[
  {
    "objectID": "block_1/511_python/511_python.html",
    "href": "block_1/511_python/511_python.html",
    "title": "Python Basics",
    "section": "",
    "text": "int, float, str, bool, list, tuple, dict, set, None\n\n\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nlen(text)\nLength of text\n\n\ntext.upper()\nUppercase\n\n\ntext.lower()\nLowercase\n\n\ntext.capitalize()\nCapitalize first letter\n\n\ntext.title()\nCapitalize first letter of each word\n\n\ntext.strip()\nRemove leading and trailing whitespace\n\n\ntext.split(' ')\nSplit string into list of words, using ’ ’ as delimiter. Default delimiter is ’ ’.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nlen(list)\nLength of list\n\n\nlist.append(item)\nAppend item to end of list\n\n\nlist.insert(index, item)\nInsert item at index\n\n\nlist.pop(n)\nRemove and return item at index n. Default n is -1\n\n\nsorted(list)\nReturns a new sorted list without modifying original list\n\n\nlist.sort()\nSort list in ascending order. To sort in descending order, use list.sort(reverse=True). (edit original list)\n\n\nlist.reverse()\nReverse list in place (edit original list)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nCreate dictionary\ndict = {'key1': 'value1', 'key2': 'value2'} or dict = dict(key1='value1', key2='value2')\n\n\nCreate empty dictionary\ndict = {} or dict = dict()\n\n\ndict['key']\nGet value of key\n\n\ndict.get('key', default)\nGet value of key, if key does not exist, return default value\n\n\ndict.pop('key')\nRemove and return value of key\n\n\ndict.keys()\nReturn list of keys\n\n\ndict.values()\nReturn list of values\n\n\ndict.items()\nReturn list of tuples (key, value)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nList\nTuple\nDictionary (dict)\nSet\n\n\n\n\nMutable\nYes\nNo\nYes\nYes\n\n\nOrdered\nYes\nYes\nNo\nNo\n\n\nIndexing\nBy index (0-based)\nBy index (0-based)\nBy key\nNo\n\n\nDuplicates\nAllowed\nAllowed\nKeys must be unique\nDuplicates not allowed\n\n\nModification\nCan change elements\nCannot change elements\nValues can be updated\nElements can be added/removed\n\n\nSyntax\nSquare brackets []\nParentheses ()\nCurly braces {}\nCurly braces {}\n\n\nUse Case\nWhen order matters\nWhen data should not change\nMapping keys to values\nFor unique values and set operations\n\n\nExample\n[1, 2, 'three']\n(1, 2, 'three')\n{'name': 'Alice', 'age': 30}\n{'apple', 'banana', 'cherry'}\n\n\n\n\n\n\n\n    # syntax: [expression for item in list]\n    # syntax: [expression for item in list if condition]\n    # syntax: [expression if condition else other_expression for item in list]\n    words = [\"the\", \"list\", \"of\", \"words\"]\n\n    x = [len(word) for word in words] # [3, 4, 2, 5]\n    x = [word for word in words if len(word) &gt; 2] # [\"the\", \"list\", \"words\"]\n    x = [word if len(word) &gt; 2 else \"short\" for word in words] # [\"the\", \"short\", \"short\", \"words\"]\n\n\n\n    # syntax: value_if_true if condition else value_if_false\n    words = [\"the\", \"list\", \"of\", \"words\"]\n\n    x = \"long list\" if len(words) &gt; 10 else \"short list\"\n\n\n\n    try:\n        # code that might raise an exception\n    except:\n        # code to handle exception\n        raise TypeError(\"Error message\") # raise exception\n\n\n\n    def function_name(arg1, arg2, arg3=default_value): # default values are optional\n        # code\n        return value\n\n    def function_name(*args): # takes in multiple arguments\n        for arg in args:\n            # code\n        return value\n\nside effects: If a function does anything other than returning a value, it is said to have side effects. An example of this is when a function changes the variables passed into it, or when a function prints something to the screen.\n\n\n\n    # Syntax: lambda arg1, arg2: expression\n    lambda x: x+1\n\n\n\n    # NumPy/SciPy style\n    def repeat_string(s, n=2):\n        \"\"\"\n        Repeat the string s, n times.\n\n        Parameters\n        ----------\n        s : str\n            the string\n        n : int, optional\n            the number of times, by default = 2\n\n        Returns\n        -------\n        str\n            the repeated string\n\n        Examples\n        --------\n        &gt;&gt;&gt; repeat_string(\"Blah\", 3)\n        \"BlahBlahBlah\"\n        \"\"\"\n        return s * n\n\n\n\n    def repeat_string(s: str, n: int = 2) -&gt; str:\n        return s * n\n\n\n\n\n    class ClassName:\n        def __init__(self, arg1, arg2):\n            # code\n        def method_name(self, arg1, arg2):\n            # code\n        @classmethod\n        def other_method_name(cls, arg1, arg2):\n            # classmethod is used to create factory methods, aka other ways to create objects\n            # code\n\n    # Inheritance\n    class SubClassName(ClassName):\n        def __init__(self, arg1, arg2):\n            super().__init__(arg1, arg2)\n            # code\n        def method2_name(self, arg1, arg2):\n            # code\n\n\n    class ClassName:\n        static_var = 0\n\n        @staticmethod\n        def method_name(arg1, arg2):\n            # code\n            ClassName.static_var += 1\n            return ClassName.static_var\n        @staticmethod\n        def reset_static_var():\n            ClassName.static_var = 0\n\n\n\n    class TestVector(unittest.TestCase):\n        def test_str1(self):\n            v1 = Vector(1, 2, 3)\n            self.assertIn(\"Vector = [1, 2, 3]\", v1.__str__())\n            self.assertEqual(len(v1.elements), 3)\n\n        def test_str2(self):\n            v1 = Vector(500)\n            self.assertIn(\"Vector = [500]\", v1.__str__())\n\n\n    TestVector = unittest.main(argv=[\"\"], verbosity=0, exit=False)\n    assert TestVector.result.wasSuccessful()\n\n\n\n\n\n\n\n\n\n\n\n\nCategory\nRule/Convention\nExample\n\n\n\n\nIndentation\nUse 4 spaces per indentation level\nif x: # four spaces here\n\n\n\nUse spaces around operators and after commas\na = b + c, d = e + f\n\n\nMaximum Line Length\nLimit all lines to a maximum of 79 characters for code, 72 for comments and docstrings\n\n\n\nImports\nAlways put imports at the top of the file\nimport os\n\n\n\nGroup imports: standard, third-party, local\nimport os import numpy as np from . import my_module\n\n\n\nUse absolute imports\nfrom my_pkg import module\n\n\nWhitespace\nAvoid extraneous whitespace\nspam(ham[1], {eggs: 2})\n\n\n\nUse blank lines to separate functions, classes, blocks of code inside functions\n\n\n\nComments\nComments should be complete sentences\n# This is a complete sentence.\n\n\n\nUse inline comments sparingly\nx = x + 1 # Increment x\n\n\nNaming Conventions\nFunction names should be lowercase with underscores\nmy_function()\n\n\n\nClass names should use CapWords convention\nMyClass\n\n\n\nConstants should be in all capital letters\nCONSTANT_NAME\n\n\nString Quotes\nUse double quotes for docstrings and single quotes for everything else when you start a project (or adhere to the project’s conventions)\n‘string’, “““docstring”“”\n\n\nExpressions and Statements\nDon’t use semicolons to terminate statements\nx = 1 (not x = 1;)\n\n\n\nUse is for identity comparisons and == for value comparisons\nif x is None\n\n\nOther Recommendations\nUse built-in types like list, dict instead of List, Dict from the typing module for simple use-cases\ndef func(a: list) -&gt; None:"
  },
  {
    "objectID": "block_1/511_python/511_python.html#data-types",
    "href": "block_1/511_python/511_python.html#data-types",
    "title": "Python Basics",
    "section": "",
    "text": "int, float, str, bool, list, tuple, dict, set, None\n\n\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nlen(text)\nLength of text\n\n\ntext.upper()\nUppercase\n\n\ntext.lower()\nLowercase\n\n\ntext.capitalize()\nCapitalize first letter\n\n\ntext.title()\nCapitalize first letter of each word\n\n\ntext.strip()\nRemove leading and trailing whitespace\n\n\ntext.split(' ')\nSplit string into list of words, using ’ ’ as delimiter. Default delimiter is ’ ’.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nlen(list)\nLength of list\n\n\nlist.append(item)\nAppend item to end of list\n\n\nlist.insert(index, item)\nInsert item at index\n\n\nlist.pop(n)\nRemove and return item at index n. Default n is -1\n\n\nsorted(list)\nReturns a new sorted list without modifying original list\n\n\nlist.sort()\nSort list in ascending order. To sort in descending order, use list.sort(reverse=True). (edit original list)\n\n\nlist.reverse()\nReverse list in place (edit original list)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nCreate dictionary\ndict = {'key1': 'value1', 'key2': 'value2'} or dict = dict(key1='value1', key2='value2')\n\n\nCreate empty dictionary\ndict = {} or dict = dict()\n\n\ndict['key']\nGet value of key\n\n\ndict.get('key', default)\nGet value of key, if key does not exist, return default value\n\n\ndict.pop('key')\nRemove and return value of key\n\n\ndict.keys()\nReturn list of keys\n\n\ndict.values()\nReturn list of values\n\n\ndict.items()\nReturn list of tuples (key, value)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nList\nTuple\nDictionary (dict)\nSet\n\n\n\n\nMutable\nYes\nNo\nYes\nYes\n\n\nOrdered\nYes\nYes\nNo\nNo\n\n\nIndexing\nBy index (0-based)\nBy index (0-based)\nBy key\nNo\n\n\nDuplicates\nAllowed\nAllowed\nKeys must be unique\nDuplicates not allowed\n\n\nModification\nCan change elements\nCannot change elements\nValues can be updated\nElements can be added/removed\n\n\nSyntax\nSquare brackets []\nParentheses ()\nCurly braces {}\nCurly braces {}\n\n\nUse Case\nWhen order matters\nWhen data should not change\nMapping keys to values\nFor unique values and set operations\n\n\nExample\n[1, 2, 'three']\n(1, 2, 'three')\n{'name': 'Alice', 'age': 30}\n{'apple', 'banana', 'cherry'}"
  },
  {
    "objectID": "block_1/511_python/511_python.html#inline-for-loop",
    "href": "block_1/511_python/511_python.html#inline-for-loop",
    "title": "Python Basics",
    "section": "",
    "text": "# syntax: [expression for item in list]\n    # syntax: [expression for item in list if condition]\n    # syntax: [expression if condition else other_expression for item in list]\n    words = [\"the\", \"list\", \"of\", \"words\"]\n\n    x = [len(word) for word in words] # [3, 4, 2, 5]\n    x = [word for word in words if len(word) &gt; 2] # [\"the\", \"list\", \"words\"]\n    x = [word if len(word) &gt; 2 else \"short\" for word in words] # [\"the\", \"short\", \"short\", \"words\"]"
  },
  {
    "objectID": "block_1/511_python/511_python.html#inline-if-else",
    "href": "block_1/511_python/511_python.html#inline-if-else",
    "title": "Python Basics",
    "section": "",
    "text": "# syntax: value_if_true if condition else value_if_false\n    words = [\"the\", \"list\", \"of\", \"words\"]\n\n    x = \"long list\" if len(words) &gt; 10 else \"short list\""
  },
  {
    "objectID": "block_1/511_python/511_python.html#try-except",
    "href": "block_1/511_python/511_python.html#try-except",
    "title": "Python Basics",
    "section": "",
    "text": "try:\n        # code that might raise an exception\n    except:\n        # code to handle exception\n        raise TypeError(\"Error message\") # raise exception"
  },
  {
    "objectID": "block_1/511_python/511_python.html#functions",
    "href": "block_1/511_python/511_python.html#functions",
    "title": "Python Basics",
    "section": "",
    "text": "def function_name(arg1, arg2, arg3=default_value): # default values are optional\n        # code\n        return value\n\n    def function_name(*args): # takes in multiple arguments\n        for arg in args:\n            # code\n        return value\n\nside effects: If a function does anything other than returning a value, it is said to have side effects. An example of this is when a function changes the variables passed into it, or when a function prints something to the screen.\n\n\n\n    # Syntax: lambda arg1, arg2: expression\n    lambda x: x+1\n\n\n\n    # NumPy/SciPy style\n    def repeat_string(s, n=2):\n        \"\"\"\n        Repeat the string s, n times.\n\n        Parameters\n        ----------\n        s : str\n            the string\n        n : int, optional\n            the number of times, by default = 2\n\n        Returns\n        -------\n        str\n            the repeated string\n\n        Examples\n        --------\n        &gt;&gt;&gt; repeat_string(\"Blah\", 3)\n        \"BlahBlahBlah\"\n        \"\"\"\n        return s * n\n\n\n\n    def repeat_string(s: str, n: int = 2) -&gt; str:\n        return s * n"
  },
  {
    "objectID": "block_1/511_python/511_python.html#classes",
    "href": "block_1/511_python/511_python.html#classes",
    "title": "Python Basics",
    "section": "",
    "text": "class ClassName:\n        def __init__(self, arg1, arg2):\n            # code\n        def method_name(self, arg1, arg2):\n            # code\n        @classmethod\n        def other_method_name(cls, arg1, arg2):\n            # classmethod is used to create factory methods, aka other ways to create objects\n            # code\n\n    # Inheritance\n    class SubClassName(ClassName):\n        def __init__(self, arg1, arg2):\n            super().__init__(arg1, arg2)\n            # code\n        def method2_name(self, arg1, arg2):\n            # code\n\n\n    class ClassName:\n        static_var = 0\n\n        @staticmethod\n        def method_name(arg1, arg2):\n            # code\n            ClassName.static_var += 1\n            return ClassName.static_var\n        @staticmethod\n        def reset_static_var():\n            ClassName.static_var = 0\n\n\n\n    class TestVector(unittest.TestCase):\n        def test_str1(self):\n            v1 = Vector(1, 2, 3)\n            self.assertIn(\"Vector = [1, 2, 3]\", v1.__str__())\n            self.assertEqual(len(v1.elements), 3)\n\n        def test_str2(self):\n            v1 = Vector(500)\n            self.assertIn(\"Vector = [500]\", v1.__str__())\n\n\n    TestVector = unittest.main(argv=[\"\"], verbosity=0, exit=False)\n    assert TestVector.result.wasSuccessful()"
  },
  {
    "objectID": "block_1/511_python/511_python.html#pep8-guidelines",
    "href": "block_1/511_python/511_python.html#pep8-guidelines",
    "title": "Python Basics",
    "section": "",
    "text": "Category\nRule/Convention\nExample\n\n\n\n\nIndentation\nUse 4 spaces per indentation level\nif x: # four spaces here\n\n\n\nUse spaces around operators and after commas\na = b + c, d = e + f\n\n\nMaximum Line Length\nLimit all lines to a maximum of 79 characters for code, 72 for comments and docstrings\n\n\n\nImports\nAlways put imports at the top of the file\nimport os\n\n\n\nGroup imports: standard, third-party, local\nimport os import numpy as np from . import my_module\n\n\n\nUse absolute imports\nfrom my_pkg import module\n\n\nWhitespace\nAvoid extraneous whitespace\nspam(ham[1], {eggs: 2})\n\n\n\nUse blank lines to separate functions, classes, blocks of code inside functions\n\n\n\nComments\nComments should be complete sentences\n# This is a complete sentence.\n\n\n\nUse inline comments sparingly\nx = x + 1 # Increment x\n\n\nNaming Conventions\nFunction names should be lowercase with underscores\nmy_function()\n\n\n\nClass names should use CapWords convention\nMyClass\n\n\n\nConstants should be in all capital letters\nCONSTANT_NAME\n\n\nString Quotes\nUse double quotes for docstrings and single quotes for everything else when you start a project (or adhere to the project’s conventions)\n‘string’, “““docstring”“”\n\n\nExpressions and Statements\nDon’t use semicolons to terminate statements\nx = 1 (not x = 1;)\n\n\n\nUse is for identity comparisons and == for value comparisons\nif x is None\n\n\nOther Recommendations\nUse built-in types like list, dict instead of List, Dict from the typing module for simple use-cases\ndef func(a: list) -&gt; None:"
  },
  {
    "objectID": "block_1/511_python/511_python.html#numpy-arrays",
    "href": "block_1/511_python/511_python.html#numpy-arrays",
    "title": "Python Basics",
    "section": "Numpy Arrays",
    "text": "Numpy Arrays\n\nDifference with lists:\n\n\n\n\n\n\n\n\nFeature\nNumpy arrays\nLists\n\n\n\n\nData type uniformity\nAll elements must be of the same data type\nNo restriction\n\n\nStorage efficiency\nStored more efficiently\nLess efficient\n\n\nVectorized operations\nSupports\nDoes not support\n\n\n\n\n\nCreating arrays:\n\n\n\n\n\n\n\n\nFunction/Method\nDescription\nExample\n\n\n\n\nnp.array([list], dtype)\nCreate an array from a list\nnp.array([1, 2, 3], dtype='int8')\n\n\nnp.arange(start, stop, step)\nCreate an array of evenly spaced values\nnp.arange(0, 10, 2) returns [0, 2, 4, 6, 8]\n\n\nnp.ones(shape, dtype)\nCreate an array of ones\nnp.ones((3, 2), dtype='int8') returns [[1, 1], [1, 1], [1, 1]]\n\n\nnp.zeros(shape, dtype)\nCreate an array of zeros\n\n\n\nnp.full(shape, fill_value, dtype)\nCreate an array of a specific value\n\n\n\nnp.random.rand(shape)\nCreate an array of random values between 0 and 1\nnp.random.randn(3, 3, 3)\n\n\n\n\n\nOther useful functions:\n\n\n\n\n\n\n\n\nFunction/Method\nDescription\nExample\n\n\n\n\narray.transpose() or array.T\nTranspose an array\n\n\n\narray.ndim\nReturns the number of dimensions\nx = np.ones((3,2)) and print(x.ndim) returns 2\n\n\narray.size\nReturns the number of elements\nprint(x.size) returns 6\n\n\narray.shape\nReturns the shape of the array\nprint(x.shape) returns (3, 2)\n\n\nnp.flip(array)\nReverse an array , default: flips row and cols\nnp.flip(array)\n\n\narray.reshape(shape)\nReshape an array\narray.reshape(2, 3) for 2 rows, 3 columns\n\n\narray.sort()\nSort an array in ascending order\nnew_arr = array.sort()\n\n\narray.flatten()\nFlatten an array, same as reshape(-1)\nnew_arr = array.flatten()\n\n\narray.concatenate()\nConcatenate arrays, default: axis=0 (adds to the bottom)\nnp.concatenate([array1, array2], axis=0)\n\n\n\nNote: For the reshape function, you can use -1 to infer dimension (e.g. array.reshape(-1, 3)). Also, the default is row-major order, but you can specify order='F' for column-major order.\n\n\narray.dtype: returns the data type\narray.astype(dtype): convert an array to a different data type\nnp.array_equal(array1, array2): check if two arrays are equal\n\n\n\nArray operations and broadcasting\n\nsmaller arrays are broadcasted to match the shape of larger arrays \nCan only broadcast if compatible in all dimensions. They are compatible if:\n\nthey are equal in size\none of them is 1\n\nChecks starting from the right-most dimension"
  },
  {
    "objectID": "block_1/511_python/511_python.html#series",
    "href": "block_1/511_python/511_python.html#series",
    "title": "Python Basics",
    "section": "Series",
    "text": "Series\n\nnumpy array with labels\ncan store any data type, string takes the most space\n\nIf any NaN, dtype of Series becomes float64\nif any mixed data types, dtype of Series becomes object\n\npd.Series(): create a Series\n\npd.Series([1, 2, 3], index=['a', 'b', 'c'], name='col1'): create a Series from a list with labels and a name\npd.Series({'a': 1, 'b': 2, 'c': 3}): create a Series from a dictionary\n\ns.index: returns the index\ns.to_numpy(): convert a Series to a numpy array\n\n\nIndexing:\n\ns['a']: returns the value at index ‘a’\ns[['a', 'b']]: returns a Series with values at indices ‘a’ and ‘b’\ns[0:3]: returns a Series with values at indices 0, 1, and 2\n\n\n\nOperations:\n\naligns values based on index\ns1 + s2: returns a Series with values at indices in both s1 and s2\n\nwill return NaN (Not a Number) if index is not in both s1 and s2\nkind of like a left join in SQL"
  },
  {
    "objectID": "block_1/511_python/511_python.html#dataframes",
    "href": "block_1/511_python/511_python.html#dataframes",
    "title": "Python Basics",
    "section": "DataFrames",
    "text": "DataFrames\n\nCreating DataFrames\n\npd.DataFrame([2d list], columns=['col1', 'col2'], index=['row1', 'row2']): creates a DataFrame from a list with column names and row names\n\n\n\n\n\n\n\n\nSource\nCode\n\n\n\n\nLists of lists\npd.DataFrame([['Quan', 7], ['Mike', 15], ['Tiffany', 3]])\n\n\nndarray\npd.DataFrame(np.array([['Quan', 7], ['Mike', 15], ['Tiffany', 3]]))\n\n\nDictionary\npd.DataFrame({\"Name\": ['Quan', 'Mike', 'Tiffany'], \"Number\": [7, 15, 3]})\n\n\nList of tuples\npd.DataFrame(zip(['Quan', 'Mike', 'Tiffany'], [7, 15, 3]))\n\n\nSeries\npd.DataFrame({\"Name\": pd.Series(['Quan', 'Mike', 'Tiffany']), \"Number\": pd.Series([7, 15, 3])})\n\n\nCsv\npd.read_csv('file.csv', sep='\\t')\n\n\n\n\n\nIndexing\n\n\n\n\n\n\n\n\nMethod\nDescription\nExample\n\n\n\n\nSingle Column\nReturns single column as a Series\ndf['col1'] or df.col1\n\n\n\nReturns single column as a DataFrame\ndf[['col1']]\n\n\nMultiple Columns\nReturns multiple columns as a DataFrame\ndf[['col1', 'col2']]\n\n\niloc (integer)\nReturns first row as a Series\ndf.iloc[0]\n\n\n\nReturns first row as a DataFrame\ndf.iloc[0:1] OR df.iloc[[0]]\n\n\n\nReturns specific rows and columns\ndf.iloc[2:5, 1:4] (rows 2-4 and columns 1-3)\n\n\n\nReturns specific rows and columns\ndf.iloc[[0, 2], [0, 2]] (rows 0 and 2 and columns 0 and 2)\n\n\nloc (label)\nReturns all rows and a specific column as a Series\ndf.loc[:, 'col1']\n\n\n\nReturns all rows and a specific column as a DataFrame\ndf.loc[:, ['col1']]\n\n\n\nReturns all rows and specific range of columns as a DataFrame\ndf.loc[:, 'col1':'col3']\n\n\n\nReturns item at row1, col1\ndf.loc['row1', 'col1']\n\n\nBoolean indexing\nReturns rows based on a boolean condition\ndf[df['col1'] &gt; 5]\n\n\nquery\nReturns rows based on a query (same as above)\ndf.query('col1 &gt; 5')\n\n\n\nNote: Indexing with just a single number like df[0] or a slice like df[0:1] without iloc or loc doesn’t work for DataFrame columns.\n\nCan do indexing with boolean with loc: df.loc[:, df['col1'] &gt; 5] Gets all rows and columns where col1 &gt; 5\nWant to *2 for rows in col2 when col1&gt;5: df.loc[df['col1'] &gt; 5, 'col2'] = 0\n\n\n\nOther useful functions\n\n\n\n\n\n\n\n\nFunction/Method\nDescription\nExample\n\n\n\n\nmax()\nReturns the maximum value\ndf['col1'].max()\n\n\nidxmax()\nReturns the index of the maximum value\ndf['col1'].idxmax()\n\n\nmin()\nReturns the minimum value\ndf['col1'].min()\n\n\nidxmin()\nReturns the index of the minimum value\ndf['col1'].idxmin()\n\n\n\nCan return row of max value of col1 by: df.iloc[[df['col1'].idxmax()]] or df.iloc[df.loc[:, 'col1'].idxmax()]"
  },
  {
    "objectID": "block_1/511_python/511_python.html#data-wrangling",
    "href": "block_1/511_python/511_python.html#data-wrangling",
    "title": "Python Basics",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nData Summary\n\nshape: returns (rows, columns)\ninfo(): returns column names, data types, and number of non-null values\ndescribe(): returns summary statistics for each column\n\n\n\nViews vs Copies\n\nCorrect way to replace: df.loc[df['Released_Year'] &gt; 2021, 'Released_Year'] = 2010"
  },
  {
    "objectID": "block_1/511_python/511_python.html#data-manipulation",
    "href": "block_1/511_python/511_python.html#data-manipulation",
    "title": "Python Basics",
    "section": "Data Manipulation",
    "text": "Data Manipulation\n\nrename(): rename columns\n\ndf.rename(columns={'old_name': 'new_name', 'old_name_2': 'new_name_2'}, inplace=True)\ninplace=True to modify DataFrame, instead of returning a new DataFrame, default is False\nrecommended to just assign to a new variable\n\ncolumns: returns column names, can change column names by assigning a list of new names\n\ndf.columns.to_list(): returns column names as list\n\n\n\nChanging Index\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\ndf.set_index('col1', inplace=True)\nSet a column as the index\n\n\ndf = pd.read_csv('file.csv', index_col='col1')\nSet index when reading in a file\n\n\ndf.reset_index()\nReset index to default, starting at 0\n\n\ndf.index = df['col1']\nDirectly modify index\n\n\ndf.index.name = \"a\"\nRename index to ‘a’\n\n\n\n\n\nAdding/ Removing Columns\n\ndrop(): remove columns\n\ndf.drop(['col1', 'col2'], axis=1, inplace=True)\ndf.drop(df.columns[5:], axis=1)\n\ninsert(): insert a column at a specific location\n\ndf.insert(0, 'col1', df['col2'])\n\ndf['new_col'] = df['col1'] + df['col2']: add a new column\n\n\n\nAdding/ Removing Rows\n\ndf.drop([5:], axis=0): remove rows 5 and after\ndf = df.iloc[5:]: returns rows 5 and after\n\n\n\nReshaping Data\n\nmelt(): unpivot a DataFrame from wide to long format\n\ne.g: df_melt = df.melt(id_vars=\"Name\", value_vars=[\"2020\", \"2019\"], var_name=\"Year\", value_name=\"Num_Courses\")\nid_vars: specifies which column should be used as identifier variables (the key)\nvalue_vars: Column(s) to unpivot. If not specified, uses all columns that are not set as id_vars.\nvar_name: select which variables to melt\nvalue_name: select which variables to keep\nignore_index=False: keeps the index, default is True (resets index)\n\npivot(): pivot a DataFrame from long to wide format\n\ne.g: df_pivot = df_melt.pivot(index=\"Name\", columns=\"Year\", values=\"Num_Courses\")\nindex: specifies which column should be used as index\ncolumns: specifies which column should be used as columns\nvalues: specifies which column should be used as values\n\n\nAfter pivot, if want to remove column names, use df.columns.name = None\n\npivot_table(): pivot a DataFrame from long to wide format, but can handle duplicate values\n\ne.g: df_pivot = df.pivot_table(index=\"Name\", columns=\"Year\", values=\"Num_Courses\", aggfunc='sum')\naggfunc: specifies how to aggregate duplicate values\n\nconcat(): concatenate DataFrames\n\ndf = pd.concat([df1, df2], axis=0, ignore_index=True)\naxis=0: concatenate along rows\naxis=1: concatenate along columns\nignore_index=True: ignore index (resets the index), default is False (leaves index as is)\n\nmerge(df1, df2, on='col1', how='inner'): merge DataFrames, analogous to join in R\n\non: specifies which column to merge on\nhow: specifies how to merge\n\ninner: only keep rows that are in both DataFrames\nouter: keep all rows from both DataFrames\nleft: keep all rows from the left DataFrame\nright: keep all rows from the right DataFrame\n\n\n\n\n\nApply Functions\n\ndf.apply(): apply a function to a DataFrame column or row-wise, takes/returns series or df\n\ndf.apply(lambda x: x['col1'] + x['col2'], axis=1): apply a function to each row\ndf.apply(lambda x: x['col1'] + x['col2'], axis=0): apply a function to each column\ndf['col1'].apply(lambda x: x + 1): apply a function to column ‘col1’\n\ndf.applymap(): apply a function to each element in a DataFrame\n\ndf.loc[:, [\"Mean Max Temp (°C)\"]].applymap(int): apply int to each element in column ’Mean Max Temp (°C)\ndf.loc[[\"Mean Max Temp (°C)\"]].astype(int): Faster…\nOnly works for DataFrames, not Series\n\n\n\n\nGrouping Data\n\ngroupby(): group data by a column\n\ndf.groupby('col1'): returns a DataFrameGroupBy object\ndf.groupby('col1').groups: returns a dictionary of groups\ndf.groupby('col1').get_group('group1'): returns a DataFrame of group1\n\ndf.agg(): aggregate data\n\ndf.groupby('col1').agg({'col2': 'mean', 'col3': 'sum'}): returns a DataFrame with mean of col2 and sum of col3 for each group (needs to be numeric, or else error)"
  },
  {
    "objectID": "block_1/511_python/511_python.html#string-dtype",
    "href": "block_1/511_python/511_python.html#string-dtype",
    "title": "Python Basics",
    "section": "String dtype",
    "text": "String dtype\n\ndf.str.func(): can apply any string function to each string in df\n\ne.g. df['col1'].str.lower(): convert all strings in col1 to lowercase\ndf['col1'].str.cat(sep=' '): concatenate all strings in col1 with a space in between, default is no space\n\nCan do regex as well:\n\ndf['col1'].str.contains(r'regex'): returns a boolean Series\ndf['col1'].str.replace(r'regex', 'new_string'): replace all strings in col1 that match regex with ‘new_string’"
  },
  {
    "objectID": "block_1/511_python/511_python.html#datetime-dtype",
    "href": "block_1/511_python/511_python.html#datetime-dtype",
    "title": "Python Basics",
    "section": "Datetime dtype",
    "text": "Datetime dtype\nfrom datetime import datetime, timedelta\n\nConstruct a datetime object: datetime(year, month, day, hour, minute, second)\ndatetime.now(): returns current datetime\ndatetime.strptime('July 9 2005, 13:54', '%B %d %Y, %H:%M'): convert a string to a datetime object\nAdd time to a datetime object: datetime + timedelta(days=1, hours=2, minutes=3, seconds=4)\n\n\nDatetime dtype in Pandas\n\npd.Timestamp('2021-07-09 13:54'): convert a string to a datetime object\n\npd.Timestamp(year, month, day, hour, minute, second): construct a datetime object\npd.Timestamp(datetime(year, month, day, hour, minute, second)): convert a datetime object to a Timestamp object\n\npd.Period('2021-07'): convert a string to a Period object\n\npd.Period(year, month): construct a Period object\npd.Period(datetime(year, month, day, hour, minute, second)): convert a datetime object to a Period objects\n\n\nConverting existing columns to datetime dtype:\n\npd.to_datetime(df['col1']): convert col1 to datetime dtype\npd.read_csv('file.csv', parse_dates=True): convert all columns with datetime dtype to datetime dtype\n\n\n\nOther datetime functions\nThe index of a DataFrame can be a datetime dtype. These functions can be applied to the index.\n\ndf.index.year: returns a Series of years\ndf.index.month: returns a Series of months\n\ndf.index.month_name(): returns a Series of month names\n\ndf.between_time('9:00', '12:00'): returns rows between 9am and 12pm\ndf.resample('D').mean(): resample data to daily and take the mean\n\nD: daily\nW: weekly\nM: monthly\nQ: quarterly\nY: yearly\n\npd.date_range(start, end, freq): returns a DatetimeIndex\n\nfreq same as above (e.g. D, W, M, Q, Y)"
  },
  {
    "objectID": "block_1/511_python/511_python.html#visualization",
    "href": "block_1/511_python/511_python.html#visualization",
    "title": "Python Basics",
    "section": "Visualization",
    "text": "Visualization\n\ndf['Distance'].plot.line(): plot a line chart\ndf['Distance'].plot.bar(): plot a bar chart\ndf['Distance'].cumsum().plot.line(): plot a line chart of cumulative sum"
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html",
    "href": "block_1/521_platforms/521_platforms.html",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "ls - list files and directories. Default behavior is to list the contents of the current working directory.\ncd - change directory. Used to navigate the filesystem. Default behavior is to change to the home directory.\npwd - print working directory. It will return the absolute path of the current working directory.\nmkdir - make directory\ntouch - create file\nrm - remove file\nrmdir - remove directory\nmv - move file. also used to rename file.\ncp - copy file\nwhich - locate a program. It will return the path of the program.\n\n\n\nThese flags allow us to modify the default behaviour of a program.\n\n-a - all\n-l - long\n-h - human readable\n-r - recursive\n-f - force\n\n\n\n\n\n. - current directory\n.. - parent directory\n~ - home directory\n/ - root directory\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nGraphical user interface (GUI)\nA user interface that relies on windows, menus, pointers, and other graphical elements\n\n\nCommand-line interface (CLI)\nA user interface that relies solely on text for commands and output, typically running in a shell.\n\n\nOperating system\nA program that provides a standard interface to whatever hardware it is running on.\n\n\nFilesystem\nThe part of the operating system that manages how files are stored and retrieved. Also used to refer to all of those files and directories or the specific way they are stored.\n\n\nSubdirectory\nA directory that is below another directory\n\n\nParent directory\nThe directory that contains another directory of interest. Going from a directory to its parent, then its parent, and so on eventually leads to the root directory of the filesystem.\n\n\nHome directory\nA directory that contains a user’s files.\n\n\nCurrent working directory\nThe folder or directory location in which the program operates. Any action taken by the program occurs relative to this directory.\n\n\nPath (in filesystem)\nA string that specifies a location in a filesystem.\n\n\nAbsolute path\nA path that points to the same location in the filesystem regardless of where it is evaluated. It is the equivalent of latitude and longitude in geography.\n\n\nRelative path\nA path whose destination is interpreted relative to some other location, such as the current working directory.\n\n\nDirectory\nAn item within a filesystem that can contain files and other directories. Also known as “folder”.\n\n\nRoot directory\nThe directory that contains everything else, either directly or indirectly.\n\n\nPrompt\nThe text printed by the shell that indicates it is ready to accept another command.\n\n\n\n\n\n\n\nSymbol\n\n\n\n\nA. Root directory (see note)\n/\n\n\nB. Parent directory\n..\n\n\nC. Current working directory\n.\n\n\nD. Home directory\n~ \n\n\nE. Command line argument\n-y or --yes\n\n\nF. Prompt (R)\n&gt;\n\n\nG. Prompt (Python)\n&gt;&gt;&gt;\n\n\nH. Prompt (Bash)\n$\n\n\n\n\n\n\n\n\ngit init - initialize a git repository\ngit add - add files to staging area.\n\nStaging area is a place where we can group files together before we “commit” them to git.\n\ngit commit - commit changes to git. Records a new version of the files in the repository.\ngit push - push changes to remote repository from local repository\ngit pull - pull changes from remote repository to local repository\ngit status - check status of git repository\ngit log - check commit history\n\ngit log --oneline - check commit history in one line\ngit log -p - check commit history with changes\n\ngit diff - check difference between two commits\ngit reset - reset git repository\n\ngit reset --hard - reset git repository to last commit\ngit reset --soft - reset git repository to last commit but keep changes\n\ngit revert - revert git repository. The difference between revert and reset is that revert creates a new commit with the changes from the commit we want to revert.\ngit stash - saves changes that you don’t want to commit immediately. It takes the dirty state of your working directory — that is, your modified tracked files and staged changes — and saves it on a stack of unfinished changes that you can reapply at any time.\n\n\n\n\nRepository - a collection of files and folders that are tracked by git.\nCommit - a snapshot of the repository at a specific point in time.\nCommit Hash - These are the commits’ hashes (SHA-1), which are used to uniquely identify the commits within a project.\nBranch - a parallel version of a repository. It is contained within the repository, but does not affect the primary or master branch allowing you to work freely without disrupting the “live” version.\n\n\n\n\n\nPublic key is used to encrypt data and private key is used to decrypt data.\nConfidentiality - only the intended recipient can decrypt the data.\nAuthentication - only the intended recipient can encrypt the data.\n\n\n\n\nSSH is more secure than HTTPS because it uses public and private keys to encrypt and decrypt data. HTTPS uses username and password to authenticate users.\n\n\n\n\nGithub pages will look in either the repository root / directory or in the repository docs/ directory for website content to render.\n\n\n\n\nDynamic Documents: Rooted in Knuth’s “literate programming” concept from 1984.\nMain Goals:\n\nWrite program code.\nCreate narratives to elucidate code function.\n\nBenefits:\n\nEnhances understanding and provides comprehensive documentation.\nGives a way to run code and view results.\nAllows text and code to be combined in a single document.\nFacilitates reproducibility of results and diagrams.\n\nPopular Formats:\n\nJupyter Notebooks (.ipynb)\nRMarkdown documents (.Rmd)\n\nKey Features:\n\nNarratives formatted with markdown.\nExecutable code:\n\nInterwoven in text (RMarkdown’s inline code).\nSeparate sections: code cells (Jupyter) or code chunks (RMarkdown).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\ngetwd()\nReturns the current working directory of the R session.\n\n\nsetwd(path)\nChanges the working directory to the specified path.\n\n\nhere::here()\nCreates file paths relative to the project’s root (where the .Rproj file is) to ensure consistent and portable references.\n\n\n\nNote: For portability, prefer here::here() over setwd() to avoid path inconsistencies across different systems.\n\n\n\n\nGlobal looks like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature/Aspect\nJupyter Notebooks\nRMarkdown\nQuarto (.qmd)\n\n\n\n\nPrimary Language\nPython\nR\nPython, R, Julia\n\n\nSecondary Language\nR (via R kernel)\nPython (via reticulate)\nR with Python (via reticulate), Python with R (via rpy2)\n\n\nCompatible Editors\nJupyterLab, VS Code\nRStudio\nRStudio, VS Code, JupyterLab\n\n\nSpecial Features\n-\n-\nSingle engine processing, cross-language integration\n\n\nRecommended Environments\nJupyterLab or VS Code\nRStudio\nRStudio (offers code-completion, incremental cell execution, and other tools for working with executable code)\n\n\n\n\n\n\n\nOn top of the RMarkdown document, you can specify a template.\nTemplates are .Rmd files that contain the YAML header and some default text.\n\n---\ntitle: \"Untitled\"\noutput: github_document / html_document / pdf_document / word_document\nauthor: \"farrandi\"\ndate: \"10/2/2023\"\n---\n\n\n\n# Same YAML header as above\n---\ntitle: \"521 Quiz 2\"\nauthor: \"Daniel Chen\"\nformat: revealjs\n---\n\n# In the morning\n\n## Getting up\n\n- Turn off alarm\n- Get out of bed\n\n## Breakfast\n\n- Eat eggs\n- Drink coffee\n\n# In the evening\n\n## Dinner\n\n- Eat spaghetti\n- Drink wine\n\n## Going to sleep\n\n- Get in bed\n- Count sheep\n\n\n\n\n\nJupyterLab supports presentations using the JavaScript framework: reveal.js.\nreveal.js is the same framework used by Quarto.\nCells for presentations are marked via the property inspector:\n\nFound at the settings wheel in the left side panel.\nSelect slide type from the dropdown menu.\n\nreveal.js presentations are two dimensional:\n\nHorizontal slides\nVertical sub-slides\n\n\n\n\n\n\n\n\n\n\n\n\nSlide Type\nDescription\nNavigation\n\n\n\n\nSlide\nStandard slide\nLeft and right arrows\n\n\nSub-slide\nSub-topic of a slide\nUp and down arrows\n\n\nFragment\nAnimated part of the previous slide, e.g. a bullet\nPart of slide animation\n\n\n- (or empty)\nAppears in same cell as previous slide\nPart of slide\n\n\nNotes\nSpeaker notes\nVisible when pressing ‘t’\n\n\nSkip\nNot included in the presentation\n-\n\n\n\n\n\n\n\nImages using ![]() or &lt;img&gt; tags don’t show up in exports.\nWorkaround: Paste images into a Markdown cell to include them as attachments. This ensures visibility in HTML and slide exports.\n\n\n\n\n\n\nVirtual Environments: Isolated Python environments that allow for the installation of packages without affecting the system’s Python installation.\nBenefits:\n\nIsolation: Packages installed in a virtual environment are isolated from the system’s Python installation.\nReproducibility: Virtual environments can be shared with others to ensure reproducibility.\nVersion control: Allow managing the versions of libraries and tools used in a project.\nCross-platform: Ensures that the project’s dependencies are consistent across different systems.\nExperimentation: Allows for experimentation with different versions of packages.\nClean environment: When starting a new project, starts with a clean slate.\nConsistency: Ensures that the project’s dependencies are consistent across different systems.\nSecurity: Isolates the project’s dependencies from the system’s Python installation.\n\n\n\n\n\nconda create -n &lt;env_name&gt; python=&lt;version&gt;: create a new environment\n\nconda env create -f path/to/environment.yml: create an environment from a file\nconda create --name live_env --clone test_env: create an environment from an existing environment\n\nconda env list: list all environments\nconda activate &lt;env_name&gt;: activate the environment\nconda deactivate: deactivate the environment\nconda env remove -n &lt;env_name&gt; --all: remove the environment\nconda env export -f environment.yml --from-history: export the environment to a file\n\n--from-history: only include packages that were explicitly installed\n\n\nManaging packages:\n\nconda config --add channels conda-forge: add a channel\nconda list: list all packages in the environment\nconda search &lt;package&gt;: search for a package\nconda install &lt;package&gt;: install a package\n\nconda install &lt;package&gt;=&lt;version&gt;: install a specific version of a package\n\nconda remove &lt;package&gt;: remove a package\n\nExample environment.yml file:\nname: test_env\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - conda\n  - python=3.7\n  - pandas==1.0.2\n  - jupyterlab\n\n\n\n\n\n\n\nComponent\nDescription\n\n\n\n\nname\n- Identifies the environment’s name. - Useful for distinguishing multiple environments.\n\n\nchannels\n- Locations where Conda searches for packages. - Default: defaults channel. - Popular options: conda-forge, bioconda.\n\n\ndependencies\n- Lists required packages for the environment. - Can specify versions or ranges. - Can include Conda or pip packages.\n\n\nprefix\n- (Optional) Directory where Conda installs the environment. - Defaults to Conda’s main directory if not provided.\n\n\n\n\n\n\n\nMake a new project in RStudio 1.1. Select setting to use renv 1.2. library(renv): to use renv\nrenv::init(): initialize the project\n\n\n\n\n\n3 Principles:\n\nMachine readable:\n\nregex and globbing friendly (avoid spaces, special characters, case sensitivity, etc.)\ndeliberate use of delimiters (e.g. _, -, .)\n\nHuman readable: Helps other people and ourselves in the future quickly understand the file structure and contents of a project/ file.\nPlays well with default ordering: Makes files more organized and easily searchable. Easy for us humans to find the files we are looking for.\n\nDates go: YYYY-MM-DD\n\n\n\n\n\n\nReproducible example\nCode formatting\nMinimal, complete, verifiable example\n\nEffective Questioning & Creating an MRE:\n\nSearch for similar questions before asking.\nClearly state the problem in the title and provide brief details in the body.\nProvide the shortest version of your code that replicates the error.\nInclude definitions if you’ve used functions or classes.\nUse toy datasets rather than real data.\nUse markdown for code to ensure readability and syntax highlighting.\nShare attempts, points of confusion, and full error tracebacks."
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#shell",
    "href": "block_1/521_platforms/521_platforms.html#shell",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "ls - list files and directories. Default behavior is to list the contents of the current working directory.\ncd - change directory. Used to navigate the filesystem. Default behavior is to change to the home directory.\npwd - print working directory. It will return the absolute path of the current working directory.\nmkdir - make directory\ntouch - create file\nrm - remove file\nrmdir - remove directory\nmv - move file. also used to rename file.\ncp - copy file\nwhich - locate a program. It will return the path of the program.\n\n\n\nThese flags allow us to modify the default behaviour of a program.\n\n-a - all\n-l - long\n-h - human readable\n-r - recursive\n-f - force\n\n\n\n\n\n. - current directory\n.. - parent directory\n~ - home directory\n/ - root directory\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nGraphical user interface (GUI)\nA user interface that relies on windows, menus, pointers, and other graphical elements\n\n\nCommand-line interface (CLI)\nA user interface that relies solely on text for commands and output, typically running in a shell.\n\n\nOperating system\nA program that provides a standard interface to whatever hardware it is running on.\n\n\nFilesystem\nThe part of the operating system that manages how files are stored and retrieved. Also used to refer to all of those files and directories or the specific way they are stored.\n\n\nSubdirectory\nA directory that is below another directory\n\n\nParent directory\nThe directory that contains another directory of interest. Going from a directory to its parent, then its parent, and so on eventually leads to the root directory of the filesystem.\n\n\nHome directory\nA directory that contains a user’s files.\n\n\nCurrent working directory\nThe folder or directory location in which the program operates. Any action taken by the program occurs relative to this directory.\n\n\nPath (in filesystem)\nA string that specifies a location in a filesystem.\n\n\nAbsolute path\nA path that points to the same location in the filesystem regardless of where it is evaluated. It is the equivalent of latitude and longitude in geography.\n\n\nRelative path\nA path whose destination is interpreted relative to some other location, such as the current working directory.\n\n\nDirectory\nAn item within a filesystem that can contain files and other directories. Also known as “folder”.\n\n\nRoot directory\nThe directory that contains everything else, either directly or indirectly.\n\n\nPrompt\nThe text printed by the shell that indicates it is ready to accept another command.\n\n\n\n\n\n\n\nSymbol\n\n\n\n\nA. Root directory (see note)\n/\n\n\nB. Parent directory\n..\n\n\nC. Current working directory\n.\n\n\nD. Home directory\n~ \n\n\nE. Command line argument\n-y or --yes\n\n\nF. Prompt (R)\n&gt;\n\n\nG. Prompt (Python)\n&gt;&gt;&gt;\n\n\nH. Prompt (Bash)\n$"
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#git-and-github",
    "href": "block_1/521_platforms/521_platforms.html#git-and-github",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "git init - initialize a git repository\ngit add - add files to staging area.\n\nStaging area is a place where we can group files together before we “commit” them to git.\n\ngit commit - commit changes to git. Records a new version of the files in the repository.\ngit push - push changes to remote repository from local repository\ngit pull - pull changes from remote repository to local repository\ngit status - check status of git repository\ngit log - check commit history\n\ngit log --oneline - check commit history in one line\ngit log -p - check commit history with changes\n\ngit diff - check difference between two commits\ngit reset - reset git repository\n\ngit reset --hard - reset git repository to last commit\ngit reset --soft - reset git repository to last commit but keep changes\n\ngit revert - revert git repository. The difference between revert and reset is that revert creates a new commit with the changes from the commit we want to revert.\ngit stash - saves changes that you don’t want to commit immediately. It takes the dirty state of your working directory — that is, your modified tracked files and staged changes — and saves it on a stack of unfinished changes that you can reapply at any time.\n\n\n\n\nRepository - a collection of files and folders that are tracked by git.\nCommit - a snapshot of the repository at a specific point in time.\nCommit Hash - These are the commits’ hashes (SHA-1), which are used to uniquely identify the commits within a project.\nBranch - a parallel version of a repository. It is contained within the repository, but does not affect the primary or master branch allowing you to work freely without disrupting the “live” version.\n\n\n\n\n\nPublic key is used to encrypt data and private key is used to decrypt data.\nConfidentiality - only the intended recipient can decrypt the data.\nAuthentication - only the intended recipient can encrypt the data.\n\n\n\n\nSSH is more secure than HTTPS because it uses public and private keys to encrypt and decrypt data. HTTPS uses username and password to authenticate users."
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#quarto-and-github-pages",
    "href": "block_1/521_platforms/521_platforms.html#quarto-and-github-pages",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "Github pages will look in either the repository root / directory or in the repository docs/ directory for website content to render."
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#dynamic-documents",
    "href": "block_1/521_platforms/521_platforms.html#dynamic-documents",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "Dynamic Documents: Rooted in Knuth’s “literate programming” concept from 1984.\nMain Goals:\n\nWrite program code.\nCreate narratives to elucidate code function.\n\nBenefits:\n\nEnhances understanding and provides comprehensive documentation.\nGives a way to run code and view results.\nAllows text and code to be combined in a single document.\nFacilitates reproducibility of results and diagrams.\n\nPopular Formats:\n\nJupyter Notebooks (.ipynb)\nRMarkdown documents (.Rmd)\n\nKey Features:\n\nNarratives formatted with markdown.\nExecutable code:\n\nInterwoven in text (RMarkdown’s inline code).\nSeparate sections: code cells (Jupyter) or code chunks (RMarkdown)."
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#rsudio-and-quarto",
    "href": "block_1/521_platforms/521_platforms.html#rsudio-and-quarto",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "Function\nDescription\n\n\n\n\ngetwd()\nReturns the current working directory of the R session.\n\n\nsetwd(path)\nChanges the working directory to the specified path.\n\n\nhere::here()\nCreates file paths relative to the project’s root (where the .Rproj file is) to ensure consistent and portable references.\n\n\n\nNote: For portability, prefer here::here() over setwd() to avoid path inconsistencies across different systems.\n\n\n\n\nGlobal looks like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature/Aspect\nJupyter Notebooks\nRMarkdown\nQuarto (.qmd)\n\n\n\n\nPrimary Language\nPython\nR\nPython, R, Julia\n\n\nSecondary Language\nR (via R kernel)\nPython (via reticulate)\nR with Python (via reticulate), Python with R (via rpy2)\n\n\nCompatible Editors\nJupyterLab, VS Code\nRStudio\nRStudio, VS Code, JupyterLab\n\n\nSpecial Features\n-\n-\nSingle engine processing, cross-language integration\n\n\nRecommended Environments\nJupyterLab or VS Code\nRStudio\nRStudio (offers code-completion, incremental cell execution, and other tools for working with executable code)\n\n\n\n\n\n\n\nOn top of the RMarkdown document, you can specify a template.\nTemplates are .Rmd files that contain the YAML header and some default text.\n\n---\ntitle: \"Untitled\"\noutput: github_document / html_document / pdf_document / word_document\nauthor: \"farrandi\"\ndate: \"10/2/2023\"\n---\n\n\n\n# Same YAML header as above\n---\ntitle: \"521 Quiz 2\"\nauthor: \"Daniel Chen\"\nformat: revealjs\n---\n\n# In the morning\n\n## Getting up\n\n- Turn off alarm\n- Get out of bed\n\n## Breakfast\n\n- Eat eggs\n- Drink coffee\n\n# In the evening\n\n## Dinner\n\n- Eat spaghetti\n- Drink wine\n\n## Going to sleep\n\n- Get in bed\n- Count sheep"
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#presentations-in-jupyterlab",
    "href": "block_1/521_platforms/521_platforms.html#presentations-in-jupyterlab",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "JupyterLab supports presentations using the JavaScript framework: reveal.js.\nreveal.js is the same framework used by Quarto.\nCells for presentations are marked via the property inspector:\n\nFound at the settings wheel in the left side panel.\nSelect slide type from the dropdown menu.\n\nreveal.js presentations are two dimensional:\n\nHorizontal slides\nVertical sub-slides\n\n\n\n\n\n\n\n\n\n\n\n\nSlide Type\nDescription\nNavigation\n\n\n\n\nSlide\nStandard slide\nLeft and right arrows\n\n\nSub-slide\nSub-topic of a slide\nUp and down arrows\n\n\nFragment\nAnimated part of the previous slide, e.g. a bullet\nPart of slide animation\n\n\n- (or empty)\nAppears in same cell as previous slide\nPart of slide\n\n\nNotes\nSpeaker notes\nVisible when pressing ‘t’\n\n\nSkip\nNot included in the presentation\n-\n\n\n\n\n\n\n\nImages using ![]() or &lt;img&gt; tags don’t show up in exports.\nWorkaround: Paste images into a Markdown cell to include them as attachments. This ensures visibility in HTML and slide exports."
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#virtual-environments",
    "href": "block_1/521_platforms/521_platforms.html#virtual-environments",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "Virtual Environments: Isolated Python environments that allow for the installation of packages without affecting the system’s Python installation.\nBenefits:\n\nIsolation: Packages installed in a virtual environment are isolated from the system’s Python installation.\nReproducibility: Virtual environments can be shared with others to ensure reproducibility.\nVersion control: Allow managing the versions of libraries and tools used in a project.\nCross-platform: Ensures that the project’s dependencies are consistent across different systems.\nExperimentation: Allows for experimentation with different versions of packages.\nClean environment: When starting a new project, starts with a clean slate.\nConsistency: Ensures that the project’s dependencies are consistent across different systems.\nSecurity: Isolates the project’s dependencies from the system’s Python installation.\n\n\n\n\n\nconda create -n &lt;env_name&gt; python=&lt;version&gt;: create a new environment\n\nconda env create -f path/to/environment.yml: create an environment from a file\nconda create --name live_env --clone test_env: create an environment from an existing environment\n\nconda env list: list all environments\nconda activate &lt;env_name&gt;: activate the environment\nconda deactivate: deactivate the environment\nconda env remove -n &lt;env_name&gt; --all: remove the environment\nconda env export -f environment.yml --from-history: export the environment to a file\n\n--from-history: only include packages that were explicitly installed\n\n\nManaging packages:\n\nconda config --add channels conda-forge: add a channel\nconda list: list all packages in the environment\nconda search &lt;package&gt;: search for a package\nconda install &lt;package&gt;: install a package\n\nconda install &lt;package&gt;=&lt;version&gt;: install a specific version of a package\n\nconda remove &lt;package&gt;: remove a package\n\nExample environment.yml file:\nname: test_env\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - conda\n  - python=3.7\n  - pandas==1.0.2\n  - jupyterlab\n\n\n\n\n\n\n\nComponent\nDescription\n\n\n\n\nname\n- Identifies the environment’s name. - Useful for distinguishing multiple environments.\n\n\nchannels\n- Locations where Conda searches for packages. - Default: defaults channel. - Popular options: conda-forge, bioconda.\n\n\ndependencies\n- Lists required packages for the environment. - Can specify versions or ranges. - Can include Conda or pip packages.\n\n\nprefix\n- (Optional) Directory where Conda installs the environment. - Defaults to Conda’s main directory if not provided.\n\n\n\n\n\n\n\nMake a new project in RStudio 1.1. Select setting to use renv 1.2. library(renv): to use renv\nrenv::init(): initialize the project"
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#file-names",
    "href": "block_1/521_platforms/521_platforms.html#file-names",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "3 Principles:\n\nMachine readable:\n\nregex and globbing friendly (avoid spaces, special characters, case sensitivity, etc.)\ndeliberate use of delimiters (e.g. _, -, .)\n\nHuman readable: Helps other people and ourselves in the future quickly understand the file structure and contents of a project/ file.\nPlays well with default ordering: Makes files more organized and easily searchable. Easy for us humans to find the files we are looking for.\n\nDates go: YYYY-MM-DD"
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#asking-questions",
    "href": "block_1/521_platforms/521_platforms.html#asking-questions",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "Reproducible example\nCode formatting\nMinimal, complete, verifiable example\n\nEffective Questioning & Creating an MRE:\n\nSearch for similar questions before asking.\nClearly state the problem in the title and provide brief details in the body.\nProvide the shortest version of your code that replicates the error.\nInclude definitions if you’ve used functions or classes.\nUse toy datasets rather than real data.\nUse markdown for code to ensure readability and syntax highlighting.\nShare attempts, points of confusion, and full error tracebacks."
  },
  {
    "objectID": "block_6/575_adv_ml/575_adv_ml.html",
    "href": "block_6/575_adv_ml/575_adv_ml.html",
    "title": "Advanced Machine Learning (Large Language Models)",
    "section": "",
    "text": "It computes the probability distribution of a sequence of words.\n\n\\(P(w_1, w_2, ..., w_t)\\)\nE.g. P(“I have read this book) &gt; P(”Eye have red this book”)\n\nCan also get the probability of the upcoming word.\n\n\\(P(w_t | w_1, w_2, ..., w_{t-1})\\)\nE.g. P(“book” | “I have read this”) &gt; P(“book” | “I have red this”)\n\n\n\n\n\nLarge language models are trained on a large corpus of text.\n\n\n\n\n\n\nHigh-level: The probability of a word depends only on the previous word (forget everything written before that).\nIdea: Predict future depending upon:\n\nThe current state\nThe probability of change\n\n\n\n\nNaive probability of a sequence of words: \\[P(w_1, w_2, ..., w_t) = P(w_1)P(w_2|w_1)P(w_3|w_1, w_2)...P(w_t|w_1, w_2, ..., w_{t-1})\\]\ne.g. \\[P(\\text{I have read this book}) = P(\\text{I})P(\\text{have}|\\text{I})P(\\text{read}|\\text{I have})P(\\text{this}|\\text{I have read})P(\\text{book}|\\text{I have read this})\\]\nOr simply: \\[P(w_1, w_2, ..., w_t) = \\prod_{i=1}^{t} P(w_i|w_{1:i-1})\\]\nBut this is hard, so in Markov model (n-grams), we only consider the n previous words. With the assumption:\n\\[P(w_t|w_1, w_2, ..., w_{t-1}) \\approx P(w_t| w_{t-1})\\]\n\n\n\n\nHave a set of states \\(S = \\{s_1, s_2, ..., s_n\\}\\).\nA set of discrete initial probabilities \\(\\pi_0 = \\{\\pi_0(s_1), \\pi_0(s_2), ..., \\pi_0(s_n)\\}\\).\nA transition matrix \\(T\\) where each \\(a_{ij}\\) is the probability of transitioning from state \\(s_i\\) to state \\(s_j\\).\n\n\\[\nT =\n\\begin{bmatrix}\n    a_{11}       & a_{12} & a_{13} & \\dots & a_{1n} \\\\\n    a_{21}       & a_{22} & a_{23} & \\dots & a_{2n} \\\\\n    \\dots \\\\\n    a_{n1}       & a_{n2} & a_{n3} & \\dots & a_{nn}\n\\end{bmatrix}\n\\]\n\nProperties:\n\n\\(0 \\leq a_{ij} \\leq 1\\)\nrows sum to 1: \\(\\sum_{j=1}^{n} a_{ij} = 1\\)\ncolumns do not need to sum to 1\nThis is assuming Homogeneous Markov chain (transition matrix does not change over time).\n\n\n\n\n\n\nPredict probabilities of sequences of states\nCompute probability of being at a state at a given time\nStationary Distribution: Find steady state after a long time\nGeneration: Generate a sequences that follows the probability of states\n\n\n\n\nSteady state after a long time.\nBasically the eigenvector of the transition matrix corresponding to the eigenvalue 1.\n\n\\[\\pi T = \\pi\\]\n\nWhere \\(\\pi\\) is the stationary probability distribution \nSufficient Condition for Uniqueness:\n\nPositive transitions (\\(a_{ij} &gt; 0\\) for all \\(i, j\\))\n\nWeaker Condition for Uniqueness:\n\nIrreducible: Can go from any state to any other state (fully connected)\nAperiodic: No fixed period (does not fall into a repetitive loop)\n\n\n\n\n\n\n\nSimilar to Naive Bayes, Markov models is just counting\nGiven \\(n\\) samples/ sequences, we can find:\n\nInitial probabilities: \\(\\pi_0(s_i) = \\frac{\\text{count}(s_i)}{n}\\)\nTransition probabilities: \\(a_{ij} = \\pi(s_i| s_j) = \\frac{\\text{count}(s_i, s_j)}{\\text{count}(s_i)} = \\frac{\\text{count of state i to j}}{\\text{count of state i to any state}}\\)\n\n\n\n\n\n\nMarkov model for NLP\nn in n-gram means \\(n-1\\) previous words are considered\n\ne.g. n=2 (bigram) means consider current word for the future\nDIFFERENT from Markov model definition bigram= markov model with n=1 (we normally use this definition in NLP)\n\nWe extend the definition of a “state” to be a sequence of words\n\ne.g. “I have read this book” -&gt; bigram states: “I have”, “have read”, “read this”, “this book”\n\nexample: “I have read this book”\n\ntrigram (n=2): \\(P(\\text{book} | \\text{read this})\\)\nn=3: \\(P(\\text{book} | \\text{have read this})\\)\n\n\nNote: n we use above is not the same as n in n-gram\n\n\n\nBest way is to embed it in an application and measure how much the application improves (extrinsic evaluation)\nOften it is expensive to run NLP pipeline\nIt is helpful to have a metric to quickly evaluate performance\nMost common intrinsic evaluation metric is perplexity\n\nLower perplexity is better (means better predictor of the words in test set)\n\n\n\n\n\nLet \\(W = w_1, w_2, ..., w_N\\) be a sequences of words.\n\\[\n\\text{Perplexity}(W) = P(w_1, w_2, ..., w_N)^{-\\frac{1}{N}} \\\\\n= \\sqrt[N]{\\frac{1}{P(w_1, w_2, ..., w_N)}}\n\\]\nFor n=1 markov model (bigram):\n\\[P(w_1, w_2, ..., w_N) = \\prod_{i=1}^{N} P(w_i|w_{i-1})\\]\nSo…\n\\[\n\\text{Perplexity}(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i|w_{i-1})}}\n\\]\n\nIncrease n will decrease perplexity =&gt; better model\nToo high still bad because of overfitting\n\n\n\n\n\n\n\n\n\nIdea: The importance of a page is determined by the importance of the pages that link to it.\nMarkov Model: The probability of being on a page at time \\(t\\) depends only on the page at time \\(t-1\\).\nTransition Matrix: The probability of going from page \\(i\\) to page \\(j\\) is the number of links from page \\(i\\) to page \\(j\\) divided by the number of links from page \\(i\\).\n\nAdd \\(\\epsilon\\) to all values so that matrix is fully connected\nNormalize so sum of each row is 1\n\nStationary Distribution: The stationary distribution of the transition matrix gives the importance of each page.\n\nIt shows the page’s long-term visit rate\n\n\n\n\n\n\n\nText is unstructured and messy\n\nNeed to “normalize”\n\n\n\n\n\nSentence segmentation: text -&gt; sentences\nWord tokenization: sentence -&gt; words\n\nProcess of identifying word boundaries\n\nCharacters for tokenization: | Character | Description | | — | — | | Space | Separate words | | dot . | Kind of ambiguous (e.g. U.S.A) | | !, ? | Kind of ambiguous too |\nHow?\n\nRegex\nUse libraries like nltk, spacy, stanza\n\n\n\n\n\n\nIn NLP we talk about:\n\nType: Unique words (element in vocabulary)\nToken: Instances of words\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nWord-Based\nCharacter-Based\n\n\n\n\nAdvantages\n- Faster training and inference due to smaller vocabulary size\n- Can handle unseen words (out-of-vocabulary) and typos by generating characters\n\n\n\n- Leverages existing knowledge of grammar and syntax through word relationships\n- More flexible for generating creative text formats like code or names\n\n\nDisadvantages\n- Requires a large vocabulary, leading to higher memory usage and computational cost\n- May struggle with complex morphology (word structure) in some languages\n\n\n\n- Can struggle with unseen words or typos (resulting in “unknown word” tokens)\n- May generate grammatically incorrect or nonsensical text due to lack of word-level context\n\n\n\n\nn-gram typically have larger state space for word-based models than character-based models\n\n\n\n\n\nRemoving stop words\nLemmatization: Convert words to their base form\nStemming: Remove suffixes\n\ne.g. automates, automatic, automation -&gt; automat\nNot actual words, but can be useful for some tasks\nBe careful, because kind of aggressive\n\n\n\n\n\n\nPart of Speech (POS) Tagging: Assigning a part of speech to each word\nNamed Entity Recognition (NER): Identifying named entities in text\nCoreference Resolution: Identifying which words refer to the same entity\nDependency Parsing: Identifying the grammatical structure of a sentence\n\n\n\n\n\n\n\n\nPython has several libraries for speech recognition.\n\nHave a module called SpeechRecognition which can access:\n\nGoogle Web Speech API\nSphinx\nWit.ai\nMicrosoft Bing Voice Recognition\nIBM Speech to Text\n\nMight need to pay for some of these services\n\nGeneral Task: Given a sequence of audio signals, want to recognize the corresponding phenomes/ words\n\nPhenomes: Distinct units of sound\n\nE.g. “cat” has 3 phenomes: “k”, “ae”, “t”. “dog” has 3 phenomes: “d”, “aa”, “g”\n\nEnglish has ~44 phenomes\n\nIt is a sequence modeling problem\nMany modern speech recognition systems use HMM\n\nHMM is also still useful in bioinformatics, financial modeling, etc.\n\n\n\n\n\n\nHidden: The state is not directly observable\n\ne.g. In speech recognition, the phenome is not directly observable. Or POS (Part of Speech) tags in text.\n\nHMM is specified by a 5-tuple \\((S, Y, \\pi, T, B)\\)\n\n\\(S\\): Set of states\n\\(Y\\): Set of observations\n\\(\\pi\\): Initial state probabilities\n\\(T\\): Transition matrix, where \\(a_{ij}\\) is the probability of transitioning from state \\(s_i\\) to state \\(s_j\\)\n\\(B\\): Emission probabilities. \\(b_j(y)\\) is the probability of observing \\(y\\) in state \\(s_j\\)\n\nYielding the state sequence and observation sequence\n\n\\[\\text{State Sequence}:Q = q_1, q_2, ..., q_T \\in S\\]\n\\[\\text{Observation Sequence}: O = o_1, o_2, ..., o_T \\in Y\\]\n\n\n\n\nThe probability of a particular state depends only on the previous state\n\n\\[P(q_i|q_0,q_1,\\dots,q_{i-1})=P(q_i|q_{i-1})\\]\n\nProbability of an observation depends only on the state.\n\n\\[P(o_i|q_0,q_1,\\dots,q_{i-1},o_0,o_1,\\dots,o_{i-1})=P(o_i|q_i)\\]\nImportant Notes:\n\nObservations are ONLY dependent on the current state\nStates are dependent on the previous state (not observations)\nEach hidden state has a probability distribution over all observations\n\n\n\n\n\nLikelihood\n\nGiven \\(\\theta = (\\pi, T, B)\\) what is the probability of observation sequence \\(O\\)?\n\nDecoding\n\nGiven an observation sequence \\(O\\) and model \\(\\theta\\). How do we choose the best state sequence \\(Q\\)?\n\nLearning\n\nGiven an observation sequence \\(O\\), how do we learn the model \\(\\theta = (\\pi, T, B)\\)?\n\n\n\n\n\n\nWhat is the probability of observing sequence \\(O\\)?\n\n\\[P(O) = \\sum\\limits_{Q} P(O,Q)\\]\nThis means we need all the possible state sequences \\(Q\\)\n\\[P(O,Q) = P(O|Q)\\times P(Q) = \\prod\\limits_{i=1}^T P(o_i|q_i) \\times \\prod\\limits_{i=1}^T P(q_i|q_{i-1})\\]\nThis is computationally inefficient. \\(O(2Tn^T)\\)\n\nNeed to find every possible state sequence \\(n^T\\), then consider each emission given the state sequence \\(T\\)\n\\(n\\) is the number of hidden states\n\\(T\\) is the length of the sequence\n\nTo solve this, we use dynamic programming (Forward Procedure)\n\n\n\nFind \\(P(O|\\theta)\\)\nMake a table of size \\(n \\times T\\) called Trellis\n\nrows: hidden states\ncolumns: time steps\n\nFill the table using the following formula:\n\nInitialization: compute first column (\\(t=0\\))\n\n\\(\\alpha_j(0) = \\pi_j b_j(o_1)\\)\n\n\\(\\pi_j\\): initial state probability\n\\(b_j(o_1)\\): emission probability\n\n\nInduction: compute the rest of the columns (\\(1 \\leq t &lt; T\\))\n\n\\(\\alpha_j(t+1) = \\sum\\limits_{i=1}^n \\alpha_i(t) a_{ij} b_j(o_{t+1})\\)\n\n\\(a_{ij}\\): transition probability from \\(i\\) to \\(j\\)\n\n\nTermination: sum over the last column (\\(t=T\\))\n\n\\(P(O|\\theta) = \\sum\\limits_{i=1}^n \\alpha_T(i)\\)\n\n\nIt is computed left to right and top to bottom\nTime complexity: \\(O(2n^2T)\\)\n\nAt each time step, need to compare states to all other states \\(n^2\\)\nBetter compared to the naive approach \\(O(2Tn^T)\\)\n\n\n\n\n\n\n\n\nTraining data: Set of observations \\(O\\) and set of state sequences \\(Q\\)\nFind parameters \\(\\theta = (\\pi, T, B)\\)\nPopular libraries in Python:\n\nhmmlearn\npomegranate\n\n\n\n\n\n\nGiven an observation sequence \\(O\\) and model \\(\\theta = (\\pi, T, B)\\), how do we choose the best state sequence \\(Q\\)?\nFind \\(Q^* = \\arg\\max_Q P(O,Q|\\theta)\\)\n\n\n\n\n\n\n\n\n\n\nForward Procedure\nViterbi Algorithm\n\n\n\n\nPurpose\nComputes the probability of observing a given sequence of emissions, given the model parameters.\nFinds the most likely sequence of hidden states that explains the observed sequence of emissions, given the model parameters.\n\n\nComputation\nComputes forward probabilities, which are the probabilities of being in a particular state at each time step given the observed sequence.\nComputes the most likely sequence of hidden states.\n\n\nProbability Calculation\nSum over all possible paths through the hidden states.\nRecursively calculates the probabilities of the most likely path up to each state at each time step.\n\n\nObjective\nComputes the likelihood of observing a given sequence of emissions.\nFinds the most probable sequence of hidden states that explains the observed sequence of emissions.\n\n\n\n\nBoth are dynamic programming algorithms with time complexity \\(O(n^2T)\\)\nViterbi Overview:\n\nStore \\(\\delta\\) and \\(\\psi\\) at each node in the trellis\n\n\\(\\delta_i(t)\\) is the max probability of the most likely path ending in trellis node at state \\(i\\) at time \\(t\\)\n\\(\\psi_i(t)\\) is the best possible previous state at time \\(t-1\\) that leads to state \\(i\\) at time \\(t\\)\n\n\n\n\n\n\n\n\\(\\delta_i(0) = \\pi_i b_i(O_0)\\)\n\nrecall \\(b_i(O_0)\\) is the emission probability and \\(\\pi_i\\) is the initial state probability\n\n\\(\\psi_i(0) = 0\\)\n\n\n\n\n\nBest path \\(\\delta_j(t)\\) to state \\(j\\) at time \\(t\\) depends on each previous state and their transition to state \\(j\\)\n\\(\\delta_j(t) = \\max\\limits_i \\{\\delta_i(t-1)a_{ij}\\} b_j(o_t)\\)\n\n\\(b_j(o_t)\\) is the emission probability of observation \\(o_t\\) given state \\(j\\)\n\n\\(\\psi_j(t) = \\arg \\max\\limits_i \\{\\delta_i(t-1)a_{ij}\\}\\)\n\n\n\n\n\nChoose the best final state\n\n\\(q_t^* = \\arg\\max\\limits_i \\delta_i(T)\\)\n\nRecursively choose the best previous state\n\n\\(q_{t-1}^* = \\psi_{q_t^*}(T)\\)\n\n\n\n\n\n\n\nWe do not always have mapping from observations to states (emission probabilities \\(B\\))\nGiven an observation sequence \\(O\\) but not the state sequence \\(Q\\), how do we choose the best parameters \\(\\theta = (\\pi, T, B)\\)?\nUse forward-backward algorithm\n\n\n\n\nReverse of the forward procedure right to left but still top to bottom\nFind the probability of observing the rest of the sequence given the current state\n\n\\(\\beta_j(t) = P(o_{t+1}, o_{t+2}, \\dots, o_T)\\)\n\n\n\n\n\n\nInitialization: set all values at last time step to 1\n\n\\(\\beta_j(T) = 1\\)\n\nInduction: compute the rest of the columns (\\(1 \\leq t &lt; T\\))\n\n\\(\\beta_i(t) = \\sum_{j=1}^N a_{ij}b_{j}(o_{t+1}) \\beta_j(t+1)\\)\n\nConclusion: sum over the first column\n\n\\(P(O|\\theta) = \\sum_{i=1}^N \\pi_i b_i(o_1) \\beta_i(1)\\)\n\n\n\n\n\n\n\nGiven observation sequence \\(O\\), no state sequence \\(S\\), how do we choose the “best” parameters \\(\\theta = (\\pi, T, B)\\)?\nWant \\(\\theta\\) that maximizes \\(P(O|\\theta)\\)\nCannot use MLE because we do not have the state sequence\nUse an unsupervised learning algorithm called Baum-Welch (Expectation-Maximization)\n\n\n\n\nInitialize \\(\\theta = (\\pi, T, B)\\) (guess) then iteratively update them\nCombines the forward (get \\(alpha\\)) and backward (get \\(beta\\)) procedures\n\\(\\alpha\\) and \\(\\beta\\) are combined to represent the probability of an entire observation sequence\nDefine \\(\\gamma\\) and \\(\\xi\\) to update the parameters\n\n\\(\\gamma_i(t)\\): probability of being in state \\(i\\) at time \\(t\\) given entire observation sequence \\(O\\)\n\\(\\xi_{ij}(t)\\): probability of transitioning from state \\(i\\) to state \\(j\\) at time \\(t\\) to \\(t+1\\) given entire observation sequence \\(O\\) regarless of previous and future states\n\nThese probabilities are used to compute the expected:\n\nNumber of times in state \\(i\\)\nNumber of transitions from state \\(i\\) to state \\(j\\)\n\nRepeat until convergence\n\n\n\n\n\n\n\nMotivation:\n\nHumans are good at identifying topics in documents.\nBut, it is difficult to do this at scale. (e.g., 1000s of documents)\n\n\n\n\n\nCommon to use unsupervised learning techniques\n\nGiven hyperparameter \\(K\\), we want to find \\(K\\) topics.\n\nIn unsupervised, a common model:\n\nInput:\n\n\\(D\\) documents\n\\(K\\) topics\n\nOutput:\n\nTopic-word association: for each topic, what words describe that topic?\nDocument-topic association: for each document, what topics are in that document?\n\n\nCommon approaches:\n\nLatent Semantic Analysis (LSA)\nLatent Dirichlet Allocation (LDA)\n\n\n\n\n\n\nSingular Value Decomposition (SVD) of the term-document matrix. See LSA notes from 563.\n\n\\[X_{n \\times d} \\approx Z_{n \\times k}W_{k \\times d}\\]\n\n\\(n\\): number of documents, \\(d\\): number of words, \\(k\\): number of topics\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nlsa_pipe = make_pipeline(\n    CountVectorizer(stop_words=\"english\"), TruncatedSVD(n_components=3)\n)\n\nZ = lsa_pipe.fit_transform(toy_df[\"text\"]);\n\n\n\n\nBayesian, generative, and unsupervised model\nDeveloped by David Blei and colleagues in 2003\n\nOne of the most cited papers in computer science\n\nDocument-topic distribution or topic proportions \\(\\theta\\):\n\nEach document is considered a mixture of topics\n\nTopic-word distribution:\n\nEach topic is considered a mixture of words\nThis is from all documents\n\n\n\n\n\nSet the number of topics \\(K\\)\nRandomly assign each word in each document to a topic\nFor each document \\(d\\):\n\nChoose a distribution over topics \\(\\theta\\) from a Dirichlet prior\n\nUse dirichlet distribution because it is conjugate priot (same form as posterior)\n\nFor each word in the document:\n\nChoose a topic from the document’s topic distribution \\(\\theta\\)\nChoose a word from the topic’s word distribution \n\n\n\n\nFit using Bayesian inference (most commonly MCMC)\n\n\n\n\n\nA Markov Chain Monte Carlo (MCMC) method\nVery accurate, but slow (alternative is Variational Inference, which is faster but less accurate)\nUsed to approximate the posterior distribution for document-topic and topic-word distributions\nMain steps:\n\nInitialization: Randomly assign each word in each document to a topic \nUpdate topic-word assignments:\n\nDecrease count of current word in both topic and document distributions\nReassign word to a new topic based on probabilities  \n\nIterate until convergence \n\n\n\n\n\n\n\n3 Main components:\n\nPreprocess corpus\nTrain LDA (use sklearn or gensim)\nInterpret results\n\n\n\n\n\nCrucial to preprocess text data before training LDA\nNeed tokenization, lowercasing, removing punctuation, stopwords\nOptionally, lemmatization or POS tagging\n\nimport spacy\n\nnlp = spacy.load(\"en_core_web_md\")\n\ndef preprocess_spacy(\n    doc,\n    min_token_len=2,\n    irrelevant_pos=[\"ADV\", \"PRON\", \"CCONJ\", \"PUNCT\", \"PART\", \"DET\", \"ADP\"],\n):\n  \"\"\"\n  Preprocess a document using spaCy\n  [Tokenize, remove stopwords, minimum token length, irrelevant POS tags, lemmatization]\n  \"\"\"\n    clean_text = []\n\n    for token in doc:\n        if (\n            token.is_stop == False  # Check if it's not a stopword\n            and len(token) &gt; min_token_len  # Check if the word meets minimum threshold\n            and token.pos_ not in irrelevant_pos\n        ):  # Check if the POS is in the acceptable POS tags\n            lemma = token.lemma_  # Take the lemma of the word\n            clean_text.append(lemma.lower())\n    return \" \".join(clean_text)\n\nwiki_df = [preprocess_spacy(doc) for doc in nlp.pipe(wiki_df[\"text\"])]\n\n\n\n\nWith sklearn:\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nvectorizer = CountVectorizer()\ndtm = vectorizer.fit_transform(wiki_df[\"text_pp\"])\n\nn_topics = 3\nlda = LatentDirichletAllocation(\n    n_components=n_topics, learning_method=\"batch\", max_iter=10, random_state=0\n)\ndocument_topics = lda.fit_transform(dtm)\n\n# Get the topic-word distribution\nsorting = np.argsort(lda.components_, axis=1)[:, ::-1]\nfeature_names = np.array(vectorizer.get_feature_names_out())\n\nWith gensim:\n\nimport gensim\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaModel\n\ncorpus = [doc.split() for doc in wiki_df[\"text_pp\"].tolist()]\ndictionary = Dictionary(corpus) # Create a vocabulary for the lda model\n\n# Get document-term matrix\ndoc_term_matrix = [dictionary.doc2bow(doc) for doc in corpus]\n\n# Train LDA model\nnum_topics = 3\n\nlda = LdaModel(\n    corpus=doc_term_matrix,\n    id2word=dictionary,\n    num_topics=num_topics,\n    random_state=42,\n    passes=10,\n)\n\nMain hyperparameters read more about them in the documentation\n\nnum_topics/ K: number of topics\nalpha: Prior on document-topic distribution\n\nHigh alpha: documents are likely to be a mixture of many topics\nLow alpha: documents are likely to be a mixture of few topics\n\neta: Prior on topic-word distribution\n\nHigh eta: topics are likely to be a mixture of many words\nLow eta: topics are likely to be a mixture of few words\n\n\n\n\n\n\n\nSince this is unsupervised, we need to interpret the topics ourselves\nIdea is to tell a story to humans and what we should care about and evaluate\nCommon methods:\n\nLook at the top words in each topic and make judgements\n\nWord Intrusion: Add a random word to the top words and see if it is noticed\n\nExtrinsic evaluation: Evaluate whether topic nodel with current hyperparameters improves the results of task or not\nQuantify topic interpretability with metrics like Coherence Score\n\nUse Gensim’s CoherenceModel to calculate coherence score\nScore is between -1 and 1, higher is better\n\n\n\n\n\n\n\n\n\n\n\nRecall when modelling sequences:\n\nOrder matters\nSequence length can vary\nNeed to capture long-term dependencies\n\nProblem with Markov models:\n\nOnly capture short-term dependencies\nSparsity problem: if there are a lot of states, the transition matrix will be very sparse\nAlso need large memory to store the n-grams\nMM do not scale well\n\nTo get closer to the ideal language model (closer to ChatGPT), here we will learn neural sequencing models.\nProblem with Feedforward Neural Networks:\n\nLose temporal information\nAll connects are fully connected and flow forward (no loops)\n\n\n\n\n\n\nRNNs are a type of neural network that can model sequences\n\nSimilar to NN, it is supervised learning\n\nSolves the limited memory problem of Markov models\n\nMemory only scales with number of words \\(O(n)\\)\n\nThey use recurrent connections to maintain a state over time.  source: Stanford CS224d slides\nConnect the hidden layer to itself\n\n# Pseudocode for RNN\nrnn = RNN()\nff = FeedForwardNN()\n\nhidden_state = [0, 0, 0] # depenpent on the number of hidden units\nfor word in input:\n    output, hidden_state = rnn(word, hidden_state)\n\nprediction = ff(hidden_state)\n\nsource: Video on RNN\n\nThe states above are hidden layers in each time step\n\nSimilar to HMMs, but state is continuous, high dimensional, and much richer\n\nEach state contains information about the whole past sequence\nNot that different from feedforward NNs\n\nStill does forward calculation\nJust have new set of weights \\(U\\) that connect previous hidden state to current hidden state\nThese weights are also trained via backpropagation\n\n\n\n\n\n\nThere are 3 weight matrices in RNNs:\n\n\\(W\\): input -&gt; hidden\n\nsize: \\(d_{\\text{input}} \\times d_{\\text{hidden}}\\)\n\n\\(V\\): hidden -&gt; output\n\nsize: \\(d_{\\text{hidden}} \\times d_{\\text{output}}\\)\n\n\\(U\\): hidden -&gt; hidden\n\nsize: \\(d_{\\text{hidden}} \\times d_{\\text{hidden}}\\)\n\n\nImportant point: All weights between time steps are shared\n\nAllows model to learn patterns that are independent of their position in the sequence\n\n\n\n\n\n\n\nComputing new state \\(h_t\\):\n\n\\(h_t = g(Uh_{t-1} + Wx_t + b_1)\\)\n\n\\(g()\\): activation function\n\\(x_t\\): input at time \\(t\\)\n\\(b_1\\): bias\n\n\nComputing output \\(y_t\\):\n\n\\(y_t = \\text{softmax}(Vh_t + b_2)\\)\n\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import RNN\n\nINPUT_SIZE = 20\nHIDDEN_SIZE = 10\nNUM_LAYERS = 1 # number of hidden layers\n\nrnn = nn.RNN(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS)\n\nh0 = torch.randn(NUM_LAYERS, HIDDEN_SIZE) # initial hidden state\n\n# PyTorch gives output and hidden state\noutput, hn = rnn(input, h0)\nSimple Sentiment RNN Class:\nclass SentimentRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n\n        super(SentimentRNN, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, text):\n        embedded = self.embedding(text)\n        output, hidden = self.rnn(embedded)\n        assert torch.equal(output[:, -1, :], hidden.squeeze(0))\n        return self.fc(hidden.squeeze(0))\n\n# Example usage\nmodel = SentimentRNN(vocab_size=vocab_size, embedding_dim=100, hidden_dim=128, output_dim=2)\n\n\n\n\nSince it is supervised, we have: training set, loss function, and backpropagation\nNeed to tailor backpropagation for RNNs\n\nSince hidden layers are connected to themselves, we need to backpropagate through time\nBackpropagation Through Time (BPTT)\n\nUnroll the RNN for a fixed number of time steps\nCalculate the loss at each time step\nSum the losses and backpropagate\nUpdate the weights\n\nSee the code for the code implementation by Andrej Karpathy\n\n\n\n\n\n\nPossible RNN architectures: \n\nSequence Labelling:\n\nE.g. Named Entity Recognition (NER) or Part-of-Speech (POS) tagging\nMany-to-many architecture\nInput are pre-trained word embeddings, outputs are tag probabilities by softmax\n\nSequence Classification:\n\nE.g. Spam detection or sentiment analysis\nSimilar to pseudo-code above, feed result of last hidden layer to a feedforward NN\nMany-to-one architecture\n\nText Generation:\n\nE.g. ChatGPT\nOne-to-many architecture\nInput is a seed, output is a sequence of words\n\nImage captioning:\n\nE.g. Show and Tell\nMany-to-many architecture\nInput is an image, output is a sequence of words\n\n\n\n\n\n\nUse sequence of outputs from one RNN as the sequence of inputs to another RNN\nGenerally outperform single-layer RNNs\nCan learn different level of abstraction in each layer\nNumber of layers is a hyperparameter, remember that higher also means more training cost\n\n\n\n\n\nUse case is in POS tagging it is useful to know words both before and after the current word\nBidirectional RNNs have two hidden layers, one for forward and one for backward\n\nCombines two independent RNNs\n\n\n\n\n\n3 Main problems:\n\nHard to remember relevant information\n\nVanishing gradients because of long sequences\nCase example: The students in the exam where the fire alarm is ringing (are) really stressed.\n\nNeed to retain information that students are plural so use “are”\n\n\nHard to optimize\nHard to parallelize\n\n\n\n\n\n\n\n\nApproach to sequence processing without using RNNs or LSTMs\nIdea: Build up richer and richer contextual representations of words across series of transformer layers\n\nContextual representation: Representation of a word that depends on the context in which it appears\n\nGPT-3: 96 layers of transformers\nBenefits:\n\nParallelization: Can process all words in parallel\nLong-range dependencies: Can learn dependencies between words that are far apart\n\nTwo main components:\n\nSelf-attention mechanism\nPositional embeddings/encodings\n\n\n\n\n\n\nGoal: To look broadly into the context and tells us how to integrate the representations of context words to build representation of a word\nIdea: Compute attention scores between each pair of words in a sentence\n\nAttention score: How much one word should focus on another word\n\nExample: “She plucked the guitar strings , ending with a melancholic note.”\n\nAttention score from note to she is low\nAttention score from note to guitar is high\nAttention score from note to melancholic is very high\n\n\n\n\nThe basic steps of the self-attention mechanism are as follows:\n\nCompare each word to every other word in the sentence (usually by dot product)\nApply softmax to derive a probability distribution over all words\nCompute a weighted sum of all words, where the weights are the probabilities from step 2\n\n\nOperations can be done in parallel\n\n\n\n\n\nQuery \\(W^Q\\): Word whose representation we are trying to compute (current focus of attention)\nKey \\(W^K\\): Word that we are comparing the query to\nValue \\(W^V\\): Word that we are trying to compute the representation of (output for the current focus of attention)\nWe can assume all of them have dimension … [TODO]\n\n\n\n\nAll diagrams in this section is made by my friend Ben Chen. Check out his blog on self-attention.\n\nAll inputs \\(a_i\\) are connected to each other to make outputs \\(b_i\\) \nThis is a breakdown of how each input \\(a_i\\) is connected to each output \\(b_i\\) using the query, key, and value \n\nIn the example our query is \\(a_1\\), and our keys are \\(a_2\\), \\(a_3\\), and \\(a_4\\)\n\n\nNote: For LLMs, not all the sequences are connected to each other, only words before the current word are connected to the current word.\n\n\n\nGet the \\(\\alpha\\) values\n\nCan either do a dot product approach (more common) \nOr an additive approach with an activation function (like tanh) \n\nApply softmax to get \\(\\alpha'\\) values\nMultiply \\(\\alpha'\\) values by the matrix product \\(W^V \\cdot A\\) to get the output \\(b_1\\)\n\n\n\n\n\nResult of the dot product can be very large\nThey are scaled before applying softmax\nCommon scaling: \\(score(x_i, x_j) = \\frac{x_i \\cdot x_j}{\\sqrt{d}}\\)\n\n\\(d\\): Dimensionsionality of the query and key vectors\n\n\n\n\n\n\nLet \\(X\\) be matrix of all input \\(x_i\\) vectors (Shape: \\(N \\times d\\))\n\n\\(Q_{N \\times d_k} = X \\cdot W^Q_{d \\times d_k}\\)\n\\(K_{N \\times d_k} = X \\cdot W^K_{d \\times d_k}\\)\n\\(V_{N \\times d_v} = X \\cdot W^V_{d \\times d_v}\\)\n\nWe can then get \\(\\alpha\\) easily by \\(Q \\times K\\) (shape: \\(N \\times N\\))\n\nRecall \\(N\\) is the number of words in the sentence\n\n\nThen to get the \\(\\text{Self Attention}(Q,K,V) = \\text{softmax}(\\frac{Q \\times K}{\\sqrt{d_k}}) \\times V\\)\nBut for LLMs, we only want to look at the words before the current word, so:\n\n\n\n\n\n\nUsing self-attention mechanism, we can learn dependencies between words, but we lose the order of words\nSolution: Add positional embeddings to the input embeddings\n\nPositional embeddings: Embeddings that encode the position of a word in a sentence\n\n\n\n\n\n\nDifferent words in a sentence can relate to each other in different ways simultaneously\n\ne.g. “The cat was scared because it didn’t recognize me in my mask”\n\nSingle attention layer might not be able to capture all these relationships\nTransformer uses multiple attention layers in parallel\n\nEach layer is called a head\nEach head learns different relationships between words\n\n\n\nsource\n\n\n\n\n\nEach Transformer block consists of:\n\nMulti-head self-attention layer\nFeed-forward neural network:\n\n\\(N\\) network\n1 hidden layer (normally higher dimensionality than input), 2 weight matrices\n\nResidual connections\n\nAdd some “skip” connections because improves learning and gives more information to the next layer\n\nLayer normalization\n\nSimilar to StandardScaler, make mean 0 and variance 1\nTo keep values in a certain range\n\n\n\n\\[\n  T^1 = \\text{SelfAttention}(X)\\\\\n  T^2 = X + T^1\\\\\n  T^3 = \\text{LayerNorm}(T^2)\\\\\n  T^4 = \\text{FFN}(T^3)\\\\\n  T^5 = T^4 + T^3\\\\\n  H = \\text{LayerNorm}(T^5)\n\\]\n\nInput and Output dimensions are matched so they can be “stacked”\n\n\n\n\n\nTake output of \\(h_N\\) and get logit vector of shape \\(1 \\times V\\) where \\(V\\) is the vocabulary size\n\n\\(h_N\\) -&gt; unembedding layer -&gt; logit vector -&gt; softmax -&gt; probability distribution\n\nThis probability distribution is used to predict the next word\nThis is a specific example of a decoder in a transformer\n\n\n\n\n\n\n\nDecoder-only Transformer\n\nExample: GPT-3\nTraining: Trained on unsupervised data\nUse case: Language modeling, text generation\n\nEncoder-only Transformer\n\nExample: BERT\nTraining: Trained on supervised data\nUse case: Text classification, question answering, sentiment analysis\n\nEncoder-Decoder Transformer\n\nExample: T5, BART\nTraining: Trained on supervised data\nUse case: Machine translation, summarization\n\n\n\n\n\nTraining: Segment corpus of text into input-output pairs\nTo predict the next word, given input words\nSelf-attention only sees words before the current word\n\nUse a causal mask to prevent the model from looking at future words\n\n\n\n\n\nOnce trained, can generate text autoregressively\n\nIncrementally generating words by sampling the next word based on previous choices\nSampling part is similar to generation with Markov models (but with more context and long-range dependencies)\n\n\n\n\n\n\n\nMainly designed for a wide range of NLP tasks (e.g., text classification)\nIt has bidirectional self-attention\n\nCan look at all words in a sentence\nCan learn dependencies between words in both directions \n\nTraining:\n\n“fill in the blank” tasks/ cloze tasks\n\nModel predicts the probability of missing words in a sentence, use cross-entropy loss\n\nMask tokens and learn to recover them\nContextual embeddings: representations created by masked language models\n\nDifferent to single vector embeddings from word2vec\nEach word has a different vector depending on the context\n\n\nTransfer learning through fine-tuning:\n\nGPT and BERT models are pre-trained on large corpora (very general)\nCan create interfaces from these models to downstream tasks\nEither freeze training or make minor adjustments to the model\n\n\n\n\n\n\nFor machine translation, summarization, etc.\nHigh level architecture:\n\nEncoder: Takes input text and creates a representation\n\nSimilar transformer blocks as in the encoder-only transformer\n\nDecoder: Takes the representation and generates the output text\n\nMore powerful block with extra cross-attention layer that can attend to all encoder words\n\nAttention mechanism: Helps the decoder focus on different parts of the input text\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nDecoder-only (e.g., GPT-3)\nEncoder-only (e.g., BERT, RoBERTa)\nEncoder-decoder (e.g., T5, BART)\n\n\n\n\nContextual Embedding Direction\nUnidirectional\nBidirectional\nBidirectional\n\n\nOutput Computation Based on\nInformation earlier in the context\nEntire context (bidirectional)\nEncoded input context\n\n\nText Generation\nCan naturally generate text completion\nCannot directly generate text\nCan generate outputs naturally\n\n\nExample\nMDS Cohort 8 is the ___\nMDS Cohort 8 is the best! → positive\nInput: Translate to Mandarin: MDS Cohort 8 is the best! Output: MDS 第八期是最棒的!\n\n\nUsage\nRecursive prediction over the sequence\nUsed for classification tasks, sequence labeling taks and many other tasks\nUsed for tasks requiring transformations of input (e.g., translation, summarization)\n\n\nTextual Context Embeddings\nProduces unidirectional contextual embeddings and token distributions\nCompute bidirectional contextual embeddings\nCompute bidirectional contextual embeddings in the encoder part and unidirectional embeddings in the decoder part\n\n\nSequence Processing\nGiven a prompt \\(X_{1:i}\\), produces embeddings for \\(X_{i+1}\\) to \\(X_{L}\\)\nContextual embeddings are used for analysis, not sequential generation\nEncode input sequence, then decode to output sequence"
  },
  {
    "objectID": "block_6/575_adv_ml/575_adv_ml.html#language-models",
    "href": "block_6/575_adv_ml/575_adv_ml.html#language-models",
    "title": "Advanced Machine Learning (Large Language Models)",
    "section": "",
    "text": "It computes the probability distribution of a sequence of words.\n\n\\(P(w_1, w_2, ..., w_t)\\)\nE.g. P(“I have read this book) &gt; P(”Eye have red this book”)\n\nCan also get the probability of the upcoming word.\n\n\\(P(w_t | w_1, w_2, ..., w_{t-1})\\)\nE.g. P(“book” | “I have read this”) &gt; P(“book” | “I have red this”)\n\n\n\n\n\nLarge language models are trained on a large corpus of text."
  },
  {
    "objectID": "block_6/575_adv_ml/575_adv_ml.html#markov-model",
    "href": "block_6/575_adv_ml/575_adv_ml.html#markov-model",
    "title": "Advanced Machine Learning (Large Language Models)",
    "section": "",
    "text": "High-level: The probability of a word depends only on the previous word (forget everything written before that).\nIdea: Predict future depending upon:\n\nThe current state\nThe probability of change\n\n\n\n\nNaive probability of a sequence of words: \\[P(w_1, w_2, ..., w_t) = P(w_1)P(w_2|w_1)P(w_3|w_1, w_2)...P(w_t|w_1, w_2, ..., w_{t-1})\\]\ne.g. \\[P(\\text{I have read this book}) = P(\\text{I})P(\\text{have}|\\text{I})P(\\text{read}|\\text{I have})P(\\text{this}|\\text{I have read})P(\\text{book}|\\text{I have read this})\\]\nOr simply: \\[P(w_1, w_2, ..., w_t) = \\prod_{i=1}^{t} P(w_i|w_{1:i-1})\\]\nBut this is hard, so in Markov model (n-grams), we only consider the n previous words. With the assumption:\n\\[P(w_t|w_1, w_2, ..., w_{t-1}) \\approx P(w_t| w_{t-1})\\]\n\n\n\n\nHave a set of states \\(S = \\{s_1, s_2, ..., s_n\\}\\).\nA set of discrete initial probabilities \\(\\pi_0 = \\{\\pi_0(s_1), \\pi_0(s_2), ..., \\pi_0(s_n)\\}\\).\nA transition matrix \\(T\\) where each \\(a_{ij}\\) is the probability of transitioning from state \\(s_i\\) to state \\(s_j\\).\n\n\\[\nT =\n\\begin{bmatrix}\n    a_{11}       & a_{12} & a_{13} & \\dots & a_{1n} \\\\\n    a_{21}       & a_{22} & a_{23} & \\dots & a_{2n} \\\\\n    \\dots \\\\\n    a_{n1}       & a_{n2} & a_{n3} & \\dots & a_{nn}\n\\end{bmatrix}\n\\]\n\nProperties:\n\n\\(0 \\leq a_{ij} \\leq 1\\)\nrows sum to 1: \\(\\sum_{j=1}^{n} a_{ij} = 1\\)\ncolumns do not need to sum to 1\nThis is assuming Homogeneous Markov chain (transition matrix does not change over time).\n\n\n\n\n\n\nPredict probabilities of sequences of states\nCompute probability of being at a state at a given time\nStationary Distribution: Find steady state after a long time\nGeneration: Generate a sequences that follows the probability of states\n\n\n\n\nSteady state after a long time.\nBasically the eigenvector of the transition matrix corresponding to the eigenvalue 1.\n\n\\[\\pi T = \\pi\\]\n\nWhere \\(\\pi\\) is the stationary probability distribution \nSufficient Condition for Uniqueness:\n\nPositive transitions (\\(a_{ij} &gt; 0\\) for all \\(i, j\\))\n\nWeaker Condition for Uniqueness:\n\nIrreducible: Can go from any state to any other state (fully connected)\nAperiodic: No fixed period (does not fall into a repetitive loop)\n\n\n\n\n\n\n\nSimilar to Naive Bayes, Markov models is just counting\nGiven \\(n\\) samples/ sequences, we can find:\n\nInitial probabilities: \\(\\pi_0(s_i) = \\frac{\\text{count}(s_i)}{n}\\)\nTransition probabilities: \\(a_{ij} = \\pi(s_i| s_j) = \\frac{\\text{count}(s_i, s_j)}{\\text{count}(s_i)} = \\frac{\\text{count of state i to j}}{\\text{count of state i to any state}}\\)\n\n\n\n\n\n\nMarkov model for NLP\nn in n-gram means \\(n-1\\) previous words are considered\n\ne.g. n=2 (bigram) means consider current word for the future\nDIFFERENT from Markov model definition bigram= markov model with n=1 (we normally use this definition in NLP)\n\nWe extend the definition of a “state” to be a sequence of words\n\ne.g. “I have read this book” -&gt; bigram states: “I have”, “have read”, “read this”, “this book”\n\nexample: “I have read this book”\n\ntrigram (n=2): \\(P(\\text{book} | \\text{read this})\\)\nn=3: \\(P(\\text{book} | \\text{have read this})\\)\n\n\nNote: n we use above is not the same as n in n-gram\n\n\n\nBest way is to embed it in an application and measure how much the application improves (extrinsic evaluation)\nOften it is expensive to run NLP pipeline\nIt is helpful to have a metric to quickly evaluate performance\nMost common intrinsic evaluation metric is perplexity\n\nLower perplexity is better (means better predictor of the words in test set)\n\n\n\n\n\nLet \\(W = w_1, w_2, ..., w_N\\) be a sequences of words.\n\\[\n\\text{Perplexity}(W) = P(w_1, w_2, ..., w_N)^{-\\frac{1}{N}} \\\\\n= \\sqrt[N]{\\frac{1}{P(w_1, w_2, ..., w_N)}}\n\\]\nFor n=1 markov model (bigram):\n\\[P(w_1, w_2, ..., w_N) = \\prod_{i=1}^{N} P(w_i|w_{i-1})\\]\nSo…\n\\[\n\\text{Perplexity}(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i|w_{i-1})}}\n\\]\n\nIncrease n will decrease perplexity =&gt; better model\nToo high still bad because of overfitting"
  },
  {
    "objectID": "block_6/575_adv_ml/575_adv_ml.html#applications-of-markov-models",
    "href": "block_6/575_adv_ml/575_adv_ml.html#applications-of-markov-models",
    "title": "Advanced Machine Learning (Large Language Models)",
    "section": "",
    "text": "Idea: The importance of a page is determined by the importance of the pages that link to it.\nMarkov Model: The probability of being on a page at time \\(t\\) depends only on the page at time \\(t-1\\).\nTransition Matrix: The probability of going from page \\(i\\) to page \\(j\\) is the number of links from page \\(i\\) to page \\(j\\) divided by the number of links from page \\(i\\).\n\nAdd \\(\\epsilon\\) to all values so that matrix is fully connected\nNormalize so sum of each row is 1\n\nStationary Distribution: The stationary distribution of the transition matrix gives the importance of each page.\n\nIt shows the page’s long-term visit rate"
  },
  {
    "objectID": "block_6/575_adv_ml/575_adv_ml.html#basic-text-preprocessing",
    "href": "block_6/575_adv_ml/575_adv_ml.html#basic-text-preprocessing",
    "title": "Advanced Machine Learning (Large Language Models)",
    "section": "",
    "text": "Text is unstructured and messy\n\nNeed to “normalize”\n\n\n\n\n\nSentence segmentation: text -&gt; sentences\nWord tokenization: sentence -&gt; words\n\nProcess of identifying word boundaries\n\nCharacters for tokenization: | Character | Description | | — | — | | Space | Separate words | | dot . | Kind of ambiguous (e.g. U.S.A) | | !, ? | Kind of ambiguous too |\nHow?\n\nRegex\nUse libraries like nltk, spacy, stanza\n\n\n\n\n\n\nIn NLP we talk about:\n\nType: Unique words (element in vocabulary)\nToken: Instances of words\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nWord-Based\nCharacter-Based\n\n\n\n\nAdvantages\n- Faster training and inference due to smaller vocabulary size\n- Can handle unseen words (out-of-vocabulary) and typos by generating characters\n\n\n\n- Leverages existing knowledge of grammar and syntax through word relationships\n- More flexible for generating creative text formats like code or names\n\n\nDisadvantages\n- Requires a large vocabulary, leading to higher memory usage and computational cost\n- May struggle with complex morphology (word structure) in some languages\n\n\n\n- Can struggle with unseen words or typos (resulting in “unknown word” tokens)\n- May generate grammatically incorrect or nonsensical text due to lack of word-level context\n\n\n\n\nn-gram typically have larger state space for word-based models than character-based models\n\n\n\n\n\nRemoving stop words\nLemmatization: Convert words to their base form\nStemming: Remove suffixes\n\ne.g. automates, automatic, automation -&gt; automat\nNot actual words, but can be useful for some tasks\nBe careful, because kind of aggressive\n\n\n\n\n\n\nPart of Speech (POS) Tagging: Assigning a part of speech to each word\nNamed Entity Recognition (NER): Identifying named entities in text\nCoreference Resolution: Identifying which words refer to the same entity\nDependency Parsing: Identifying the grammatical structure of a sentence"
  },
  {
    "objectID": "block_6/575_adv_ml/575_adv_ml.html#hidden-markov-models",
    "href": "block_6/575_adv_ml/575_adv_ml.html#hidden-markov-models",
    "title": "Advanced Machine Learning (Large Language Models)",
    "section": "",
    "text": "Python has several libraries for speech recognition.\n\nHave a module called SpeechRecognition which can access:\n\nGoogle Web Speech API\nSphinx\nWit.ai\nMicrosoft Bing Voice Recognition\nIBM Speech to Text\n\nMight need to pay for some of these services\n\nGeneral Task: Given a sequence of audio signals, want to recognize the corresponding phenomes/ words\n\nPhenomes: Distinct units of sound\n\nE.g. “cat” has 3 phenomes: “k”, “ae”, “t”. “dog” has 3 phenomes: “d”, “aa”, “g”\n\nEnglish has ~44 phenomes\n\nIt is a sequence modeling problem\nMany modern speech recognition systems use HMM\n\nHMM is also still useful in bioinformatics, financial modeling, etc.\n\n\n\n\n\n\nHidden: The state is not directly observable\n\ne.g. In speech recognition, the phenome is not directly observable. Or POS (Part of Speech) tags in text.\n\nHMM is specified by a 5-tuple \\((S, Y, \\pi, T, B)\\)\n\n\\(S\\): Set of states\n\\(Y\\): Set of observations\n\\(\\pi\\): Initial state probabilities\n\\(T\\): Transition matrix, where \\(a_{ij}\\) is the probability of transitioning from state \\(s_i\\) to state \\(s_j\\)\n\\(B\\): Emission probabilities. \\(b_j(y)\\) is the probability of observing \\(y\\) in state \\(s_j\\)\n\nYielding the state sequence and observation sequence\n\n\\[\\text{State Sequence}:Q = q_1, q_2, ..., q_T \\in S\\]\n\\[\\text{Observation Sequence}: O = o_1, o_2, ..., o_T \\in Y\\]\n\n\n\n\nThe probability of a particular state depends only on the previous state\n\n\\[P(q_i|q_0,q_1,\\dots,q_{i-1})=P(q_i|q_{i-1})\\]\n\nProbability of an observation depends only on the state.\n\n\\[P(o_i|q_0,q_1,\\dots,q_{i-1},o_0,o_1,\\dots,o_{i-1})=P(o_i|q_i)\\]\nImportant Notes:\n\nObservations are ONLY dependent on the current state\nStates are dependent on the previous state (not observations)\nEach hidden state has a probability distribution over all observations\n\n\n\n\n\nLikelihood\n\nGiven \\(\\theta = (\\pi, T, B)\\) what is the probability of observation sequence \\(O\\)?\n\nDecoding\n\nGiven an observation sequence \\(O\\) and model \\(\\theta\\). How do we choose the best state sequence \\(Q\\)?\n\nLearning\n\nGiven an observation sequence \\(O\\), how do we learn the model \\(\\theta = (\\pi, T, B)\\)?\n\n\n\n\n\n\nWhat is the probability of observing sequence \\(O\\)?\n\n\\[P(O) = \\sum\\limits_{Q} P(O,Q)\\]\nThis means we need all the possible state sequences \\(Q\\)\n\\[P(O,Q) = P(O|Q)\\times P(Q) = \\prod\\limits_{i=1}^T P(o_i|q_i) \\times \\prod\\limits_{i=1}^T P(q_i|q_{i-1})\\]\nThis is computationally inefficient. \\(O(2Tn^T)\\)\n\nNeed to find every possible state sequence \\(n^T\\), then consider each emission given the state sequence \\(T\\)\n\\(n\\) is the number of hidden states\n\\(T\\) is the length of the sequence\n\nTo solve this, we use dynamic programming (Forward Procedure)\n\n\n\nFind \\(P(O|\\theta)\\)\nMake a table of size \\(n \\times T\\) called Trellis\n\nrows: hidden states\ncolumns: time steps\n\nFill the table using the following formula:\n\nInitialization: compute first column (\\(t=0\\))\n\n\\(\\alpha_j(0) = \\pi_j b_j(o_1)\\)\n\n\\(\\pi_j\\): initial state probability\n\\(b_j(o_1)\\): emission probability\n\n\nInduction: compute the rest of the columns (\\(1 \\leq t &lt; T\\))\n\n\\(\\alpha_j(t+1) = \\sum\\limits_{i=1}^n \\alpha_i(t) a_{ij} b_j(o_{t+1})\\)\n\n\\(a_{ij}\\): transition probability from \\(i\\) to \\(j\\)\n\n\nTermination: sum over the last column (\\(t=T\\))\n\n\\(P(O|\\theta) = \\sum\\limits_{i=1}^n \\alpha_T(i)\\)\n\n\nIt is computed left to right and top to bottom\nTime complexity: \\(O(2n^2T)\\)\n\nAt each time step, need to compare states to all other states \\(n^2\\)\nBetter compared to the naive approach \\(O(2Tn^T)\\)\n\n\n\n\n\n\n\n\nTraining data: Set of observations \\(O\\) and set of state sequences \\(Q\\)\nFind parameters \\(\\theta = (\\pi, T, B)\\)\nPopular libraries in Python:\n\nhmmlearn\npomegranate\n\n\n\n\n\n\nGiven an observation sequence \\(O\\) and model \\(\\theta = (\\pi, T, B)\\), how do we choose the best state sequence \\(Q\\)?\nFind \\(Q^* = \\arg\\max_Q P(O,Q|\\theta)\\)\n\n\n\n\n\n\n\n\n\n\nForward Procedure\nViterbi Algorithm\n\n\n\n\nPurpose\nComputes the probability of observing a given sequence of emissions, given the model parameters.\nFinds the most likely sequence of hidden states that explains the observed sequence of emissions, given the model parameters.\n\n\nComputation\nComputes forward probabilities, which are the probabilities of being in a particular state at each time step given the observed sequence.\nComputes the most likely sequence of hidden states.\n\n\nProbability Calculation\nSum over all possible paths through the hidden states.\nRecursively calculates the probabilities of the most likely path up to each state at each time step.\n\n\nObjective\nComputes the likelihood of observing a given sequence of emissions.\nFinds the most probable sequence of hidden states that explains the observed sequence of emissions.\n\n\n\n\nBoth are dynamic programming algorithms with time complexity \\(O(n^2T)\\)\nViterbi Overview:\n\nStore \\(\\delta\\) and \\(\\psi\\) at each node in the trellis\n\n\\(\\delta_i(t)\\) is the max probability of the most likely path ending in trellis node at state \\(i\\) at time \\(t\\)\n\\(\\psi_i(t)\\) is the best possible previous state at time \\(t-1\\) that leads to state \\(i\\) at time \\(t\\)\n\n\n\n\n\n\n\n\\(\\delta_i(0) = \\pi_i b_i(O_0)\\)\n\nrecall \\(b_i(O_0)\\) is the emission probability and \\(\\pi_i\\) is the initial state probability\n\n\\(\\psi_i(0) = 0\\)\n\n\n\n\n\nBest path \\(\\delta_j(t)\\) to state \\(j\\) at time \\(t\\) depends on each previous state and their transition to state \\(j\\)\n\\(\\delta_j(t) = \\max\\limits_i \\{\\delta_i(t-1)a_{ij}\\} b_j(o_t)\\)\n\n\\(b_j(o_t)\\) is the emission probability of observation \\(o_t\\) given state \\(j\\)\n\n\\(\\psi_j(t) = \\arg \\max\\limits_i \\{\\delta_i(t-1)a_{ij}\\}\\)\n\n\n\n\n\nChoose the best final state\n\n\\(q_t^* = \\arg\\max\\limits_i \\delta_i(T)\\)\n\nRecursively choose the best previous state\n\n\\(q_{t-1}^* = \\psi_{q_t^*}(T)\\)\n\n\n\n\n\n\n\nWe do not always have mapping from observations to states (emission probabilities \\(B\\))\nGiven an observation sequence \\(O\\) but not the state sequence \\(Q\\), how do we choose the best parameters \\(\\theta = (\\pi, T, B)\\)?\nUse forward-backward algorithm\n\n\n\n\nReverse of the forward procedure right to left but still top to bottom\nFind the probability of observing the rest of the sequence given the current state\n\n\\(\\beta_j(t) = P(o_{t+1}, o_{t+2}, \\dots, o_T)\\)\n\n\n\n\n\n\nInitialization: set all values at last time step to 1\n\n\\(\\beta_j(T) = 1\\)\n\nInduction: compute the rest of the columns (\\(1 \\leq t &lt; T\\))\n\n\\(\\beta_i(t) = \\sum_{j=1}^N a_{ij}b_{j}(o_{t+1}) \\beta_j(t+1)\\)\n\nConclusion: sum over the first column\n\n\\(P(O|\\theta) = \\sum_{i=1}^N \\pi_i b_i(o_1) \\beta_i(1)\\)\n\n\n\n\n\n\n\nGiven observation sequence \\(O\\), no state sequence \\(S\\), how do we choose the “best” parameters \\(\\theta = (\\pi, T, B)\\)?\nWant \\(\\theta\\) that maximizes \\(P(O|\\theta)\\)\nCannot use MLE because we do not have the state sequence\nUse an unsupervised learning algorithm called Baum-Welch (Expectation-Maximization)\n\n\n\n\nInitialize \\(\\theta = (\\pi, T, B)\\) (guess) then iteratively update them\nCombines the forward (get \\(alpha\\)) and backward (get \\(beta\\)) procedures\n\\(\\alpha\\) and \\(\\beta\\) are combined to represent the probability of an entire observation sequence\nDefine \\(\\gamma\\) and \\(\\xi\\) to update the parameters\n\n\\(\\gamma_i(t)\\): probability of being in state \\(i\\) at time \\(t\\) given entire observation sequence \\(O\\)\n\\(\\xi_{ij}(t)\\): probability of transitioning from state \\(i\\) to state \\(j\\) at time \\(t\\) to \\(t+1\\) given entire observation sequence \\(O\\) regarless of previous and future states\n\nThese probabilities are used to compute the expected:\n\nNumber of times in state \\(i\\)\nNumber of transitions from state \\(i\\) to state \\(j\\)\n\nRepeat until convergence"
  },
  {
    "objectID": "block_6/575_adv_ml/575_adv_ml.html#topic-modeling",
    "href": "block_6/575_adv_ml/575_adv_ml.html#topic-modeling",
    "title": "Advanced Machine Learning (Large Language Models)",
    "section": "",
    "text": "Motivation:\n\nHumans are good at identifying topics in documents.\nBut, it is difficult to do this at scale. (e.g., 1000s of documents)\n\n\n\n\n\nCommon to use unsupervised learning techniques\n\nGiven hyperparameter \\(K\\), we want to find \\(K\\) topics.\n\nIn unsupervised, a common model:\n\nInput:\n\n\\(D\\) documents\n\\(K\\) topics\n\nOutput:\n\nTopic-word association: for each topic, what words describe that topic?\nDocument-topic association: for each document, what topics are in that document?\n\n\nCommon approaches:\n\nLatent Semantic Analysis (LSA)\nLatent Dirichlet Allocation (LDA)\n\n\n\n\n\n\nSingular Value Decomposition (SVD) of the term-document matrix. See LSA notes from 563.\n\n\\[X_{n \\times d} \\approx Z_{n \\times k}W_{k \\times d}\\]\n\n\\(n\\): number of documents, \\(d\\): number of words, \\(k\\): number of topics\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nlsa_pipe = make_pipeline(\n    CountVectorizer(stop_words=\"english\"), TruncatedSVD(n_components=3)\n)\n\nZ = lsa_pipe.fit_transform(toy_df[\"text\"]);\n\n\n\n\nBayesian, generative, and unsupervised model\nDeveloped by David Blei and colleagues in 2003\n\nOne of the most cited papers in computer science\n\nDocument-topic distribution or topic proportions \\(\\theta\\):\n\nEach document is considered a mixture of topics\n\nTopic-word distribution:\n\nEach topic is considered a mixture of words\nThis is from all documents\n\n\n\n\n\nSet the number of topics \\(K\\)\nRandomly assign each word in each document to a topic\nFor each document \\(d\\):\n\nChoose a distribution over topics \\(\\theta\\) from a Dirichlet prior\n\nUse dirichlet distribution because it is conjugate priot (same form as posterior)\n\nFor each word in the document:\n\nChoose a topic from the document’s topic distribution \\(\\theta\\)\nChoose a word from the topic’s word distribution \n\n\n\n\nFit using Bayesian inference (most commonly MCMC)\n\n\n\n\n\nA Markov Chain Monte Carlo (MCMC) method\nVery accurate, but slow (alternative is Variational Inference, which is faster but less accurate)\nUsed to approximate the posterior distribution for document-topic and topic-word distributions\nMain steps:\n\nInitialization: Randomly assign each word in each document to a topic \nUpdate topic-word assignments:\n\nDecrease count of current word in both topic and document distributions\nReassign word to a new topic based on probabilities  \n\nIterate until convergence \n\n\n\n\n\n\n\n3 Main components:\n\nPreprocess corpus\nTrain LDA (use sklearn or gensim)\nInterpret results\n\n\n\n\n\nCrucial to preprocess text data before training LDA\nNeed tokenization, lowercasing, removing punctuation, stopwords\nOptionally, lemmatization or POS tagging\n\nimport spacy\n\nnlp = spacy.load(\"en_core_web_md\")\n\ndef preprocess_spacy(\n    doc,\n    min_token_len=2,\n    irrelevant_pos=[\"ADV\", \"PRON\", \"CCONJ\", \"PUNCT\", \"PART\", \"DET\", \"ADP\"],\n):\n  \"\"\"\n  Preprocess a document using spaCy\n  [Tokenize, remove stopwords, minimum token length, irrelevant POS tags, lemmatization]\n  \"\"\"\n    clean_text = []\n\n    for token in doc:\n        if (\n            token.is_stop == False  # Check if it's not a stopword\n            and len(token) &gt; min_token_len  # Check if the word meets minimum threshold\n            and token.pos_ not in irrelevant_pos\n        ):  # Check if the POS is in the acceptable POS tags\n            lemma = token.lemma_  # Take the lemma of the word\n            clean_text.append(lemma.lower())\n    return \" \".join(clean_text)\n\nwiki_df = [preprocess_spacy(doc) for doc in nlp.pipe(wiki_df[\"text\"])]\n\n\n\n\nWith sklearn:\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nvectorizer = CountVectorizer()\ndtm = vectorizer.fit_transform(wiki_df[\"text_pp\"])\n\nn_topics = 3\nlda = LatentDirichletAllocation(\n    n_components=n_topics, learning_method=\"batch\", max_iter=10, random_state=0\n)\ndocument_topics = lda.fit_transform(dtm)\n\n# Get the topic-word distribution\nsorting = np.argsort(lda.components_, axis=1)[:, ::-1]\nfeature_names = np.array(vectorizer.get_feature_names_out())\n\nWith gensim:\n\nimport gensim\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaModel\n\ncorpus = [doc.split() for doc in wiki_df[\"text_pp\"].tolist()]\ndictionary = Dictionary(corpus) # Create a vocabulary for the lda model\n\n# Get document-term matrix\ndoc_term_matrix = [dictionary.doc2bow(doc) for doc in corpus]\n\n# Train LDA model\nnum_topics = 3\n\nlda = LdaModel(\n    corpus=doc_term_matrix,\n    id2word=dictionary,\n    num_topics=num_topics,\n    random_state=42,\n    passes=10,\n)\n\nMain hyperparameters read more about them in the documentation\n\nnum_topics/ K: number of topics\nalpha: Prior on document-topic distribution\n\nHigh alpha: documents are likely to be a mixture of many topics\nLow alpha: documents are likely to be a mixture of few topics\n\neta: Prior on topic-word distribution\n\nHigh eta: topics are likely to be a mixture of many words\nLow eta: topics are likely to be a mixture of few words\n\n\n\n\n\n\n\nSince this is unsupervised, we need to interpret the topics ourselves\nIdea is to tell a story to humans and what we should care about and evaluate\nCommon methods:\n\nLook at the top words in each topic and make judgements\n\nWord Intrusion: Add a random word to the top words and see if it is noticed\n\nExtrinsic evaluation: Evaluate whether topic nodel with current hyperparameters improves the results of task or not\nQuantify topic interpretability with metrics like Coherence Score\n\nUse Gensim’s CoherenceModel to calculate coherence score\nScore is between -1 and 1, higher is better"
  },
  {
    "objectID": "block_6/575_adv_ml/575_adv_ml.html#recurrent-neural-networks-rnns",
    "href": "block_6/575_adv_ml/575_adv_ml.html#recurrent-neural-networks-rnns",
    "title": "Advanced Machine Learning (Large Language Models)",
    "section": "",
    "text": "Recall when modelling sequences:\n\nOrder matters\nSequence length can vary\nNeed to capture long-term dependencies\n\nProblem with Markov models:\n\nOnly capture short-term dependencies\nSparsity problem: if there are a lot of states, the transition matrix will be very sparse\nAlso need large memory to store the n-grams\nMM do not scale well\n\nTo get closer to the ideal language model (closer to ChatGPT), here we will learn neural sequencing models.\nProblem with Feedforward Neural Networks:\n\nLose temporal information\nAll connects are fully connected and flow forward (no loops)\n\n\n\n\n\n\nRNNs are a type of neural network that can model sequences\n\nSimilar to NN, it is supervised learning\n\nSolves the limited memory problem of Markov models\n\nMemory only scales with number of words \\(O(n)\\)\n\nThey use recurrent connections to maintain a state over time.  source: Stanford CS224d slides\nConnect the hidden layer to itself\n\n# Pseudocode for RNN\nrnn = RNN()\nff = FeedForwardNN()\n\nhidden_state = [0, 0, 0] # depenpent on the number of hidden units\nfor word in input:\n    output, hidden_state = rnn(word, hidden_state)\n\nprediction = ff(hidden_state)\n\nsource: Video on RNN\n\nThe states above are hidden layers in each time step\n\nSimilar to HMMs, but state is continuous, high dimensional, and much richer\n\nEach state contains information about the whole past sequence\nNot that different from feedforward NNs\n\nStill does forward calculation\nJust have new set of weights \\(U\\) that connect previous hidden state to current hidden state\nThese weights are also trained via backpropagation\n\n\n\n\n\n\nThere are 3 weight matrices in RNNs:\n\n\\(W\\): input -&gt; hidden\n\nsize: \\(d_{\\text{input}} \\times d_{\\text{hidden}}\\)\n\n\\(V\\): hidden -&gt; output\n\nsize: \\(d_{\\text{hidden}} \\times d_{\\text{output}}\\)\n\n\\(U\\): hidden -&gt; hidden\n\nsize: \\(d_{\\text{hidden}} \\times d_{\\text{hidden}}\\)\n\n\nImportant point: All weights between time steps are shared\n\nAllows model to learn patterns that are independent of their position in the sequence\n\n\n\n\n\n\n\nComputing new state \\(h_t\\):\n\n\\(h_t = g(Uh_{t-1} + Wx_t + b_1)\\)\n\n\\(g()\\): activation function\n\\(x_t\\): input at time \\(t\\)\n\\(b_1\\): bias\n\n\nComputing output \\(y_t\\):\n\n\\(y_t = \\text{softmax}(Vh_t + b_2)\\)\n\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import RNN\n\nINPUT_SIZE = 20\nHIDDEN_SIZE = 10\nNUM_LAYERS = 1 # number of hidden layers\n\nrnn = nn.RNN(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS)\n\nh0 = torch.randn(NUM_LAYERS, HIDDEN_SIZE) # initial hidden state\n\n# PyTorch gives output and hidden state\noutput, hn = rnn(input, h0)\nSimple Sentiment RNN Class:\nclass SentimentRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n\n        super(SentimentRNN, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, text):\n        embedded = self.embedding(text)\n        output, hidden = self.rnn(embedded)\n        assert torch.equal(output[:, -1, :], hidden.squeeze(0))\n        return self.fc(hidden.squeeze(0))\n\n# Example usage\nmodel = SentimentRNN(vocab_size=vocab_size, embedding_dim=100, hidden_dim=128, output_dim=2)\n\n\n\n\nSince it is supervised, we have: training set, loss function, and backpropagation\nNeed to tailor backpropagation for RNNs\n\nSince hidden layers are connected to themselves, we need to backpropagate through time\nBackpropagation Through Time (BPTT)\n\nUnroll the RNN for a fixed number of time steps\nCalculate the loss at each time step\nSum the losses and backpropagate\nUpdate the weights\n\nSee the code for the code implementation by Andrej Karpathy\n\n\n\n\n\n\nPossible RNN architectures: \n\nSequence Labelling:\n\nE.g. Named Entity Recognition (NER) or Part-of-Speech (POS) tagging\nMany-to-many architecture\nInput are pre-trained word embeddings, outputs are tag probabilities by softmax\n\nSequence Classification:\n\nE.g. Spam detection or sentiment analysis\nSimilar to pseudo-code above, feed result of last hidden layer to a feedforward NN\nMany-to-one architecture\n\nText Generation:\n\nE.g. ChatGPT\nOne-to-many architecture\nInput is a seed, output is a sequence of words\n\nImage captioning:\n\nE.g. Show and Tell\nMany-to-many architecture\nInput is an image, output is a sequence of words\n\n\n\n\n\n\nUse sequence of outputs from one RNN as the sequence of inputs to another RNN\nGenerally outperform single-layer RNNs\nCan learn different level of abstraction in each layer\nNumber of layers is a hyperparameter, remember that higher also means more training cost\n\n\n\n\n\nUse case is in POS tagging it is useful to know words both before and after the current word\nBidirectional RNNs have two hidden layers, one for forward and one for backward\n\nCombines two independent RNNs\n\n\n\n\n\n3 Main problems:\n\nHard to remember relevant information\n\nVanishing gradients because of long sequences\nCase example: The students in the exam where the fire alarm is ringing (are) really stressed.\n\nNeed to retain information that students are plural so use “are”\n\n\nHard to optimize\nHard to parallelize"
  },
  {
    "objectID": "block_6/575_adv_ml/575_adv_ml.html#transformers",
    "href": "block_6/575_adv_ml/575_adv_ml.html#transformers",
    "title": "Advanced Machine Learning (Large Language Models)",
    "section": "",
    "text": "Approach to sequence processing without using RNNs or LSTMs\nIdea: Build up richer and richer contextual representations of words across series of transformer layers\n\nContextual representation: Representation of a word that depends on the context in which it appears\n\nGPT-3: 96 layers of transformers\nBenefits:\n\nParallelization: Can process all words in parallel\nLong-range dependencies: Can learn dependencies between words that are far apart\n\nTwo main components:\n\nSelf-attention mechanism\nPositional embeddings/encodings\n\n\n\n\n\n\nGoal: To look broadly into the context and tells us how to integrate the representations of context words to build representation of a word\nIdea: Compute attention scores between each pair of words in a sentence\n\nAttention score: How much one word should focus on another word\n\nExample: “She plucked the guitar strings , ending with a melancholic note.”\n\nAttention score from note to she is low\nAttention score from note to guitar is high\nAttention score from note to melancholic is very high\n\n\n\n\nThe basic steps of the self-attention mechanism are as follows:\n\nCompare each word to every other word in the sentence (usually by dot product)\nApply softmax to derive a probability distribution over all words\nCompute a weighted sum of all words, where the weights are the probabilities from step 2\n\n\nOperations can be done in parallel\n\n\n\n\n\nQuery \\(W^Q\\): Word whose representation we are trying to compute (current focus of attention)\nKey \\(W^K\\): Word that we are comparing the query to\nValue \\(W^V\\): Word that we are trying to compute the representation of (output for the current focus of attention)\nWe can assume all of them have dimension … [TODO]\n\n\n\n\nAll diagrams in this section is made by my friend Ben Chen. Check out his blog on self-attention.\n\nAll inputs \\(a_i\\) are connected to each other to make outputs \\(b_i\\) \nThis is a breakdown of how each input \\(a_i\\) is connected to each output \\(b_i\\) using the query, key, and value \n\nIn the example our query is \\(a_1\\), and our keys are \\(a_2\\), \\(a_3\\), and \\(a_4\\)\n\n\nNote: For LLMs, not all the sequences are connected to each other, only words before the current word are connected to the current word.\n\n\n\nGet the \\(\\alpha\\) values\n\nCan either do a dot product approach (more common) \nOr an additive approach with an activation function (like tanh) \n\nApply softmax to get \\(\\alpha'\\) values\nMultiply \\(\\alpha'\\) values by the matrix product \\(W^V \\cdot A\\) to get the output \\(b_1\\)\n\n\n\n\n\nResult of the dot product can be very large\nThey are scaled before applying softmax\nCommon scaling: \\(score(x_i, x_j) = \\frac{x_i \\cdot x_j}{\\sqrt{d}}\\)\n\n\\(d\\): Dimensionsionality of the query and key vectors\n\n\n\n\n\n\nLet \\(X\\) be matrix of all input \\(x_i\\) vectors (Shape: \\(N \\times d\\))\n\n\\(Q_{N \\times d_k} = X \\cdot W^Q_{d \\times d_k}\\)\n\\(K_{N \\times d_k} = X \\cdot W^K_{d \\times d_k}\\)\n\\(V_{N \\times d_v} = X \\cdot W^V_{d \\times d_v}\\)\n\nWe can then get \\(\\alpha\\) easily by \\(Q \\times K\\) (shape: \\(N \\times N\\))\n\nRecall \\(N\\) is the number of words in the sentence\n\n\nThen to get the \\(\\text{Self Attention}(Q,K,V) = \\text{softmax}(\\frac{Q \\times K}{\\sqrt{d_k}}) \\times V\\)\nBut for LLMs, we only want to look at the words before the current word, so:\n\n\n\n\n\n\nUsing self-attention mechanism, we can learn dependencies between words, but we lose the order of words\nSolution: Add positional embeddings to the input embeddings\n\nPositional embeddings: Embeddings that encode the position of a word in a sentence\n\n\n\n\n\n\nDifferent words in a sentence can relate to each other in different ways simultaneously\n\ne.g. “The cat was scared because it didn’t recognize me in my mask”\n\nSingle attention layer might not be able to capture all these relationships\nTransformer uses multiple attention layers in parallel\n\nEach layer is called a head\nEach head learns different relationships between words\n\n\n\nsource\n\n\n\n\n\nEach Transformer block consists of:\n\nMulti-head self-attention layer\nFeed-forward neural network:\n\n\\(N\\) network\n1 hidden layer (normally higher dimensionality than input), 2 weight matrices\n\nResidual connections\n\nAdd some “skip” connections because improves learning and gives more information to the next layer\n\nLayer normalization\n\nSimilar to StandardScaler, make mean 0 and variance 1\nTo keep values in a certain range\n\n\n\n\\[\n  T^1 = \\text{SelfAttention}(X)\\\\\n  T^2 = X + T^1\\\\\n  T^3 = \\text{LayerNorm}(T^2)\\\\\n  T^4 = \\text{FFN}(T^3)\\\\\n  T^5 = T^4 + T^3\\\\\n  H = \\text{LayerNorm}(T^5)\n\\]\n\nInput and Output dimensions are matched so they can be “stacked”\n\n\n\n\n\nTake output of \\(h_N\\) and get logit vector of shape \\(1 \\times V\\) where \\(V\\) is the vocabulary size\n\n\\(h_N\\) -&gt; unembedding layer -&gt; logit vector -&gt; softmax -&gt; probability distribution\n\nThis probability distribution is used to predict the next word\nThis is a specific example of a decoder in a transformer"
  },
  {
    "objectID": "block_6/575_adv_ml/575_adv_ml.html#types-of-transformers",
    "href": "block_6/575_adv_ml/575_adv_ml.html#types-of-transformers",
    "title": "Advanced Machine Learning (Large Language Models)",
    "section": "",
    "text": "Decoder-only Transformer\n\nExample: GPT-3\nTraining: Trained on unsupervised data\nUse case: Language modeling, text generation\n\nEncoder-only Transformer\n\nExample: BERT\nTraining: Trained on supervised data\nUse case: Text classification, question answering, sentiment analysis\n\nEncoder-Decoder Transformer\n\nExample: T5, BART\nTraining: Trained on supervised data\nUse case: Machine translation, summarization\n\n\n\n\n\nTraining: Segment corpus of text into input-output pairs\nTo predict the next word, given input words\nSelf-attention only sees words before the current word\n\nUse a causal mask to prevent the model from looking at future words\n\n\n\n\n\nOnce trained, can generate text autoregressively\n\nIncrementally generating words by sampling the next word based on previous choices\nSampling part is similar to generation with Markov models (but with more context and long-range dependencies)\n\n\n\n\n\n\n\nMainly designed for a wide range of NLP tasks (e.g., text classification)\nIt has bidirectional self-attention\n\nCan look at all words in a sentence\nCan learn dependencies between words in both directions \n\nTraining:\n\n“fill in the blank” tasks/ cloze tasks\n\nModel predicts the probability of missing words in a sentence, use cross-entropy loss\n\nMask tokens and learn to recover them\nContextual embeddings: representations created by masked language models\n\nDifferent to single vector embeddings from word2vec\nEach word has a different vector depending on the context\n\n\nTransfer learning through fine-tuning:\n\nGPT and BERT models are pre-trained on large corpora (very general)\nCan create interfaces from these models to downstream tasks\nEither freeze training or make minor adjustments to the model\n\n\n\n\n\n\nFor machine translation, summarization, etc.\nHigh level architecture:\n\nEncoder: Takes input text and creates a representation\n\nSimilar transformer blocks as in the encoder-only transformer\n\nDecoder: Takes the representation and generates the output text\n\nMore powerful block with extra cross-attention layer that can attend to all encoder words\n\nAttention mechanism: Helps the decoder focus on different parts of the input text\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nDecoder-only (e.g., GPT-3)\nEncoder-only (e.g., BERT, RoBERTa)\nEncoder-decoder (e.g., T5, BART)\n\n\n\n\nContextual Embedding Direction\nUnidirectional\nBidirectional\nBidirectional\n\n\nOutput Computation Based on\nInformation earlier in the context\nEntire context (bidirectional)\nEncoded input context\n\n\nText Generation\nCan naturally generate text completion\nCannot directly generate text\nCan generate outputs naturally\n\n\nExample\nMDS Cohort 8 is the ___\nMDS Cohort 8 is the best! → positive\nInput: Translate to Mandarin: MDS Cohort 8 is the best! Output: MDS 第八期是最棒的!\n\n\nUsage\nRecursive prediction over the sequence\nUsed for classification tasks, sequence labeling taks and many other tasks\nUsed for tasks requiring transformations of input (e.g., translation, summarization)\n\n\nTextual Context Embeddings\nProduces unidirectional contextual embeddings and token distributions\nCompute bidirectional contextual embeddings\nCompute bidirectional contextual embeddings in the encoder part and unidirectional embeddings in the decoder part\n\n\nSequence Processing\nGiven a prompt \\(X_{1:i}\\), produces embeddings for \\(X_{i+1}\\) to \\(X_{L}\\)\nContextual embeddings are used for analysis, not sequential generation\nEncode input sequence, then decode to output sequence"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Made by Nando (@farrandi)\nThis is a list of notes from my classes in the Master of Data Science program (2023/2024) at the University of British Columbia.\nI hope you find them useful!"
  },
  {
    "objectID": "block_5/553_stat_inf_2/553_stat_inf_2.html",
    "href": "block_5/553_stat_inf_2/553_stat_inf_2.html",
    "title": "Statistical Inference II",
    "section": "",
    "text": "Recall DSCI 552: Statistical Interference I and Distribution Cheatsheet\n\nUse Observed Data (from random sample) to make Inferences about Population Parameters\n\ne.g. \\(\\mu\\), \\(\\sigma^2\\), median, etc.\nFind Point Estimates and Confidence Intervals for these parameters.\n\nLatent Population vs. Observable Variables\n\nLatent Population is the population that we are interested in, but we can’t observe it directly.\nObservable Variables are the variables that we can observe directly.\ne.g. Online ad click data to estimate the total lifetime revenue.\n\n\n\n\n\n\nVery flexible\n\nCan handle: missing data, complex models, non-iid, etc.\n\nValid inference for any (finite) amount of data\nThe population parameters are treated as random variables\n\nEasy to interpret uncertainty of the population parameters\n\n\n\n\\[\\text{Posterier} \\propto \\text{Likelihood} \\times \\text{Prior}\\]\n\nPrior: What we know about the parameter before we see the data (prior knowledge)\nLikelihood: How the data is generated given the parameter\nPosterior: What we know about the parameter after we see the data.\n\nGood for prediction, inference, and decision making.\n\n\n\n\nRecursive Updating: As we get more data, we can update our prior to get a new posterior.\n\n\n\n\n\nA simplified mathematical model for some reality (For both Frequentist and Bayesian)\nGenerative because it can make synthetic data\nExamples:\n\nWe can incorporate noise in measurements (e.g., outputs coming from the model).\nThey can be overly simplified models with incomplete measurements (e.g., rainy day model).\nThey can even incorporate unobservable latent variables (e.g., hypothetical tennis rankings).\n\n\n\n\n\n\nStan is a probabilistic programming language for Bayesian inference\nrstan is an R interface to Stan\nList of common distributions in stan:\n\nbernoulli(theta)\nbernoulli_logit(alpha)\nbinomial(n, theta)\nbeta_binomial(n, alpha, beta)\npoisson(lambda)\nneg_binomial(alpha, beta)\ngamma_poisson(lambda, alpha)\n\n\n\n\n\nCode the generative model in Stan\nSpecify observed values of data to estimate using rstan\nGenerate synthetic data from the model\nPerform inference on the synthetic data\n\nonly data generated from the model is used for inference\n\n\nNote: Generative model is all you need (and get).\n\n\n\n\n\n\n\n\n\n\n\nLikelihood\nProbability\n\n\n\n\nhow plausible a given distributional parameter is given some observed data\nchance that some outcome of interest will happen for a particular random variable\n\n\n\\(P(\\theta \\| X = x)\\)\n\\(P(X = x \\| \\theta)\\)\n\n\nbounded to 0 and 1\nunbounded to 0 and 1\n\n\n\n\n\n\n\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\n\n\\(P(A^c|B) = 1 - P(A|B)\\)\nIF \\(A\\) and \\(B\\) are independent, then \\(P(A|B) = P(A)\\)\n\nSo \\(P(B|A) = P(B)\\)\n\n\n\n\n\nLet \\(\\theta\\) be a parameter of interest and \\(Y\\) be the observed data.\n\nPrior: \\(P(\\theta)\\)\n\n\\(P(\\theta^c) = 1 - P(\\theta)\\)\n\nLikelihood of the data given the parameter:\n\n\\(\\ell(\\theta|Y) = P(Y|\\theta)\\)\n\nPosterior (what we want to find): \\(P(\\theta|Y)\\)\n\n\\[P(\\theta|Y) = \\frac{P(Y|\\theta)P(\\theta)}{P(Y)}\\]\n\\[\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{normalization constant}}\\]\n\\[\\text{posterior} \\propto \\text{prior} \\times \\text{likelihood}\\]\n\nOnce we have the posterior, we have everything we need to make decisions.\n\n\n\n\nProperties:\n\nHidden variables of interest are random (prior distribution)\nUse posterior (conditional distribution of hidden variables given observation) to capture uncertainty\n\nE.g. posterior: \\(P(A|B) = 0.3\\),\n\nThere is a 30% chance of \\(A\\) if \\(B\\) is true.\n\n\n\n\n\n\n\nMAP is a Bayesian approach to MLE\n\n\n\n\n\n\n\n\nMLE\nMAP\n\n\n\n\nFinding value that maximizes likelihood\nFinding value that maximizes posterior\n\n\nOnly uses observed data\nUses observed data and prior\n\n\n\\(\\hat{\\theta}_{\\text{MLE}} = \\text{argmax}_{\\theta} P(D\\|\\theta)\\)\n\\(\\hat{\\theta}_{\\text{MAP}} = \\text{argmax}_{\\theta} P(\\theta\\|D)\\)\n\n\n\n\n\n\n\nBig advantage:\n\nIt formulates every problem in one common framework\n\nFinal goal: Take samples from the posterior distribution\nComputer does most of the heavy lifting\n\nAll we need to do is good model design and critical analysis of the results\n\nCharacteristics:\n\n\n\n\nQuestion: Pose a scientific question\nDesign: Formulate variables and create a probabilistic model for them. Prior knowledge is included here!\nInference: Get posterior samples from the model\nCheck: If the samples are from your posterior\nAnalyze: Use the samples to answer the question\n\n\n\n\n\nInferential: Using observed data \\(Y\\) to make inferences about the population/ latent variable \\(\\theta\\)\nPredictive: Using observed data \\(Y\\) to make predictions about future data \\(Y'\\)\n\n\n\n\n\n\nOne of the most foundational Bayesian models\nRecall \\(Posterior \\propto Likelihood \\times Prior\\)\n\nBinomial: The likelihood function\n\n\\(Y | \\pi \\sim Binomial(n, \\pi) \\text{ where } \\pi \\in [0,1]\\)\n\nBayesian thinking: \\(Y\\) is a random variable (population parameters are no longer fixed)\nBeta: Prior distribution of parameter of interest \\(\\pi\\)\n\n\\(\\pi \\sim Beta(a, b)\\)\n\n\n\n\n\n\\[\\pi \\sim Beta(a, b)\\]\n\nPDF: \\(f(\\pi) = \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)} \\pi^{a - 1} (1 - \\pi)^{b - 1} \\text{  for  } 0 \\leq \\pi \\leq 1\\)\nMean: \\(\\frac{a}{a + b}\\)\nVariance: \\(\\frac{ab}{(a + b)^2(a + b + 1)}\\)\nMode: \\(\\frac{a - 1}{a + b - 2} \\text{ when } a, b &gt; 1\\)\n\n\n\n\n\nOne of the biggest challenges in Bayesian statistics\nNeed to rely on subject matter prior knowledge\ne.g. Collect information from previous studies and plot a histogram of the data, then fit a beta distribution to it\nbayesrule package in R has a function summarize_beta_binomial(a, b) to summarize the beta distribution\nPDF of binomial distribution:\n\n\\(f(y | \\pi) = \\binom{n}{y} \\pi^y (1 - \\pi)^{n - y}\\)\n\n\n\n\n\n\\[ Posterior \\propto Likelihood \\times Prior \\]\n\\[ f(\\pi | Y) \\propto f(Y | \\pi) \\times f(\\pi) \\]\nusing the beta-binomial model:\n\\[ f(\\pi | Y) \\propto \\binom{n}{y} \\pi^y (1 - \\pi)^{n - y} \\times \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)} \\pi^{a - 1} (1 - \\pi)^{b - 1}\\]\nSimplify (remove non-\\(\\pi\\) terms):\n\\[ f(\\pi | Y) \\propto \\pi^{y + a - 1} (1 - \\pi)^{n - y + b - 1} \\]\nWe recognize this as the kernel of a beta distribution:\n\\[ f(\\pi | Y) \\propto Beta(a + y, b + n - y) \\]\n\nKernel: The part of the expression that depends on the variable of interest\n\n\n\n\n\nPosterior: \\(f(\\pi | Y) = Beta(a + y, b + n - y)\\)\n\nMean: \\(\\frac{a + y}{a + b + n}\\)\nVariance: \\(\\frac{(a + y)(b + n - y)}{(a + b + n)^2(a + b + n + 1)}\\)\nMode: \\(\\frac{a + y - 1}{a + b + n - 2}\\)\n\nMode is the value of \\(\\pi\\) that maximizes the posterior distribution/ peak (MAP/ Maximum A Posteriori)\n\n\nCan also use summarize_beta_binomial(a, b, n, y) to summarize the posterior distribution\n\na and b are the parameters of the prior beta distribution\nn is the number of trials\ny is the number of successes\n\nCan also use plot_beta_binomial(a, b, n, y) to plot the prior and posterior distributions (also from bayesrule package)\n\n\n\n\nCredible Interval: Range of plausible values for the parameter.\n\nWidth: measures variability of the posterior distribution\n\nUse function qbeta in R to calculate the quantiles of the beta distribution\nFor a given a,b,n,y, the 95% credible interval is qbeta(c(0.025, 0.975), shape1 = a + y, shape2 = b + n - y)\n95% CI means:\n\nThere is a 95% posterior probability that the true value of \\(\\pi\\) is between \\(L\\) and \\(U\\)\n\n\n\n\n\n\n\n\nBayesian model is a big joint probability distribution \\(P(Y, Y', \\theta)\\)\n\nObservations: \\(Y\\)\nLatent variables: \\(\\theta\\)\nPredictions: \\(Y'\\)\n\nDid inferential approach in Beta-Binomial model (previous section). Now another approach:\n\n\\[P(Y, Y', \\theta) = P(Y, Y' | \\theta) \\times P(\\theta)\\]\n\nGenerate \\(\\theta\\) from the prior \\(P(\\theta)\\)\nGenerate \\(Y, Y'\\) given \\(\\theta\\) from likelihood \\(P(Y, Y' | \\theta)\\)\n\n\nConstructing the Likelihood:\n\nFormulate the data type (int, real, categorical, etc.) and support (positive, negative, etc.)\nFigure out which variables are fixed covariates and which are random variables\nPick a distribution that match the type and support. The distribution will have some unknown parameters - need a prior for these parameters\n\n\n\n\n\n\nGoal: Generate samples from the posterior distribution\nProblem: The posterior is often intractable (can’t be solved analytically)\nSolution: Use MCMC to generate samples from the posterior\nMonte Carlo Algorithm\n\nNeed closed analytical form of the posterior \\(f(\\theta | Y)\\) (e.g. Beta-Binomial model or Gamma-Poisson model)\nBuild independent MC sample \\(\\{\\Theta_1, \\Theta_2, \\ldots, \\Theta_n\\}\\) from \\(f(\\Theta | Y)\\) by:\n\nDrawing \\(\\Theta_i\\) from \\(f(\\Theta | Y)\\)\nGo there\n\n\nIs a random walk in the space of \\(\\theta\\)\nCalled a Markov Chain because the next state depends only on the current state \\(\\theta^{(t)} \\rightarrow \\theta^{(t+1)}\\)\n\n\n\n\nAllows us to obtain an approximation of the posterior distribution \\(f(\\Theta | Y)\\) via MC \\(\\{\\Theta_1, \\Theta_2, \\ldots, \\Theta_n\\}\\).\nNext \\(\\Theta_{t+1}\\) is selected by:\n\nProposing a new value \\(\\Theta'\\) from a proposal distribution \\(q(\\Theta' | \\Theta_t)\\) (e.g. Uniform, Normal, etc.)\nDecide whether to accept or reject \\(\\Theta'\\) based on acceptance probability \\(\\alpha\\): \\[\\alpha = min\\left(1, \\frac{f(\\Theta') \\ell(\\Theta' | Y)}{f(\\Theta_t) \\ell(\\Theta_t| Y)}\\right)\\]\n\nThen obtain the next via bernoulli trial with probability \\(\\alpha\\) for success \\(\\Theta^{(t+1)} = \\Theta'\\)\n\n\n\n\n\n\n\nSome considerations:\n\nWarm-up: Discard the first \\(n\\) samples to allow the chain to converge\nThinning: Only keep every \\(n\\)th sample to reduce autocorrelation\n\nskip the first \\(n\\) samples and then keep every \\(n\\)th sample\n\n\n\n\\[\\text{Num of approx posterior samples} = \\frac{\\text{iter} - \\text{warmup}}{\\text{thin}}\\]\n\n\n\n\nPrior: \\(\\lambda \\sim Gamma(s, r)\\)\nLikelihood: \\(Y_i | \\lambda \\sim Poisson(\\lambda)\\)\n\n\\[\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}\\]\n\\[f(\\lambda | Y) \\propto \\ell(\\lambda | Y) \\times f(\\lambda)\\]\n\nPosterior: \\(\\lambda | Y \\sim Gamma(s + \\sum Y_i, r + n)\\) \nOur prior: \\(\\lambda \\sim Gamma(s=150, r=40)\\)\n\n{stan output.var='gamma_poisson_stan'}\ndata {\n  int&lt;lower=1&gt; n; // number of rows in training data\n  int&lt;lower=0&gt; count[n]; // array of observed counts (integer)\n  real&lt;lower=0&gt; s; // prior shape Gamma parameter\n  real&lt;lower=0&gt; r; // prior rate Gamma parameter\n\n}\n\nparameters {\n  real&lt;lower=0&gt; lambda; // parameter of interest\n}\n\nmodel {\n  lambda ~ gamma(s,r); // prior distribution of lambda\n  count ~ poisson(lambda); // Poisson likelihood, can be complex formula too\n}\nbird_dictionary &lt;- list(\n  n = nrow(observed_evidence),\n  count = as.integer(observed_evidence$count),\n  s = 150,\n  r = 40\n)\n\nposterior_lambda &lt;- sampling(\n  object = gamma_poisson_stan,\n  data = bird_dictionary,\n  chains = 1,\n  iter = 10000,\n  warmup = 1000,\n  thin = 5,\n  seed = 553,\n  algorithm = \"NUTS\"\n)\n\nposterior_lambda &lt;- as.data.frame(posterior_lambda)\nlibrary(ggplot2)\nlibrary(bayesplot)\n\n# Plot the prior, likelihood, and posterior\nplot &lt;- plot_gamma_poisson(\n    shape = 150, rate = 40,\n    sum_y = sum(observed_evidence$count),\n    n = nrow(observed_evidence),\n)\n\n# Plot the posterior distribution from stan\nposterior_plot &lt;- posterior_lambda %&gt;%\n  ggplot(aes(x = lambda)) +\n  geom_histogram(aes(y = after_stat(count)),\n                 bins = 30,\n                 fill = \"lightblue\",\n                 color = \"black\",\n                 alpha = 0.5)\n((See lab 2 for more examples))\n\n\n\n\n\nComparable to OLS\n\n\n\n\nThis data is from bayesrules package in R.\nWe want to consider the effects of temp_feel (numeric) and weekend (boolean) on rides (count).\n\n\nFor OLS would use Poisson regression since output is count data.\nlm(rides ~ temp_feel + weekend, data = bikeshare)\n\n\n\nFor Bayesian, we need to specify priors for the coefficients and the variance.\n\\[Y_i \\sim \\mathcal{N}(\\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2}, \\sigma^2)\\]\nwhere\n\n\\(X_{i1}\\) is temp_feel\n\\(X_{i2}\\) is weekend: 1 if weekend, 0 if not\n\\(\\beta_0\\) is the intercept ~ \\(Gamma(7.5, 1)\\)\n\\(\\beta_1\\) is the coefficient for temp_feel ~ \\(\\mathcal{N}(0, 1000^2)\\)\n\\(\\beta_2\\) is the coefficient for weekend ~ \\(\\mathcal{N}(0, 1000^2)\\)\n\\(\\sigma^2\\) is the variance ~ \\(IG(0.001, 0.001)\\)\n\nInverse Gamma distribution (popular for variance priors &gt; 0)\n\n\n\n\n\n\nUse stan to sample from the posterior distribution.\n\n{r bikerides_stan}\n\ndata {\n  int&lt;lower=0&gt; n;                               // training sample size\n  vector[n] y;                                  // response vector\n  vector[n] x_1;                                // regressor 1 vector\n  vector[n] x_2;                                // regressor 2 vector\n  real pred_x_1;                                // fixed value for regressor 1\n  real pred_x_2;                                // fixed value for regressor 2\n}\n\nparameters {\n  real&lt;lower=0&gt; beta_0;                         // intercept with lower bound\n  real beta_1;                                  // regression coefficient 1\n  real beta_2;                                  // regression coefficient 2\n  real&lt;lower=0&gt; sigma;                          // common standard deviation with lower bound\n  }\n\nmodel {\n  beta_0 ~ gamma(7.5, 1);                       // alpha = 7.5 and beta = 1\n  beta_1 ~ normal(0, 1000);                     // mu_b1 = 0 and sigma_b1 = 1000\n  beta_2 ~ normal(0, 1000);                     // mu_b2 = 0 and sigma_b2 = 1000\n  sigma ~ inv_gamma(0.001, 0.001);              // eta = 0.001 and lambda = 0.001\n  y ~ normal(beta_0 + beta_1 * x_1 + beta_2 * x_2, sigma);\n}\ngenerated quantities {\n  real y_pred = normal_rng(beta_0 + beta_1 * pred_x_1 + beta_2 * pred_x_2, sigma);\n}\n\ngenerated quantities creates a posterior predictive distribution y_pred which takes into account:\n\nPosterior variablility in the parameters (from joint posterior distribution of the parameters)\nSampling variability in the data (Each prediction should deviate from its posterior prediction) so add random noise\n\nIt executes after obtaining the posterior samples for the parameters.\n\nThen, in R need a dictionary to pass into stan.\nmodel_matrix &lt;- as.data.frame(model.matrix(rides ~ temp_feel + weekend, data = bikeshare_data))\nmodel_matrix\n\nbikerides_dictionary &lt;- list(\n  n = nrow(bikeshare_data),\n  y = bikeshare_data$rides,\n  x_1 = bikeshare_data$temp_feel,\n  x_2 = model_matrix$weekendTRUE,\n  pred_x_1 = 75,\n  pred_x_2 = 1\n)\nNow compile the Stan model:\nposterior_bikeshare &lt;- stan(\n  model_code = bikerides_stan,\n  data = bikerides_dictionary,\n  chains = 1,\n  iter = 40000,\n  warmup = 20000,\n  thin = 60,\n  seed = 553,\n)\n\n# view the posterior summary\nround(summary(posterior_bikeshare)$summary, 2)[-6, c(\"mean\", \"sd\", \"2.5%\", \"97.5%\")]\n\nThe 2.5% and 97.5% quantiles are the 95% credible intervals.\n\nIf the interval contains 0, then the coefficient is not significant.\nIf the interval is large, then the model is not very certain about the coefficient (i.e. model is not capturing the right systematic component).\n\nThere is also 2.5% and 97.5% quantiles for the y_pred which is the posterior predictive distribution for a 95% prediction interval.\n\n\n\n\n\n\n\nUsing a Tinder example where we want to infer the prob of finding a partner if we use Tinder.\n\n\\(X_i \\sim \\text{Bernoulli}(\\pi)\\) for each person \\(i\\)\n\n\\(\\pi\\) is the probability of finding a partner\n\nPrior: \\(\\pi \\sim \\text{Beta}(a, b)\\)\nLikelihood: \\(Y|\\pi \\sim \\text{Binomial}(n, \\pi)\\)\nPosterior: \\(\\pi|y \\sim \\text{Beta}(a', b') = \\text{Beta}(a+y, b+n-y)\\)\n\n\n\n\n\nClaim: In any city like Vancouver, more than 15% of the single people who use the Tinder app will eventually find a partner.\n\nNull Hypothesis: \\(\\pi \\leq 0.15\\)\nAlternative Hypothesis: \\(\\pi &gt; 0.15\\) (associated with the claim)\n\nIn Bayesian, we use the posterior and get probability for each hypothesis (unlike frequentist).\nUse pbeta function to get the probability\n\nIf posterior \\(Beta(a'=24, b'=192)\\) from likelihood of 20 successes out of 200 trials\nThen \\(H_0: P(\\pi \\leq 0.15 | y=20) = \\int_0^{0.15} f(\\pi | y=20) d\\pi\\)\nequal to pbeta(0.15, 24, 192) \n\nIn the Tinder example, we get 2 probabilities:\n\n\\(P(H_0 | y=20) = P(\\pi \\leq 0.15 | y=20) = 0.957\\)\n\\(P(H_a | y=20) = P(\\pi &gt; 0.15 | y=20) = 0.043\\)\n\nCan get Posterior Odds that \\(\\pi &gt; 0.15\\) by dividing the two probabilities.\n\n\\[\\text{Posterior Odds} = \\frac{P(H_a | y=20)}{P(H_0 | y=20)} = \\frac{0.043}{0.957} = 0.045\\]\n\nInterpretation: For \\(y=20\\), \\(\\pi\\) is 22 times(\\(\\frac{1}{0.045}\\)) more likely to be less than or equal to 0.15 compared to being greater than 0.15 using our posterior model.\n\n\n\n\n\nDo the same for prior odds\n\n\\[\\text{Prior Odds} = \\frac{P(H_a)}{P(H_0)}\\]\n\\[\\text{Bayes Factor} = \\frac{\\text{Posterior Odds}}{\\text{Prior Odds}}\\]\n\nBayes Factor = 1: Plausibility of \\(H_a\\) stays the same even after new data\nBayes Factor &gt; 1: Plausibility of \\(H_a\\) increases after new data\nBayes Factor &lt;br 1: Plausibility of \\(H_a\\) decreases after new data\n\n\n\nIf from MCMC, we cannot get analytical solution for Bayes Factor since no exact PDF to integrate.\nCan use bayesfactor package in R to get Bayes Factor.\n\nSolution: Empirical cumulative distribution function (ECDF) of the posterior samples to approximate the posterior distribution. Use ecdf function in R.\n\n\n\n\n\n\nLets say we want to figure out: whether or not 15% of the single people who use the Tinder app will eventually find a partner (in any city like Vancouver)\n\n\\(H_0: \\pi = 0.15\\)\n\\(H_a: \\pi \\neq 0.15\\)\n\n\n\\[P(\\pi = 0.15 | y=20) = \\int_{0.15}^{0.15} f(\\pi | y=20) d\\pi = 0\\]\n\nDoes not work since =0, so add a range of values (e.g. \\(\\pm 0.10\\))\n\n\\(H_0: \\pi \\in [0.05, 0.25]\\)\n\\(H_a: \\pi \\notin [0.05, 0.25]\\)\n\nThen find the credible interval for the posterior distribution of \\(\\pi\\).\n\nqbeta(c(0.025, 0.975), 24, 192)\nThen based on the results:\n\nIf credible interval falls within the range of \\(H_0\\), then we are in favor of \\(H_0\\), with 95% probability.\n\n\n\n\n\n\nComparable to logistic regression\n\n\n\n\nLet \\(Y_i \\in \\{0, 1\\}\\)\nResponse is assumed as: \\[Y_i | \\beta_0 \\beta_1 \\sim \\text{Bernoulli}(\\pi_i) \\]\n\nwith link function:\n\\[log(\\frac{\\pi_i}{1-\\pi_i}) = \\beta_0 + \\beta_1 X_i\\]\nThat is our likelihood.\n\n\n\n\nParameter of interest: \\(\\beta_0, \\beta_1\\)\nLet us assume as follows:\n\n\\(\\beta_0 \\sim N(\\mu=0, \\sigma^2=100^2)\\)\n\\(\\beta_1 \\sim N(\\mu=0, \\sigma^2=100^2)\\)\n\nAssume 0 because we do not know if there is any association, also variace is high to reflect the uncertainty.\n\n\n\n\n\n\n\nNo need warmup and thin as it is only prior.\n\nprior_stan_climate_change &lt;- \"parameters {\nreal beta_0;\nreal beta_1;\n}\nmodel {\nbeta_0 ~ normal(0, 100);\nbeta_1 ~ normal(0, 100);\n}\"\nprior_climate &lt;- stan(\n  model_code = prior_stan_climate_change,\n  chains = 1,\n  iter = 1000,\n  warmup = 0, # Can be 0 since it is only prior\n  thin = 1, # No need to thin as well\n  seed = 553\n)\n\n\n\nposterior_stan_climate_change &lt;- \"data {\nint&lt;lower=0&gt; n;                          // number of observations\nvector[n] income;                        // regressor income\nint&lt;lower=0,upper=1&gt; climate_change[n];  // setting the response variable as binary\n}\nparameters {\nreal beta_0;\nreal beta_1;\n}\nmodel {\nbeta_0 ~ normal(0, 100);\nbeta_1 ~ normal(0, 100);\nclimate_change ~ bernoulli_logit(beta_0 + beta_1 * income);\n}\"\nclimate_dictionary &lt;- list(\n  n = nrow(pulse_training),\n  income = pulse_training$income,\n  climate_change = as.integer(pulse_training$climate_change)\n)\n\nposterior_climate &lt;- stan(\n  model_code = posterior_stan_climate_change,\n  data = climate_dictionary,\n  chains = 1,\n  iter = 21000,\n  warmup = 1000,\n  thin = 20,\n  seed = 553\n)\n\n\n\n\n\n\n\n\n\n\n\nFrequentist BLR\nBayesian BLR\n\n\n\n\nEstimates the MLE (Maximum Likelihood Estimation)\nEstimates the posterior distribution\n\n\nEstimates the standard errors\nEstimates the posterior distribution\n\n\nEstimates the confidence intervals\nEstimates the credible intervals\n\n\n\n\nA big advantage of Bayesian: did not need to derive any Maximum Likelihood steps\nMeaning of Bayesian coefficients:\n\ne.g. estimate of \\(\\beta_1\\) is 0.009, 95% credible interval is (0.005, 0.0134), since 0 is not in the posterior credible interval:\nFor each unit increase in income, a subject is 1.009 times more likely to believe in climate change\nThere is a 95% posterior probability that the true value of \\(\\beta_1\\) lies between 0.005 and 0.0134\n\n\n\n\n\n\n\nComplete Pooled Model: A model that pools all the data together and estimates a single parameter for all the data\ne.g. have a dataset of multiple rocket types and their launchs, and want to estimate the probability of a rocket launch succeeding\n\nOnly 1 \\(\\pi\\) for all the rocket types\n\n\n\\[\n\\text{likelihood:} \\qquad X_i|\\pi \\sim \\text{Binomial}(n_i, \\pi) \\quad \\text{for } i = 1, \\dots, 367\\\\\n\\text{prior:} \\qquad \\pi \\sim \\text{Beta}(a = 1, b = 1).\n\\]\n\nDoes not allow us to infer any probability of an individual group\n\n\n\n\n\nNon-pooled Model: A model that estimates a parameter for each group of data\ne.g. have a dataset of multiple rocket types and their launchs, and want to estimate the probability of a rocket launch succeeding\n\nHave a \\(\\pi_i\\) for each rocket type\n\n\n\\[\n\\text{likelihood:} \\qquad X_i|\\pi_i \\sim \\text{Binomial}(n_i, \\pi_i) \\quad \\text{for } i = 1, \\dots, 367\\\\\n\\text{prior:} \\qquad \\pi_i \\sim \\text{Beta}(a = 1, b = 1).\n\\]\n\nBetter when comparing groups with different sample sizes\n\nUsing MLE will not be reliable when sample sizes are small\ne.g. Binomial distribution with \\(n=1\\)\n\nDrawbacks:\n\nCannot generalize to new groups\nCannot take valuable information from other groups\n\n\n\n\n\n\nHierarchical Bayesian Model: A model that estimates a parameter for each group of data, but also estimates a distribution of parameters for all the groups\nKey feature: Nesting of parameters over multiple levels\n\nCommon to have a variable in the model prior is itself a random variable (needs another prior)\n\nOr a variable in the likelihood is itself a random variable (needs another prior)\n\n\ne.g. have a dataset of multiple rocket types and their launchs, and want to estimate the probability of a rocket launch succeeding\n\nHave a \\(\\pi_i\\) for each rocket type\nAlso have random variables \\(\\pi_i \\sim \\text{Beta}(a, b)\\)\n\n\\(a\\) and \\(b\\) are parameters and no longer hyperparameters\nnew hyperparameters are the priors for \\(a\\) and \\(b\\)\n\n\n\n\\[\n\\text{likelihood:} \\qquad X_i|\\pi_i \\sim \\text{Binomial}(n_i, \\pi_i) \\quad \\text{for } i = 1, \\dots, 367\\\\\n\\text{priors:} \\qquad \\pi_i \\sim \\text{Beta}(a, b) \\\\\n\\quad a \\sim \\text{Gamma}(0.001, 0.001) \\\\\n\\qquad b \\sim \\text{Gamma}(0.001, 0.001).\n\\]\n\ne.g2 dataset of number of freethrows made and attempts by a Basketball player for multiple seasons\n\nGood because can predict free throw percentage for a new season\nHave a \\(n_i\\) as number of attempts that season and \\(\\pi_i\\) is free throw percentage for that season\n\n\\[\n\\text{likelihood:} \\qquad X_i|\\pi_i \\sim \\text{Binomial}(n_i, \\pi_i) \\quad \\text{where i=season number}\\\\\n\\text{prior:} \\qquad \\pi_i \\sim \\text{Beta}(a,b) \\\\\n\\quad a \\sim \\text{Gamma}(0.001, 0.001) \\\\\n\\qquad b \\sim \\text{Gamma}(0.001, 0.001).\n\\]\nBasically combines the best of of the complete pooled model and the non-pooled model:\n\nUse valuable info from all groups to infer the success probability of a specific group\nCan get posterior predictive distribution for new group\n\n\n\n\n\nFor the \\(a\\) and \\(b\\) we need continous and non-negative priors\nHence, we use the Gamma distribution _ this tends to assign small values to \\(a\\) and \\(b\\) which is good for the prior\n\n\n\n\n\nUsing Heirarchical Bayesian, we will get narrower Credible Intervals compared to the non-pooled model\nBorrowing Strength: The estimates will be more precise because we are using information from all the groups\n\nIn the case of rockets, its the \\(a\\) and \\(b\\) that are being shared\n\nHelps learn parameters and reduce posterior variance\n\n\n\n\n\nUse posterior means of \\(a\\) and \\(b\\) to get the posterior means of \\(\\pi_i\\) for all groups (in rocket example)\n\n\n\n\n\nIn Stan, need to add a new block generated quantities to get the posterior predictive distribution for a new group\n\ngenerated quantities {\n  real&lt;lower=0,upper=1&gt; pi_pred = beta_rng(a, b);\n}\n\nThis generates a new \\(\\pi\\) for a new group from the posterior distribution of \\(a\\) and \\(b\\)\nMean of pi_pred (new group) is similar to the posterior mean of \\(\\pi\\) for all groups\n\nbecause it is the best bayesian model we can obtain without further covariates/ features\n\n\n\n\n\n\n\nCommon pitfalls of MCMC:\n\nNot enough iterations\nNot enough thinning\n\nTo make it semi-independent\n\nNot enough burn-in\n\nDiagnostic to check if MCMC work and is giving “good” samples for the approx posterior dist\n\n\n\n\nIllustrate posterior sampling by a chain.\nAll chains are overlaid on top of each other (without warm-up)\nIdeal:\n\nNo trend\nNo chain stuck in a local mode\n\n\nlibrary(bayesplot)\n\ntraceplot &lt;- mcmc_trace(mcmc_object,\n                        pars = c(\"param1\", \"param2\"),\n                        size = 0.1,\n                        facet_args = list(nrow = 3))\n\nGood example of no trend and no chain stuck in a local mode\n\n\n\n\nOverlays the density plot of the parameter of interest with the MCMC posterior samples.\n\nlibrary(bayesplot)\n\ndensityplot &lt;- mcmc_dens_overlay(mcmc_object,\n                         pars = c(\"param1\", \"param2\"),\n                         facet_args = list(nrow = 3))\n\n\n\n\n\n\\(N_{eff}\\): Number of independent samples needed to give an accurate MCMC posterior approximation.\nLet \\(N\\) be the length of the chain, then we expect:\n\n\\(N_{eff} \\leq N\\) is normal and expected\n\\(N_{eff} \\approx N\\) is ideal\n\nCompare with ratio of \\(N_{eff}/N\\):\n\nIf \\(N_{eff}/N &gt; 1\\), Means need more samples to be as good as Monte Carlo simulation\n\nCan increase iter OR increase thinning\n\nIf \\(N_{eff}/N &lt; 1\\), Means need less samples to be as good as Monte Carlo simulation\n\nCan decrease iter OR decrease thinning\n\n\n\nlibrary(bayesplot)\n\neff_sample_size &lt;- neff_ratio(mcmc_object,\n                    pars = c(\"param1\", \"param2\"))\n\nessplot &lt;- mcmc_neff(eff_sample_size, size = 2)\n\n\n\n\n\nMCMC is not entirely independent, so we need to check the autocorrelation of the chain\nWant to see the autocorrelation drop to 0 quickly =&gt; shows semi-independence\n\nIdeally before lag 5\n\n\nlibrary(bayesplot)\n\nacf &lt;- mcmc_acf(mcmc_object,\n                pars = c(\"param1\", \"param2\"))\n\n\n\n\n\nEvaluate within-chain and between-chain variance to check if they are roughly the same\nwarning: Sometimes does not make sense for discrete variables\n\n\\[ \\hat{R} = \\sqrt{ 1 + \\frac{b^2}{s^2} } \\approx 1 \\]\n\n\\(b^2\\): Between-chain variance\n\\(s^2\\): Within-chain variance\n\nlibrary(bayesplot)\n\ngelman_rubin &lt;- mcmc_rhat(mcmc_object, size = 2)"
  },
  {
    "objectID": "block_5/553_stat_inf_2/553_stat_inf_2.html#review-of-frquentist-statistical-inference",
    "href": "block_5/553_stat_inf_2/553_stat_inf_2.html#review-of-frquentist-statistical-inference",
    "title": "Statistical Inference II",
    "section": "",
    "text": "Recall DSCI 552: Statistical Interference I and Distribution Cheatsheet\n\nUse Observed Data (from random sample) to make Inferences about Population Parameters\n\ne.g. \\(\\mu\\), \\(\\sigma^2\\), median, etc.\nFind Point Estimates and Confidence Intervals for these parameters.\n\nLatent Population vs. Observable Variables\n\nLatent Population is the population that we are interested in, but we can’t observe it directly.\nObservable Variables are the variables that we can observe directly.\ne.g. Online ad click data to estimate the total lifetime revenue."
  },
  {
    "objectID": "block_5/553_stat_inf_2/553_stat_inf_2.html#introduction-to-bayesian-statistics",
    "href": "block_5/553_stat_inf_2/553_stat_inf_2.html#introduction-to-bayesian-statistics",
    "title": "Statistical Inference II",
    "section": "",
    "text": "Very flexible\n\nCan handle: missing data, complex models, non-iid, etc.\n\nValid inference for any (finite) amount of data\nThe population parameters are treated as random variables\n\nEasy to interpret uncertainty of the population parameters\n\n\n\n\\[\\text{Posterier} \\propto \\text{Likelihood} \\times \\text{Prior}\\]\n\nPrior: What we know about the parameter before we see the data (prior knowledge)\nLikelihood: How the data is generated given the parameter\nPosterior: What we know about the parameter after we see the data.\n\nGood for prediction, inference, and decision making.\n\n\n\n\nRecursive Updating: As we get more data, we can update our prior to get a new posterior.\n\n\n\n\n\nA simplified mathematical model for some reality (For both Frequentist and Bayesian)\nGenerative because it can make synthetic data\nExamples:\n\nWe can incorporate noise in measurements (e.g., outputs coming from the model).\nThey can be overly simplified models with incomplete measurements (e.g., rainy day model).\nThey can even incorporate unobservable latent variables (e.g., hypothetical tennis rankings).\n\n\n\n\n\n\nStan is a probabilistic programming language for Bayesian inference\nrstan is an R interface to Stan\nList of common distributions in stan:\n\nbernoulli(theta)\nbernoulli_logit(alpha)\nbinomial(n, theta)\nbeta_binomial(n, alpha, beta)\npoisson(lambda)\nneg_binomial(alpha, beta)\ngamma_poisson(lambda, alpha)\n\n\n\n\n\nCode the generative model in Stan\nSpecify observed values of data to estimate using rstan\nGenerate synthetic data from the model\nPerform inference on the synthetic data\n\nonly data generated from the model is used for inference\n\n\nNote: Generative model is all you need (and get).\n\n\n\n\n\n\n\n\n\n\n\nLikelihood\nProbability\n\n\n\n\nhow plausible a given distributional parameter is given some observed data\nchance that some outcome of interest will happen for a particular random variable\n\n\n\\(P(\\theta \\| X = x)\\)\n\\(P(X = x \\| \\theta)\\)\n\n\nbounded to 0 and 1\nunbounded to 0 and 1"
  },
  {
    "objectID": "block_5/553_stat_inf_2/553_stat_inf_2.html#conditional-probability",
    "href": "block_5/553_stat_inf_2/553_stat_inf_2.html#conditional-probability",
    "title": "Statistical Inference II",
    "section": "",
    "text": "\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\n\n\\(P(A^c|B) = 1 - P(A|B)\\)\nIF \\(A\\) and \\(B\\) are independent, then \\(P(A|B) = P(A)\\)\n\nSo \\(P(B|A) = P(B)\\)"
  },
  {
    "objectID": "block_5/553_stat_inf_2/553_stat_inf_2.html#bayes-theorem",
    "href": "block_5/553_stat_inf_2/553_stat_inf_2.html#bayes-theorem",
    "title": "Statistical Inference II",
    "section": "",
    "text": "Let \\(\\theta\\) be a parameter of interest and \\(Y\\) be the observed data.\n\nPrior: \\(P(\\theta)\\)\n\n\\(P(\\theta^c) = 1 - P(\\theta)\\)\n\nLikelihood of the data given the parameter:\n\n\\(\\ell(\\theta|Y) = P(Y|\\theta)\\)\n\nPosterior (what we want to find): \\(P(\\theta|Y)\\)\n\n\\[P(\\theta|Y) = \\frac{P(Y|\\theta)P(\\theta)}{P(Y)}\\]\n\\[\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{normalization constant}}\\]\n\\[\\text{posterior} \\propto \\text{prior} \\times \\text{likelihood}\\]\n\nOnce we have the posterior, we have everything we need to make decisions.\n\n\n\n\nProperties:\n\nHidden variables of interest are random (prior distribution)\nUse posterior (conditional distribution of hidden variables given observation) to capture uncertainty\n\nE.g. posterior: \\(P(A|B) = 0.3\\),\n\nThere is a 30% chance of \\(A\\) if \\(B\\) is true."
  },
  {
    "objectID": "block_5/553_stat_inf_2/553_stat_inf_2.html#maximum-a-posteriori-estimation-map-and-maximum-likelihood-estimation-mle",
    "href": "block_5/553_stat_inf_2/553_stat_inf_2.html#maximum-a-posteriori-estimation-map-and-maximum-likelihood-estimation-mle",
    "title": "Statistical Inference II",
    "section": "",
    "text": "MAP is a Bayesian approach to MLE\n\n\n\n\n\n\n\n\nMLE\nMAP\n\n\n\n\nFinding value that maximizes likelihood\nFinding value that maximizes posterior\n\n\nOnly uses observed data\nUses observed data and prior\n\n\n\\(\\hat{\\theta}_{\\text{MLE}} = \\text{argmax}_{\\theta} P(D\\|\\theta)\\)\n\\(\\hat{\\theta}_{\\text{MAP}} = \\text{argmax}_{\\theta} P(\\theta\\|D)\\)"
  },
  {
    "objectID": "block_5/553_stat_inf_2/553_stat_inf_2.html#the-bayesian-modelling",
    "href": "block_5/553_stat_inf_2/553_stat_inf_2.html#the-bayesian-modelling",
    "title": "Statistical Inference II",
    "section": "",
    "text": "Big advantage:\n\nIt formulates every problem in one common framework\n\nFinal goal: Take samples from the posterior distribution\nComputer does most of the heavy lifting\n\nAll we need to do is good model design and critical analysis of the results\n\nCharacteristics:\n\n\n\n\nQuestion: Pose a scientific question\nDesign: Formulate variables and create a probabilistic model for them. Prior knowledge is included here!\nInference: Get posterior samples from the model\nCheck: If the samples are from your posterior\nAnalyze: Use the samples to answer the question\n\n\n\n\n\nInferential: Using observed data \\(Y\\) to make inferences about the population/ latent variable \\(\\theta\\)\nPredictive: Using observed data \\(Y\\) to make predictions about future data \\(Y'\\)"
  },
  {
    "objectID": "block_5/553_stat_inf_2/553_stat_inf_2.html#beta-binomial-model",
    "href": "block_5/553_stat_inf_2/553_stat_inf_2.html#beta-binomial-model",
    "title": "Statistical Inference II",
    "section": "",
    "text": "One of the most foundational Bayesian models\nRecall \\(Posterior \\propto Likelihood \\times Prior\\)\n\nBinomial: The likelihood function\n\n\\(Y | \\pi \\sim Binomial(n, \\pi) \\text{ where } \\pi \\in [0,1]\\)\n\nBayesian thinking: \\(Y\\) is a random variable (population parameters are no longer fixed)\nBeta: Prior distribution of parameter of interest \\(\\pi\\)\n\n\\(\\pi \\sim Beta(a, b)\\)\n\n\n\n\n\n\\[\\pi \\sim Beta(a, b)\\]\n\nPDF: \\(f(\\pi) = \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)} \\pi^{a - 1} (1 - \\pi)^{b - 1} \\text{  for  } 0 \\leq \\pi \\leq 1\\)\nMean: \\(\\frac{a}{a + b}\\)\nVariance: \\(\\frac{ab}{(a + b)^2(a + b + 1)}\\)\nMode: \\(\\frac{a - 1}{a + b - 2} \\text{ when } a, b &gt; 1\\)\n\n\n\n\n\nOne of the biggest challenges in Bayesian statistics\nNeed to rely on subject matter prior knowledge\ne.g. Collect information from previous studies and plot a histogram of the data, then fit a beta distribution to it\nbayesrule package in R has a function summarize_beta_binomial(a, b) to summarize the beta distribution\nPDF of binomial distribution:\n\n\\(f(y | \\pi) = \\binom{n}{y} \\pi^y (1 - \\pi)^{n - y}\\)\n\n\n\n\n\n\\[ Posterior \\propto Likelihood \\times Prior \\]\n\\[ f(\\pi | Y) \\propto f(Y | \\pi) \\times f(\\pi) \\]\nusing the beta-binomial model:\n\\[ f(\\pi | Y) \\propto \\binom{n}{y} \\pi^y (1 - \\pi)^{n - y} \\times \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)} \\pi^{a - 1} (1 - \\pi)^{b - 1}\\]\nSimplify (remove non-\\(\\pi\\) terms):\n\\[ f(\\pi | Y) \\propto \\pi^{y + a - 1} (1 - \\pi)^{n - y + b - 1} \\]\nWe recognize this as the kernel of a beta distribution:\n\\[ f(\\pi | Y) \\propto Beta(a + y, b + n - y) \\]\n\nKernel: The part of the expression that depends on the variable of interest\n\n\n\n\n\nPosterior: \\(f(\\pi | Y) = Beta(a + y, b + n - y)\\)\n\nMean: \\(\\frac{a + y}{a + b + n}\\)\nVariance: \\(\\frac{(a + y)(b + n - y)}{(a + b + n)^2(a + b + n + 1)}\\)\nMode: \\(\\frac{a + y - 1}{a + b + n - 2}\\)\n\nMode is the value of \\(\\pi\\) that maximizes the posterior distribution/ peak (MAP/ Maximum A Posteriori)\n\n\nCan also use summarize_beta_binomial(a, b, n, y) to summarize the posterior distribution\n\na and b are the parameters of the prior beta distribution\nn is the number of trials\ny is the number of successes\n\nCan also use plot_beta_binomial(a, b, n, y) to plot the prior and posterior distributions (also from bayesrule package)\n\n\n\n\nCredible Interval: Range of plausible values for the parameter.\n\nWidth: measures variability of the posterior distribution\n\nUse function qbeta in R to calculate the quantiles of the beta distribution\nFor a given a,b,n,y, the 95% credible interval is qbeta(c(0.025, 0.975), shape1 = a + y, shape2 = b + n - y)\n95% CI means:\n\nThere is a 95% posterior probability that the true value of \\(\\pi\\) is between \\(L\\) and \\(U\\)"
  },
  {
    "objectID": "block_5/553_stat_inf_2/553_stat_inf_2.html#designing-the-model",
    "href": "block_5/553_stat_inf_2/553_stat_inf_2.html#designing-the-model",
    "title": "Statistical Inference II",
    "section": "",
    "text": "Bayesian model is a big joint probability distribution \\(P(Y, Y', \\theta)\\)\n\nObservations: \\(Y\\)\nLatent variables: \\(\\theta\\)\nPredictions: \\(Y'\\)\n\nDid inferential approach in Beta-Binomial model (previous section). Now another approach:\n\n\\[P(Y, Y', \\theta) = P(Y, Y' | \\theta) \\times P(\\theta)\\]\n\nGenerate \\(\\theta\\) from the prior \\(P(\\theta)\\)\nGenerate \\(Y, Y'\\) given \\(\\theta\\) from likelihood \\(P(Y, Y' | \\theta)\\)\n\n\nConstructing the Likelihood:\n\nFormulate the data type (int, real, categorical, etc.) and support (positive, negative, etc.)\nFigure out which variables are fixed covariates and which are random variables\nPick a distribution that match the type and support. The distribution will have some unknown parameters - need a prior for these parameters"
  },
  {
    "objectID": "block_5/553_stat_inf_2/553_stat_inf_2.html#markov-chain-monte-carlo-mcmc",
    "href": "block_5/553_stat_inf_2/553_stat_inf_2.html#markov-chain-monte-carlo-mcmc",
    "title": "Statistical Inference II",
    "section": "",
    "text": "Goal: Generate samples from the posterior distribution\nProblem: The posterior is often intractable (can’t be solved analytically)\nSolution: Use MCMC to generate samples from the posterior\nMonte Carlo Algorithm\n\nNeed closed analytical form of the posterior \\(f(\\theta | Y)\\) (e.g. Beta-Binomial model or Gamma-Poisson model)\nBuild independent MC sample \\(\\{\\Theta_1, \\Theta_2, \\ldots, \\Theta_n\\}\\) from \\(f(\\Theta | Y)\\) by:\n\nDrawing \\(\\Theta_i\\) from \\(f(\\Theta | Y)\\)\nGo there\n\n\nIs a random walk in the space of \\(\\theta\\)\nCalled a Markov Chain because the next state depends only on the current state \\(\\theta^{(t)} \\rightarrow \\theta^{(t+1)}\\)\n\n\n\n\nAllows us to obtain an approximation of the posterior distribution \\(f(\\Theta | Y)\\) via MC \\(\\{\\Theta_1, \\Theta_2, \\ldots, \\Theta_n\\}\\).\nNext \\(\\Theta_{t+1}\\) is selected by:\n\nProposing a new value \\(\\Theta'\\) from a proposal distribution \\(q(\\Theta' | \\Theta_t)\\) (e.g. Uniform, Normal, etc.)\nDecide whether to accept or reject \\(\\Theta'\\) based on acceptance probability \\(\\alpha\\): \\[\\alpha = min\\left(1, \\frac{f(\\Theta') \\ell(\\Theta' | Y)}{f(\\Theta_t) \\ell(\\Theta_t| Y)}\\right)\\]\n\nThen obtain the next via bernoulli trial with probability \\(\\alpha\\) for success \\(\\Theta^{(t+1)} = \\Theta'\\)\n\n\n\n\n\n\n\nSome considerations:\n\nWarm-up: Discard the first \\(n\\) samples to allow the chain to converge\nThinning: Only keep every \\(n\\)th sample to reduce autocorrelation\n\nskip the first \\(n\\) samples and then keep every \\(n\\)th sample\n\n\n\n\\[\\text{Num of approx posterior samples} = \\frac{\\text{iter} - \\text{warmup}}{\\text{thin}}\\]\n\n\n\n\nPrior: \\(\\lambda \\sim Gamma(s, r)\\)\nLikelihood: \\(Y_i | \\lambda \\sim Poisson(\\lambda)\\)\n\n\\[\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}\\]\n\\[f(\\lambda | Y) \\propto \\ell(\\lambda | Y) \\times f(\\lambda)\\]\n\nPosterior: \\(\\lambda | Y \\sim Gamma(s + \\sum Y_i, r + n)\\) \nOur prior: \\(\\lambda \\sim Gamma(s=150, r=40)\\)\n\n{stan output.var='gamma_poisson_stan'}\ndata {\n  int&lt;lower=1&gt; n; // number of rows in training data\n  int&lt;lower=0&gt; count[n]; // array of observed counts (integer)\n  real&lt;lower=0&gt; s; // prior shape Gamma parameter\n  real&lt;lower=0&gt; r; // prior rate Gamma parameter\n\n}\n\nparameters {\n  real&lt;lower=0&gt; lambda; // parameter of interest\n}\n\nmodel {\n  lambda ~ gamma(s,r); // prior distribution of lambda\n  count ~ poisson(lambda); // Poisson likelihood, can be complex formula too\n}\nbird_dictionary &lt;- list(\n  n = nrow(observed_evidence),\n  count = as.integer(observed_evidence$count),\n  s = 150,\n  r = 40\n)\n\nposterior_lambda &lt;- sampling(\n  object = gamma_poisson_stan,\n  data = bird_dictionary,\n  chains = 1,\n  iter = 10000,\n  warmup = 1000,\n  thin = 5,\n  seed = 553,\n  algorithm = \"NUTS\"\n)\n\nposterior_lambda &lt;- as.data.frame(posterior_lambda)\nlibrary(ggplot2)\nlibrary(bayesplot)\n\n# Plot the prior, likelihood, and posterior\nplot &lt;- plot_gamma_poisson(\n    shape = 150, rate = 40,\n    sum_y = sum(observed_evidence$count),\n    n = nrow(observed_evidence),\n)\n\n# Plot the posterior distribution from stan\nposterior_plot &lt;- posterior_lambda %&gt;%\n  ggplot(aes(x = lambda)) +\n  geom_histogram(aes(y = after_stat(count)),\n                 bins = 30,\n                 fill = \"lightblue\",\n                 color = \"black\",\n                 alpha = 0.5)\n((See lab 2 for more examples))"
  },
  {
    "objectID": "block_5/553_stat_inf_2/553_stat_inf_2.html#bayesian-normal-linear-regression",
    "href": "block_5/553_stat_inf_2/553_stat_inf_2.html#bayesian-normal-linear-regression",
    "title": "Statistical Inference II",
    "section": "",
    "text": "Comparable to OLS\n\n\n\n\nThis data is from bayesrules package in R.\nWe want to consider the effects of temp_feel (numeric) and weekend (boolean) on rides (count).\n\n\nFor OLS would use Poisson regression since output is count data.\nlm(rides ~ temp_feel + weekend, data = bikeshare)\n\n\n\nFor Bayesian, we need to specify priors for the coefficients and the variance.\n\\[Y_i \\sim \\mathcal{N}(\\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2}, \\sigma^2)\\]\nwhere\n\n\\(X_{i1}\\) is temp_feel\n\\(X_{i2}\\) is weekend: 1 if weekend, 0 if not\n\\(\\beta_0\\) is the intercept ~ \\(Gamma(7.5, 1)\\)\n\\(\\beta_1\\) is the coefficient for temp_feel ~ \\(\\mathcal{N}(0, 1000^2)\\)\n\\(\\beta_2\\) is the coefficient for weekend ~ \\(\\mathcal{N}(0, 1000^2)\\)\n\\(\\sigma^2\\) is the variance ~ \\(IG(0.001, 0.001)\\)\n\nInverse Gamma distribution (popular for variance priors &gt; 0)\n\n\n\n\n\n\nUse stan to sample from the posterior distribution.\n\n{r bikerides_stan}\n\ndata {\n  int&lt;lower=0&gt; n;                               // training sample size\n  vector[n] y;                                  // response vector\n  vector[n] x_1;                                // regressor 1 vector\n  vector[n] x_2;                                // regressor 2 vector\n  real pred_x_1;                                // fixed value for regressor 1\n  real pred_x_2;                                // fixed value for regressor 2\n}\n\nparameters {\n  real&lt;lower=0&gt; beta_0;                         // intercept with lower bound\n  real beta_1;                                  // regression coefficient 1\n  real beta_2;                                  // regression coefficient 2\n  real&lt;lower=0&gt; sigma;                          // common standard deviation with lower bound\n  }\n\nmodel {\n  beta_0 ~ gamma(7.5, 1);                       // alpha = 7.5 and beta = 1\n  beta_1 ~ normal(0, 1000);                     // mu_b1 = 0 and sigma_b1 = 1000\n  beta_2 ~ normal(0, 1000);                     // mu_b2 = 0 and sigma_b2 = 1000\n  sigma ~ inv_gamma(0.001, 0.001);              // eta = 0.001 and lambda = 0.001\n  y ~ normal(beta_0 + beta_1 * x_1 + beta_2 * x_2, sigma);\n}\ngenerated quantities {\n  real y_pred = normal_rng(beta_0 + beta_1 * pred_x_1 + beta_2 * pred_x_2, sigma);\n}\n\ngenerated quantities creates a posterior predictive distribution y_pred which takes into account:\n\nPosterior variablility in the parameters (from joint posterior distribution of the parameters)\nSampling variability in the data (Each prediction should deviate from its posterior prediction) so add random noise\n\nIt executes after obtaining the posterior samples for the parameters.\n\nThen, in R need a dictionary to pass into stan.\nmodel_matrix &lt;- as.data.frame(model.matrix(rides ~ temp_feel + weekend, data = bikeshare_data))\nmodel_matrix\n\nbikerides_dictionary &lt;- list(\n  n = nrow(bikeshare_data),\n  y = bikeshare_data$rides,\n  x_1 = bikeshare_data$temp_feel,\n  x_2 = model_matrix$weekendTRUE,\n  pred_x_1 = 75,\n  pred_x_2 = 1\n)\nNow compile the Stan model:\nposterior_bikeshare &lt;- stan(\n  model_code = bikerides_stan,\n  data = bikerides_dictionary,\n  chains = 1,\n  iter = 40000,\n  warmup = 20000,\n  thin = 60,\n  seed = 553,\n)\n\n# view the posterior summary\nround(summary(posterior_bikeshare)$summary, 2)[-6, c(\"mean\", \"sd\", \"2.5%\", \"97.5%\")]\n\nThe 2.5% and 97.5% quantiles are the 95% credible intervals.\n\nIf the interval contains 0, then the coefficient is not significant.\nIf the interval is large, then the model is not very certain about the coefficient (i.e. model is not capturing the right systematic component).\n\nThere is also 2.5% and 97.5% quantiles for the y_pred which is the posterior predictive distribution for a 95% prediction interval."
  },
  {
    "objectID": "block_5/553_stat_inf_2/553_stat_inf_2.html#bayesian-hypothesis-testing",
    "href": "block_5/553_stat_inf_2/553_stat_inf_2.html#bayesian-hypothesis-testing",
    "title": "Statistical Inference II",
    "section": "",
    "text": "Using a Tinder example where we want to infer the prob of finding a partner if we use Tinder.\n\n\\(X_i \\sim \\text{Bernoulli}(\\pi)\\) for each person \\(i\\)\n\n\\(\\pi\\) is the probability of finding a partner\n\nPrior: \\(\\pi \\sim \\text{Beta}(a, b)\\)\nLikelihood: \\(Y|\\pi \\sim \\text{Binomial}(n, \\pi)\\)\nPosterior: \\(\\pi|y \\sim \\text{Beta}(a', b') = \\text{Beta}(a+y, b+n-y)\\)\n\n\n\n\n\nClaim: In any city like Vancouver, more than 15% of the single people who use the Tinder app will eventually find a partner.\n\nNull Hypothesis: \\(\\pi \\leq 0.15\\)\nAlternative Hypothesis: \\(\\pi &gt; 0.15\\) (associated with the claim)\n\nIn Bayesian, we use the posterior and get probability for each hypothesis (unlike frequentist).\nUse pbeta function to get the probability\n\nIf posterior \\(Beta(a'=24, b'=192)\\) from likelihood of 20 successes out of 200 trials\nThen \\(H_0: P(\\pi \\leq 0.15 | y=20) = \\int_0^{0.15} f(\\pi | y=20) d\\pi\\)\nequal to pbeta(0.15, 24, 192) \n\nIn the Tinder example, we get 2 probabilities:\n\n\\(P(H_0 | y=20) = P(\\pi \\leq 0.15 | y=20) = 0.957\\)\n\\(P(H_a | y=20) = P(\\pi &gt; 0.15 | y=20) = 0.043\\)\n\nCan get Posterior Odds that \\(\\pi &gt; 0.15\\) by dividing the two probabilities.\n\n\\[\\text{Posterior Odds} = \\frac{P(H_a | y=20)}{P(H_0 | y=20)} = \\frac{0.043}{0.957} = 0.045\\]\n\nInterpretation: For \\(y=20\\), \\(\\pi\\) is 22 times(\\(\\frac{1}{0.045}\\)) more likely to be less than or equal to 0.15 compared to being greater than 0.15 using our posterior model.\n\n\n\n\n\nDo the same for prior odds\n\n\\[\\text{Prior Odds} = \\frac{P(H_a)}{P(H_0)}\\]\n\\[\\text{Bayes Factor} = \\frac{\\text{Posterior Odds}}{\\text{Prior Odds}}\\]\n\nBayes Factor = 1: Plausibility of \\(H_a\\) stays the same even after new data\nBayes Factor &gt; 1: Plausibility of \\(H_a\\) increases after new data\nBayes Factor &lt;br 1: Plausibility of \\(H_a\\) decreases after new data\n\n\n\nIf from MCMC, we cannot get analytical solution for Bayes Factor since no exact PDF to integrate.\nCan use bayesfactor package in R to get Bayes Factor.\n\nSolution: Empirical cumulative distribution function (ECDF) of the posterior samples to approximate the posterior distribution. Use ecdf function in R.\n\n\n\n\n\n\nLets say we want to figure out: whether or not 15% of the single people who use the Tinder app will eventually find a partner (in any city like Vancouver)\n\n\\(H_0: \\pi = 0.15\\)\n\\(H_a: \\pi \\neq 0.15\\)\n\n\n\\[P(\\pi = 0.15 | y=20) = \\int_{0.15}^{0.15} f(\\pi | y=20) d\\pi = 0\\]\n\nDoes not work since =0, so add a range of values (e.g. \\(\\pm 0.10\\))\n\n\\(H_0: \\pi \\in [0.05, 0.25]\\)\n\\(H_a: \\pi \\notin [0.05, 0.25]\\)\n\nThen find the credible interval for the posterior distribution of \\(\\pi\\).\n\nqbeta(c(0.025, 0.975), 24, 192)\nThen based on the results:\n\nIf credible interval falls within the range of \\(H_0\\), then we are in favor of \\(H_0\\), with 95% probability."
  },
  {
    "objectID": "block_5/553_stat_inf_2/553_stat_inf_2.html#bayesian-binary-logistic-regression",
    "href": "block_5/553_stat_inf_2/553_stat_inf_2.html#bayesian-binary-logistic-regression",
    "title": "Statistical Inference II",
    "section": "",
    "text": "Comparable to logistic regression\n\n\n\n\nLet \\(Y_i \\in \\{0, 1\\}\\)\nResponse is assumed as: \\[Y_i | \\beta_0 \\beta_1 \\sim \\text{Bernoulli}(\\pi_i) \\]\n\nwith link function:\n\\[log(\\frac{\\pi_i}{1-\\pi_i}) = \\beta_0 + \\beta_1 X_i\\]\nThat is our likelihood.\n\n\n\n\nParameter of interest: \\(\\beta_0, \\beta_1\\)\nLet us assume as follows:\n\n\\(\\beta_0 \\sim N(\\mu=0, \\sigma^2=100^2)\\)\n\\(\\beta_1 \\sim N(\\mu=0, \\sigma^2=100^2)\\)\n\nAssume 0 because we do not know if there is any association, also variace is high to reflect the uncertainty.\n\n\n\n\n\n\n\nNo need warmup and thin as it is only prior.\n\nprior_stan_climate_change &lt;- \"parameters {\nreal beta_0;\nreal beta_1;\n}\nmodel {\nbeta_0 ~ normal(0, 100);\nbeta_1 ~ normal(0, 100);\n}\"\nprior_climate &lt;- stan(\n  model_code = prior_stan_climate_change,\n  chains = 1,\n  iter = 1000,\n  warmup = 0, # Can be 0 since it is only prior\n  thin = 1, # No need to thin as well\n  seed = 553\n)\n\n\n\nposterior_stan_climate_change &lt;- \"data {\nint&lt;lower=0&gt; n;                          // number of observations\nvector[n] income;                        // regressor income\nint&lt;lower=0,upper=1&gt; climate_change[n];  // setting the response variable as binary\n}\nparameters {\nreal beta_0;\nreal beta_1;\n}\nmodel {\nbeta_0 ~ normal(0, 100);\nbeta_1 ~ normal(0, 100);\nclimate_change ~ bernoulli_logit(beta_0 + beta_1 * income);\n}\"\nclimate_dictionary &lt;- list(\n  n = nrow(pulse_training),\n  income = pulse_training$income,\n  climate_change = as.integer(pulse_training$climate_change)\n)\n\nposterior_climate &lt;- stan(\n  model_code = posterior_stan_climate_change,\n  data = climate_dictionary,\n  chains = 1,\n  iter = 21000,\n  warmup = 1000,\n  thin = 20,\n  seed = 553\n)\n\n\n\n\n\n\n\n\n\n\n\nFrequentist BLR\nBayesian BLR\n\n\n\n\nEstimates the MLE (Maximum Likelihood Estimation)\nEstimates the posterior distribution\n\n\nEstimates the standard errors\nEstimates the posterior distribution\n\n\nEstimates the confidence intervals\nEstimates the credible intervals\n\n\n\n\nA big advantage of Bayesian: did not need to derive any Maximum Likelihood steps\nMeaning of Bayesian coefficients:\n\ne.g. estimate of \\(\\beta_1\\) is 0.009, 95% credible interval is (0.005, 0.0134), since 0 is not in the posterior credible interval:\nFor each unit increase in income, a subject is 1.009 times more likely to believe in climate change\nThere is a 95% posterior probability that the true value of \\(\\beta_1\\) lies between 0.005 and 0.0134"
  },
  {
    "objectID": "block_5/553_stat_inf_2/553_stat_inf_2.html#complete-pooled-model",
    "href": "block_5/553_stat_inf_2/553_stat_inf_2.html#complete-pooled-model",
    "title": "Statistical Inference II",
    "section": "",
    "text": "Complete Pooled Model: A model that pools all the data together and estimates a single parameter for all the data\ne.g. have a dataset of multiple rocket types and their launchs, and want to estimate the probability of a rocket launch succeeding\n\nOnly 1 \\(\\pi\\) for all the rocket types\n\n\n\\[\n\\text{likelihood:} \\qquad X_i|\\pi \\sim \\text{Binomial}(n_i, \\pi) \\quad \\text{for } i = 1, \\dots, 367\\\\\n\\text{prior:} \\qquad \\pi \\sim \\text{Beta}(a = 1, b = 1).\n\\]\n\nDoes not allow us to infer any probability of an individual group"
  },
  {
    "objectID": "block_5/553_stat_inf_2/553_stat_inf_2.html#non-pooled-model",
    "href": "block_5/553_stat_inf_2/553_stat_inf_2.html#non-pooled-model",
    "title": "Statistical Inference II",
    "section": "",
    "text": "Non-pooled Model: A model that estimates a parameter for each group of data\ne.g. have a dataset of multiple rocket types and their launchs, and want to estimate the probability of a rocket launch succeeding\n\nHave a \\(\\pi_i\\) for each rocket type\n\n\n\\[\n\\text{likelihood:} \\qquad X_i|\\pi_i \\sim \\text{Binomial}(n_i, \\pi_i) \\quad \\text{for } i = 1, \\dots, 367\\\\\n\\text{prior:} \\qquad \\pi_i \\sim \\text{Beta}(a = 1, b = 1).\n\\]\n\nBetter when comparing groups with different sample sizes\n\nUsing MLE will not be reliable when sample sizes are small\ne.g. Binomial distribution with \\(n=1\\)\n\nDrawbacks:\n\nCannot generalize to new groups\nCannot take valuable information from other groups"
  },
  {
    "objectID": "block_5/553_stat_inf_2/553_stat_inf_2.html#hierarchical-bayesian-model",
    "href": "block_5/553_stat_inf_2/553_stat_inf_2.html#hierarchical-bayesian-model",
    "title": "Statistical Inference II",
    "section": "",
    "text": "Hierarchical Bayesian Model: A model that estimates a parameter for each group of data, but also estimates a distribution of parameters for all the groups\nKey feature: Nesting of parameters over multiple levels\n\nCommon to have a variable in the model prior is itself a random variable (needs another prior)\n\nOr a variable in the likelihood is itself a random variable (needs another prior)\n\n\ne.g. have a dataset of multiple rocket types and their launchs, and want to estimate the probability of a rocket launch succeeding\n\nHave a \\(\\pi_i\\) for each rocket type\nAlso have random variables \\(\\pi_i \\sim \\text{Beta}(a, b)\\)\n\n\\(a\\) and \\(b\\) are parameters and no longer hyperparameters\nnew hyperparameters are the priors for \\(a\\) and \\(b\\)\n\n\n\n\\[\n\\text{likelihood:} \\qquad X_i|\\pi_i \\sim \\text{Binomial}(n_i, \\pi_i) \\quad \\text{for } i = 1, \\dots, 367\\\\\n\\text{priors:} \\qquad \\pi_i \\sim \\text{Beta}(a, b) \\\\\n\\quad a \\sim \\text{Gamma}(0.001, 0.001) \\\\\n\\qquad b \\sim \\text{Gamma}(0.001, 0.001).\n\\]\n\ne.g2 dataset of number of freethrows made and attempts by a Basketball player for multiple seasons\n\nGood because can predict free throw percentage for a new season\nHave a \\(n_i\\) as number of attempts that season and \\(\\pi_i\\) is free throw percentage for that season\n\n\\[\n\\text{likelihood:} \\qquad X_i|\\pi_i \\sim \\text{Binomial}(n_i, \\pi_i) \\quad \\text{where i=season number}\\\\\n\\text{prior:} \\qquad \\pi_i \\sim \\text{Beta}(a,b) \\\\\n\\quad a \\sim \\text{Gamma}(0.001, 0.001) \\\\\n\\qquad b \\sim \\text{Gamma}(0.001, 0.001).\n\\]\nBasically combines the best of of the complete pooled model and the non-pooled model:\n\nUse valuable info from all groups to infer the success probability of a specific group\nCan get posterior predictive distribution for new group\n\n\n\n\n\nFor the \\(a\\) and \\(b\\) we need continous and non-negative priors\nHence, we use the Gamma distribution _ this tends to assign small values to \\(a\\) and \\(b\\) which is good for the prior\n\n\n\n\n\nUsing Heirarchical Bayesian, we will get narrower Credible Intervals compared to the non-pooled model\nBorrowing Strength: The estimates will be more precise because we are using information from all the groups\n\nIn the case of rockets, its the \\(a\\) and \\(b\\) that are being shared\n\nHelps learn parameters and reduce posterior variance\n\n\n\n\n\nUse posterior means of \\(a\\) and \\(b\\) to get the posterior means of \\(\\pi_i\\) for all groups (in rocket example)\n\n\n\n\n\nIn Stan, need to add a new block generated quantities to get the posterior predictive distribution for a new group\n\ngenerated quantities {\n  real&lt;lower=0,upper=1&gt; pi_pred = beta_rng(a, b);\n}\n\nThis generates a new \\(\\pi\\) for a new group from the posterior distribution of \\(a\\) and \\(b\\)\nMean of pi_pred (new group) is similar to the posterior mean of \\(\\pi\\) for all groups\n\nbecause it is the best bayesian model we can obtain without further covariates/ features"
  },
  {
    "objectID": "block_5/553_stat_inf_2/553_stat_inf_2.html#mcmc-diagnostics",
    "href": "block_5/553_stat_inf_2/553_stat_inf_2.html#mcmc-diagnostics",
    "title": "Statistical Inference II",
    "section": "",
    "text": "Common pitfalls of MCMC:\n\nNot enough iterations\nNot enough thinning\n\nTo make it semi-independent\n\nNot enough burn-in\n\nDiagnostic to check if MCMC work and is giving “good” samples for the approx posterior dist\n\n\n\n\nIllustrate posterior sampling by a chain.\nAll chains are overlaid on top of each other (without warm-up)\nIdeal:\n\nNo trend\nNo chain stuck in a local mode\n\n\nlibrary(bayesplot)\n\ntraceplot &lt;- mcmc_trace(mcmc_object,\n                        pars = c(\"param1\", \"param2\"),\n                        size = 0.1,\n                        facet_args = list(nrow = 3))\n\nGood example of no trend and no chain stuck in a local mode\n\n\n\n\nOverlays the density plot of the parameter of interest with the MCMC posterior samples.\n\nlibrary(bayesplot)\n\ndensityplot &lt;- mcmc_dens_overlay(mcmc_object,\n                         pars = c(\"param1\", \"param2\"),\n                         facet_args = list(nrow = 3))\n\n\n\n\n\n\\(N_{eff}\\): Number of independent samples needed to give an accurate MCMC posterior approximation.\nLet \\(N\\) be the length of the chain, then we expect:\n\n\\(N_{eff} \\leq N\\) is normal and expected\n\\(N_{eff} \\approx N\\) is ideal\n\nCompare with ratio of \\(N_{eff}/N\\):\n\nIf \\(N_{eff}/N &gt; 1\\), Means need more samples to be as good as Monte Carlo simulation\n\nCan increase iter OR increase thinning\n\nIf \\(N_{eff}/N &lt; 1\\), Means need less samples to be as good as Monte Carlo simulation\n\nCan decrease iter OR decrease thinning\n\n\n\nlibrary(bayesplot)\n\neff_sample_size &lt;- neff_ratio(mcmc_object,\n                    pars = c(\"param1\", \"param2\"))\n\nessplot &lt;- mcmc_neff(eff_sample_size, size = 2)\n\n\n\n\n\nMCMC is not entirely independent, so we need to check the autocorrelation of the chain\nWant to see the autocorrelation drop to 0 quickly =&gt; shows semi-independence\n\nIdeally before lag 5\n\n\nlibrary(bayesplot)\n\nacf &lt;- mcmc_acf(mcmc_object,\n                pars = c(\"param1\", \"param2\"))\n\n\n\n\n\nEvaluate within-chain and between-chain variance to check if they are roughly the same\nwarning: Sometimes does not make sense for discrete variables\n\n\\[ \\hat{R} = \\sqrt{ 1 + \\frac{b^2}{s^2} } \\approx 1 \\]\n\n\\(b^2\\): Between-chain variance\n\\(s^2\\): Within-chain variance\n\nlibrary(bayesplot)\n\ngelman_rubin &lt;- mcmc_rhat(mcmc_object, size = 2)"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses\nExamples:\n\nClustering\nDimensionality Reduction\n\n\n\n\n\n\n\n\nLabeling data is expensive and time-consuming\nClustering is a way to group based on similarity\n\nCan get a sense of the structure of the data without labeled responses\n\nClustering: Task of partitioning data into groups called clusters based on similarity\nGoal:\n\nPoints in the same cluster should be as similar as possible\nPoints in different clusters should be as different as possible\n\nCan have multiple different clusters in the same dataset\n\ne.g. in dataset of food can have clusters of:\n\nappetizers, main courses, desserts\ncuisines\nhealthy vs unhealthy\n\n\n\n\nComparisons with kNN: | k-Means | k-Nearest Neighbors | | ——- | ——————– | | Unsupervised | Supervised | | Clustering | Classification | | Parametric | Non-parametric | | k = number of clusters | k = number of neighbors |\n\n\n\n\nData Exploration\n\nSummarize/ compress data\nPartition data into groups\n\nCustomer Segmentation\n\nGroup customers based on purchasing behavior\nTarget marketing\n\nDocument Clustering\n\nGroup similar documents together\nTopic modeling Other applications: Medical imaging, Anomaly detection, Image segmentation, Inputting missing data, data compression, etc.\n\n\n\n\n\n\n\nOne of the most popular clustering algorithms\nSimple and easy to implement\nBasic Algorithm:\n\nRandomly initialize \\(K\\) centroids\nAssign each data point to the nearest centroid\nUpdate the centroids to the mean of the data points assigned to it\nRepeat steps 2 and 3 until convergence\n\nProperties:\n\nWill always converge (not necessarily to the right answer)\nSensitive to intialization\nTerminates when centroids do not change\nMakes linear decision boundaries\nMUST SCALE DATA before applying K-means\n\nBecause K-means uses distance\nIf features are on different scales, the clustering will be biased towards the features with larger scales\n\n\n\n\n\nInput:\n\n\\(X\\): Set of \\(n\\) data points\n\\(K\\): Number of clusters\n\nOutput:\n\n\\(K\\) clusters with centroids \\(\\mu_1, \\mu_2, \\ldots, \\mu_K\\)\n\n\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=3, n_init='auto')\nkmeans.fit(X); # only need X\n\nkmeans.labels_ # cluster assignments\nkmeans.cluster_centers_ # cluster centroids\nkmeans.predict(new_data) # predict cluster for new data\n\n\n\nK is a hyperparameter\n\nAs K increases -&gt; smaller clusters\n\nNo perfect way to choose K\n\n\nElbow Method:\n\n\n\nSpecific to k-means\n\\(\\text{Inertia} = \\text{sum of intra-cluster distances}\\)\n\nSum of centroid to point distances of all points in the cluster of all clusters \\(\\sum_{i=1}^{K} \\sum_{x \\in C_i} ||x - \\mu_i||^2\\)\n\nInertia decreases as K increases, until it reaches 0 when K = n\nPlot inertia vs K\n\nElbow point: point where inertia starts to decrease more slowly\n\nChoose K at elbow point\n\nfrom yellowbrick.cluster import KElbowVisualizer\n\n model = KMeans(n_init='auto')\n visualizer = KElbowVisualizer(model, k=(1, 10))\n\n visualizer.fit(XX)  # Fit the data to the visualizer\n visualizer.show();\n\nSilhouette Score: \n\n\nNot dependent on cluster centers -&gt; can be used to compare different clustering algorithms\nGets worst as K increases, since being closer to neigouring clusters \\[\\text{Silhouette Score} = \\frac{b - a}{\\max(a, b)}\\]\n\\(a\\): Mean distance between a sample and all other points in the same cluster\n\\(b\\): Mean distance between a sample and all other points in the next nearest cluster\nRange: \\([-1, 1]\\)\n\n1: Object is well matched to its own cluster and poorly matched to neighboring clusters (BEST)\n0: Object is not matched to its own cluster and might be better in neighboring clusters\n-1: Object is poorly matched to its own cluster and well matched to neighboring clusters (WORST)\n\n\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nmodel = KMeans(2, n_init='auto', random_state=42)\nvisualizer = SilhouetteVisualizer(model, colors=\"yellowbrick\")\nvisualizer.fit(XX)  # Fit the data to the visualizer\nvisualizer.show();\n\ny-axis: Sample number (similar thickness = balanced cluster sizes)\nx-axis: Silhouette score\n\n\n\n\n\n\n\nMotivation:\n\nK-means makes linear decision boundaries\nGMMs can have more flexible cluster shapes\n\n\n\nfrom sklearn.mixture import GaussianMixture\n\ngmm = GaussianMixture(n_components=3, covariance_type='full')\n\ngmm.fit(X_train)\ngmm_labels = gmm.predict(X_train)\n\n# Get values\ngmm.means_ # cluster means (size: K x n_features)\ngmm.covariances_ # cluster covariances (size: K x n_features x n_features)\ngmm.weights_ # cluster weights (size: K)\n\ncovariance_type:\n\nfull: Each component has its own general covariance matrix\n\nsize: \\(K \\times n\\_features \\times n\\_features\\)\n\ntied: All components share the same general covariance matrix\n\nsize: \\(n\\_features \\times n\\_features\\)\n\ndiag: Each component has its own diagonal covariance matrix\n\nsize: \\(K \\times n\\_features\\)\n\nspherical: Each component has its own single variance\n\nsize: \\(K\\)\n\n\n\n\n\n\\[P(x) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)\\]\n\n\\(P(x)\\): Probability of observing \\(x\\)\n\\(\\pi_k\\): Weight of the \\(k\\)th Gaussian (between 0 and 1)\n\\(k\\): Number of clusters\n\\(\\mathcal{N}(x | \\mu_k, \\Sigma_k)\\): Gaussian distribution with mean \\(\\mu_k\\) and covariance \\(\\Sigma_k\\)\n\nGenerative Model: models the probability of a data point being generated from the mixture of Gaussians\nThe generative story of the model assumes that each data point in the dataset is generated from one of the Gaussian components\n\nChoose \\(k\\) with probability \\(\\pi_k\\)\nGenerate data point from \\(\\mathcal{N}(x | \\mu_k, \\Sigma_k)\\)\n\nHigh Level Algorithm (non-convex optimization):\n\nInitialize \\(\\pi_k, \\mu_k, \\Sigma_k\\) (sensitive to init, can init with k-means)\nE-step: Compute the probability of each data point belonging to each cluster\nM-step: Update \\(\\pi_k, \\mu_k, \\Sigma_k\\) to maximize the likelihood of the data\nRepeat steps 2 and 3 until convergence\n\nUnder the hood, GMMs use the Expectation-Maximization (EM) algorithm.\n\nBasic idea: treat cluster assignments as hidden variables and iteratively update them\nE-step: for each point, compute the probability of it belonging to each cluster\nM-step: for each cluster, update the parameters to maximize the likelihood of the data\n\nOther Properties:\n\nCan constrain the covariance matrix\nNumber of clusters is a hyperparameter and has a significant impact on the model\n\n\n\n\n\n\n\nDensity-Based Spatial Clustering of Applications with Noise\nIdea: Clusters are dense regions in the data space, separated by low-density regions\nAddresses K-Means’ weaknesses:\n\nNo need to specify number of clusters\nCan find clusters of arbitrary shapes\nCan identify points that don’t belong to any cluster (points don’t have to belong to any cluster, label = -1)\nInitialization is not a problem\n\nComparison to K-means:\n\ndoes not have to assign all points to clusters\nno predict method unlike K-means\nnon-parametric\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nCan find clusters of arbitrary shapes\ncannot predict new data\n\n\nCan detect outliers\nNeeds tuning of 2 non-trivial hyperparameters\n\n\n\n\nDBSCAN Failure Cases:\n\nDifferent densities of clusters\n\n\nfrom sklearn.cluster import DBSCAN\n\ndbscan = DBSCAN(eps=0.5, min_samples=5)\ndbscan.fit(X)\n\ndbscan.labels_\n\neps (default=0.5): maximum distance between two samples for one to be considered as in the neighborhood of the other.\nmin_samples (default = 5): number of samples in a neighborhood for a point to be considered as a core point\n\n\n\n\nKinds of points:\n\nCore point: A point that has at least min_samples points within eps of it\nBorder point: A point that is within eps of a core point, but has less than min_samples points within eps of it\nNoise point: A point that is neither a core point nor a border point\n\nAlgorithm:\n\nrandomly pick a point that has not been visited\nCheck if it’s a core point\n\nSee eps distance around the point if there are min_samples points\n\nIf yes, start a cluster around this point\nCheck if neighbors are core points and repeat\nOnce no more core points, pick another random point and repeat\n\n\n\n\n\n\nSilhouette Method\n\nCannot use elbow method because no centroids\n\n\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nmodel = DBSCAN(eps=0.5, min_samples=5)\nmodel.fit(X)\n\n# Silhoutte is designed for k-means, so we need to do this\nn_clusters = len(set(model.labels_))\ndbscan.n_clusters = n_clusters\ndbscan.predict = lambda x: model.labels_\n\nvisualizer = SilhouetteVisualizer(dbscan, colors='yellowbrick')\nvisualizer.fit(X)\nvisualizer.show()\n\n\n\n\n\nHard to decide how many clusters\n\nSo get complete picture of similarity between points then decide\n\nMain idea:\n\nStart with each point as a cluster\nMerge the closest clusters\nRepeat until only a single cluster remains (n-1 steps)\n\nVisualized as a dendrogram\n\nfrom matplotlib import pyplot as plt\nfrom scipy.preprocessing import StandardScaler\nfrom scipy.cluster.hierarchy import dendrogram, ward\n\nX_scaled = StandardScaler().fit_transform(X)\n\nlinkage_array = ward(X_scaled) # see below for linkage criteria\n\n# Plot the dendrogram\nax = plt.subplot()\ndendrogram(linkage_array, ax=ax, color_threshold=3)\n\n\n\n\nx-axis: data points\ny-axis: distance between clusters\nIs a tree like plot\n\nNew parent node for each 2 clusters that are merged\n\nLength of the vertical line at the point of merging is the distance between the clusters\n\n\n\n\n\nLinkage Criteria determines how to find similarity between clusters\nSingle Linkage:\n\nMerge smallest min distance between points in two clusters\nCan lead to chaining\nscipy.cluster.hierarchy.single\n\nComplete Linkage:\n\nMerge smallest max distance between points in two clusters\nCan lead to crowding (tight clusters)\nscipy.cluster.hierarchy.complete\n\nAverage Linkage:\n\nMerge smallest mean distance between points in two clusters\nscipy.cluster.hierarchy.average\n\nWard’s Method:\n\nMinimizes the variance of the clusters being merged\nLeads to equally sized clusters\nscipy.cluster.hierarchy.ward\n\n\n\n\n\n\nTruncation:\n\nscipy.cluster.hierarchy.dendrogram has a truncate_mode parameter\nTwo levels:\n\nlastp: show last p merged clusters (only p nodes are shown)\nlevel: level is all nodes with p merges from the root\n\n\n\n\n\nFlatten\n\nDirectly cut the dendogram at certain condition (e.g. distance or max number of clusters)\n\n\nfrom scipy.cluster.hierarchy import fcluster\n\n# 3 is the max distance\nhier_labels = fcluster(linkage_array, 3 , criterion='distance')\n\n# Based on max number of clusters (4 max clusters)\nhier_labels = fcluster(linkage_array, 4 , criterion='maxclust')\n\n\n\n\n\n\n\nCurse of Dimensionality\n\nAs the number of dimensions increases, the volume of the space increases so fast that the available data become sparse\n\nData Visualization\n\nIt is difficult to visualize data in high dimensions\n\nComputational Efficiency\n\nMany algorithms are computationally infeasible in high dimensions\n\n\n\n\n\n\nGoal\n\nFind a low-dimensional representation of the data that captures as much of the variance as possible\n\nApproach\n\nFind the lower dimension hyperplane that minimizes the reconstruction error\nModel is the best-fit hyperplane\n\n\n\n\n\n\\[X = ZW\\]\n\\[(n \\times d) = (n \\times k) \\cdot (k \\times d)\\]\nUsually \\(k &lt;&lt; d\\)\n\n\\(X\\): original data, (\\(n \\times d\\))\n\\(Z\\): coordinates in the lower dimension, (\\(n \\times k\\))\n\\(W\\): lower dimension hyperplane, (\\(k \\times d\\))\n\n\\(W\\) is the principal components\nRows of \\(W\\) are orthogonal to each other\n\n\nCan reconstruct the original data with some error:\n\\[\\hat{X} = ZW\\]\nNote: if \\(k = d\\), then \\(Z\\) is not necessarily \\(X\\) but \\(\\hat{X} = X\\) (Frobenius norm)\n\nObjective/ Loss Function:\n\nMinimize reconstruction error \\(\\|ZW - X\\|_F^2\\)\n\nFrobeinus norm \\(||A||_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n a_{ij}^2}\\)\n\nNOT the same as least squares\n\nLS is vertical distance, PCA is orthogonal distance\n\n\n\n\n\n\n\nSingular Value Decomposition (SVD)\n\\[A = USV^T\\]\n\n\\(A\\): original data, (\\(n \\times d\\))\n\\(U\\): left singular vectors, (\\(n \\times n\\))\n\northonormal columns \\(U_i^TU_j = 0\\) for all \\(i \\neq j\\)\n\n\\(S\\): diagonal matrix of singular values, (\\(n \\times d\\))\n\nsquare root of eigenvalues of \\(A^TA\\) or \\(AA^T\\)\ncorresponds to the variance of the data along the principal components (in decreasing order)\n\n\\(V\\): right singular vectors, (\\(d \\times d\\))\n\northonormal columns (eigenvectors)\nprincipal components (get the first \\(k\\) columns)\n\n\nFor dimensionality reduction, we can use the first \\(k\\) columns of \\(U\\) and the first \\(k\\) rows of \\(V^T\\) (equal to first \\(k\\) columns of \\(V\\))\nPCA Algorithm\n\nCenter the data (subtract the mean of each column)\nCompute the SVD of the centered data to get the principal components \\(W\\).\n\n\\(W\\) is the first \\(k\\) columns of \\(V\\)\n\nVariance of each PC is the square of the singular value \\(s_i^2\\)\nDrop the PCs with the smallest variance\n\nUniquess of PCA\n\nPCA are not unique, similar to eigenvectors\nCan add constraints to make it closer to unique:\n\nNormalization: \\(||w_i|| = 1\\)\nOrthogonality: \\(w_i^Tw_j = 0\\) for all \\(i \\neq j\\)\nSequential PCA: \\(w_1^Tw_2 = 0\\), \\(w_2^Tw_3 = 0\\), etc.\n\nThe principal components are unique up to sign\n\n\n\n\n\n\nNo definitive rule\nCan look at:\n\nExplained variance ratio\nReconstructions plot\n\n\nfrom sklearn.decomposition import PCA\npca = PCA()\npca.fit(samples)\n\n# Plot the explained variances\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\n\n\n\n\nPCA can be used to remove multicollinearity\nConcept: the principal components are orthogonal to each other so they should not be correlated\n\n\n\n\n\nData Compression\n\nCan use the first \\(k\\) principal components to represent the data\n\nFeature Extraction\n\nCan use the first \\(k\\) principal components as features\n\nVisualization of High-Dimensional Data\n\nCan visualize the data in 2D or 3D by using the first 2 or 3 principal components\n\nDimensionality Reduction\nAnomaly Detection\n\nCan use the reconstruction error to detect anomalies/ outliers (if the error is too large)\nOutliers = high reconstruction error\n\n\n\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\npca = PCA(n_components=2)\npipeline = make_pipeline(StandardScaler(), pca)\n\n# Fit the pipeline to 'samples'\nZ = pipeline.fit_transform(samples)\nX_hat = pipeline.inverse_transform(Z)\n\n# Get the principal components\nprint(pca.components_)\n\n\n\n\nPCA is a generalization of K-means\nK-means is a special case of PCA where the principal components are the cluster centers\nK-means each example is expressed with only one component (one-hot encoding) but in PCA it is a linear combination of all components\n\n\n\n\n\n\nDo not center the data and just use SVD\nUseful for sparse data (e.g. text data in a bag-of-words model)\nIt is also referred to as Latent Semantic Indexing (LSI)\nTruncatedSVD in sklearn is used for LSA\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import make_pipeline\n\nlsa_pipe = make_pipeline(CountVectorizer(stop_words='english'),\n                             TruncatedSVD(n_components=2))\n\nlsa_transformed = lsa_pipe.fit_transform(df['text'])\n\n\n\n\nUseful for when data is created with several independent sources\n\ne.g. music with different instruments\n\nProperties:\n\nCoefficients and basis vectors (components) are non-negative\n\nUnlike in PCA you can subtract, e.g. \\(X_i = 14W_0 - 2W_2\\)\nSince cannot cancel out =&gt; more interpretable\n\nComponents are neither orthogonal to each other nor are they sorted by the variance explained by them\nData is not centered\nWill get different results for different number of components\n\nn_components=2 will point at extreme, n_components=1 will point at mean\nUnlike PCA, where first component points at the direction of maximum variance regardless of the number of components\n\nSlower than PCA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifferenciating Feature\nPCA\nNMF\nLSA\n\n\n\n\nPrimary Use\nDimensionality reduction, feature extraction\nFeature extraction, source separation\nDimensionality reduction, semantic analysis\n\n\nData types/ constraints\nLinear data, centered data\nNon-negative data, non-centered data\nSparse data (e.g., text data), not centered\n\n\nOutput components\nOrthogonal components, sorted by variance explained\nNon-negative components, not orthogonal or sorted\nComponents from SVD, not necessarily orthogonal or sorted\n\n\nInterpretability\nLess interpretable due to orthogonality\nMore interpretable due to non-negativity\nMore interpretable, particularly in semantic analysis contexts\n\n\n\n\n\n\n\nMotivation: You can understand a word by the context/company it keeps.\n\n\n\n\nStandard approach: put words in vector space and the distance between words is the similarity between them.\n\n\n\nword2vec is unsupervised/ semi-supervised learning because:\n\nclosely related to dimensionality reduction + extracting meaninggful representation from raw data\ndo not need any labeled data\nrunning text is used as supervision signal\n\n\n\n\n\n\nOne-hot representation:\n\nSimplest way to represent a word\nOHE vector is a vector of all 0s except for a 1 at the index of the word in the vocabulary\n\nrows = words in sentence, columns = words in vocabulary\n\nDisadvantages:\n\nHigh dimensionality\nNo notion of similarity between words (dot product = 0)\nNo notion of context\n\n\nTerm-term co-occurrence matrix:\n\nA matrix where each row and column corresponds to a word in the vocabulary\nThe value in the i-th row and j-th column is the number of times word i and word j appear together in a context window\n\nContext window: a fixed-size window that slides over the text (e.g. window size = 2 means 2 words to the left and 2 words to the right)\n\nDisadvantages:\n\nHigh dimensionality\nSparse\nDoes not capture polysemy (multiple meanings of a word)\n\n\n\n\n\n\n\nTerm-term co-occurrence matrix is sparse and high-dimensional\nBetter to learn short and dense vectors for words\n\nEasier to store and train\nBetter generalization\n\nApproaches:\n\nLatent Semantic Analysis (LSA): Use SVD to reduce the dimensionality of the term-term co-occurrence matrix\n\nWorks better for small datasets compared to word2vec\n\nWord2Vec: Use neural networks to learn word embeddings\n\n\n\n\n\n\nCreate short and dense word embeddings using neural networks\nIdea: Predict the context of a word given the word itself\n\nSkip-gram: Predict the context words given the target word\nContinuous Bag of Words (CBOW): Predict the target word given the context words\n\nTwo moderately efficient training algorithms:\n\nHierarchical softmax: Use a binary tree to represent all words in the vocabulary\nNegative sampling: Treat the problem as a binary classification problem\n\n\n\n\n\nPredict the context words given the target word\nNN to obtain short and dense word vectors\nArchitecture:\n\n\n\nInput layer: one-hot encoded vector of the target word (size = \\(V \\times 1\\))\n\n\\(W\\) = input layer to hidden layer weights (size = \\(V \\times d\\)) \\(\\text{hidden} = W^T \\times \\text{input}\\)\n\nHidden layer: linear transformation (no activation function) to obtain the word vector (size = \\(d \\times 1\\))\n\n\\(W_c\\) = hidden layer to output layer weights (size = \\(V \\times d\\)) \\(\\text{output} = W_c \\times \\text{hidden}\\)\n\nOutput layer: softmax layer to predict the context words (size = \\(V \\times 1\\))\n\nReturns a one-hot encoded vector of the context word\n\nThe dense representation of the word:\n\n\\(W\\): word embedding matrix (size = \\(V \\times d\\))\n\nThis is the main output of the algorithm\n\n\\(W_c\\): shared context embedding matrix (size = \\(V \\times d\\))\n\nTrain multiple target+context pairs until the weights converge \nExample:\n\n“Add freshly squeezed [pineapple] juice to your smoothie.\n\nTarget word: pineapple\nNN outputs probability distribution of context words: {squeezed, juice}\n\n\n\n\n\n\\[\\arg \\max\\limits_\\theta \\prod\\limits_{(w_c,w_t) \\in D} P(w_c|w_t;\\theta) \\approx \\prod\\limits_{(w_c,w_t) \\in D} \\frac{e^{w_c.w_t}}{\\sum\\limits_{\\substack{c' \\in V}} e^{w_{c'}.w_t}}\\]\n\nWant to get the context word with the highest probability given the target word\n\\(w_t\\) → target word\n\\(w_c\\) → context word\n\\(D\\) → the set of all target and context pairs from the text\n\\(P(w_c|w_t;\\theta)\\) → probability of context word given the target word\nAssumption: maximizing this objective would lead to good word embeddings\n\n\n\n\n\nDimensionality of word vectors (\\(d\\))\nWindow size:\n\nSmall window size: captures more syntactic information (e.g. verb-noun relationships)\nLarge window size: captures more semantic information (e.g. country-capital relationships)\n\n\n\n\n\n\n\n\nword2vec\nwikipedia2vec: for 12 languages\nGloVe: based on GloVe algorithm (Stanford)\nfastText pre-trained embeddings for 294 languages\n\n\n\n\n\nCan do analogy tasks\n\ne.g. man to king as women to (queen)\nMAN : KING :: WOMAN : ?\nsolce by: \\(\\vec{X} = \\vec{\\text{KING}} − \\vec{\\text{MAN}} + \\vec{\\text{WOMAN}}\\)\n\nThere are some biases in the word embeddings because they are trained on biased data\n\n\n\n\n\n\n\n\n\nNLP library by Facebook research\nIncludes an algorithm which is an extension to word2vec\nHelps deal with unknown words elegantly\nBreaks words into several n-gram subwords\nExample: trigram sub-words for berry are ber, err, rry\n\nEmbedding(berry) = embedding(ber) + embedding(err) + embedding(rry)\n\n\n\n\n\n\nStarts with the co-occurrence matrix\n\nCo-occurrence can be interpreted as an indicator of semantic proximity of words\n\nTakes advantage of global count statistics\nPredicts co-occurrence ratios\nLoss based on word frequency\n\n\n\n\n\n\n\nUsing word embeddings for various ML (NLP) tasks\nBefore using it for application, need to evaluate quality of word embeddings:\n\nExamine a number of word pairs for similarity scores (use TOEFL MCQ dataset)\nExamine different analogies for stereotypes and biases they encode\nVisualize embeddings in two dimensions\n\n\n\n\n\nMotivation: You can understand a document by the context/company it keeps.\nAssuming we have reasonable representations of words, we can represent a paragraph/ document as:\n\nAverage embeddings\nConcatenate embeddings\n\n\n\n\n\nDo it with spacy\nWe do not necessarily get expected representation of text\n\ne.g. “Machine Learning” and “Learning Machine” will have same representation\n\nFor long sentences or documents, this can get very noisy (mix of different signals)\n\nimport spacy\nnlp = spacy.load('en_core_web_md')\n\nnlp(\"empty\").vector[0:10] # access word vector\n\ndoc = nlp(\"I like apples and oranges\")\ndoc_vec = doc.vector # shape (300,)\n\ndoc2 = nlp(\"I like bananas and grapes\")\ndoc.similarity(doc2) # check similarity by averaging word vectors\n\n\n\n\nspacy’s mlp.pipe() takes iterable of texts and returns an iterable of Doc objects.\n\nX_train_embeddings = pd.DataFrame([text.vector for text in nlp.pipe(X_train)]) # shape (n, 300)\nX_test_embeddings = pd.DataFrame([text.vector for text in nlp.pipe(X_test)])\n\nlgr = LogisticRegression(max_iter=1000) # from sklearn.linear_model\nlgr.fit(X_train_embeddings, y_train)\n\nspaCy uses corpus of text styles from telephone conversations, newswire, newsgroups, broadcast news, weblogs, and conversational telephone speech.\nMight need to train your own for medical, tweets, etc.\n\n\n\n\n\n\nNot so common, but it’s possible\nComparisons:\n\nWords -&gt; Products\nSentences -&gt; Purchase history of users\nVocabulary -&gt; Products\n\n\n\n\n\n\n\nsource\n\nManifold: Lower-dimensional structure embedded within a higher-dimensional space (can be curved/ twisted)\nManifold Learning: Techniques to learn the structure of the manifold from the data\n\nBased on the idea of finding low dimensional representation that preserves the distances between points as best as possible\nReal-world data often lies on a low-dimensional manifold\n\nCommon methods:\n\nMulti-dimensional scaling (MDS)\nISOMAP\nLocally linear embedding (LLE)\nt-SNE\nUMAP [best one]\n\n\n\n\n\nt-SNE applies non-linear transformation to the data\n\nPCA is a linear dimensionality reduction technique\n\nMostly used for visualization\nDoes not construct an explicit mapping function from the high-dimensional space to the low-dimensional space\n\nIt optimizes the position of the points in the low-dimensional space\n\nHyperparameters:\n\nperplexity: Number of nearest neighbors to consider\nlearning_rate: Step size for gradient descent\nn_iter: Number of iterations\n\nCons:\n\nslow and does not scale well to large datasets\nrandom initialization can lead to different results\nsensitive to hyperparameters (perplexity)\nNeed to re-run when new data is added\n\n\n\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.datasets import load_digits\n\ndigits = load_digits()\n\ntsne = TSNE(n_components=2, random_state=42)\ndigits_tsne = tsne.fit_transform(digits.data)\n\nInput: \nPCA output: \nt-SNE output: \n\n\n\n\n\nIdea: Preserve the similarity between points in high-dimensional space in the low-dimensional space\n\nIn high-dimensional space,\n\nCompute pairwise similarity between points as probabilities\nSimilarity b/w \\(x_i\\) and \\(x_j\\) is \\(p_{ij}\\)\n\n\\(p_{ij}\\) is calculated using Gaussian distribution, centered at \\(x_i\\)\n\nIt is the density of \\(x_j\\) under the Gaussian centered at \\(x_i\\)\n\\(p_{ij}\\) high if \\(x_i\\) and \\(x_j\\) are close to each other (and low if far)\n\nVariance \\(\\sigma^2\\) of the Gaussian is influenced by perplexity hyperparameter\n\nperplexity is a measure of effective number of neighbors to consider\nHigher perplexity, larger variance, more neighbors\n\n\n\nIn low-dimensional space,\n\nRandomly initialize points in low-dimensional space (e.g. PCA)\nCalculates a similar set of pairwise probabilities \\(q_{ij}\\) in the low-dimensional space\n\n\\(q_{ij}\\) is calculated using t-distribution (NOT Gaussian) to mitigate crowding problem\n\nMakes sure points are not crowded together\n\nt-distribution has heavier tails than Gaussian\n\nAssigns a higher probability to points that are far apart\n\n\n\nLoss function\n\nMinimize the difference between \\(p_{ij}\\) and \\(q_{ij}\\) using gradient descent (use Kullback-Leibler divergence)\n\n\\[KL(P||Q) = \\sum_{i,j}p_{ij}\\log\\left(\\frac{p_{ij}}{q_{ij}}\\right)\\]\n\n\n\n\nPerplexity is a measure of effective number of neighbors to consider\n\nLow: consider fewer neighbors, smaller variance\nHigh: consider more neighbors, larger variance\n\n\n\n\n\n\n\n\n\nA recommender suggests a particular product or service to users they are likely to consume.\nWhy is it important?\n\nEverything we buy or consume is influenced by this (music, shopping, movies, etc.)\nIt is the core of success for many companies (e.g. spotify, amazon, netflix, etc.)\nTool to reduce the effort of users to find what they want\n\nEthical considerations:\n\nCan lead to filter bubbles (e.g. political views, etc.)\nCan lead to privacy issues (e.g. tracking user behavior)\n\n\n\n\n\nData:\n\npurchase history\nuser-system interactions (e.g. clicks, likes, etc.)\nfeatures of the items (e.g. genre, price, etc.)\n\nApproaches:\n\nCollaborative filtering:\n\nUnsupervised learning\nHave labels \\(y_{ij}\\) (ratings for user \\(i\\) and item \\(j\\))\nLearn latent features of users and items\n\nContent-based filtering:\n\nSupervised learning\nExtract features of items/ users to predict ratings\n\nHybrid methods:\n\nCombine both approaches\n\n\n\n\n\n\n\n\n\n\nAlso referred to as the \\(Y\\) matrix\nNot actually used in real life because it will be very large (also sparse)\nTrain and validation will have same number of rows (users) and columns (items)\n\n\\(N\\) users and \\(M\\) items\n\\(Y_{ij}\\) is the rating of user \\(i\\) for item \\(j\\)\n\n\n\n\npredict rating \\(\\neq\\) regression or classification:\n\nIt is a different problem because we don’t have a target variable\nWe have to predict the missing values in the utility matrix\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\nratings = pd.read_csv('ratings.csv')\n\nN = len(np.unique(ratings[users]))\nM = len(np.unique(ratings[items]))\n\nuser_mapper = dict(zip(np.unique(ratings['users']), list(range(N))))\nitem_mapper = dict(zip(np.unique(ratings['items']), list(range(M))))\nuser_inv_mapper = dict(zip(list(range(N)), np.unique(ratings['users'])))\nitem_inv_mapper = dict(zip(list(range(M)), np.unique(ratings['items'])))\n\nMap to get user/item id -&gt; indices (utility matrix)\nInverse map to get indices -&gt; user/item id\n\ndef create_Y_from_ratings(\n    data, N, M, user_mapper, item_mapper, user_key=\"user_id\", item_key=\"movie_id\"):  # Function to create a dense utility matrix\n\n    Y = np.zeros((N, M))\n    Y.fill(np.nan)\n    for index, val in data.iterrows():\n        n = user_mapper[val[user_key]]\n        m = item_mapper[val[item_key]]\n        Y[n, m] = val[\"rating\"]\n\n    return Y\n\nY_mat = create_Y_from_ratings(toy_ratings, N, M, user_mapper, item_mapper)\n\n\n\n\n\nNo notion of “accurate” recommendations, but still need to evaluate\nUnsupervised learning but split the data and evaluate \n\nSPLIT TRAIN /VALID ON RATINGS, NOT UTILITY MATRIX\nUtility matrix of train and validation will be the same\nCode shown below, not really going to use y\n\n\nfrom sklearn.model_selection import train_test_split\n\nX = toy_ratings.copy()\ny = toy_ratings[user_key]\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\ntrain_mat = create_Y_from_ratings(X_train, N, M, user_mapper, item_mapper)\nvalid_mat = create_Y_from_ratings(X_valid, N, M, user_mapper, item_mapper)\n\nRMSE:\n\nIt is the most common metric\nIt compares the predicted ratings with the actual ratings\n\n\n\n\n\n\nGlobal Average:\n\nPredict everything as the global average rating\nIt is a very simple model\n\nPer-User Average:\n\nPredict everything as the average rating of the user\n\nPer-Item Average:\n\nPredict everything as the average rating of the item\n\nPer-User and Per-Item Average:\n\nPredict everything as the average of the user and the item\n\nKNN:\n\nCalculate distance between examples usign features where neither value is missing\n\nfrom sklearn.impute import KNNImputer\n\n# assume train_mat is the utility matrix\nimputer = KNNImputer(n_neighbors=2, keep_empty_features=True)\ntrain_mat_imp = imputer.fit_transform(train_mat)\n\n\n\n\nClustering:\n\nCluster the items, then recommend items from the same cluster\n\nGraphs and BFS:\n\nCreate a graph of users and items\nUse BFS to recommend items\n\n\n\n\n\n\n\n\nUnsupervised learning\nIntuition:\n\nPeople who agreed in the past are likely to agree again in future\nLeverage social information for recommendations\n\nPCA ?:\n\nTo learn latent features of users and items\nRun on utility matrix\nProblem: missing values\n\nPCA loss function \\(f(Z,W) = \\sum_{i,j} ||W^TZ_{ij} - Y_{ij}||^2\\)\nCannot use SVD directly because have many missing values AND missing values make SVD undefined\n\nSolutions:\n\nImpute the values to do PCA\n\nBUT, will introduce bias (distort the data)\nResult will be dominated by the imputed values\n\nSumming over only available values\n\nProne to overfitting\n\nCollaborative Filtering Loss Function:\n\nOnly consider the available values\nAdd L2-reg to the loss function for W and Z\n\\(f(Z,W) = \\sum_{i,j} ||W^TZ_{ij} - Y_{ij}||^2 + \\frac{\\lambda_1}{2}||W||^2 + \\frac{\\lambda_2}{2}||Z||^2\\)\nThis accounts for the missing values and the regularization terms prevent overfitting (representations are not too complex)\nThis improved the RMSE score bby 7% in the Netflix competition\nOptimize using SGD (stoachastic gradient descent) and WALS (weighted alternating least squares)\n\n\n\nOther Notes:\n\nResult can be outside the range of the ratings\nWill have problems with cold start (new users or items)\n\n\n\n\n\n\n\\(Z\\) is no longer the points in the new hyperplane and \\(W\\) is no longer the weights\n\\(Z\\):\n\nEach row is a user\nMaps users to latent feature of items\n\n\\(W\\):\n\nEach col is an item\nMaps items to latent feature of users\n\n\n\n\n\n\nhttps://surprise.readthedocs.io/en/stable/index.html\n\nimport surprise\nfrom surprise import SVD, Dataset, Reader, accuracy\n\n# Load the data\nreader = Reader()\ndata = Dataset.load_from_df(ratings[['users', 'items', 'ratings']], reader)\n\n# Train-test split\ntrainset, validset = train_test_split(data, test_size=0.2, random_state=42)\n\n# PCA-like model\nk=2\nalgo = SVD(n_factors=k, random_state=42)\nalgo.fit(trainset)\n\n# Predictions\npreds = algo.test(validset.build_testset())\n\n# RMSE\naccuracy.rmse(preds)\n\nCan also cross-validate\n\nfrom surprise.model_selection import cross_validate\n\ncross_validate(algo, data, measures=['RMSE', \"MAE\"], cv=5, verbose=True)\n\n\n\n\n\nSource: Google ML\n\nCosine:\n\n\\(d(x,y) = \\frac{x \\cdot y}{||x|| \\cdot ||y||}\\)\nCollinear = 1, orthogonal = 0 (want a value close to 1)\nIt is the angle between the two vectors\nRank (high to low): C, A, B\n\nEuclidean:\n\n\\(d(x,y) = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2}\\)\nIt is the straight line distance between the two points (want smaller distance)\nRank (high to low): B, C, A\n\nDot Product:\n\n\\(d(x,y) = x \\cdot y\\)\nIt is the projection of one vector onto the other\nIf vectors are normalized, it is the same as cosine similarity (want larger value)\nRank (high to low): A, B, C\n\n\n\n\n\n\nSupervised learning\nDoes not make use of social network / information\nSolves the cold start problem (can recommend items to new users/items)\nAssumes that we have features of items and/or users to predict ratings\nCreate a user profile for each user\n\nTreat rating prediction as a regression problem\nHave a regression model for each user\n\n\n\n\n\nLoad ratings_df (contains user_id, movie_id, and rating)\n\nAlso make the user and movie mappers.\n\n\ntoy_ratings = pd.read_csv(\"data/toy_ratings.csv\")\n\nN = len(np.unique(toy_ratings[\"user_id\"]))\nM = len(np.unique(toy_ratings[\"movie_id\"]))\n\nuser_key = \"user_id\" # Name of user\nitem_key = \"movie_id\" # Name of movie\n\n# Turns the name into a number (id)\nuser_mapper = dict(zip(np.unique(toy_ratings[user_key]), list(range(N))))\nitem_mapper = dict(zip(np.unique(toy_ratings[item_key]), list(range(M))))\n\n# Turns the number (id) back into a name\nuser_inverse_mapper = dict(zip(list(range(N)), np.unique(toy_ratings[user_key])))\nitem_inverse_mapper = dict(zip(list(range(M)), np.unique(toy_ratings[item_key])))\n\nLoad movie features. This is a matrix of shape (n_movies, n_features)\n\nIndex of movie features is movie id/name\nFeatures can be genre, director, actors, etc.\n\n\nimport pandas as pd\n\nmovie_feats_df = pd.read_csv(\"data/toy_movie_feats.csv\", index_col=0)\nZ = movie_feats_df.to_numpy()\n\n\nBuild a user profile. For each user, we will get the ratings and the corresponding movie features.\n\nResults in a dictionary (key: user, value: numpy array of size (n_ratings, n_genres))\n\nn_ratings: number of movies rated by the user\n\n\n\nfrom collections import defaultdict\n\ndef get_X_y_per_user(ratings, d=item_feats.shape[1]):\n    \"\"\"\n    Returns X and y for each user.\n\n    Parameters:\n    ----------\n    ratings : pandas.DataFrame\n         ratings data as a dataframe\n\n    d : int\n        number of item features\n\n    Return:\n    ----------\n        dictionaries containing X and y for all users\n    \"\"\"\n    lr_y = defaultdict(list)\n    lr_X = defaultdict(list)\n\n    for index, val in ratings.iterrows():\n        n = user_mapper[val[user_key]]\n        m = item_mapper[val[item_key]]\n        lr_X[n].append(item_feats[m])\n        lr_y[n].append(val[\"rating\"])\n\n    for n in lr_X:\n        lr_X[n] = np.array(lr_X[n])\n        lr_y[n] = np.array(lr_y[n])\n\n    return lr_X, lr_y\n\ndef get_user_profile(user_name):\n  \"\"\"\n  Get the user profile based on the user name\n\n  e.g. get_user_profile(\"user1\")\n  \"\"\"\n    X = X_train_usr[user_mapper[user_name]]\n    y = y_train_usr[user_mapper[user_name]]\n    items = rated_items[user_mapper[user_name]]\n    movie_names = [item_inverse_mapper[item] for item in items]\n    print(\"Profile for user: \", user_name)\n    profile_df = pd.DataFrame(X, columns=movie_feats_df.columns, index=movie_names)\n    profile_df[\"ratings\"] = y\n    return profile_df\n# Using the helper functions\nXt,yt = get_X_y_per_user(X_train)\nXv,yv = get_X_y_per_user(X_valid)\n\n# Check the user profile\nget_user_profile(\"Nando\")\n\nSupervised learning. Train a regression model for each user.\n\nWe will use Ridge regression model for this example.\n\n\nfrom sklearn.linear_model import Ridge\n\nmodels = dict()\n# Make utility matrix\npred_lin_reg = np.zeros((N, M))\n\nfor n in range(N):\n  models[n] = Ridge()\n  models[n].fit(Xt[n], yt[n])\n  pred_lin_reg[n] = models[n].predict(item_feats)\n\n\n\n\n\n\n\n\n\n\n\nCollaborative Filtering\nContent Based Filtering\n\n\n\n\n\\(\\hat{y}_{ij} = w_j^T z_i\\)\n\\(\\hat{y}_{ij} = w_i^T x_{ij}\\)\n\n\n\\(w_j^T\\): “hidden” embedding for feature \\(j\\)  \\(z_i\\): “hidden” embedding for user \\(i\\)\n\\(w_i\\): feature vector for user \\(i\\)  \\(x_{ij}\\): feature \\(j\\) for user \\(i\\)\n\n\n(+) Makes use of social network / information\n(-) Does not make use of social network / information\n\n\n(-) Cold start problem\n(+) Solves the cold start problem\n\n\n(+) No feature engineering required\n(-) Requires feature engineering\n\n\n(-) Hard to interpret\n(+) Easy to interpret\n\n\n(-) Cannot capture unique user preferences\n(+) Can capture unique user preferences (since each model is unique)\n\n\n(+) More diverse recommendations\n(-) Less diverse recommendations (hardly recommend an item outside the user’s profile)\n\n\n\n\n\n\n\nBest RMSE \\(\\neq\\) Best Recommender\nNeed to consider simplicity, interpretatibility, code maintainability, etc.\n\nThe Netflix Prize: The winning solution was never implemented\n\nOther things to consider:\n\nDiversity: If someone buys a tennis racket, they might not want to buy another tennis racket\nFreshness: New items (new items need to be recommended for it to be successful)\nTrust: Explain your recommendation to the user\nPersistence: If the same recommendation is made over and over again, it might not be a good recommendation\nSocial influence: If a friend buys something, you might want to buy it too\n\nAlso need to consider ethical implications\n\nFilter bubbles\nRecommending harmful content"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html#unsupervised-learning-introduction",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html#unsupervised-learning-introduction",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses\nExamples:\n\nClustering\nDimensionality Reduction"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html#k-means-clustering",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html#k-means-clustering",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Labeling data is expensive and time-consuming\nClustering is a way to group based on similarity\n\nCan get a sense of the structure of the data without labeled responses\n\nClustering: Task of partitioning data into groups called clusters based on similarity\nGoal:\n\nPoints in the same cluster should be as similar as possible\nPoints in different clusters should be as different as possible\n\nCan have multiple different clusters in the same dataset\n\ne.g. in dataset of food can have clusters of:\n\nappetizers, main courses, desserts\ncuisines\nhealthy vs unhealthy\n\n\n\n\nComparisons with kNN: | k-Means | k-Nearest Neighbors | | ——- | ——————– | | Unsupervised | Supervised | | Clustering | Classification | | Parametric | Non-parametric | | k = number of clusters | k = number of neighbors |\n\n\n\n\nData Exploration\n\nSummarize/ compress data\nPartition data into groups\n\nCustomer Segmentation\n\nGroup customers based on purchasing behavior\nTarget marketing\n\nDocument Clustering\n\nGroup similar documents together\nTopic modeling Other applications: Medical imaging, Anomaly detection, Image segmentation, Inputting missing data, data compression, etc.\n\n\n\n\n\n\n\nOne of the most popular clustering algorithms\nSimple and easy to implement\nBasic Algorithm:\n\nRandomly initialize \\(K\\) centroids\nAssign each data point to the nearest centroid\nUpdate the centroids to the mean of the data points assigned to it\nRepeat steps 2 and 3 until convergence\n\nProperties:\n\nWill always converge (not necessarily to the right answer)\nSensitive to intialization\nTerminates when centroids do not change\nMakes linear decision boundaries\nMUST SCALE DATA before applying K-means\n\nBecause K-means uses distance\nIf features are on different scales, the clustering will be biased towards the features with larger scales\n\n\n\n\n\nInput:\n\n\\(X\\): Set of \\(n\\) data points\n\\(K\\): Number of clusters\n\nOutput:\n\n\\(K\\) clusters with centroids \\(\\mu_1, \\mu_2, \\ldots, \\mu_K\\)\n\n\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=3, n_init='auto')\nkmeans.fit(X); # only need X\n\nkmeans.labels_ # cluster assignments\nkmeans.cluster_centers_ # cluster centroids\nkmeans.predict(new_data) # predict cluster for new data\n\n\n\nK is a hyperparameter\n\nAs K increases -&gt; smaller clusters\n\nNo perfect way to choose K\n\n\nElbow Method:\n\n\n\nSpecific to k-means\n\\(\\text{Inertia} = \\text{sum of intra-cluster distances}\\)\n\nSum of centroid to point distances of all points in the cluster of all clusters \\(\\sum_{i=1}^{K} \\sum_{x \\in C_i} ||x - \\mu_i||^2\\)\n\nInertia decreases as K increases, until it reaches 0 when K = n\nPlot inertia vs K\n\nElbow point: point where inertia starts to decrease more slowly\n\nChoose K at elbow point\n\nfrom yellowbrick.cluster import KElbowVisualizer\n\n model = KMeans(n_init='auto')\n visualizer = KElbowVisualizer(model, k=(1, 10))\n\n visualizer.fit(XX)  # Fit the data to the visualizer\n visualizer.show();\n\nSilhouette Score: \n\n\nNot dependent on cluster centers -&gt; can be used to compare different clustering algorithms\nGets worst as K increases, since being closer to neigouring clusters \\[\\text{Silhouette Score} = \\frac{b - a}{\\max(a, b)}\\]\n\\(a\\): Mean distance between a sample and all other points in the same cluster\n\\(b\\): Mean distance between a sample and all other points in the next nearest cluster\nRange: \\([-1, 1]\\)\n\n1: Object is well matched to its own cluster and poorly matched to neighboring clusters (BEST)\n0: Object is not matched to its own cluster and might be better in neighboring clusters\n-1: Object is poorly matched to its own cluster and well matched to neighboring clusters (WORST)\n\n\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nmodel = KMeans(2, n_init='auto', random_state=42)\nvisualizer = SilhouetteVisualizer(model, colors=\"yellowbrick\")\nvisualizer.fit(XX)  # Fit the data to the visualizer\nvisualizer.show();\n\ny-axis: Sample number (similar thickness = balanced cluster sizes)\nx-axis: Silhouette score"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html#gaussian-mixture-models-high-level-overview",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html#gaussian-mixture-models-high-level-overview",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Motivation:\n\nK-means makes linear decision boundaries\nGMMs can have more flexible cluster shapes\n\n\n\nfrom sklearn.mixture import GaussianMixture\n\ngmm = GaussianMixture(n_components=3, covariance_type='full')\n\ngmm.fit(X_train)\ngmm_labels = gmm.predict(X_train)\n\n# Get values\ngmm.means_ # cluster means (size: K x n_features)\ngmm.covariances_ # cluster covariances (size: K x n_features x n_features)\ngmm.weights_ # cluster weights (size: K)\n\ncovariance_type:\n\nfull: Each component has its own general covariance matrix\n\nsize: \\(K \\times n\\_features \\times n\\_features\\)\n\ntied: All components share the same general covariance matrix\n\nsize: \\(n\\_features \\times n\\_features\\)\n\ndiag: Each component has its own diagonal covariance matrix\n\nsize: \\(K \\times n\\_features\\)\n\nspherical: Each component has its own single variance\n\nsize: \\(K\\)\n\n\n\n\n\n\\[P(x) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)\\]\n\n\\(P(x)\\): Probability of observing \\(x\\)\n\\(\\pi_k\\): Weight of the \\(k\\)th Gaussian (between 0 and 1)\n\\(k\\): Number of clusters\n\\(\\mathcal{N}(x | \\mu_k, \\Sigma_k)\\): Gaussian distribution with mean \\(\\mu_k\\) and covariance \\(\\Sigma_k\\)\n\nGenerative Model: models the probability of a data point being generated from the mixture of Gaussians\nThe generative story of the model assumes that each data point in the dataset is generated from one of the Gaussian components\n\nChoose \\(k\\) with probability \\(\\pi_k\\)\nGenerate data point from \\(\\mathcal{N}(x | \\mu_k, \\Sigma_k)\\)\n\nHigh Level Algorithm (non-convex optimization):\n\nInitialize \\(\\pi_k, \\mu_k, \\Sigma_k\\) (sensitive to init, can init with k-means)\nE-step: Compute the probability of each data point belonging to each cluster\nM-step: Update \\(\\pi_k, \\mu_k, \\Sigma_k\\) to maximize the likelihood of the data\nRepeat steps 2 and 3 until convergence\n\nUnder the hood, GMMs use the Expectation-Maximization (EM) algorithm.\n\nBasic idea: treat cluster assignments as hidden variables and iteratively update them\nE-step: for each point, compute the probability of it belonging to each cluster\nM-step: for each cluster, update the parameters to maximize the likelihood of the data\n\nOther Properties:\n\nCan constrain the covariance matrix\nNumber of clusters is a hyperparameter and has a significant impact on the model"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html#dbscan",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html#dbscan",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Density-Based Spatial Clustering of Applications with Noise\nIdea: Clusters are dense regions in the data space, separated by low-density regions\nAddresses K-Means’ weaknesses:\n\nNo need to specify number of clusters\nCan find clusters of arbitrary shapes\nCan identify points that don’t belong to any cluster (points don’t have to belong to any cluster, label = -1)\nInitialization is not a problem\n\nComparison to K-means:\n\ndoes not have to assign all points to clusters\nno predict method unlike K-means\nnon-parametric\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nCan find clusters of arbitrary shapes\ncannot predict new data\n\n\nCan detect outliers\nNeeds tuning of 2 non-trivial hyperparameters\n\n\n\n\nDBSCAN Failure Cases:\n\nDifferent densities of clusters\n\n\nfrom sklearn.cluster import DBSCAN\n\ndbscan = DBSCAN(eps=0.5, min_samples=5)\ndbscan.fit(X)\n\ndbscan.labels_\n\neps (default=0.5): maximum distance between two samples for one to be considered as in the neighborhood of the other.\nmin_samples (default = 5): number of samples in a neighborhood for a point to be considered as a core point\n\n\n\n\nKinds of points:\n\nCore point: A point that has at least min_samples points within eps of it\nBorder point: A point that is within eps of a core point, but has less than min_samples points within eps of it\nNoise point: A point that is neither a core point nor a border point\n\nAlgorithm:\n\nrandomly pick a point that has not been visited\nCheck if it’s a core point\n\nSee eps distance around the point if there are min_samples points\n\nIf yes, start a cluster around this point\nCheck if neighbors are core points and repeat\nOnce no more core points, pick another random point and repeat\n\n\n\n\n\n\nSilhouette Method\n\nCannot use elbow method because no centroids\n\n\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nmodel = DBSCAN(eps=0.5, min_samples=5)\nmodel.fit(X)\n\n# Silhoutte is designed for k-means, so we need to do this\nn_clusters = len(set(model.labels_))\ndbscan.n_clusters = n_clusters\ndbscan.predict = lambda x: model.labels_\n\nvisualizer = SilhouetteVisualizer(dbscan, colors='yellowbrick')\nvisualizer.fit(X)\nvisualizer.show()"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html#hierarchical-clustering",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html#hierarchical-clustering",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Hard to decide how many clusters\n\nSo get complete picture of similarity between points then decide\n\nMain idea:\n\nStart with each point as a cluster\nMerge the closest clusters\nRepeat until only a single cluster remains (n-1 steps)\n\nVisualized as a dendrogram\n\nfrom matplotlib import pyplot as plt\nfrom scipy.preprocessing import StandardScaler\nfrom scipy.cluster.hierarchy import dendrogram, ward\n\nX_scaled = StandardScaler().fit_transform(X)\n\nlinkage_array = ward(X_scaled) # see below for linkage criteria\n\n# Plot the dendrogram\nax = plt.subplot()\ndendrogram(linkage_array, ax=ax, color_threshold=3)\n\n\n\n\nx-axis: data points\ny-axis: distance between clusters\nIs a tree like plot\n\nNew parent node for each 2 clusters that are merged\n\nLength of the vertical line at the point of merging is the distance between the clusters\n\n\n\n\n\nLinkage Criteria determines how to find similarity between clusters\nSingle Linkage:\n\nMerge smallest min distance between points in two clusters\nCan lead to chaining\nscipy.cluster.hierarchy.single\n\nComplete Linkage:\n\nMerge smallest max distance between points in two clusters\nCan lead to crowding (tight clusters)\nscipy.cluster.hierarchy.complete\n\nAverage Linkage:\n\nMerge smallest mean distance between points in two clusters\nscipy.cluster.hierarchy.average\n\nWard’s Method:\n\nMinimizes the variance of the clusters being merged\nLeads to equally sized clusters\nscipy.cluster.hierarchy.ward\n\n\n\n\n\n\nTruncation:\n\nscipy.cluster.hierarchy.dendrogram has a truncate_mode parameter\nTwo levels:\n\nlastp: show last p merged clusters (only p nodes are shown)\nlevel: level is all nodes with p merges from the root\n\n\n\n\n\nFlatten\n\nDirectly cut the dendogram at certain condition (e.g. distance or max number of clusters)\n\n\nfrom scipy.cluster.hierarchy import fcluster\n\n# 3 is the max distance\nhier_labels = fcluster(linkage_array, 3 , criterion='distance')\n\n# Based on max number of clusters (4 max clusters)\nhier_labels = fcluster(linkage_array, 4 , criterion='maxclust')"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html#principal-component-analysis-pca",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html#principal-component-analysis-pca",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Curse of Dimensionality\n\nAs the number of dimensions increases, the volume of the space increases so fast that the available data become sparse\n\nData Visualization\n\nIt is difficult to visualize data in high dimensions\n\nComputational Efficiency\n\nMany algorithms are computationally infeasible in high dimensions\n\n\n\n\n\n\nGoal\n\nFind a low-dimensional representation of the data that captures as much of the variance as possible\n\nApproach\n\nFind the lower dimension hyperplane that minimizes the reconstruction error\nModel is the best-fit hyperplane\n\n\n\n\n\n\\[X = ZW\\]\n\\[(n \\times d) = (n \\times k) \\cdot (k \\times d)\\]\nUsually \\(k &lt;&lt; d\\)\n\n\\(X\\): original data, (\\(n \\times d\\))\n\\(Z\\): coordinates in the lower dimension, (\\(n \\times k\\))\n\\(W\\): lower dimension hyperplane, (\\(k \\times d\\))\n\n\\(W\\) is the principal components\nRows of \\(W\\) are orthogonal to each other\n\n\nCan reconstruct the original data with some error:\n\\[\\hat{X} = ZW\\]\nNote: if \\(k = d\\), then \\(Z\\) is not necessarily \\(X\\) but \\(\\hat{X} = X\\) (Frobenius norm)\n\nObjective/ Loss Function:\n\nMinimize reconstruction error \\(\\|ZW - X\\|_F^2\\)\n\nFrobeinus norm \\(||A||_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n a_{ij}^2}\\)\n\nNOT the same as least squares\n\nLS is vertical distance, PCA is orthogonal distance\n\n\n\n\n\n\n\nSingular Value Decomposition (SVD)\n\\[A = USV^T\\]\n\n\\(A\\): original data, (\\(n \\times d\\))\n\\(U\\): left singular vectors, (\\(n \\times n\\))\n\northonormal columns \\(U_i^TU_j = 0\\) for all \\(i \\neq j\\)\n\n\\(S\\): diagonal matrix of singular values, (\\(n \\times d\\))\n\nsquare root of eigenvalues of \\(A^TA\\) or \\(AA^T\\)\ncorresponds to the variance of the data along the principal components (in decreasing order)\n\n\\(V\\): right singular vectors, (\\(d \\times d\\))\n\northonormal columns (eigenvectors)\nprincipal components (get the first \\(k\\) columns)\n\n\nFor dimensionality reduction, we can use the first \\(k\\) columns of \\(U\\) and the first \\(k\\) rows of \\(V^T\\) (equal to first \\(k\\) columns of \\(V\\))\nPCA Algorithm\n\nCenter the data (subtract the mean of each column)\nCompute the SVD of the centered data to get the principal components \\(W\\).\n\n\\(W\\) is the first \\(k\\) columns of \\(V\\)\n\nVariance of each PC is the square of the singular value \\(s_i^2\\)\nDrop the PCs with the smallest variance\n\nUniquess of PCA\n\nPCA are not unique, similar to eigenvectors\nCan add constraints to make it closer to unique:\n\nNormalization: \\(||w_i|| = 1\\)\nOrthogonality: \\(w_i^Tw_j = 0\\) for all \\(i \\neq j\\)\nSequential PCA: \\(w_1^Tw_2 = 0\\), \\(w_2^Tw_3 = 0\\), etc.\n\nThe principal components are unique up to sign\n\n\n\n\n\n\nNo definitive rule\nCan look at:\n\nExplained variance ratio\nReconstructions plot\n\n\nfrom sklearn.decomposition import PCA\npca = PCA()\npca.fit(samples)\n\n# Plot the explained variances\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\n\n\n\n\nPCA can be used to remove multicollinearity\nConcept: the principal components are orthogonal to each other so they should not be correlated\n\n\n\n\n\nData Compression\n\nCan use the first \\(k\\) principal components to represent the data\n\nFeature Extraction\n\nCan use the first \\(k\\) principal components as features\n\nVisualization of High-Dimensional Data\n\nCan visualize the data in 2D or 3D by using the first 2 or 3 principal components\n\nDimensionality Reduction\nAnomaly Detection\n\nCan use the reconstruction error to detect anomalies/ outliers (if the error is too large)\nOutliers = high reconstruction error\n\n\n\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\npca = PCA(n_components=2)\npipeline = make_pipeline(StandardScaler(), pca)\n\n# Fit the pipeline to 'samples'\nZ = pipeline.fit_transform(samples)\nX_hat = pipeline.inverse_transform(Z)\n\n# Get the principal components\nprint(pca.components_)\n\n\n\n\nPCA is a generalization of K-means\nK-means is a special case of PCA where the principal components are the cluster centers\nK-means each example is expressed with only one component (one-hot encoding) but in PCA it is a linear combination of all components"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html#lsa-latent-semantic-analysis",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html#lsa-latent-semantic-analysis",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Do not center the data and just use SVD\nUseful for sparse data (e.g. text data in a bag-of-words model)\nIt is also referred to as Latent Semantic Indexing (LSI)\nTruncatedSVD in sklearn is used for LSA\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import make_pipeline\n\nlsa_pipe = make_pipeline(CountVectorizer(stop_words='english'),\n                             TruncatedSVD(n_components=2))\n\nlsa_transformed = lsa_pipe.fit_transform(df['text'])"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html#nmf-non-negative-matrix-factorization",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html#nmf-non-negative-matrix-factorization",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Useful for when data is created with several independent sources\n\ne.g. music with different instruments\n\nProperties:\n\nCoefficients and basis vectors (components) are non-negative\n\nUnlike in PCA you can subtract, e.g. \\(X_i = 14W_0 - 2W_2\\)\nSince cannot cancel out =&gt; more interpretable\n\nComponents are neither orthogonal to each other nor are they sorted by the variance explained by them\nData is not centered\nWill get different results for different number of components\n\nn_components=2 will point at extreme, n_components=1 will point at mean\nUnlike PCA, where first component points at the direction of maximum variance regardless of the number of components\n\nSlower than PCA"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html#comparison-of-pca-lsa-and-nmf",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html#comparison-of-pca-lsa-and-nmf",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Differenciating Feature\nPCA\nNMF\nLSA\n\n\n\n\nPrimary Use\nDimensionality reduction, feature extraction\nFeature extraction, source separation\nDimensionality reduction, semantic analysis\n\n\nData types/ constraints\nLinear data, centered data\nNon-negative data, non-centered data\nSparse data (e.g., text data), not centered\n\n\nOutput components\nOrthogonal components, sorted by variance explained\nNon-negative components, not orthogonal or sorted\nComponents from SVD, not necessarily orthogonal or sorted\n\n\nInterpretability\nLess interpretable due to orthogonality\nMore interpretable due to non-negativity\nMore interpretable, particularly in semantic analysis contexts"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html#word-embeddings",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html#word-embeddings",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Motivation: You can understand a word by the context/company it keeps.\n\n\n\n\nStandard approach: put words in vector space and the distance between words is the similarity between them.\n\n\n\nword2vec is unsupervised/ semi-supervised learning because:\n\nclosely related to dimensionality reduction + extracting meaninggful representation from raw data\ndo not need any labeled data\nrunning text is used as supervision signal\n\n\n\n\n\n\nOne-hot representation:\n\nSimplest way to represent a word\nOHE vector is a vector of all 0s except for a 1 at the index of the word in the vocabulary\n\nrows = words in sentence, columns = words in vocabulary\n\nDisadvantages:\n\nHigh dimensionality\nNo notion of similarity between words (dot product = 0)\nNo notion of context\n\n\nTerm-term co-occurrence matrix:\n\nA matrix where each row and column corresponds to a word in the vocabulary\nThe value in the i-th row and j-th column is the number of times word i and word j appear together in a context window\n\nContext window: a fixed-size window that slides over the text (e.g. window size = 2 means 2 words to the left and 2 words to the right)\n\nDisadvantages:\n\nHigh dimensionality\nSparse\nDoes not capture polysemy (multiple meanings of a word)\n\n\n\n\n\n\n\nTerm-term co-occurrence matrix is sparse and high-dimensional\nBetter to learn short and dense vectors for words\n\nEasier to store and train\nBetter generalization\n\nApproaches:\n\nLatent Semantic Analysis (LSA): Use SVD to reduce the dimensionality of the term-term co-occurrence matrix\n\nWorks better for small datasets compared to word2vec\n\nWord2Vec: Use neural networks to learn word embeddings\n\n\n\n\n\n\nCreate short and dense word embeddings using neural networks\nIdea: Predict the context of a word given the word itself\n\nSkip-gram: Predict the context words given the target word\nContinuous Bag of Words (CBOW): Predict the target word given the context words\n\nTwo moderately efficient training algorithms:\n\nHierarchical softmax: Use a binary tree to represent all words in the vocabulary\nNegative sampling: Treat the problem as a binary classification problem\n\n\n\n\n\nPredict the context words given the target word\nNN to obtain short and dense word vectors\nArchitecture:\n\n\n\nInput layer: one-hot encoded vector of the target word (size = \\(V \\times 1\\))\n\n\\(W\\) = input layer to hidden layer weights (size = \\(V \\times d\\)) \\(\\text{hidden} = W^T \\times \\text{input}\\)\n\nHidden layer: linear transformation (no activation function) to obtain the word vector (size = \\(d \\times 1\\))\n\n\\(W_c\\) = hidden layer to output layer weights (size = \\(V \\times d\\)) \\(\\text{output} = W_c \\times \\text{hidden}\\)\n\nOutput layer: softmax layer to predict the context words (size = \\(V \\times 1\\))\n\nReturns a one-hot encoded vector of the context word\n\nThe dense representation of the word:\n\n\\(W\\): word embedding matrix (size = \\(V \\times d\\))\n\nThis is the main output of the algorithm\n\n\\(W_c\\): shared context embedding matrix (size = \\(V \\times d\\))\n\nTrain multiple target+context pairs until the weights converge \nExample:\n\n“Add freshly squeezed [pineapple] juice to your smoothie.\n\nTarget word: pineapple\nNN outputs probability distribution of context words: {squeezed, juice}\n\n\n\n\n\n\\[\\arg \\max\\limits_\\theta \\prod\\limits_{(w_c,w_t) \\in D} P(w_c|w_t;\\theta) \\approx \\prod\\limits_{(w_c,w_t) \\in D} \\frac{e^{w_c.w_t}}{\\sum\\limits_{\\substack{c' \\in V}} e^{w_{c'}.w_t}}\\]\n\nWant to get the context word with the highest probability given the target word\n\\(w_t\\) → target word\n\\(w_c\\) → context word\n\\(D\\) → the set of all target and context pairs from the text\n\\(P(w_c|w_t;\\theta)\\) → probability of context word given the target word\nAssumption: maximizing this objective would lead to good word embeddings\n\n\n\n\n\nDimensionality of word vectors (\\(d\\))\nWindow size:\n\nSmall window size: captures more syntactic information (e.g. verb-noun relationships)\nLarge window size: captures more semantic information (e.g. country-capital relationships)\n\n\n\n\n\n\n\n\nword2vec\nwikipedia2vec: for 12 languages\nGloVe: based on GloVe algorithm (Stanford)\nfastText pre-trained embeddings for 294 languages\n\n\n\n\n\nCan do analogy tasks\n\ne.g. man to king as women to (queen)\nMAN : KING :: WOMAN : ?\nsolce by: \\(\\vec{X} = \\vec{\\text{KING}} − \\vec{\\text{MAN}} + \\vec{\\text{WOMAN}}\\)\n\nThere are some biases in the word embeddings because they are trained on biased data\n\n\n\n\n\n\n\n\n\nNLP library by Facebook research\nIncludes an algorithm which is an extension to word2vec\nHelps deal with unknown words elegantly\nBreaks words into several n-gram subwords\nExample: trigram sub-words for berry are ber, err, rry\n\nEmbedding(berry) = embedding(ber) + embedding(err) + embedding(rry)\n\n\n\n\n\n\nStarts with the co-occurrence matrix\n\nCo-occurrence can be interpreted as an indicator of semantic proximity of words\n\nTakes advantage of global count statistics\nPredicts co-occurrence ratios\nLoss based on word frequency"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html#word-embedding-applications",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html#word-embedding-applications",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Using word embeddings for various ML (NLP) tasks\nBefore using it for application, need to evaluate quality of word embeddings:\n\nExamine a number of word pairs for similarity scores (use TOEFL MCQ dataset)\nExamine different analogies for stereotypes and biases they encode\nVisualize embeddings in two dimensions\n\n\n\n\n\nMotivation: You can understand a document by the context/company it keeps.\nAssuming we have reasonable representations of words, we can represent a paragraph/ document as:\n\nAverage embeddings\nConcatenate embeddings\n\n\n\n\n\nDo it with spacy\nWe do not necessarily get expected representation of text\n\ne.g. “Machine Learning” and “Learning Machine” will have same representation\n\nFor long sentences or documents, this can get very noisy (mix of different signals)\n\nimport spacy\nnlp = spacy.load('en_core_web_md')\n\nnlp(\"empty\").vector[0:10] # access word vector\n\ndoc = nlp(\"I like apples and oranges\")\ndoc_vec = doc.vector # shape (300,)\n\ndoc2 = nlp(\"I like bananas and grapes\")\ndoc.similarity(doc2) # check similarity by averaging word vectors\n\n\n\n\nspacy’s mlp.pipe() takes iterable of texts and returns an iterable of Doc objects.\n\nX_train_embeddings = pd.DataFrame([text.vector for text in nlp.pipe(X_train)]) # shape (n, 300)\nX_test_embeddings = pd.DataFrame([text.vector for text in nlp.pipe(X_test)])\n\nlgr = LogisticRegression(max_iter=1000) # from sklearn.linear_model\nlgr.fit(X_train_embeddings, y_train)\n\nspaCy uses corpus of text styles from telephone conversations, newswire, newsgroups, broadcast news, weblogs, and conversational telephone speech.\nMight need to train your own for medical, tweets, etc.\n\n\n\n\n\n\nNot so common, but it’s possible\nComparisons:\n\nWords -&gt; Products\nSentences -&gt; Purchase history of users\nVocabulary -&gt; Products"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html#manifold-learning",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html#manifold-learning",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "source\n\nManifold: Lower-dimensional structure embedded within a higher-dimensional space (can be curved/ twisted)\nManifold Learning: Techniques to learn the structure of the manifold from the data\n\nBased on the idea of finding low dimensional representation that preserves the distances between points as best as possible\nReal-world data often lies on a low-dimensional manifold\n\nCommon methods:\n\nMulti-dimensional scaling (MDS)\nISOMAP\nLocally linear embedding (LLE)\nt-SNE\nUMAP [best one]\n\n\n\n\n\nt-SNE applies non-linear transformation to the data\n\nPCA is a linear dimensionality reduction technique\n\nMostly used for visualization\nDoes not construct an explicit mapping function from the high-dimensional space to the low-dimensional space\n\nIt optimizes the position of the points in the low-dimensional space\n\nHyperparameters:\n\nperplexity: Number of nearest neighbors to consider\nlearning_rate: Step size for gradient descent\nn_iter: Number of iterations\n\nCons:\n\nslow and does not scale well to large datasets\nrandom initialization can lead to different results\nsensitive to hyperparameters (perplexity)\nNeed to re-run when new data is added\n\n\n\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.datasets import load_digits\n\ndigits = load_digits()\n\ntsne = TSNE(n_components=2, random_state=42)\ndigits_tsne = tsne.fit_transform(digits.data)\n\nInput: \nPCA output: \nt-SNE output: \n\n\n\n\n\nIdea: Preserve the similarity between points in high-dimensional space in the low-dimensional space\n\nIn high-dimensional space,\n\nCompute pairwise similarity between points as probabilities\nSimilarity b/w \\(x_i\\) and \\(x_j\\) is \\(p_{ij}\\)\n\n\\(p_{ij}\\) is calculated using Gaussian distribution, centered at \\(x_i\\)\n\nIt is the density of \\(x_j\\) under the Gaussian centered at \\(x_i\\)\n\\(p_{ij}\\) high if \\(x_i\\) and \\(x_j\\) are close to each other (and low if far)\n\nVariance \\(\\sigma^2\\) of the Gaussian is influenced by perplexity hyperparameter\n\nperplexity is a measure of effective number of neighbors to consider\nHigher perplexity, larger variance, more neighbors\n\n\n\nIn low-dimensional space,\n\nRandomly initialize points in low-dimensional space (e.g. PCA)\nCalculates a similar set of pairwise probabilities \\(q_{ij}\\) in the low-dimensional space\n\n\\(q_{ij}\\) is calculated using t-distribution (NOT Gaussian) to mitigate crowding problem\n\nMakes sure points are not crowded together\n\nt-distribution has heavier tails than Gaussian\n\nAssigns a higher probability to points that are far apart\n\n\n\nLoss function\n\nMinimize the difference between \\(p_{ij}\\) and \\(q_{ij}\\) using gradient descent (use Kullback-Leibler divergence)\n\n\\[KL(P||Q) = \\sum_{i,j}p_{ij}\\log\\left(\\frac{p_{ij}}{q_{ij}}\\right)\\]\n\n\n\n\nPerplexity is a measure of effective number of neighbors to consider\n\nLow: consider fewer neighbors, smaller variance\nHigh: consider more neighbors, larger variance"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html#recommender-systems-introduction",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html#recommender-systems-introduction",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "A recommender suggests a particular product or service to users they are likely to consume.\nWhy is it important?\n\nEverything we buy or consume is influenced by this (music, shopping, movies, etc.)\nIt is the core of success for many companies (e.g. spotify, amazon, netflix, etc.)\nTool to reduce the effort of users to find what they want\n\nEthical considerations:\n\nCan lead to filter bubbles (e.g. political views, etc.)\nCan lead to privacy issues (e.g. tracking user behavior)\n\n\n\n\n\nData:\n\npurchase history\nuser-system interactions (e.g. clicks, likes, etc.)\nfeatures of the items (e.g. genre, price, etc.)\n\nApproaches:\n\nCollaborative filtering:\n\nUnsupervised learning\nHave labels \\(y_{ij}\\) (ratings for user \\(i\\) and item \\(j\\))\nLearn latent features of users and items\n\nContent-based filtering:\n\nSupervised learning\nExtract features of items/ users to predict ratings\n\nHybrid methods:\n\nCombine both approaches"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html#recommender-systems-structure",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html#recommender-systems-structure",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Also referred to as the \\(Y\\) matrix\nNot actually used in real life because it will be very large (also sparse)\nTrain and validation will have same number of rows (users) and columns (items)\n\n\\(N\\) users and \\(M\\) items\n\\(Y_{ij}\\) is the rating of user \\(i\\) for item \\(j\\)\n\n\n\n\npredict rating \\(\\neq\\) regression or classification:\n\nIt is a different problem because we don’t have a target variable\nWe have to predict the missing values in the utility matrix\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\nratings = pd.read_csv('ratings.csv')\n\nN = len(np.unique(ratings[users]))\nM = len(np.unique(ratings[items]))\n\nuser_mapper = dict(zip(np.unique(ratings['users']), list(range(N))))\nitem_mapper = dict(zip(np.unique(ratings['items']), list(range(M))))\nuser_inv_mapper = dict(zip(list(range(N)), np.unique(ratings['users'])))\nitem_inv_mapper = dict(zip(list(range(M)), np.unique(ratings['items'])))\n\nMap to get user/item id -&gt; indices (utility matrix)\nInverse map to get indices -&gt; user/item id\n\ndef create_Y_from_ratings(\n    data, N, M, user_mapper, item_mapper, user_key=\"user_id\", item_key=\"movie_id\"):  # Function to create a dense utility matrix\n\n    Y = np.zeros((N, M))\n    Y.fill(np.nan)\n    for index, val in data.iterrows():\n        n = user_mapper[val[user_key]]\n        m = item_mapper[val[item_key]]\n        Y[n, m] = val[\"rating\"]\n\n    return Y\n\nY_mat = create_Y_from_ratings(toy_ratings, N, M, user_mapper, item_mapper)\n\n\n\n\n\nNo notion of “accurate” recommendations, but still need to evaluate\nUnsupervised learning but split the data and evaluate \n\nSPLIT TRAIN /VALID ON RATINGS, NOT UTILITY MATRIX\nUtility matrix of train and validation will be the same\nCode shown below, not really going to use y\n\n\nfrom sklearn.model_selection import train_test_split\n\nX = toy_ratings.copy()\ny = toy_ratings[user_key]\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\ntrain_mat = create_Y_from_ratings(X_train, N, M, user_mapper, item_mapper)\nvalid_mat = create_Y_from_ratings(X_valid, N, M, user_mapper, item_mapper)\n\nRMSE:\n\nIt is the most common metric\nIt compares the predicted ratings with the actual ratings\n\n\n\n\n\n\nGlobal Average:\n\nPredict everything as the global average rating\nIt is a very simple model\n\nPer-User Average:\n\nPredict everything as the average rating of the user\n\nPer-Item Average:\n\nPredict everything as the average rating of the item\n\nPer-User and Per-Item Average:\n\nPredict everything as the average of the user and the item\n\nKNN:\n\nCalculate distance between examples usign features where neither value is missing\n\nfrom sklearn.impute import KNNImputer\n\n# assume train_mat is the utility matrix\nimputer = KNNImputer(n_neighbors=2, keep_empty_features=True)\ntrain_mat_imp = imputer.fit_transform(train_mat)\n\n\n\n\nClustering:\n\nCluster the items, then recommend items from the same cluster\n\nGraphs and BFS:\n\nCreate a graph of users and items\nUse BFS to recommend items"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html#collaborative-filtering",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html#collaborative-filtering",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Unsupervised learning\nIntuition:\n\nPeople who agreed in the past are likely to agree again in future\nLeverage social information for recommendations\n\nPCA ?:\n\nTo learn latent features of users and items\nRun on utility matrix\nProblem: missing values\n\nPCA loss function \\(f(Z,W) = \\sum_{i,j} ||W^TZ_{ij} - Y_{ij}||^2\\)\nCannot use SVD directly because have many missing values AND missing values make SVD undefined\n\nSolutions:\n\nImpute the values to do PCA\n\nBUT, will introduce bias (distort the data)\nResult will be dominated by the imputed values\n\nSumming over only available values\n\nProne to overfitting\n\nCollaborative Filtering Loss Function:\n\nOnly consider the available values\nAdd L2-reg to the loss function for W and Z\n\\(f(Z,W) = \\sum_{i,j} ||W^TZ_{ij} - Y_{ij}||^2 + \\frac{\\lambda_1}{2}||W||^2 + \\frac{\\lambda_2}{2}||Z||^2\\)\nThis accounts for the missing values and the regularization terms prevent overfitting (representations are not too complex)\nThis improved the RMSE score bby 7% in the Netflix competition\nOptimize using SGD (stoachastic gradient descent) and WALS (weighted alternating least squares)\n\n\n\nOther Notes:\n\nResult can be outside the range of the ratings\nWill have problems with cold start (new users or items)\n\n\n\n\n\n\n\\(Z\\) is no longer the points in the new hyperplane and \\(W\\) is no longer the weights\n\\(Z\\):\n\nEach row is a user\nMaps users to latent feature of items\n\n\\(W\\):\n\nEach col is an item\nMaps items to latent feature of users\n\n\n\n\n\n\nhttps://surprise.readthedocs.io/en/stable/index.html\n\nimport surprise\nfrom surprise import SVD, Dataset, Reader, accuracy\n\n# Load the data\nreader = Reader()\ndata = Dataset.load_from_df(ratings[['users', 'items', 'ratings']], reader)\n\n# Train-test split\ntrainset, validset = train_test_split(data, test_size=0.2, random_state=42)\n\n# PCA-like model\nk=2\nalgo = SVD(n_factors=k, random_state=42)\nalgo.fit(trainset)\n\n# Predictions\npreds = algo.test(validset.build_testset())\n\n# RMSE\naccuracy.rmse(preds)\n\nCan also cross-validate\n\nfrom surprise.model_selection import cross_validate\n\ncross_validate(algo, data, measures=['RMSE', \"MAE\"], cv=5, verbose=True)"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html#distance-metrics",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html#distance-metrics",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Source: Google ML\n\nCosine:\n\n\\(d(x,y) = \\frac{x \\cdot y}{||x|| \\cdot ||y||}\\)\nCollinear = 1, orthogonal = 0 (want a value close to 1)\nIt is the angle between the two vectors\nRank (high to low): C, A, B\n\nEuclidean:\n\n\\(d(x,y) = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2}\\)\nIt is the straight line distance between the two points (want smaller distance)\nRank (high to low): B, C, A\n\nDot Product:\n\n\\(d(x,y) = x \\cdot y\\)\nIt is the projection of one vector onto the other\nIf vectors are normalized, it is the same as cosine similarity (want larger value)\nRank (high to low): A, B, C"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html#content-based-filtering",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html#content-based-filtering",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Supervised learning\nDoes not make use of social network / information\nSolves the cold start problem (can recommend items to new users/items)\nAssumes that we have features of items and/or users to predict ratings\nCreate a user profile for each user\n\nTreat rating prediction as a regression problem\nHave a regression model for each user\n\n\n\n\n\nLoad ratings_df (contains user_id, movie_id, and rating)\n\nAlso make the user and movie mappers.\n\n\ntoy_ratings = pd.read_csv(\"data/toy_ratings.csv\")\n\nN = len(np.unique(toy_ratings[\"user_id\"]))\nM = len(np.unique(toy_ratings[\"movie_id\"]))\n\nuser_key = \"user_id\" # Name of user\nitem_key = \"movie_id\" # Name of movie\n\n# Turns the name into a number (id)\nuser_mapper = dict(zip(np.unique(toy_ratings[user_key]), list(range(N))))\nitem_mapper = dict(zip(np.unique(toy_ratings[item_key]), list(range(M))))\n\n# Turns the number (id) back into a name\nuser_inverse_mapper = dict(zip(list(range(N)), np.unique(toy_ratings[user_key])))\nitem_inverse_mapper = dict(zip(list(range(M)), np.unique(toy_ratings[item_key])))\n\nLoad movie features. This is a matrix of shape (n_movies, n_features)\n\nIndex of movie features is movie id/name\nFeatures can be genre, director, actors, etc.\n\n\nimport pandas as pd\n\nmovie_feats_df = pd.read_csv(\"data/toy_movie_feats.csv\", index_col=0)\nZ = movie_feats_df.to_numpy()\n\n\nBuild a user profile. For each user, we will get the ratings and the corresponding movie features.\n\nResults in a dictionary (key: user, value: numpy array of size (n_ratings, n_genres))\n\nn_ratings: number of movies rated by the user\n\n\n\nfrom collections import defaultdict\n\ndef get_X_y_per_user(ratings, d=item_feats.shape[1]):\n    \"\"\"\n    Returns X and y for each user.\n\n    Parameters:\n    ----------\n    ratings : pandas.DataFrame\n         ratings data as a dataframe\n\n    d : int\n        number of item features\n\n    Return:\n    ----------\n        dictionaries containing X and y for all users\n    \"\"\"\n    lr_y = defaultdict(list)\n    lr_X = defaultdict(list)\n\n    for index, val in ratings.iterrows():\n        n = user_mapper[val[user_key]]\n        m = item_mapper[val[item_key]]\n        lr_X[n].append(item_feats[m])\n        lr_y[n].append(val[\"rating\"])\n\n    for n in lr_X:\n        lr_X[n] = np.array(lr_X[n])\n        lr_y[n] = np.array(lr_y[n])\n\n    return lr_X, lr_y\n\ndef get_user_profile(user_name):\n  \"\"\"\n  Get the user profile based on the user name\n\n  e.g. get_user_profile(\"user1\")\n  \"\"\"\n    X = X_train_usr[user_mapper[user_name]]\n    y = y_train_usr[user_mapper[user_name]]\n    items = rated_items[user_mapper[user_name]]\n    movie_names = [item_inverse_mapper[item] for item in items]\n    print(\"Profile for user: \", user_name)\n    profile_df = pd.DataFrame(X, columns=movie_feats_df.columns, index=movie_names)\n    profile_df[\"ratings\"] = y\n    return profile_df\n# Using the helper functions\nXt,yt = get_X_y_per_user(X_train)\nXv,yv = get_X_y_per_user(X_valid)\n\n# Check the user profile\nget_user_profile(\"Nando\")\n\nSupervised learning. Train a regression model for each user.\n\nWe will use Ridge regression model for this example.\n\n\nfrom sklearn.linear_model import Ridge\n\nmodels = dict()\n# Make utility matrix\npred_lin_reg = np.zeros((N, M))\n\nfor n in range(N):\n  models[n] = Ridge()\n  models[n].fit(Xt[n], yt[n])\n  pred_lin_reg[n] = models[n].predict(item_feats)"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html#collaborative-vs-content-based-filtering",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html#collaborative-vs-content-based-filtering",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Collaborative Filtering\nContent Based Filtering\n\n\n\n\n\\(\\hat{y}_{ij} = w_j^T z_i\\)\n\\(\\hat{y}_{ij} = w_i^T x_{ij}\\)\n\n\n\\(w_j^T\\): “hidden” embedding for feature \\(j\\)  \\(z_i\\): “hidden” embedding for user \\(i\\)\n\\(w_i\\): feature vector for user \\(i\\)  \\(x_{ij}\\): feature \\(j\\) for user \\(i\\)\n\n\n(+) Makes use of social network / information\n(-) Does not make use of social network / information\n\n\n(-) Cold start problem\n(+) Solves the cold start problem\n\n\n(+) No feature engineering required\n(-) Requires feature engineering\n\n\n(-) Hard to interpret\n(+) Easy to interpret\n\n\n(-) Cannot capture unique user preferences\n(+) Can capture unique user preferences (since each model is unique)\n\n\n(+) More diverse recommendations\n(-) Less diverse recommendations (hardly recommend an item outside the user’s profile)"
  },
  {
    "objectID": "block_5/563_unsup_learn/563_unsup_learn.html#beyond-error-rate-in-recommender-systems",
    "href": "block_5/563_unsup_learn/563_unsup_learn.html#beyond-error-rate-in-recommender-systems",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Best RMSE \\(\\neq\\) Best Recommender\nNeed to consider simplicity, interpretatibility, code maintainability, etc.\n\nThe Netflix Prize: The winning solution was never implemented\n\nOther things to consider:\n\nDiversity: If someone buys a tennis racket, they might not want to buy another tennis racket\nFreshness: New items (new items need to be recommended for it to be successful)\nTrust: Explain your recommendation to the user\nPersistence: If the same recommendation is made over and over again, it might not be a good recommendation\nSocial influence: If a friend buys something, you might want to buy it too\n\nAlso need to consider ethical implications\n\nFilter bubbles\nRecommending harmful content"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html",
    "href": "block_2/531_viz/531_viz.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Easier to understand data when it is visualized\nAnscombe’s quartet: 4 datasets with the same mean, variance, correlation, and linear regression line but look very different when plotted\nWe are using high-level declarative (altair, ggplot) instead of low-level imperative (matplotlib) \n\n\n\n\n\ngrammar: alt.Chart(data).mark_type().encode(x,y).properties()\nexample gallery: https://altair-viz.github.io/gallery/index.html\n\nimport altair as alt\n\n# to unlimit the number of rows displayed (5000 by default)\nalt.data_transformers.enable('vegafusion')\n\nalt.Chart(cars, title=\"My plot title\").mark_line(opacity=0.6).encode(\n    x=alt.X(\"Year:T\"), # encoders\n    y=alt.Y(\"mean(Miles_per_Gallon)\"),  # same as doing `y=cars.groupby('Year')['Miles_per_Gallon'].mean()`\n    color=alt.Color(\"Origin\"),\n).facet(\n    \"region\",\n    columns=2, # number of columns in the facet\n)\n\n\n\n\n\n\n\n\n\n\n\nMark Type\nDescription\nExample Use Case\n\n\n\n\nmark_area\nFilled area plots.\nDisplay quantitative data over an interval.\n\n\nmark_bar\nBar charts.\nCompare discrete categories or time intervals.\n\n\nmark_circle, mark_square, mark_geoshape\nGeometric shape marks.\nVisualize point data in 2D space (circle, square) or geographic regions and shapes (geoshape).\n\n\nmark_line\nLine charts.\nShow trends over a continuous domain.\n\n\nmark_point\nScatter plots with customizable point shapes.\nVisualize point data with various symbols.\n\n\nmark_rect\nRectangular marks, used in heatmaps.\nDisplay data in 2D grid-like structures.\n\n\nmark_boxplot\nBoxplots.\nSummarize data distribution with quartiles.\n\n\n\n\nno violin plot but can do mark_circle with y and color the same variable and size as count().\npandas explode df.explode() to explode a column of lists into multiple rows\n\n\n\n\nalt.X(\"Year:T\") is the same as alt.X(\"Year\", type=\"temporal\")\ncan do alt.X().scale() to change the scale of the axis\n\n.stack() to stack the axis\n.scale() to change the scale\n\n.scale(type=\"log\") to change to log scale\n.scale(range=(0, 100)) to change the domain\n.scale(zero=False) to not include 0 in the domain\n\n.bin() to bin the axis, makes histogram if used on mark_bar()\n\ndefault: stack=True\ntakes in binwidth or maxbins\n\n.sort() to sort the axis\n\n.sort(“x”) to sort by x (ascending)\n.sort(“-x”) to sort by x (descending)\n\n.title(\"\"): change the title of the axis\n\n\n\n\n\n\n\n\nData Type\nShorthand Code\nDescription\n\n\n\n\nquantitative\nQ\na continuous real-valued quantity\n\n\nordinal\nO\na discrete ordered quantity\n\n\nnominal\nN\na discrete unordered category\n\n\ntemporal\nT\na time or date value\n\n\ngeojson\nG\na geographic shape\n\n\n\nhttps://altair-viz.github.io/user_guide/encodings/index.html#encoding-data-types\n\n\n\nalt.Chart(movies).mark_point(opacity=0.3, size=10).encode(\n     alt.X(alt.repeat('row')).type('quantitative'),\n     alt.Y(alt.repeat('column')).type('quantitative')\n).properties(\n    width=200,\n    height=200\n).repeat(\n    column=['runtime', 'revenue', 'budget'],\n    row=['runtime', 'revenue', 'budget']\n)\n\n\n\ncorr_df = (\n    movies\n    .corr('spearman', numeric_only=True)\n    .abs()                      # Use abs for negative correlation to stand out\n    .stack()                    # Get df into long format for altair\n    .reset_index(name='corr'))  # Name the index that is reset to avoid name collision\n\nalt.Chart(corr_df).mark_circle().encode(\n    x='level_0',\n    y='level_1',\n    size='corr',\n    color='corr'\n)\n\n\n\n\n\ngrammar: ggplot(data, aes(x, y)) + geom_type() + facet_wrap() + theme()\n\nread_csv(url) |&gt;\n    ggplot(aes(x = Year, y = Miles_per_Gallon, color = Origin)) +\n    geom_line(stat = \"summary\", fun = mean, alpha = 0.6) +\n    # stat_summary(geom = \"line\", fun = mean) # alternative way of doing the same thing\n    facet_wrap(~ region, ncol = 2) +\n    # properties\n    ggtitle(\"My plot title\") +\n    labs(x = \"Year\", y = \"Miles per Gallon\")\n\nin ggplot, fill and color are different\n\nfill is the color of the inside of the shape (bars, area, etc.)\ncolor is the color of the outline of the shape (points, lines, etc.)\n\n\n\n\n\n\n\n\n\n\n\n\nGeom Function\nDescription\nExample Use Case\n\n\n\n\ngeom_area\nFilled area plots.\nDisplay quantitative data over an interval.\n\n\ngeom_bar & geom_col\nBar charts.\nCompare discrete categories. geom_col is a special case of geom_bar where height is pre-determined.\n\n\ngeom_point, geom_tile, geom_polygon\nGeometric shape marks.\ngeom_point for scatter plots, geom_tile for heatmaps, and geom_polygon for custom shapes.\n\n\ngeom_line\nLine charts.\nShow trends over a continuous domain.\n\n\ngeom_smooth\nAdds a smoothed conditional mean.\nFit and display a trend line for scattered data.\n\n\ngeom_boxplot\nBoxplots.\nSummarize data distribution with quartiles.\n\n\ngeom_histogram\nHistograms.\nDisplay frequency distributions.\n\n\ngeom_text & geom_label\nDisplay text and labels.\nAnnotate plots with text or labeled rectangles.\n\n\ngeom_jitter\nPoints with a small amount of random noise.\nDisplay individual data points without overlap.\n\n\ngeom_path\nConnect observations in the order they appear.\nDisplay paths or trajectories.\n\n\ngeom_violin\nViolin plots.\nCombine boxplot and kernel density estimation.\n\n\ngeom_rug\nMarginal rug plots.\nDisplay 1D data distribution on plot margins.\n\n\n\n\n\n\n\nreorder() to reorder the axis\n\ngm2018 %&gt;%\n    add_count(region) %&gt;%\n    ggplot(aes(y = reorder(region, n))) + # y is region, reorder by its count (n)\n    geom_bar()\n\n\n\n\nRepeated Charts\n\nlibrary(GGally)\nGGally::ggpairs(movies %&gt;% select_if(is.numeric), progress = FALSE)\n\nCorrelation plot\n\nGGally::ggcorr(movies)\n\n\n\n\n\nFaceting displays groups based on a dataset variable in separate subplots.\nIt’s useful for splitting data over additional categorical variables.\nAvoids overloading a single chart with too much information.\nUsing color groupings in a histogram can obscure differences between groups.\nFaceting each group in separate subplots clarifies distribution comparisons.\n\n\n\n\n\n\n\n\n\n\n\nshows only 1 value of distribution, can show 3 with error bars:\n\nDot plots\nBar plots\n\n\n\n\n\nbox plots: normally shows 5 values (min, max, median, 25th percentile, 75th percentile)\nHistograms\n\ncons: binning required and distribution sensitive to bin settings\n\nKDE\n\nshows density of data points\ngives a smooth curve because uses “gaussian” (default) centered at each data point\nNo binning required (not sensitive to bin width or bin location)\nless affected by sparse data\ncons: y-axis is density, not count (hard to interpret)\n\nViolin plots: mirror of density estimates (KDE)\n\nmore would show higher density, unlike box plots (smaller box plots would show higher density)\ncons: over-smoothing of data (when too little data points)\n\n\n\n\n\n\nscatter plots: shows all data points (can overlap and saturate)\njitter plots: scatter plots with jittering over a small width in y-axis\nraincloud plots: violin plots with jittering\n\n\n\n\n\n\n\nalt.Chart(df).transform_density('x', as_=['x', 'density']).mark_area().encode(x='x:Q', y='density:Q')\nalt.Chart(df).explode('col_name'): explode a column (of list normally) into multiple rows"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#intro-to-data-visualization",
    "href": "block_2/531_viz/531_viz.html#intro-to-data-visualization",
    "title": "Data Visualization",
    "section": "",
    "text": "Easier to understand data when it is visualized\nAnscombe’s quartet: 4 datasets with the same mean, variance, correlation, and linear regression line but look very different when plotted\nWe are using high-level declarative (altair, ggplot) instead of low-level imperative (matplotlib)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#plotting-in-python-altair",
    "href": "block_2/531_viz/531_viz.html#plotting-in-python-altair",
    "title": "Data Visualization",
    "section": "",
    "text": "grammar: alt.Chart(data).mark_type().encode(x,y).properties()\nexample gallery: https://altair-viz.github.io/gallery/index.html\n\nimport altair as alt\n\n# to unlimit the number of rows displayed (5000 by default)\nalt.data_transformers.enable('vegafusion')\n\nalt.Chart(cars, title=\"My plot title\").mark_line(opacity=0.6).encode(\n    x=alt.X(\"Year:T\"), # encoders\n    y=alt.Y(\"mean(Miles_per_Gallon)\"),  # same as doing `y=cars.groupby('Year')['Miles_per_Gallon'].mean()`\n    color=alt.Color(\"Origin\"),\n).facet(\n    \"region\",\n    columns=2, # number of columns in the facet\n)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#different-mark_types",
    "href": "block_2/531_viz/531_viz.html#different-mark_types",
    "title": "Data Visualization",
    "section": "",
    "text": "Mark Type\nDescription\nExample Use Case\n\n\n\n\nmark_area\nFilled area plots.\nDisplay quantitative data over an interval.\n\n\nmark_bar\nBar charts.\nCompare discrete categories or time intervals.\n\n\nmark_circle, mark_square, mark_geoshape\nGeometric shape marks.\nVisualize point data in 2D space (circle, square) or geographic regions and shapes (geoshape).\n\n\nmark_line\nLine charts.\nShow trends over a continuous domain.\n\n\nmark_point\nScatter plots with customizable point shapes.\nVisualize point data with various symbols.\n\n\nmark_rect\nRectangular marks, used in heatmaps.\nDisplay data in 2D grid-like structures.\n\n\nmark_boxplot\nBoxplots.\nSummarize data distribution with quartiles.\n\n\n\n\nno violin plot but can do mark_circle with y and color the same variable and size as count().\npandas explode df.explode() to explode a column of lists into multiple rows\n\n\n\n\nalt.X(\"Year:T\") is the same as alt.X(\"Year\", type=\"temporal\")\ncan do alt.X().scale() to change the scale of the axis\n\n.stack() to stack the axis\n.scale() to change the scale\n\n.scale(type=\"log\") to change to log scale\n.scale(range=(0, 100)) to change the domain\n.scale(zero=False) to not include 0 in the domain\n\n.bin() to bin the axis, makes histogram if used on mark_bar()\n\ndefault: stack=True\ntakes in binwidth or maxbins\n\n.sort() to sort the axis\n\n.sort(“x”) to sort by x (ascending)\n.sort(“-x”) to sort by x (descending)\n\n.title(\"\"): change the title of the axis\n\n\n\n\n\n\n\n\nData Type\nShorthand Code\nDescription\n\n\n\n\nquantitative\nQ\na continuous real-valued quantity\n\n\nordinal\nO\na discrete ordered quantity\n\n\nnominal\nN\na discrete unordered category\n\n\ntemporal\nT\na time or date value\n\n\ngeojson\nG\na geographic shape\n\n\n\nhttps://altair-viz.github.io/user_guide/encodings/index.html#encoding-data-types\n\n\n\nalt.Chart(movies).mark_point(opacity=0.3, size=10).encode(\n     alt.X(alt.repeat('row')).type('quantitative'),\n     alt.Y(alt.repeat('column')).type('quantitative')\n).properties(\n    width=200,\n    height=200\n).repeat(\n    column=['runtime', 'revenue', 'budget'],\n    row=['runtime', 'revenue', 'budget']\n)\n\n\n\ncorr_df = (\n    movies\n    .corr('spearman', numeric_only=True)\n    .abs()                      # Use abs for negative correlation to stand out\n    .stack()                    # Get df into long format for altair\n    .reset_index(name='corr'))  # Name the index that is reset to avoid name collision\n\nalt.Chart(corr_df).mark_circle().encode(\n    x='level_0',\n    y='level_1',\n    size='corr',\n    color='corr'\n)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#plotting-in-r-ggplot2",
    "href": "block_2/531_viz/531_viz.html#plotting-in-r-ggplot2",
    "title": "Data Visualization",
    "section": "",
    "text": "grammar: ggplot(data, aes(x, y)) + geom_type() + facet_wrap() + theme()\n\nread_csv(url) |&gt;\n    ggplot(aes(x = Year, y = Miles_per_Gallon, color = Origin)) +\n    geom_line(stat = \"summary\", fun = mean, alpha = 0.6) +\n    # stat_summary(geom = \"line\", fun = mean) # alternative way of doing the same thing\n    facet_wrap(~ region, ncol = 2) +\n    # properties\n    ggtitle(\"My plot title\") +\n    labs(x = \"Year\", y = \"Miles per Gallon\")\n\nin ggplot, fill and color are different\n\nfill is the color of the inside of the shape (bars, area, etc.)\ncolor is the color of the outline of the shape (points, lines, etc.)\n\n\n\n\n\n\n\n\n\n\n\n\nGeom Function\nDescription\nExample Use Case\n\n\n\n\ngeom_area\nFilled area plots.\nDisplay quantitative data over an interval.\n\n\ngeom_bar & geom_col\nBar charts.\nCompare discrete categories. geom_col is a special case of geom_bar where height is pre-determined.\n\n\ngeom_point, geom_tile, geom_polygon\nGeometric shape marks.\ngeom_point for scatter plots, geom_tile for heatmaps, and geom_polygon for custom shapes.\n\n\ngeom_line\nLine charts.\nShow trends over a continuous domain.\n\n\ngeom_smooth\nAdds a smoothed conditional mean.\nFit and display a trend line for scattered data.\n\n\ngeom_boxplot\nBoxplots.\nSummarize data distribution with quartiles.\n\n\ngeom_histogram\nHistograms.\nDisplay frequency distributions.\n\n\ngeom_text & geom_label\nDisplay text and labels.\nAnnotate plots with text or labeled rectangles.\n\n\ngeom_jitter\nPoints with a small amount of random noise.\nDisplay individual data points without overlap.\n\n\ngeom_path\nConnect observations in the order they appear.\nDisplay paths or trajectories.\n\n\ngeom_violin\nViolin plots.\nCombine boxplot and kernel density estimation.\n\n\ngeom_rug\nMarginal rug plots.\nDisplay 1D data distribution on plot margins.\n\n\n\n\n\n\n\nreorder() to reorder the axis\n\ngm2018 %&gt;%\n    add_count(region) %&gt;%\n    ggplot(aes(y = reorder(region, n))) + # y is region, reorder by its count (n)\n    geom_bar()\n\n\n\n\nRepeated Charts\n\nlibrary(GGally)\nGGally::ggpairs(movies %&gt;% select_if(is.numeric), progress = FALSE)\n\nCorrelation plot\n\nGGally::ggcorr(movies)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#faceting",
    "href": "block_2/531_viz/531_viz.html#faceting",
    "title": "Data Visualization",
    "section": "",
    "text": "Faceting displays groups based on a dataset variable in separate subplots.\nIt’s useful for splitting data over additional categorical variables.\nAvoids overloading a single chart with too much information.\nUsing color groupings in a histogram can obscure differences between groups.\nFaceting each group in separate subplots clarifies distribution comparisons."
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#most-basic-fast-to-make",
    "href": "block_2/531_viz/531_viz.html#most-basic-fast-to-make",
    "title": "Data Visualization",
    "section": "",
    "text": "shows only 1 value of distribution, can show 3 with error bars:\n\nDot plots\nBar plots"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#a-bit-better",
    "href": "block_2/531_viz/531_viz.html#a-bit-better",
    "title": "Data Visualization",
    "section": "",
    "text": "box plots: normally shows 5 values (min, max, median, 25th percentile, 75th percentile)\nHistograms\n\ncons: binning required and distribution sensitive to bin settings\n\nKDE\n\nshows density of data points\ngives a smooth curve because uses “gaussian” (default) centered at each data point\nNo binning required (not sensitive to bin width or bin location)\nless affected by sparse data\ncons: y-axis is density, not count (hard to interpret)\n\nViolin plots: mirror of density estimates (KDE)\n\nmore would show higher density, unlike box plots (smaller box plots would show higher density)\ncons: over-smoothing of data (when too little data points)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#showing-all-data",
    "href": "block_2/531_viz/531_viz.html#showing-all-data",
    "title": "Data Visualization",
    "section": "",
    "text": "scatter plots: shows all data points (can overlap and saturate)\njitter plots: scatter plots with jittering over a small width in y-axis\nraincloud plots: violin plots with jittering"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#plotting-density",
    "href": "block_2/531_viz/531_viz.html#plotting-density",
    "title": "Data Visualization",
    "section": "",
    "text": "alt.Chart(df).transform_density('x', as_=['x', 'density']).mark_area().encode(x='x:Q', y='density:Q')\nalt.Chart(df).explode('col_name'): explode a column (of list normally) into multiple rows"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#some-notes",
    "href": "block_2/531_viz/531_viz.html#some-notes",
    "title": "Data Visualization",
    "section": "Some notes",
    "text": "Some notes\n\nsame data can give different messages\nkeep same color mapping for same categories across plots\nlabels horizontally &gt; vertically\nPie charts aint it when:\n\ncategories &gt; 3\ncomparing &gt; 1 pie charts\n\nDon’t have too many lines in a line chart\n\nunless you gray out the lines and highlight the one you want to show\n\n\n\nOverplotting\n\nToo many points in a plot\nSome solutions:\n\nlower size\nlower opacity (alpha in ggplot)\n\nIf you have too many points, you can use a heatmap\n\nggplot: gromm::geom_bin2d(bins=40) or gromm::geom_hex(bins=40)\n\nhex is preferred because better granularity\n\n\nmatch group colours in different charts\n\nggplot(diamonds) +\n    aes(x=carat, y=price) +\n    geom_hex(bins=40)\n\naltair:\n\nalt.Chart(diamonds).mark_rect().encode(\n  alt.X('carat', bin=alt.Bin(maxbins=40)),\n  alt.Y('price', binsalt.Bin(maxbins=40)),\n  alt.Color('count()'))\n\n\nAxes Formatting\n\nPython Altair:\n\n# clip=True to remove points outside of the plot axis\nalt.Chart(diamonds,\n        title='Diamonds',\n        subtitle='Price vs. Carat'\n    ).mark_rect(clip=True).encode(\n  alt.X('carat',\n    bin=alt.Bin(maxbins=40),\n    title='Carat', # set the axis title, blank if no title\n    scale=alt.Scale(domain=(0, 3)), # set the axis extent\n    reverse=True # reverse=True to flip the axis (largest value on the left)\n    # axis=None # remove the axis (no labels no titles)\n    ),\n  alt.Y('price',\n    binsalt.Bin(maxbins=40),\n    title='Price',\n    scale=alt.Scale(domain=(0, 2000)),\n    axis=alt.Axis(format='$s') # set the axis format 1000 -&gt; $1.0k\n    ),\n  alt.Color('count()'))\n\nR ggplot:\n\nggplot(diamonds) +\n    aes(x=carat, y=price) +\n    geom_hex(bins=40) +\n    scale_x_continuous(\n        limits=c(0, 3),\n        expand=c(0, 0)) + # expand=c(0, 0) to remove padding [c(mult, add)] for both sides\n    scale_y_continuous(\n        limits=c(0, 2000),\n        trans=\"reverse\", # to flip the axis\n        labels=scales::label_dollar()) + # to format the axis labels\n    labs(x=\"Carat\", y=\"Price\", fill=\"Number\", # to set the axis labels\n        title=\"Diamonds\", subtitle=\"Price vs. Carat\") + # to set the title and subtitle\n    theme(axis.title.x=element_blank(), # to remove the axis title\n        axis.text.x=element_blank(), # to remove the axis labels\n        axis.ticks.x=element_blank()) +# to remove the axis ticks\n    # theme_void() # to remove everything (no labels no titles)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#trendlines",
    "href": "block_2/531_viz/531_viz.html#trendlines",
    "title": "Data Visualization",
    "section": "Trendlines",
    "text": "Trendlines\n\n(Rolling) Mean: average of a subset of data\n\nto communicate to general public\n\nLinear Regression: line of best fit\n\nfor extrapolation\n\npoints +  points.mark_line(size=3).transform_regression(\n  'Year',\n  'Horsepower',\n  groupby=['Origin'],\n  method='poly', # 'linear' (default), 'log', 'exp', 'pow', 'quad', 'poly'\n)\nLoess: linear regression with a moving window (subset of data)\n\npython:\n\n\npoints +  points.mark_line(size=3).transform_loess(\n    'Year',\n    'Horsepower',\n    groupby=['Origin'],\n    bandwidth=0.8, # 0.3 (default), 1 (max is similar to linear regression)\n)\n\nR:\n\nggplot(cars) +\n    aes(x = Year,\n        y = Horsepower,\n        color = Origin,\n        fill = Origin) + # fill the CI area around the line\n    geom_point() +\n    geom_smooth(se = FALSE,\n        span = 0.8, # similar to bandwidth\n        method = \"loess\") # method = \"lm\" for linear regression"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#some-general-rules",
    "href": "block_2/531_viz/531_viz.html#some-general-rules",
    "title": "Data Visualization",
    "section": "Some general rules",
    "text": "Some general rules\n\nin Altair: `alt.Color(‘species’).scale(scheme=‘dark2’, reverse=True)\nin ggplot: scale_color_brewer(palette='Dark2')\n\n\n\n\n\n\n\n\n\n\nData Type\nVariation\nColor Map Type\nExample\n\n\n\n\nNumerical\nVary value\nSequential (Perceptually uniform)\nNumber of people (0-100)\n\n\nCategorical\nVary hue\nCategorical\nSubject (math, science, english)\n\n\nOrdered\nVary hue and value\nNot specified\nLow, medium, high\n\n\nCyclic\nVary hue and value\nCyclic\nDegrees (0-360) or days of week (0-6)\n\n\nData with natural center\nNot specified\nDiverging\nTemperature (0-100)\n\n\n\n \n \n\nAlso consider intuitive colors\n\n(e.g. red for hot, blue for cold)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#add-annotations",
    "href": "block_2/531_viz/531_viz.html#add-annotations",
    "title": "Data Visualization",
    "section": "Add annotations",
    "text": "Add annotations\n\nin Altair:\n\nbars = alt.Chart(wheat).mark_bar().encode(\n    x='year:O',\n    y=\"wheat\",\n    color=alt.Color('highlight').legend(None)\n)\nbars + bars.mark_text(dy=-5).encode(text='wheat')\n\n# Or for line plot\nlines = alt.Chart(stocks).mark_line().encode(\n    x='date',\n    y='price',\n    color=alt.Color('symbol').legend(None)\n)\n\ntext = alt.Chart(stock_order).mark_text(dx=20).encode(\n    x='date',\n    y='price',\n    text='symbol',\n    color='symbol'\n)\n\nlines + text\n\nin ggplot:\n\nggplot(wheat) +\n    aes(x = year,\n        y = wheat,\n        fill = highlight,\n        label = wheat) +\n    geom_bar(stat = 'identity', color = 'white') +\n    geom_text(vjust=-0.3)\n\n# Or for line plot\nggplot(stocks) +\n    aes(x = date,\n        y = price,\n        color = symbol,\n        label = symbol) +\n    geom_line() +\n    geom_text(data = stock_order, vjust=-1) +\n    ggthemes::scale_color_tableau() +\n    theme(legend.position = 'none')\n\nFrequency Framing\n\npeople normally judge probabilities wrongly\nNormalize the counts and show each individual count"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#error-bars",
    "href": "block_2/531_viz/531_viz.html#error-bars",
    "title": "Data Visualization",
    "section": "Error bars",
    "text": "Error bars\n\nSpecify what kind of error bar\n\nmin, max\nstd dev\nstd error\n95% confidence interval\n\nin Altair:\n\npoints.mark_errorband(extent='ci') # always 95% confidence interval\n\n# good way\nerr_bars = alt.Chart(cars).mark_errorbar(extent='ci', rule=alt.LineConfig(size=2)).encode(\n  x='Horsepower',\n  y='Origin'\n)\n\n(err_bars.mark_tick(color='lightgrey') + # show ticks\nerr_bars + # error bars\nerr_bars.mark_point(color='black').encode(x='mean(Horsepower)')) # mean as a point\n\nIn ggplot: use Hmisc::mean_cl_boot()\n\nggplot(cars) +\n    aes(x = Horsepower,\n        y = Origin) +\n    geom_point(shape = '|', color='grey', size=5) + # show ticks\n    geom_pointrange(stat = 'summary', fun.data = mean_cl_boot, size = 0.7) # error bar + mean as a point\n\n# For line mean and errobar\n... + geom_line(stat = 'summary', fun = mean) + # mean as a line\n    geom_ribbon(stat = 'summary', fun.data = mean_cl_boot, alpha=0.5, color = NA) # error bar as a ribbon"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#statistical-vs-practical-significance",
    "href": "block_2/531_viz/531_viz.html#statistical-vs-practical-significance",
    "title": "Data Visualization",
    "section": "Statistical vs practical significance",
    "text": "Statistical vs practical significance\n\nStatistical significance: is the difference between two groups statistically significant?\nPractical significance: is the difference between two groups practically significant?"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#figure-composition",
    "href": "block_2/531_viz/531_viz.html#figure-composition",
    "title": "Data Visualization",
    "section": "Figure composition",
    "text": "Figure composition\n\nPython\n\nvertically: plot1 & plot2 or alt.vconcat(plot1, plot2)\nhorizontally: plot1 | plot2 or alt.hconcat(plot1, plot2)\nadd title: (plot1 | plot2).properties(title='title')\n\n\n\nR\n\nuse package patchwork\nvertically: plot_grid(plot1, plot2, ncol=1)\nhorizontally: plot_grid(plot1, plot2, nrow=1)\nAdd labels: plot_grid(plot1, plot2, labels=c('A', 'B'))\nSet width: plot_grid(plot1, plot2, rel_widths=c(1, 2))"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#interactive-visualization",
    "href": "block_2/531_viz/531_viz.html#interactive-visualization",
    "title": "Data Visualization",
    "section": "Interactive visualization",
    "text": "Interactive visualization\n\nPanning and zooming\n\nAdd .interactive() to the end of the chart\n\nalt.Chart(cars).mark_circle().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n).interactive()\n\n\nDetails on demand\n\nAdd a tooltip to show details on demand\n\nalt.Chart(cars).mark_circle().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    tooltip=['Name', 'Origin']\n)\n\n\nInterval selection\n\nUse alt.selection_interval() to select a range of data points\n\nformat: alt.condition(check, if_true, if_false)\n\n\nbrush = alt.selection_interval(\n    encodings=['x'], # only select x axis, default is both x and y\n    resolve='union' # default is 'global', which means all charts are linked\n  )\n\npoints = alt.Chart(cars).mark_circle().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    # Use alt.condition to do a selection\n    color=alt.condition(brush, 'Origin', alt.value('lightgray'))\n).add_params(\n    brush\n)\n\n# linking different plots\npoints | points.encode(x='Acceleration')\n\n\nClick selection\n\ndefault: on='click'\n\nclick = alt.selection_point(fields=['Origin'], on='mouseover', bind='legend')\n\nbars = alt.Chart(cars).mark_bar().encode(\n    x='count()',\n    y='Origin',\n    color='Origin',\n    opacity=alt.condition(click, alt.value(0.9), alt.value(0.2))\n).add_params(\n    click\n)\n\n\nFiltering data based on selection\n\nuse transform_filter to filter data based on selection\n\nbrush = alt.selection_interval()\nclick = alt.selection_point(fields=['Origin'], bind='legend')\n\npoints = alt.Chart(cars).mark_circle().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    color=alt.condition(brush, 'Origin', alt.value('lightgray')),\n    opacity=alt.condition(click, alt.value(0.9), alt.value(0.2))\n).add_params(\n    brush\n)\n\nbars = alt.Chart(cars).mark_bar().encode(\n    x='count()',\n    y='Origin',\n    color='Origin',\n    opacity=alt.condition(click, alt.value(0.9), alt.value(0.2))\n).transform_filter( # changes bar plot based on selection of points\n    brush\n)\n\n(points & bars).add_params(click)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#pairwise-comparisons",
    "href": "block_2/531_viz/531_viz.html#pairwise-comparisons",
    "title": "Data Visualization",
    "section": "Pairwise comparisons",
    "text": "Pairwise comparisons\n\nPython:\n\npoints = alt.Chart(scores_this_year).mark_circle(size=50, color='black', opacity=1).encode(\n    alt.X('score_type'),\n    alt.Y('score'),\n    alt.Detail('time')).properties(width=300)\npoints.mark_line(size=1.8, opacity=0.8\n    ).encode(\n        alt.Color('diff',\n        scale=alt.Scale(scheme='blueorange', domain=(-6, 6))) # diverging colormap\n        # scale(range=['coral', 'green', 'steelblue']) # for categorial [negative, neutral, positive]\n    ) +\n    points\n\nR:\n\nggplot(scores_this_year) +\n    aes(x = score_type,\n        y = score,\n       group = time) +\n    geom_line(aes(color = self_belief), size = 0.8) +\n    geom_point(size=3) + labs(x='') +\n    # colour the lines by diverging colormap (sometimes not a good idea)\n    scale_color_distiller(palette = 'PuOr', limits = c(-5, 5))"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html",
    "href": "block_2/552_stat_inter/552_stat_inter.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Term\nDefinition\n\n\n\n\npoint_estimate\nA summary statistic calculated from a random sample that estimates an unknown population parameter of interest.\n\n\npopulation\nThe entire set of entities objects of interest.\n\n\npopulation_parameter\nA numerical summary value about the population.\n\n\nsample\nA collected subset of observations from a population.\n\n\nobservation\nA quantity or quality (or a set of these) from a single member of a population.\n\n\nsampling_distribution\nA distribution of point estimates, where each point estimate was calculated from a different random sample coming from the same population.\n\n\n\n\nTrue population parameter is denoted with Greek letters. (e.g. μ for mean)\nEstimated population parameter is denoted with Greek letters with a hat. (e.g. μ̂ for mean)\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nDescriptive\nSummarizes a characteristic of a set of data without interpretation.\n- What is the frequency of bacterial illnesses in a data set?  - How many people live in each US state?\n\n\nExploratory\nAnalyzes data to identify patterns, trends, or relationships between variables.\n- Do certain diets correlate with bacterial illnesses?  - Does air pollution correlate with life expectancy in different US regions?\n\n\nInferential\nAnalyzes patterns, trends, or relationships in a representative sample to quantify applicability to the whole population. Estimation with associated randomness.\n- Is eating 5 servings of fruits and vegetables associated with fewer bacterial illnesses?  - Is gestational length different for first born babies?\n\n\nPredictive\nAims to predict measurements or labels for individuals. Not focused on causes but on predictions.\n- How many viral illnesses will someone have next year?  - What political party will someone vote for in the next US election?\n\n\nCausal\nInquires if changing one factor will change another factor in a population. Sometimes design allows for causal interpretation.\n- Does eating 5 servings of fruits and vegetables cause fewer bacterial illnesses?  - Does smoking lead to cancer?\n\n\nMechanistic\nSeeks to explain the underlying mechanism of observed patterns or relationships.\n- How do changes in diet reduce bacterial illnesses?  - How does airplane wing design affect air flow and decrease drag?\n\n\n\n\n\n\nDefine the population of interest.\nSelect the right sampling method according to the specific characteristics of our population of interest.\nSelect our sample size (Power Analysis).\nCollect the sampled data.\nMeasure and calculate the sample statistic.\nInfer the population value based on this sample statistic while accounting for sampling uncertainty.\n\n\n\n\n\n\n\n\nPopulation\nSample\n\n\n\n\n\\(\\pi_E\\): population proportion\n\\(\\hat{\\pi}_E\\): sample proportion\n\n\n\\(\\mu\\): population mean\n\\(\\bar{\\mu}\\): sample mean\n\n\n\nPopulation Distribution:\n\nThe sample distribution is of a similar shape to the population distribution.\nThe sample point estimates are identical to the values for the true population parameter we are trying to estimate.\n\nSample Distribution of 1 Sample:\n\nTaking a random sample and calculating a point estimate is a “good guess” of the unknown population parameter you are interested in.\nAs the sample size increases:\n\nthe sampling distribution becomes narrower.\nmore sample point estimates are closer to the true population mean.\nthe sampling distribution appears more bell-shaped.\n\n\nSampling Distribution of Sample Means:\n\nThe sampling distribution is centered at the true population mean.\nMost sample means are at or very near the same value as the true population mean.\nThe sample distribution (if representative) is an estimate of the population distribution.\nThe sampling distribution of the sample means is not necessarily the same shape as the distribution of the population distribution and tends to be more symmetrical and bell-shaped.\n\n\n\n\nset.seed(1) # for reproducibility\nsample1 &lt;- rep_sample_n(df, size = 100, reps = 10000, replace = TRUE)\n\ndefault is sampling with replacement, but can be changed with replace = FALSE\nsize is the number of samples to draw\nrep is the number of times to repeat the sampling process\n\n\n\n\npop_dist &lt;- multi_family_strata %&gt;%\n  ggplot(aes(x = current_land_value)) +\n  geom_histogram(bins = 50) +\n  xlab(\"current land value\") +\n  ggtitle(\"Population distribution\")"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#terms-and-definitions",
    "href": "block_2/552_stat_inter/552_stat_inter.html#terms-and-definitions",
    "title": "Statistical Inference",
    "section": "",
    "text": "Term\nDefinition\n\n\n\n\npoint_estimate\nA summary statistic calculated from a random sample that estimates an unknown population parameter of interest.\n\n\npopulation\nThe entire set of entities objects of interest.\n\n\npopulation_parameter\nA numerical summary value about the population.\n\n\nsample\nA collected subset of observations from a population.\n\n\nobservation\nA quantity or quality (or a set of these) from a single member of a population.\n\n\nsampling_distribution\nA distribution of point estimates, where each point estimate was calculated from a different random sample coming from the same population.\n\n\n\n\nTrue population parameter is denoted with Greek letters. (e.g. μ for mean)\nEstimated population parameter is denoted with Greek letters with a hat. (e.g. μ̂ for mean)"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#types-of-statistical-questions",
    "href": "block_2/552_stat_inter/552_stat_inter.html#types-of-statistical-questions",
    "title": "Statistical Inference",
    "section": "",
    "text": "Type\nDescription\nExample\n\n\n\n\nDescriptive\nSummarizes a characteristic of a set of data without interpretation.\n- What is the frequency of bacterial illnesses in a data set?  - How many people live in each US state?\n\n\nExploratory\nAnalyzes data to identify patterns, trends, or relationships between variables.\n- Do certain diets correlate with bacterial illnesses?  - Does air pollution correlate with life expectancy in different US regions?\n\n\nInferential\nAnalyzes patterns, trends, or relationships in a representative sample to quantify applicability to the whole population. Estimation with associated randomness.\n- Is eating 5 servings of fruits and vegetables associated with fewer bacterial illnesses?  - Is gestational length different for first born babies?\n\n\nPredictive\nAims to predict measurements or labels for individuals. Not focused on causes but on predictions.\n- How many viral illnesses will someone have next year?  - What political party will someone vote for in the next US election?\n\n\nCausal\nInquires if changing one factor will change another factor in a population. Sometimes design allows for causal interpretation.\n- Does eating 5 servings of fruits and vegetables cause fewer bacterial illnesses?  - Does smoking lead to cancer?\n\n\nMechanistic\nSeeks to explain the underlying mechanism of observed patterns or relationships.\n- How do changes in diet reduce bacterial illnesses?  - How does airplane wing design affect air flow and decrease drag?\n\n\n\n\n\n\nDefine the population of interest.\nSelect the right sampling method according to the specific characteristics of our population of interest.\nSelect our sample size (Power Analysis).\nCollect the sampled data.\nMeasure and calculate the sample statistic.\nInfer the population value based on this sample statistic while accounting for sampling uncertainty."
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#sampling",
    "href": "block_2/552_stat_inter/552_stat_inter.html#sampling",
    "title": "Statistical Inference",
    "section": "",
    "text": "Population\nSample\n\n\n\n\n\\(\\pi_E\\): population proportion\n\\(\\hat{\\pi}_E\\): sample proportion\n\n\n\\(\\mu\\): population mean\n\\(\\bar{\\mu}\\): sample mean\n\n\n\nPopulation Distribution:\n\nThe sample distribution is of a similar shape to the population distribution.\nThe sample point estimates are identical to the values for the true population parameter we are trying to estimate.\n\nSample Distribution of 1 Sample:\n\nTaking a random sample and calculating a point estimate is a “good guess” of the unknown population parameter you are interested in.\nAs the sample size increases:\n\nthe sampling distribution becomes narrower.\nmore sample point estimates are closer to the true population mean.\nthe sampling distribution appears more bell-shaped.\n\n\nSampling Distribution of Sample Means:\n\nThe sampling distribution is centered at the true population mean.\nMost sample means are at or very near the same value as the true population mean.\nThe sample distribution (if representative) is an estimate of the population distribution.\nThe sampling distribution of the sample means is not necessarily the same shape as the distribution of the population distribution and tends to be more symmetrical and bell-shaped."
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#drawing-samples-in-r-rsamplerep_sample_n",
    "href": "block_2/552_stat_inter/552_stat_inter.html#drawing-samples-in-r-rsamplerep_sample_n",
    "title": "Statistical Inference",
    "section": "",
    "text": "set.seed(1) # for reproducibility\nsample1 &lt;- rep_sample_n(df, size = 100, reps = 10000, replace = TRUE)\n\ndefault is sampling with replacement, but can be changed with replace = FALSE\nsize is the number of samples to draw\nrep is the number of times to repeat the sampling process"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#plotting-histograms-in-r",
    "href": "block_2/552_stat_inter/552_stat_inter.html#plotting-histograms-in-r",
    "title": "Statistical Inference",
    "section": "",
    "text": "pop_dist &lt;- multi_family_strata %&gt;%\n  ggplot(aes(x = current_land_value)) +\n  geom_histogram(bins = 50) +\n  xlab(\"current land value\") +\n  ggtitle(\"Population distribution\")"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#more-on-bootstrapping",
    "href": "block_2/552_stat_inter/552_stat_inter.html#more-on-bootstrapping",
    "title": "Statistical Inference",
    "section": "More on Bootstrapping",
    "text": "More on Bootstrapping\n\nMean of the bootstrap sample is an estimate of the sample mean not the population mean. (unlike the sampling distribution of the sample mean)\nSpread of bootstrap sample is of similar shape to the sampling distribution of the sample mean.\n\nThis is because we used a bootstrap sample size that was the same as the original sample size.\nIf sample size is larger than original sample: underestimate the spread.\nThis is because the empirical sample distribution is an estimate of the population distribution."
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#implementation-in-r",
    "href": "block_2/552_stat_inter/552_stat_inter.html#implementation-in-r",
    "title": "Statistical Inference",
    "section": "Implementation in R",
    "text": "Implementation in R\nset.seed(2485) # DO NOT CHANGE!\n\n# Take a sample from the population\nsample_1 &lt;- multi_family_strata |&gt;\n    rep_sample_n(size = 10) |&gt;\n\nbootstrap_means_10 &lt;- sample_1 |&gt;\n    ungroup() |&gt; # Remove grouping\n    # Removes the replicate column from rep_sample_n\n    select(current_land_value) |&gt; # Only select the column we want\n\n    # Bootstrap from the sample 2000 times\n    rep_sample_n(size = 10, reps = 2000, replace = T) |&gt;\n    # Calculate the mean for each bootstrap sample\n    group_by(replicate) |&gt;\n    summarise(mean_land_value = mean(current_land_value))\n\nbootstrap_means_10"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#calculating-confidence-intervals",
    "href": "block_2/552_stat_inter/552_stat_inter.html#calculating-confidence-intervals",
    "title": "Statistical Inference",
    "section": "Calculating Confidence Intervals",
    "text": "Calculating Confidence Intervals\n\nOne way : use the middle 95% of the distribution of bootstrap sample estimates to determine our endpoints. (2.5th and 97.5th percentiles)\nSpecific to a sample, not a population\nConfidence: yes or no on whether the interval contains the population parameter\nHigher sample size = narrower confidence interval\n\nIncreasing sample size increases the precision of our estimate\n\n\n\nWhat does 95% confidence Interval mean?\n\nIf we were to repeat this process over and over again and calculate the 95% CI many times,\n\n95% of the time expect true population parameter to be in the confidence interval\n\nThe confidence level is the percentage of time that the interval will contain the true population parameter if we were to repeat the process over and over again.\nOther confidence levels: 90%, 99%\n\nhigher confidence level, wider the interval = more likely to contain the population parameter - higher for use cases where we need to be more confident that the interval contains the population parameter (e.g. medical trials)\n\n\nUsing the null distribution, the \\(p\\)-value is the area to the right of \\(\\delta^*\\) and to the left of \\(-\\delta^*\\). In other words, the \\(p\\)-value is doubled for the two-tailed test. Conclusion: If we fail to reject the null hypothesis for a one-sided test, we would definitely not be able to reject it for a two-sided test."
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#steps-to-calculate-a-confidence-interval",
    "href": "block_2/552_stat_inter/552_stat_inter.html#steps-to-calculate-a-confidence-interval",
    "title": "Statistical Inference",
    "section": "Steps to calculate a confidence interval",
    "text": "Steps to calculate a confidence interval\n\nCalculate the true population parameter (e.g. mean), normally we don’t know this\nGet a sample from a population of size n\n\nset.seed(552) # For reproducibility.\nsample &lt;- rep_sample_n(listings, size = 40)\n\nLook at the sample and decide on parameter of interest for distribution (e.g. median)\nBootstrap the sample m times (e.g. 10,000 times) with replacement from the existing sample and get the required statistic (e.g. median) for each bootstrap sample.\n\n\nmust be the same size as the original sample\n\nset.seed(552) # For reproducibility.\nbootstrap_estimates &lt;- sample %&gt;%\n  specify(response = room_type, success = \"Entire home/apt\") %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"prop\")\nbootstrap_estimates\n\nGet the decided parameter of interest (e.g., median) for each of the bootstrap samples and make a distribution\nCalculate the confidence interval (e.g. 95%)\n\nuse infer::get_confidence_interval()\nreturns a tibble with the lower and upper bounds of the confidence interval\n\n\nget_confidence_interval(bootstrap_estimates, level = 0.90, type = \"percentile\")"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#fundamentals-of-hypothesis-testing",
    "href": "block_2/552_stat_inter/552_stat_inter.html#fundamentals-of-hypothesis-testing",
    "title": "Statistical Inference",
    "section": "Fundamentals of Hypothesis Testing",
    "text": "Fundamentals of Hypothesis Testing\n\nNull hypothesis: a statement of “no effect” or “no difference”\n\n\\(H_0: p_{control} - p_{variation} = \\delta = 0\\), where:\n\n\\(p_{control}\\): values of the control group\n\\(p_{variation}\\) values from the variation group\n\n\nAlternative hypothesis / \\(H_a\\): a statement of an effect or difference\n\n\\(H_a: p_{control} - p_{variation} = \\delta \\neq 0\\)\nclaim to seek statistical evidence for\n\nalpha: the probability of rejecting the null hypothesis when it is true, typically 0.05\n\nthe probability of a type I error\nthe probability of a false positive\n\\(\\alpha = P(\\text{reject } H_0 \\mid H_0 \\text{ is true})\\) \n\nProvided a strong enough statistical evidence, we can reject the null hypothesis and accept the alternative hypothesis\nObserved test statistic: \\(\\delta^*\\)\n\ne.g. \\(\\delta^* = \\hat{m}_{chinstrap} - \\hat{m}_{adelie}\\)\n\nwhere \\(\\hat{m}_{chinstrap}\\) is the sample mean (estimator) body mass of Chinstrap penguins"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#framework-for-hypothesis-testing-6-steps",
    "href": "block_2/552_stat_inter/552_stat_inter.html#framework-for-hypothesis-testing-6-steps",
    "title": "Statistical Inference",
    "section": "Framework for Hypothesis Testing (6 Steps)",
    "text": "Framework for Hypothesis Testing (6 Steps)\n\nDefine your null and alternative hypotheses.\n\nNull: The mean body mass of Chinstrap and Adelie are the same. \\(\\mu_{Chinstrap} - \\mu_{Adelie} = 0\\)\nAlt: The mean body mass of Chinstrap and Adelie are different. \\(\\mu_{Chinstrap} - \\mu_{Adelie} \\neq 0\\)\n\nwhere \\(\\mu_{Chinstrap}\\) is the population mean body mass of Chinstrap penguins\n\n\nCompute the observed test statistic coming from your original sample.\n\n\\(\\delta^* = \\hat{m}_{chinstrap} - \\hat{m}_{adelie}\\)\n\n\nchinstrap_adelie # data frame with chinstrap and adelie penguins only\n\nchinstrap_adelie_test_stat &lt;- chinstrap_adelie |&gt;\n  specify(formula = body_mass_g ~ species) |&gt;\n  calculate(\n  stat = \"diff in means\",\n  order = c(\"Chinstrap\", \"Adelie\")\n)\n\nSimulate the null hypothesis being true and calculate their corresponding test statistics.\n\ne.g. by randomly shuffling the data =&gt; any observed difference is due to chance\npermutation test: randomly shuffle the data and calculate the test statistic for each permutation (Without replacement)\nwould be a normal distribution about 0 (two-tailed test)\n\n\n# Running permutation test\nchinstrap_adelie_null_distribution &lt;- chinstrap_adelie |&gt;\n  specify(formula = body_mass_g ~ species) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  calculate(\n  stat = \"diff in means\",\n  order = c(\"Chinstrap\", \"Adelie\")\n  )\n\nGenerate the null distribution using these test statistics.\nObserve where the observed test statistic falls in the distribution\n\nif it falls in the extreme 5% of the distribution, we reject the null hypothesis\ni.e. if the p-value is less than \\(\\alpha\\), we reject the null hypothesis\n\nIf \\(\\delta\\) is near the extremes past some threshold defined with a significance level \\(\\alpha\\), we reject the null hypothesis. Otherwise, we fail to reject the null hypothesis."
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#p-value",
    "href": "block_2/552_stat_inter/552_stat_inter.html#p-value",
    "title": "Statistical Inference",
    "section": "P-Value",
    "text": "P-Value\n\nthe probability of observing a test statistic equally or more extreme than the one you observed, given that the null hypothesis is true\nuse get_p_value function from infer R library\n\nchinstrap_adelie_p_value &lt;- chinstrap_adelie_null_distribution|&gt;\n  get_p_value(chinstrap_adelie_test_stat, direction = \"both\")\n\nResults:\n\nReject Null Hypothesis if p-value &lt; \\(\\alpha\\)\nFail to Reject Null Hypothesis if p-value &gt; \\(\\alpha\\)"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#maximum-likelihood-estimation",
    "href": "block_2/552_stat_inter/552_stat_inter.html#maximum-likelihood-estimation",
    "title": "Statistical Inference",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\nDefinition:\n\nUsed to FIND estimators\nIDEA: Given a set of data and a statistical model (assumed pdf), MLE finds the parameter values that make the observed data most probable.\n\n\nSome key ideas:\n\nNeed to make distributional assumptions about the data, to specify the likelihood function\nNeed to consider nature (discrete/continuous) of the data\nGet the parameters that maximize the likelihood function\n\n\n\n\nLikelihood Function:\n\n\\(P(dataset | params) = l(\\lambda | y_1, y_2, ...)\\)\n\nprob mass/density function of the data given the parameters\n\npdf gives density not probability (area under curve == 1)\n\ngenerally: product of individual pdfs because iid\n\n\n\\[\\prod_{i=1}^n f_{Y_i}(y_i| \\lambda)\\]\n\nContext: Used when you have a specific set of outcomes (data) and you want to understand how likely different parameter values are, given that data.\n\nexample: Have a dataset of 3 obs (\\(y_1, y_2, y_3\\)) and want to know how likely it is that \\(\\lambda = 2\\). Assume exponential distribution.\n\\[l(\\lambda | y_1, y_2, y_3) = \\prod_{i=1}^3 f_{Y_i}(y_i| \\lambda) = \\prod_{i=1}^3 \\lambda e^{-\\lambda y_i}\\]\n\nin R: dexp(y, rate = lambda, log = FALSE)\n\n\n\nObjective of MLE:\n\nThe aim is to find the parameter values that maximize this likelihood function.\n\n\\[w \\in argmax\\{P(dataset | params)\\} = \\prod_{i=1}^3 f_{Y_i}(y_i| \\lambda)\\]\nbut values are very small, so we take the log of the likelihood function to simplify the math.\n\\[w \\in argmax\\{\\log(P(dataset | params))\\} = \\sum_{i=1}^n \\ln f_{Y_i}(y_i| \\lambda) \\]\n\n\nProcedure:\n\nChoose likelihood function\nTake the log of the likelihood function\nDifferentiate the log-likelihood function with respect to the parameters to get max\nSolve for the parameters\n\nin R can do this:\nexp_values &lt;- tibble(\n  possible_lambdas = seq(0.01, 0.2, 0.001),\n  likelihood = map_dbl(possible_lambdas, ~ prod(dexp(sample_n30$values, .))),\n  log_likelihood = map_dbl(possible_lambdas, ~ log(prod(dexp(sample_n30$values, .))))\n)\n\nR optim::optimize()\n\noptimize(f, interval, maximum = TRUE)\n\nf: function to be optimized\ninterval: vector of length 2 giving the start and end of the interval\nmaximum: logical, should the function be maximized?\n\ne.g.\n# Log-likelihood function\nLL &lt;- function(l) log(prod(dexp(sample$values, l)))\n\noptimize(LL, c(0.01, 0.2), maximum = TRUE)\n\n\n\n\nProperties of MLE:\n\nConsistency: As the sample size grows, the MLE converges in probability to the true parameter value.\nAsymptotic Normality: For many models, as the sample size grows large, the distribution of the MLE approaches a normal distribution.\nEfficiency: Among the class of consistent estimators, MLE often has the smallest variance (is the most efficient) under certain regularity conditions.\nMLE can be biased, but it is asymptotically unbiased (i.e. as the sample size increases, the bias goes to 0)\n\n\n\nLimitations:\n\nRequires a specified model for the underlying data distribution. If the model is incorrect, MLE can give biased estimates.\nComputation can be challenging, especially for complex models or when there’s no closed-form solution."
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#central-limit-theorem",
    "href": "block_2/552_stat_inter/552_stat_inter.html#central-limit-theorem",
    "title": "Statistical Inference",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nPopulation and Sampling\n\nLets say population of N samples, each with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nGet a sample of n iid random samples \\(X_1, X_2, ..., X_n\\) from the population.\n\n\\[ \\bar{X} = \\frac{1}{n} \\sum*{i=1}^n X_i \\] \\[ S^2 = \\frac{1}{n-1} \\sum*{i=1}^n (X_i - \\bar{X})^2 \\]\n\nn-1 is to make it unbiased\n\n\n\nDefinition:\n\\(\\bar{X} \\dot\\sim N(\\mu, \\frac{\\sigma^2}{n})\\) as \\(n \\rightarrow \\infty\\)\n\nThe sampling distribution of the sample mean of a population distribution converges to a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\) as the sample size \\(n\\) gets larger when we sample with replacement. \nNOTE: standard deviation of the sampling distribution of the sample mean is called the standard error of the sample mean.\n\n\n\nAssumptions\n\nThe sample size is large enough (at least 30)\n\nunless the population is normal\n\nThe sample is drawn from a population with a finite variance and mean.\nThe samples are iid (independent and identically distributed) from the population.\nEach category in the population should have a sufficient number of samples. (e.g. #success at least 10)\n\nWhen we do inference using the CLT, we required a large sample for two reasons:\n\nThe sampling distribution of \\(\\bar{X}\\) tends to be closer to normal when the sample size is large.\nThe calculated standard error is typically very accurate when using a large sample.\n\n\n\nConfidence Interval using CLT\n\\[ CI = Point \\; Estimate \\pm Margin \\; of \\; Error \\]\n\\[ MoE = Z_x \\times SE = Z_x \\times \\frac{\\sigma}{\\sqrt{n}} \\]\nwhere \\(Z_x\\) is the z-score for the desired confidence level.\n\n95% confidence level: \\(Z_x = Z_{0.025} = 1.96\\)\n\nrecal in R: qnorm(0.975) = 1.96\n\n\n\n\nConfidence Interval for Proportions\n\nsample proportion: \\(\\hat{p} = \\frac{\\sum{X_i}}{n}\\)\n\nit is a sample mean of 0 and 1\n\nRule of thumb:\n\nsuccess: at least 10\nfailure: at least 10\n\n\nCI for proportions:\n\\[ \\hat{p} \\pm Z_x \\times \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\]\nwhere standard error is \\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) since the variance of a bernoulli distribution is \\(p(1-p)\\)"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#hypothesis-testing-via-normal-and-t-testing",
    "href": "block_2/552_stat_inter/552_stat_inter.html#hypothesis-testing-via-normal-and-t-testing",
    "title": "Statistical Inference",
    "section": "Hypothesis Testing via normal and T-testing",
    "text": "Hypothesis Testing via normal and T-testing\n\nSimple confidence interval Test\n\nIf we want to check if mean of population is equal to a value\n\nCan reasonably conclude that the population mean is not equal to the value if the value is not in the confidence interval\nIf the value is in the confidence interval, we cannot conclude that the population mean is equal to the value\n\nNEED TO DO A HYPOTHESIS TEST\n\n\n\n\n\nHypothesis testing\n\nPermutation test (can work on any estimator)\nStudent’s t-test (only works on sample mean)\nProportion test (only works on sample proportion)\n\nNote: All hypothesis tests are done under the null hypothesis\n\n\nP-Value\n\nthe probability of observing a test statistic equally or more extreme than the one you observed, given that the null hypothesis is true\n\nsmall p-value: observed test statistic is unlikely under the null hypothesis\n\nuse get_p_value function from infer R library\nResults:\n\nReject Null Hypothesis if p-value &lt; \\(\\alpha\\)\nFail to Reject Null Hypothesis if p-value &gt; \\(\\alpha\\)\n\n\n\nR code examples\n\nget p-value of normal distribution: pnorm(Z, mean = 0, sd = 1, lower.tail = FALSE)\n\n\n\n\nPermutation Hypothesis test (6 Steps)\n\nDefine your estimator (mean, median, sd, etc.)\nDefine your null and alternative hypotheses. (population)\nCompute the observed test statistic (sample) coming from your original sample.\nSimulate the null hypothesis being true and calculate their corresponding test statistics.\n\ne.g. by randomly shuffling the data =&gt; any observed difference is due to chance\nwould be a normal distribution about 0 (two-tailed test)\n\nGenerate the null distribution using these test statistics.\nObserve where the observed test statistic falls in the distribution\n\nif it falls in the extreme 5% of the distribution, we reject the null hypothesis\ni.e. if the p-value is less than \\(\\alpha\\), we reject the null hypothesis\n\nIf \\(\\delta\\) is near the extremes past some threshold defined with a significance level \\(\\alpha\\), we reject the null hypothesis. Otherwise, we fail to reject the null hypothesis.\n\n\n\nProportion test\n\nDefine test statistic \\(\\delta = p_{control} - p_{variation}\\)\n\n\nAlso define null and alternative hypothesis\n\n\nDefine theory-based CLT test statistic \\[Z = \\frac{\\hat{p}_{control} - \\hat{p}_{variation}}{\\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_C} + \\frac{1}{n_V})}}\\] where: \\[\\hat{p} = \\frac{\\sum_{i=1}^{n_C} X_{iC} + \\sum_{i=1}^{n_V} X_{iV}}{n_C + n_V}\\]\n\nnumer: sample stat + effect size (difference in proportions)\ndenom: standard error of 2 sample\n\nSimulates to Normal distribution with mean 0 and standard deviation\n\n\nFind p-value and see if it is less than \\(\\alpha\\)\nOr find the CI and see if Z is in the CI\n\nIf Z is in the CI, we fail to reject the null hypothesis\n\n\n\nproportion test in R\n\nuse prop.test() function in R\n\nx is the number of successes in Bernoulli trials\nn is the number of trials\nalternative is the alternative hypothesis\n\nless, greater, two.sided\n\nconf.level is the confidence level\ncorrect = FALSE means we are using the normal approximation\n\nif correct = TRUE, we are using the exact binomial test\n\n\n\nprop.test(x = click_summary$success, n = click_summary$n,\n  correct = FALSE, alternative = \"less\"\n)\n\n\n\nPearson’s Chi-Squared Test\n\nTo identify whether two categorical variables are independent or not\n\nSteps:\n\nMake a contingency table\n\nlibrary(janitor)\n\n# Makes table with webpage, n, and num_success as columns\ncont_table_AB &lt;- click_through %&gt;%\n  tabyl(webpage, click_target)\n\nDo some calulations, e.g. for 2x2 table:\n\n\\[\\chi^2 = \\sum_{i=1}^2 \\sum_{j=1}^2 \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\]\n\n\nChi-Squared Test in R\nchisq.test(cont_table_AB, correct = FALSE)\nwhere:\n\n\nT Test\n\nUses the CLT to approximate the sampling distribution of the sample mean\n\nONLY works on sample mean (because CLT only works on sample mean)\nONLY works on continuous data (because CLT only works on continuous data)\ncan be any distribution, because CLT makes it normal\n\nBasically similar to normal test but we have small sample size\nDegrees of freedom = n - 1 where n is the sample size\n\nas df increases, the t-distribution approaches the standard normal distribution\n\nTest Statistic = (sample mean - population mean) / standard error\n\nstandard error = standard deviation / sqrt(n)\nmore formally: \\[t = \\frac{\\bar{x} - \\mu}{s/\\sqrt{n}}\\]\n\nnumerator: observed difference in sample means\ndenominator: standard error of the sampling distribution of the two-sample difference in means for the sample size we collected.\n\n\nOne sample t-test\n\nNull hypothesis: the population mean is equal to a value \\(\\mu = \\mu_0\\)\nTest statistic: \\[t = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}\\]\n\ns : sample standard deviation\n\nIn R use t.test() function\n\nx is the sample data\nmu is the value of the population mean\nalternative is the alternative hypothesis\n\nless, greater, two.sided\n\nconf.level is the confidence level\ncorrect = FALSE means we are using the normal approximation\n\nif correct = TRUE, we are using the exact binomial test\n\n\n\n\n\nTwo sample t-test\n\nNull hypothesis: the population mean of two groups are equal \\(\\delta = \\mu_1 - \\mu_2 = 0\\)\nTest statistic (if equal variance, $ &lt; 2):\n\n\\[t = \\frac{(\\bar{x}_1 - \\bar{x}_2) - \\delta_0}{S_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]\n\n\\(S_p\\) is the pooled standard deviation \\[S_p = \\sqrt{\\frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}}\\]\ndegrees of freedom: \\(df = n_1 + n_2 - 2\\)\nin R, use t.test() function\n\nformula is the formula for the test statistic variable ~ group\ndata is the data frame\nmu hypothesis value of \\(\\delta\\)\nalternative is the alternative hypothesis\n\nless, greater, two.sided\n\nconf.level is the confidence level\nvar.equal = TRUE means we are assuming equal variance\n\nif var.equal = FALSE, we are assuming unequal variance\n\n\n\n\n\n\nComparison with simulation hypothesis test\n\n\n\n\n\n\n\n\n\nTest Type\nFactor\nImpact on Distribution / Statistic\nImpact on p-value\n\n\n\n\nSimulation Hypothesis\nSample Size\nIncrease =&gt; narrower null distribution\nDecrease\n\n\n\nSample Variance\nIncrease =&gt; increase simulated null variance\nIncrease\n\n\n\nEffect Size\nIncrease =&gt; more extreme test statistic\nIncrease (more likely to reject null)\n\n\nNormal/T-Testing\nEffect Size\nIn numerator of test statistic\nIncrease (if other factors remain same)\n\n\n\nSample Variance\nIncrease =&gt; increase standard error\nIncrease\n\n\n\nSample Size\nIncrease =&gt; decrease standard error\nDecrease"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#errors-in-inference",
    "href": "block_2/552_stat_inter/552_stat_inter.html#errors-in-inference",
    "title": "Statistical Inference",
    "section": "Errors in Inference",
    "text": "Errors in Inference\n\nTypes of Errors\n\nType I error: False positive $ P( H_0 | H_0 ) = $ \nType II error: False negative $ P( H_0 | H_0 ) = $\n\n\n\n\n\n\n\n\n\nDecision\nTrue Condition Positive\nTrue Condition Negative\n\n\n\n\nTest Positive\nCorrect (True Positive)\nType I Error (False Positive)\n\n\nTest Negative\nType II Error (False Negative)\nCorrect (True Negative)\n\n\n\n\n\nVisual representation of errors\n\nParameters:\n\n\\(\\beta\\), power = 1 - \\(\\beta\\)\n\\(\\alpha\\): sets type I error rate (how often we reject the null hypothesis when it is true)\ncohen’s d : effect size\n\n\\(d = \\frac{\\mu_1 - \\mu_2}{\\sigma}\\), where:\n\n\\(\\mu_1\\): mean of group 1\n\\(\\mu_2\\): mean of group 2\n\\(\\sigma\\): standard deviation of the population"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html",
    "href": "block_3/573_model_sel/573_model_sel.html",
    "title": "Model Selection",
    "section": "",
    "text": "Need to evaluate the performance of a classifier not just by accuracy\n\n\n\n\nLinear regression with sigmoid function\nHyperparameter: C (default 1.0)\n\nLower C =&gt; more regularization =&gt; smaller coefficients =&gt; underfitting\n\n\nfrom sklearn.linear_model import LogisticRegression\n\npipe_lr = make_pipeline(preprocessor, LogisticRegression())\npd.DataFrame(cross_validate(pipe_lr, X_train, y_train, cv=10, return_train_score=True))\n\n\n\n\n\nChoose the positive case based on which class is more important to us or which class is rare\n\nspotting a class (e.g. fraud/ spam/ disease detection)\n\nCan do this in python:\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_predict\n\n# gets values without plot\nconfusion_matrix(y_train, cross_val_predict(pipe_lr, X_train, y_train, cv=5))\n\n# get values with plot\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nConfusionMatrixDisplay.from_estimator(pipe_lr, X_train, y_train, display_labels=['Not Fraud', 'Fraud']);\nOutput of confusion matrix by default:\n             | Predicted F [0]| Predicted T [1]\nActual F [0] | TN             | FP\nActual T [1] | FN             | TP\n\n\n\nPrecision \\[precision = \\frac{TP}{PP} =  \\frac{TP}{TP + FP}\\]\nRecall or True positive rate (TPR) \\[recall = \\frac{TP}{P} = \\frac{TP}{TP + FN}\\]\nf1-score \\[f1 = 2 \\cdot \\frac{precision * recall}{precision + recall} = 2 \\frac{1}{\\frac{1}{precision} + \\frac{1}{recall}}\\]\n\n\n\n\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_train, cross_val_predict(pipe_lr, X_train, y_train, cv=5)))\n\n# output\n#               precision    recall  f1-score   support\n\n#        Fraud       0.89      0.63      0.74       102\n#    Non fraud       1.00      1.00      1.00     59708\n\n#     accuracy                           1.00     59810\n#    macro avg       0.94      0.81      0.87     59810\n# weighted avg       1.00      1.00      1.00     59810\n\n\n\n\n\ntradeoff between precision and recall\nsetting a threshold is called setting the operating point\nChanging the threshold of the classifier (default of logistic regression is 0.5)\n\nremember predict_proba returns the probability of the positive class, if higher than threshold, then positive class\nas threshold increases, higher bar =&gt; precision increases and recall decreases (less FP but less TP)\nas threshold decreases, lower bar =&gt; precision decreases and recall increases (more TP but more FP)\n\n\n\n\nfrom sklearn.metrics import PrecisionRecallDisplay\n\nPrecisionRecallDisplay.from_estimator(\n    pipe_lr,\n    X_valid,\n    y_valid,\n    pos_label='Fraud',\n    name='LR',  # For Logistic Regression\n);\n\n\nWant the curve to be as close to the top right corner as possible (100% precision and 100% recall)\n\nThreshold of 0 in bottom left =&gt; All points are predicted to be positive =&gt; 100% recall and 0% precision\nThreshold of 1 in top right =&gt; All points are predicted to be negative =&gt; 0% recall and 100% precision\n\n\nOr if want just the values:\nfrom sklearn.metrics import precision_recall_curve\n\npd.DataFrame(\n    precision_recall_curve(\n        y_valid,\n        pipe_lr.predict_proba(X_valid)[:, fraud_column],\n        pos_label='Fraud',\n    ),\n    index=['precision', 'recall', 'threshold']\n).T\n\n\n\n\nArea under the precision recall curve\nHigher is better (0 is worst, 1 is best)\nAlso supports multi-class using one-vs-rest approach + averaging\nIMPORTANT:\n\nF1 score is for a given threshold and measures the quality of predict.\nAP score is a summary across thresholds and measures the quality of predict_proba.\n\n\nfrom sklearn.metrics import average_precision_score\n\nap_lr = average_precision_score(y_valid, pipe_lr.predict_proba(X_valid)[:, fraud_column], pos_label='Fraud')\n\n\n\nAP score\nf1 score\n\n\n\n\nmeasures quality of predict_proba\nmeasures quality of predict\n\n\n\n\nThese two metrics do not always agree*\n\n\n\n\n\n\nReceiver Operating Characteristic: plots the true positive rate (recall) against the false positive rate (1 - specificity)\n\nInstead of plotting precision against recall, plots recall (TPR) against FPR\n\n\n\\[FPR = \\frac{FP}{N} = \\frac{FP}{FP + TN}\\]\nfrom sklearn.metrics import RocCurveDisplay\n\nRocCurveDisplay.from_estimator(\n    pipe_lr,\n    X_valid,\n    y_valid,\n    pos_label='Fraud'\n);\n\n\nIdeal is top left corner (100% TPR and 0% FPR)\nLower left (Threshold of 1) =&gt; 0% TPR and 0% FPR\nUpper right (Threshold of 0) =&gt; 100% TPR and 100% FPR\n\n\n\n\nArea under the ROC curve\nAUC is the probability that a randomly chosen positive point has a higher score than a randomly chosen negative point\n\nAUC of 1.0: all positive points have a higher score than all negative points.\nAUC of 0.5: means random chance.\n\n\n\n\n\n\n\nIf we balance the classes:\n\nPrecision goes down, FP goes up\nRecall goes up, FN goes down\nF1 score goes down\nAP score goes down\nAUC score goes down\ngenerally reduce accuracy\n\n\n\n\n\nClass weights: penalize the minority class more\n\nLogisticRegression(\n  max_iter=500,\n  # give more importance to \"Fraud\" class (10x)\n  class_weight={'Non fraud': 1, 'Fraud': 10}\n\n  # class_weight='balanced' =&gt; automatically give more importance to minority class\n  )\n\n\n\n\n\n\n\n\n\nfrom sklearn.dummy import DummyRegressor\n\ndummy = DummyRegressor(strategy='mean') # default\npd.DataFrame(cross_validate(dummy, X_train, y_train, cv=10, return_train_score=True))\n\n\n\n\nLinear regression with L2 regularization\nHyperparameter: alpha (default 1.0)\n\nHigher alpha =&gt; more regularization =&gt; smaller coefficients =&gt; underfitting\nmore regularization : smaller coefficients =&gt; less sensitive to changes in the input features =&gt; less likely to overfit\n\n\nfrom sklearn.linear_model import Ridge\n\nlr_pipe = make_pipeline(preprocessor, Ridge())\npd.DataFrame(cross_validate(lr_pipe, X_train, y_train, cv=10, return_train_score=True))\n\n# fit model\nlr_pipe.fit(X_train, y_train)\n\n# get coefficients\ndf = pd.DataFrame(\n    data={\"coefficients\": lr_pipe.named_steps[\"ridge\"].coef_}, index=feature_names)\n\ndf.sort_values(\"coefficients\", ascending=False)\n\n\n\n\nRidge with cross-validation to find the best alpha\n\nfrom sklearn.linear_model import RidgeCV\n\nalphas = 10.0 ** np.arange(-6, 6, 1)\n\nridgecv_pipe = make_pipeline(preprocessor,\n  RidgeCV(alphas=alphas, cv=10))\nridgecv_pipe.fit(X_train, y_train);\n\n# best alpha\nbest_alpha = ridgecv_pipe.named_steps[\"ridgecv\"].alpha_\n\n\n\n\n\nCannot use equality since we are predicting a continuous variable (not classification)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetric\nDescription\nFormula\nMin Value\nMax Value\nCode Example (sklearn)\n\n\n\n\nMSE (Mean Squared Error)\nMeasures the average of the squares of the errors, i.e., the average squared difference between the estimated values and the actual value.\n\\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(Y_i - \\hat{Y}_i)^2\\]\n0 (perfect)\n∞\nfrom sklearn.metrics import mean_squared_error  mse = mean_squared_error(y_true, y_pred)\n\n\nRMSE (Root Mean Squared Error)\nSquare root of the MSE. It measures the standard deviation of the prediction errors or residuals.\n\\[RMSE = \\sqrt{MSE}\\]\n0 (perfect)\n∞\nrmse = mean_squared_error(y_true, y_pred, squared=False)\n\n\nMAE (Mean Absolute Error)\nMeasures the average of the absolute errors.\n\\[MAE = \\frac{1}{n}\\sum_{i=1}^{n}\\left\\| Y_i - \\hat{Y}_i \\right\\| \\]\n0 (perfect)\n∞\nfrom sklearn.metrics import mean_absolute_error  mae = mean_absolute_error(y_true, y_pred)\n\n\nMAPE (Mean Absolute Percentage Error)\nMeasures the average of the absolute percentage errors.\n\\[MAPE = \\frac{1}{n}\\sum_{i=1}^{n}\\left\\| \\frac{Y_i - \\hat{Y}_i}{Y_i} \\right\\| \\times 100\\% \\]\n0 % (perfect)\n∞ %\nfrom sklearn.metrics import mean_absolute_percentage_error  mape = mean_absolute_percentage_error(y_true, y_pred)\n\n\nR² (Coefficient of Determination)\nMeasures how well future samples are likely to be predicted by the model.\n\\[R^2 = 1 - \\frac{\\sum_{i=1}^{n}(Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^{n}(Y_i - \\bar{Y})^2}\\]\n-ve [if worst than mean]\n1 (perfect)\nfrom sklearn.metrics import r2_score  r2 = r2_score(y_true, y_pred)\n\n\n\n\nMAE is less sensitive to outliers than MSE\n\n\n\n\n\nCan plot the actual vs predicted values to see the error (and plot line of gradient 1 = perfect prediction)\n\nfrom sklearn.metrics import PredictionErrorDisplay\n\nPredictionErrorDisplay.from_estimator(\n    ridge_tuned,\n    X_train,\n    y_train,\n    kind='actual_vs_predicted',\n    subsample=None  # show all predictions\n)\n\n\nCan plot the residuals (error) vs predicted values to see if there is a pattern (e.g. heteroscedasticity)\n\nfrom sklearn.metrics import PredictionErrorDisplay\n\nPredictionErrorDisplay.from_estimator(\n    ridge_tuned,\n    X_train,\n    y_train,\n    subsample=None  # show all predictions\n)\n\n\n\n\n\n\nNeed to have good data to build a good model.\nBad quality data:\n\nMissing values\nFew observations overall or in a specific group\nBiased sample (not representative of the population)\nNon-independent observations\nInaccurate measurements\nFabricated data\nOut of bounds values\nObsure column names\nTypos (spelling differences in categorical variables)\nUsing multiple values to represent same thing (e.g. NA, NONE, NULL, NaN)\nIncorrect data types\n\n\n\nBetter features help more than a better model.\n\nGood features would ideally:\n\ncapture the most important information\nallow learning with few examples\ngeneralize to new scenarios\n\nTrade-off for simple and expressive features:\n\nsimple features: overfitting risk is low but low score\nexpressive features: overfitting risk is high but high score\n\n\n\n\n\n\n\n\n\n\n\n\nPreprocessing\nEngineering\n\n\n\n\nstep of cleaning and preparing the data for analysis\ncreating new features from existing data\n\n\ngenerally HAVE to do, or error\noptional but can improve model performance\n\n\ne.g. scaling, normalization, imputation\ne.g. one-hot encoding, binning, get more data, group-wise normalization, transformation\n\n\n\n\nFeature Selection: removing irrelevant features, normally done after feature engineering\n\n\n\n\n\nCommon guidelines:\n\nIs not unique or random\nHas a variance (not constant)\nAdds unique variance (is not constant transformation)\n\nchanging units (e.g. from meters to feet) is not feature engineering\n\nIs ideally interpretable\n\nExamples:\n\nLooking up and getting additional data\nDiscretization (binning): e.g. age -&gt; age group\nGroup-wise normalization: express feature relative to group mean/median\nTransformation: e.g. height, weight -&gt; BMI\n\n\n\n\nparameter: degree\n\nhigher degree: more expressive features =&gt; more overfitting\n\nTry to capture non-linear relationships\n\nLinear regression can only capture lines, planes, hyperplanes\n\ne.g. add a squared feature (feat1^2)\n\n\n\n\n\nDo polynomial then StandardScaler\n\navoids polynomials being very large\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2)\n# degree: max degree of polynomial features\nX_enc = poly_feats.fit_transform(X_toy)\npd.DataFrame(X_enc, columns=poly_feats.get_feature_names_out()).head()\nDegree example:\n\n\n\nDimension\nDegree\nExamples\n\n\n\n\n1\n2\n1, x, x^2\n\n\n2\n2\n1, x, y, x^2, xy, y^2\n\n\n\n\n\n\n\\[ \\hat{y} = Xw \\]\n\nX: design matrix (n x d)\nw: weights (d x 1)\ny: target (n x 1)\n\nFor polynomial features:\n\\(Z = [1, x, x^2]\\)\n\\[ \\hat{y} = Zw = \\begin{bmatrix} 1 & x_1 & x_1^2 \\\\ 1 & x_2 & x_2^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x_n^2 \\end{bmatrix} \\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\end{bmatrix} = \\begin{bmatrix} w_0 + w_1x_1 + w_2x_1^2 \\\\ w_0 + w_1x_2 + w_2x_2^2 \\\\ \\vdots \\\\ w_0 + w_1x_n + w_2x_n^2 \\end{bmatrix} \\]\n\n\n\n\nPolynomial features can be expensive to compute\nKernel trick: compute dot product between two vectors in a higher dimensional space without computing the transformation explicitly\n\n\n\n\n\n\n\nimport seaborn as sns\n\nsns.heatmap(df.corr(), annot=True)\n\nShows correlation between features\nExtremely simplistic:\n\nOnly looks at linear correlation\nOnly looks at pairwise correlation (isolates features)\n\n\n\n\n\n\nCorrelation among features might make coefficients completely uninterpretable.\nFairly straightforward to interpret coefficients of ordinal features.\nIn categorical features, it’s often helpful to consider one category as a reference point and think about relative importance.\nFor numeric features, relative importance is meaningful after scaling.\nYou have to be careful about the scale of the feature when interpreting the coefficients.\nRemember that explaining the model explaining the data or explaining how the world works.\nThe coefficients tell us only about the model and they might not accurately reflect the data.\n\n\n\n\n\n\nMore features =&gt; more complex model =&gt; overfitting\nFeature selection find features that are important for prediction and remove the rest.\n\nWhy?\n\nInterpretability: easier to explain with fewer features\nComputation: faster to train simpler models (curse of dimensionality)\nData collection: cheaper to collect fewer features\nStorage: less space to store fewer features\nFundamental Tradeoff: reduce overfitting by reducing complexity\n\nHow?\n\ndomain knowledge\nautomatic feature selection\n\nmodel-based selection\nrecursive feature elimination\nforward/backward selection\n\n\n\n\n\nFast and easy\nArbitrary, does not take into account feature combinations\ne.g. correlation threshold, variance threshold\n\n\n\n\n\nUse a supervised model to judge the importance of each feature, and keep only the most important ones.\nCan use different model than the final estimator.\nUse a model that provides some measure of feature importance. (e.g. decision trees, linear models, etc.)\n\nGeneral Steps:\n\nSelect a threshold for which features to keep.\nDiscard features below the threshold.\n\nfrom sklearn.feature_selection import SelectFromModel\n\nselect_lr = SelectFromModel(Ridge(), threshold=\"median\")\n\n# Add feature selection to pipeline (after preprocessing)\npipe = make_pipeline(preprocessor, select_lr, Ridge())\n\n# access selected features\npipe.named_steps[\"selectfrommodel\"]\n# can do: .n_features_in_, .threshold_, .get_support()\n\n\n\n\nRecursively remove features, build a model on those features that remain, and then repeatedly construct a model and remove the weakest feature until a specified number of features remain.\nIteratively eliminates unimportant features.\nComputationally expensive\n\n\n\n\nDecide the number of features to select.\nAssign importances to features, e.g. by fitting a model and looking at coef_ (Linear models) or feature_importances_ (RandomForestClassifier).\nRemove the least important feature.\nRepeat steps 2-3 until only \\(k\\) features are remaining.\n\nfrom sklearn.feature_selection import RFE\n\nrfe = RFE(Ridge(), n_features_to_select=120)\n\npipe_rf_rfe = make_pipeline(preprocessor, rfe, RandomForestRegressor(random_state=42))\n\nresults[\"rfe + rfe\"] = mean_std_cross_val_scores(\n    pipe_rf_rfe, X_train, y_train, return_train_score=True\n)\nNote: for categorical features, RFE might remove some OHE features. This is an unresolved issue.\n\n\n\n\nRFE but using CV to find the optimal number of features to keep.\nvery slow since there is CV within CV.\n\nfrom sklearn.feature_selection import RFECV\n\nrfecv = RFECV(Ridge())\n\npipe_rf_rfecv = make_pipeline(\n    preprocessor, rfecv, RandomForestRegressor(random_state=42)\n)\npipe_rf_rfecv.fit(X_train, y_train);\n\n# optimal number of features\npipe_rfe_ridgecv.named_steps[\"rfecv\"].n_features_\n# which features were selected\npipe_rfe_ridgecv.named_steps[\"rfecv\"].support_\n\n\n\n\n\nNot based on feature importance.\nDefine a scoring function \\(f(S)\\) that measures the quality of a subset of features \\(S\\).\nSearch through all possible subsets of features and pick the subset with the best score.\n\ne.g. for A, B, C search through {}, {A}, {B}, {C}, {A, B}, {A, C}, {B, C}, {A, B, C}…\n# subsets = \\(2^p\\) where \\(p\\) is the number of features\n\nForward selection: start with no features and add one at a time.\n\nadd the feature that results in the best score.\n\nBackward selection: start with all features and remove one at a time.\n\nremove the feature that results in the best score.\n\n\nfrom sklearn.feature_selection import SequentialFeatureSelector\n\npipe_forward = make_pipeline(\n    preprocessor,\n    SequentialFeatureSelector(Ridge(), direction=\"forward\"),\n    RandomForestRegressor(random_state=42),\n)\n\n# results['rf_forward_fs'] = mean_std_cross_val_scores(pipe_forward, X_train, y_train, return_train_score=True)\n\npipe_forward.fit(X_train, y_train)\n\n\n\n\nA feature relevance is only defines in the context of other features.\n\nAdding/removing features can change the importance of other features.\n\nRelevance != causality\nThe methods don’t always work.\n\n\n\n\n\n\nA function that measures how well a model fits the data\nThe goal is to minimize the loss function\n\nsmaller loss \\(\\rightarrow\\) better model\n\nCaptures what is important to minimize\nCommon loss functions:\n\nLeast squares loss\nAbsolute error loss\nHuber loss\n\n\n\n\n\n\n\n\n\n\n\n\nScoring Metric\nLoss Function\n\n\n\n\nReport the results (often several metrics)\nUsed to train/ fit the model\n\n\nFind best model hyperparams\nFind best model parameters\n\n\nCan pick those suitable for the task\nConstrainted (need to be differentiable)\n\n\n\n\n\n\n\n\n\n\\[J(\\omega) = \\sum_{i=1}^{n} (y_i - \\omega^T x_i)^2 \\]\n\nDefine loss as sum of squared errors (difference between prediction and actual value)\nPenalizes heavily for large errors\n\nsensitive to outliers\n\nFor a well-defined OLS, there is a unique solution (not always the case)\n\n\\(\\omega = (X^T X)^{-1} X^T y\\)\n\n\\(X^T X\\) must be invertible\n\n\n\n\n\n\n\\[J(\\omega) = \\sum_{i=1}^{n} |y_i - \\omega^T x_i| \\]\n\nLess sensitive to outliers\nBut minimization is harder\n\nnot differentiable at 0\nno closed form solution\n\nAlternative: Huber Loss\n\nbehaves like OLS for small errors\n\n\n\n\n\n\n\\[ y_i w^T x_i = \\begin{cases} \\text{correct} & y_i w^T x_i &gt; 0 \\\\ \\text{incorrect} & y_i w^T x_i &lt; 0 \\end{cases} \\]\n\n\\(y_i\\): positive\n\ncorrect: \\(w^T x_i &gt; 0 \\rightarrow y_i w^T x_i &gt; 0\\)\nincorrect: \\(w^T x_i &lt; 0 \\rightarrow y_i w^T x_i &lt; 0\\)\n\n\\(y_i\\): negative\n\ncorrect: \\(w^T x_i &lt; 0 \\rightarrow y_i w^T x_i &gt; 0\\)\nincorrect: \\(w^T x_i &gt; 0 \\rightarrow y_i w^T x_i &lt; 0\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0-1 Loss\n Exponential Loss \n Hinge Loss \n Logistic Loss \n\n\n\n\ndescription\nOnly cares about whether the prediction is correct or not\nPunishes the wrong predictions exponentially, value gets smaller as more confident for correct predictions\n- Confident and correct predictions are not penalized- Grows linearly for incorrect predictions\n- Used in logistic regression- Smooths the degenerate 0-1 loss with log-sum-exp\n\n\nfunction\n\\[\\mathbb{1}\\{y \\neq w^T x\\}\\]\n\\[e^{-y w^T x}\\]\n\\[\\max(0, 1 - y w^T x)\\]\n\\[\\log(1 + e^{-y w^T x})\\]\n\n\nloss\n- when incorrect, loss is 1- when correct, loss is 0- Total loss is the sum of all losses (number of incorrect predictions)\n- when incorrect, loss is large- when correct, loss is small\n- if correct \\(y w^T x &gt; 1\\), \\(1-y w^T x &lt;0\\) - if incorrect \\(y w^T x &lt; 1\\), \\(1-y w^T x &gt;0\\)\n- Convex, differentiable, and smooth\n\n\n\n\nHinge loss + L2 regularization = SVM\n\n\n\n\nLet \\(z = y_i w^T x_i\\)\n\nSigmoid loss: \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)\n\nmaps to [0, 1] (probability)\nfor predict_proba\n\nLogistic loss: \\(\\log(1 + e^{-z})\\)\n\nmaps \\(y_i w^T x_i\\) to \\([0, \\infty]\\) = loss contribution for a single example\nimportant for fit\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nmodel.predict(X_test)\nmodel.predict_proba(X_test)\n\nhyperparameters:\n\nC: inverse of regularization strength\n\nsmaller C \\(\\rightarrow\\) stronger regularization \\(\\rightarrow\\) smaller weights \\(\\rightarrow\\) simpler model \\(\\rightarrow\\) underfitting\n\n\n\n\n\n\n\n\\[J(w) = \\text{loss} + \\lambda \\cdot \\text{regularization} \\]\n\nmore complex model =&gt; higher regularization =&gt; higher cost\n\n\n\n\n\nadding penalty on model complexity (to reduce overfitting)\nBenefits:\n\nreduce overfitting\nless prone to outliers\n\nKey Idea: Pick the line/ hyperplane with the smalles slope (simplest model)\n\n\n\n\n\n\n\n\n\nL0 Norm\nL1 Norm\nL2 Norm\n\n\n\n\nNumber of non-zero elements\nSum of absolute values\nSquare root of the sum of squared values\n\n\n\\(\\|\\|w\\|\\|_0\\)\n\\(\\|\\|w\\|\\|_1\\)\n\\(\\|\\|w\\|\\|_2\\)\n\n\nN/A\n\\(\\sum_{i=1}^{n} \\|x_i\\|\\)\n\\(\\sqrt{\\sum_{i=1}^{n} x_i^2}\\)\n\n\n\n\n\n\n\\[J(\\omega) = ||X\\omega - y||_2^2 + \\lambda ||\\omega||_0\\]\n\nWhere \\(||\\omega||_0\\) is the L0-norm: number of non-zero elements in \\(\\omega\\)\nTo increase the degrees of freedom by 1, need to decrease the error by \\(\\lambda\\)\n\ndegrees of freedom: number of parameters in the model\nsmaller DOF is preferred\n\nHard to optimize since it is non-convex and non-differentiable\n\n\n\n\n\\[J(\\omega) = ||X\\omega - y||_2^2 + \\lambda ||\\omega||_2^2\\]\n\nWhere \\(||\\omega||_2^2\\) is the L2-norm: sum of squared values of \\(\\omega\\)\nWeights decrease, but DO NOT become zero\nBig \\(\\lambda\\) =&gt; more regularization =&gt; lower weights =&gt; simpler model =&gt; underfitting\n\n\\(\\lambda=0\\) same as OLS\n\nAs \\(\\lambda\\) increases (simpler model):\n\n\\(||X\\omega - y||_2^2\\) increases (less accurate)\n\\(||\\omega||_2^2\\) decreases (smaller weights)\n\n\n\n\n\nTheory: As n grows, \\(\\lambda\\) should be in the range of \\([0,\\sqrt{n}]\\)\nDo this by optimizing validation error or CV error\n\n\n\n\nfrom sklearn.linear_model import Ridge\nridge = Ridge(alpha=1.0) # alpha is lambda\nridge.fit(X_train, y_train)\n\n\n\n\n\\[J(\\omega) = ||X\\omega - y||_2^2 + \\lambda ||\\omega||_1\\]\n\nSimilar to L2 but uses L1-norm instead.\n\nboth shrink weights and result in simpler models\n\nWeights can become zero\n\nsome features are completely ignored\nkinda like feature selection\n\n\n\n\n\nsparsity: linear function with many zero-valued coefficients\nL0 and L1 regularization =&gt; sparse models\n\n\n\n\nfrom sklearn.linear_model import Lasso\nlasso = Lasso(alpha=1.0) # alpha is lambda\nlasso.fit(X_train, y_train)\n\n\n\n\n\nScaling does not matter:\n\nDecision trees/ Naive Bayes/ Random Forests: only look at 1 feature at a time\nLeast Squares Loss: just change weights\n\nScaling matters:\n\nKNN: distance will be affected by large values\nRegularized Least Squares: regularization term will be affected by large values\n\n\n\n\n\n\nL2 regularization: only 1 unique solution\n\ne.g. 3 collinear features =&gt; all equal weights (1/3)\n\nL1 regularization: many solutions\n\ne.g. 3 collinear features =&gt; (1,0,0), (0,1,0), (0,0,1), (1/2,1/2,0), etc.\n\n\n\n\n\n\\[J(\\omega) = ||X\\omega - y||_2^2 + \\lambda(\\alpha ||\\omega||_1 + (1-\\alpha)||\\omega||_2^2)\\]\n\n\\(\\lambda\\) is the regularization parameter\n\\(\\alpha\\) promotes:\n\nsparcity in L1\nsmoothness in L2\n\nFunction is strictly convex, so there is a unique solution (no collinearity problem)\n\nfrom sklearn.linear_model import ElasticNet\nelastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5) # alpha is lambda, l1_ratio is alpha\nelastic_net.fit(X_train, y_train)\n\n\n\n\nKey Idea: Groups make better decisions than individuals, especially when the group members are diverse.\nTree-based is the most successful ensemble method\n\nRandom Forest\nGradient Boosted Trees\n\nWays to combine models:\n\nAveraging\nStacking\n\n\n\n\n\nDoes not make the individual models better but adds diversity =&gt; better ensemble\nThink of it visually as high variance (not precise) and low bias (accurate on average)\n((insert image from slides))\n\n\n\n\nRF\nBoosting\n\n\n\n\nMany independent trees\nTrees are dependent on each other\n\n\nrandom diversity\nnon-random diversity\n\n\ncan be parallelized\nsequential\n\n\n\n\n\n\nTrain each model on a different subset of the data\nBootstrap Aggregation: Sampling with replacement.\n\n\n\n\n\n\nBagging + Random set of features at each node.\nDeep trees, normally full depth.\nGeneral Idea:\n\nSingle tree is likely to overfit\nUse a collection of diverse trees\nEach tree overfits on some part of the data but reduce overfitting by averaging\n\nAt each node:\n\nrandomly select a subset of features (independednt from each node)\nfind best split among the subset of features\ngrow tree to full depth\n\nPrediction:\n\nVote the trees\n\n\n\n\n\nuses soft prediction (avg predict_proba)\neasy to fit in parallel (each tree is independent)\nSome important hyperparameters:\n\nn_estimators: number of trees (higher = more complex)\nmax_depth: max depth of each tree (higher = more complex)\nmax_features: max number of features to consider at each split (higher = more complex)\n\nDefaults:\n\nClassification: sqrt(# features)\nRegression: # features / 3 or # features\n\n\n\nmax_features and n_estimators get better scores as values increase but more computationally expensive\n\ndifferent from standard “hyperparameter tuning”\n\n\n\n\n\n\n\nRandom Forest + Random Thresholds (instead of the best threshold)\nIndividually, each tree is not that good\nEach tree is grown to full depth\nIn sklearn: ExtraTreesClassifier\n((insert image about the variance comparison))\n\n\n\n\n\nBoost performance through reweighting of data\n\n\nFit initial model on training data\nreweight the training data so that samples that the initial model got wrong became more important\nFit the next model on the reweighted data\n\nwill prioritize the samples that the previous model performed poorly on\n\nRepeat until you have a set of models\nCombine the models into a single model\n\n\n\n\n\nbetter than AdaBoost\nKey Idea: Trying to decrease the residuals of the previous model by creating new tree that can predict the residuals (then use it as correction)\n\nNo randomization\ncombine multiple shallow (depth 1-5) trees\nbuilt sequentially\n\nWas computationally expensive but now it is not:\n\nXGBoost, LightGBM, CatBoost\n\n\n\nConstruct a base tree (root node), no splits\nBuild next tree based on errors of previous tree\n\nthe errors are the residuals\n\nCombine the tree from step 1 with the tree from step 2\nRepeat until max number of estimators is reached\nCombine the trees into a single model\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightbgm.sklearn import LGBMClassifier\n\nImportant Hyperparameters:\n\nn_estimators: number of boosting rounds (higher = more complex)\nlearning_rate: controls how strongly each tree tries to correct the mistakes of the previous trees (higher = more corrections = more complex)\nmax_depth: max depth of each tree (higher = more complex)\nscale_pos_weight: balancing of positive and negative weights\n\n\n\n\n\n\n\n\nTake vote of all models\n\n\n\n\n\nvoting='soft' uses predict_proba and averages the probabilities\n\ngood if trust predict_proba\n\nvoting='hard' uses predict and takes the majority vote\n\nfrom sklearn.ensemble import VotingClassifier\n\naveraging_model = VotingClassifier(\n    list(classifiers.items()), voting=\"soft\"\n)  # need the list() here for cross-validation to work!\n\n\n\n\nKey idea: Weighted voting via simple model\n\nuse outputs as inputs to a new model\n\nUse a simpler model like logistic regression as “meta-model”\nEach model is a feature for the meta-model\n\n\n\n\n\n\n\n\n\n\nLooking at feature importance can help us understand the model\nWhy?\n\nDebugging: find systematic errors\nDiagnosing bias: find underlying biases\nFeature engineering: find new features\nFeature selection: find unimportant features\nModel interpretability can be leveraged by domain experts to diagnose systematic errors and underlying biases of complex ML systems.\n\n\n\n\n\nKey idea: How much does each feature contribute to the model?\nUse a tree-based model (e.g. random forest) to measure feature importance\n\nDo not use single decision tree because it is not stable\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nEasy to interpret\nNot stable (can change a lot with small changes in data)\n\n\nCan be used for feature selection\nCan be biased towards high cardinality* features (e.g. ID)\n\n\nCan be used for feature engineering (e.g. interaction features)\nCan be biased towards numerical features (e.g. age)\n\n\nna\nCan only be used for tree-based models (e.g. random forest)\n\n\n\n*high cardinality: a feature with a lot of unique values\n\n\n\n\nRandomly shuffle a feature and measure how much the model performance changes\nNo issue with high cardinality features\nCan add a random noise column to measure feature importance\n\nCan be used as a threshold to remove features\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nCan be used for any model (not just tree-based models)\nCorrelated features are still an issue\n\n\nno issue with high cardinality features\nWe do not know if they contribute positively or negatively to the model\n\n\nSimple concept, easy to understand\n\n\n\nUse validation data, so not picking up features that overfit\n\n\n\n\n\n\nfrom sklearn.inspection import permutation_importance\n\nperm_importance = permutation_importance(\n    model,\n    X_test,\n    y_test,\n    n_repeats=10,\n    random_state=42,\n    n_jobs=-1\n)\n\ncan be used on non sklearn models\ndo not have a sign\nonly tell us importance not direction of importance\n\n\n\n\n\n\nWhat is the contribution of each feature to the prediction?\nThe order of the features matters due to interactions\nCould use backward/ forward selection to find the best subset of features but very expensive\nInstead, see how the prediction changes when we add a feature\nShapley values: the average marginal contribution of a feature value over all possible coalitions\n\nMarginal contribution: the difference in the prediction with and without the feature\nCoalition: a subset of features\n\n\nread: https://towardsdatascience.com/using-shap-values-to-explain-how-your-machine-learning-model-works-732b3f40e137\n\n\n\nIndicate the contribution of each feature to the prediction\nSame units as the prediction (for classification, it is probability)\n\n\n\n\nimport shap\n\nX_test_enc = pd.DataFrame(\n    data = pipe[:-1].transform(X_test),\n    columns = feature_names,\n    index = X_test.index\n)\n\nexplainer = shap.Explainer(pipe[-1])\nexplanation = explainer(X_test_enc)\n\n# [sample, feature, class], class 1 is the positive class\nexplanation[:,:,1].values\n\n# visualize\n# also [sample, feature, class]\nshap.plots.waterfall(explanation[2,:,1])\n\n# or force plot: compresses the waterfall plot\nshap.plots.force(explanation[2,:,1])\n\n# summary plit to show impact of each feature\nshap.plots.beeswarm(explanation[:,:,1])\nshap.summary_plot(explanation[:,:,1])"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#classification-metrics",
    "href": "block_3/573_model_sel/573_model_sel.html#classification-metrics",
    "title": "Model Selection",
    "section": "",
    "text": "Need to evaluate the performance of a classifier not just by accuracy\n\n\n\n\nLinear regression with sigmoid function\nHyperparameter: C (default 1.0)\n\nLower C =&gt; more regularization =&gt; smaller coefficients =&gt; underfitting\n\n\nfrom sklearn.linear_model import LogisticRegression\n\npipe_lr = make_pipeline(preprocessor, LogisticRegression())\npd.DataFrame(cross_validate(pipe_lr, X_train, y_train, cv=10, return_train_score=True))\n\n\n\n\n\nChoose the positive case based on which class is more important to us or which class is rare\n\nspotting a class (e.g. fraud/ spam/ disease detection)\n\nCan do this in python:\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_predict\n\n# gets values without plot\nconfusion_matrix(y_train, cross_val_predict(pipe_lr, X_train, y_train, cv=5))\n\n# get values with plot\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nConfusionMatrixDisplay.from_estimator(pipe_lr, X_train, y_train, display_labels=['Not Fraud', 'Fraud']);\nOutput of confusion matrix by default:\n             | Predicted F [0]| Predicted T [1]\nActual F [0] | TN             | FP\nActual T [1] | FN             | TP\n\n\n\nPrecision \\[precision = \\frac{TP}{PP} =  \\frac{TP}{TP + FP}\\]\nRecall or True positive rate (TPR) \\[recall = \\frac{TP}{P} = \\frac{TP}{TP + FN}\\]\nf1-score \\[f1 = 2 \\cdot \\frac{precision * recall}{precision + recall} = 2 \\frac{1}{\\frac{1}{precision} + \\frac{1}{recall}}\\]\n\n\n\n\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_train, cross_val_predict(pipe_lr, X_train, y_train, cv=5)))\n\n# output\n#               precision    recall  f1-score   support\n\n#        Fraud       0.89      0.63      0.74       102\n#    Non fraud       1.00      1.00      1.00     59708\n\n#     accuracy                           1.00     59810\n#    macro avg       0.94      0.81      0.87     59810\n# weighted avg       1.00      1.00      1.00     59810\n\n\n\n\n\ntradeoff between precision and recall\nsetting a threshold is called setting the operating point\nChanging the threshold of the classifier (default of logistic regression is 0.5)\n\nremember predict_proba returns the probability of the positive class, if higher than threshold, then positive class\nas threshold increases, higher bar =&gt; precision increases and recall decreases (less FP but less TP)\nas threshold decreases, lower bar =&gt; precision decreases and recall increases (more TP but more FP)\n\n\n\n\nfrom sklearn.metrics import PrecisionRecallDisplay\n\nPrecisionRecallDisplay.from_estimator(\n    pipe_lr,\n    X_valid,\n    y_valid,\n    pos_label='Fraud',\n    name='LR',  # For Logistic Regression\n);\n\n\nWant the curve to be as close to the top right corner as possible (100% precision and 100% recall)\n\nThreshold of 0 in bottom left =&gt; All points are predicted to be positive =&gt; 100% recall and 0% precision\nThreshold of 1 in top right =&gt; All points are predicted to be negative =&gt; 0% recall and 100% precision\n\n\nOr if want just the values:\nfrom sklearn.metrics import precision_recall_curve\n\npd.DataFrame(\n    precision_recall_curve(\n        y_valid,\n        pipe_lr.predict_proba(X_valid)[:, fraud_column],\n        pos_label='Fraud',\n    ),\n    index=['precision', 'recall', 'threshold']\n).T\n\n\n\n\nArea under the precision recall curve\nHigher is better (0 is worst, 1 is best)\nAlso supports multi-class using one-vs-rest approach + averaging\nIMPORTANT:\n\nF1 score is for a given threshold and measures the quality of predict.\nAP score is a summary across thresholds and measures the quality of predict_proba.\n\n\nfrom sklearn.metrics import average_precision_score\n\nap_lr = average_precision_score(y_valid, pipe_lr.predict_proba(X_valid)[:, fraud_column], pos_label='Fraud')\n\n\n\nAP score\nf1 score\n\n\n\n\nmeasures quality of predict_proba\nmeasures quality of predict\n\n\n\n\nThese two metrics do not always agree*\n\n\n\n\n\n\nReceiver Operating Characteristic: plots the true positive rate (recall) against the false positive rate (1 - specificity)\n\nInstead of plotting precision against recall, plots recall (TPR) against FPR\n\n\n\\[FPR = \\frac{FP}{N} = \\frac{FP}{FP + TN}\\]\nfrom sklearn.metrics import RocCurveDisplay\n\nRocCurveDisplay.from_estimator(\n    pipe_lr,\n    X_valid,\n    y_valid,\n    pos_label='Fraud'\n);\n\n\nIdeal is top left corner (100% TPR and 0% FPR)\nLower left (Threshold of 1) =&gt; 0% TPR and 0% FPR\nUpper right (Threshold of 0) =&gt; 100% TPR and 100% FPR\n\n\n\n\nArea under the ROC curve\nAUC is the probability that a randomly chosen positive point has a higher score than a randomly chosen negative point\n\nAUC of 1.0: all positive points have a higher score than all negative points.\nAUC of 0.5: means random chance.\n\n\n\n\n\n\n\nIf we balance the classes:\n\nPrecision goes down, FP goes up\nRecall goes up, FN goes down\nF1 score goes down\nAP score goes down\nAUC score goes down\ngenerally reduce accuracy\n\n\n\n\n\nClass weights: penalize the minority class more\n\nLogisticRegression(\n  max_iter=500,\n  # give more importance to \"Fraud\" class (10x)\n  class_weight={'Non fraud': 1, 'Fraud': 10}\n\n  # class_weight='balanced' =&gt; automatically give more importance to minority class\n  )"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#regression-metrics",
    "href": "block_3/573_model_sel/573_model_sel.html#regression-metrics",
    "title": "Model Selection",
    "section": "",
    "text": "from sklearn.dummy import DummyRegressor\n\ndummy = DummyRegressor(strategy='mean') # default\npd.DataFrame(cross_validate(dummy, X_train, y_train, cv=10, return_train_score=True))\n\n\n\n\nLinear regression with L2 regularization\nHyperparameter: alpha (default 1.0)\n\nHigher alpha =&gt; more regularization =&gt; smaller coefficients =&gt; underfitting\nmore regularization : smaller coefficients =&gt; less sensitive to changes in the input features =&gt; less likely to overfit\n\n\nfrom sklearn.linear_model import Ridge\n\nlr_pipe = make_pipeline(preprocessor, Ridge())\npd.DataFrame(cross_validate(lr_pipe, X_train, y_train, cv=10, return_train_score=True))\n\n# fit model\nlr_pipe.fit(X_train, y_train)\n\n# get coefficients\ndf = pd.DataFrame(\n    data={\"coefficients\": lr_pipe.named_steps[\"ridge\"].coef_}, index=feature_names)\n\ndf.sort_values(\"coefficients\", ascending=False)\n\n\n\n\nRidge with cross-validation to find the best alpha\n\nfrom sklearn.linear_model import RidgeCV\n\nalphas = 10.0 ** np.arange(-6, 6, 1)\n\nridgecv_pipe = make_pipeline(preprocessor,\n  RidgeCV(alphas=alphas, cv=10))\nridgecv_pipe.fit(X_train, y_train);\n\n# best alpha\nbest_alpha = ridgecv_pipe.named_steps[\"ridgecv\"].alpha_\n\n\n\n\n\nCannot use equality since we are predicting a continuous variable (not classification)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetric\nDescription\nFormula\nMin Value\nMax Value\nCode Example (sklearn)\n\n\n\n\nMSE (Mean Squared Error)\nMeasures the average of the squares of the errors, i.e., the average squared difference between the estimated values and the actual value.\n\\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(Y_i - \\hat{Y}_i)^2\\]\n0 (perfect)\n∞\nfrom sklearn.metrics import mean_squared_error  mse = mean_squared_error(y_true, y_pred)\n\n\nRMSE (Root Mean Squared Error)\nSquare root of the MSE. It measures the standard deviation of the prediction errors or residuals.\n\\[RMSE = \\sqrt{MSE}\\]\n0 (perfect)\n∞\nrmse = mean_squared_error(y_true, y_pred, squared=False)\n\n\nMAE (Mean Absolute Error)\nMeasures the average of the absolute errors.\n\\[MAE = \\frac{1}{n}\\sum_{i=1}^{n}\\left\\| Y_i - \\hat{Y}_i \\right\\| \\]\n0 (perfect)\n∞\nfrom sklearn.metrics import mean_absolute_error  mae = mean_absolute_error(y_true, y_pred)\n\n\nMAPE (Mean Absolute Percentage Error)\nMeasures the average of the absolute percentage errors.\n\\[MAPE = \\frac{1}{n}\\sum_{i=1}^{n}\\left\\| \\frac{Y_i - \\hat{Y}_i}{Y_i} \\right\\| \\times 100\\% \\]\n0 % (perfect)\n∞ %\nfrom sklearn.metrics import mean_absolute_percentage_error  mape = mean_absolute_percentage_error(y_true, y_pred)\n\n\nR² (Coefficient of Determination)\nMeasures how well future samples are likely to be predicted by the model.\n\\[R^2 = 1 - \\frac{\\sum_{i=1}^{n}(Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^{n}(Y_i - \\bar{Y})^2}\\]\n-ve [if worst than mean]\n1 (perfect)\nfrom sklearn.metrics import r2_score  r2 = r2_score(y_true, y_pred)\n\n\n\n\nMAE is less sensitive to outliers than MSE\n\n\n\n\n\nCan plot the actual vs predicted values to see the error (and plot line of gradient 1 = perfect prediction)\n\nfrom sklearn.metrics import PredictionErrorDisplay\n\nPredictionErrorDisplay.from_estimator(\n    ridge_tuned,\n    X_train,\n    y_train,\n    kind='actual_vs_predicted',\n    subsample=None  # show all predictions\n)\n\n\nCan plot the residuals (error) vs predicted values to see if there is a pattern (e.g. heteroscedasticity)\n\nfrom sklearn.metrics import PredictionErrorDisplay\n\nPredictionErrorDisplay.from_estimator(\n    ridge_tuned,\n    X_train,\n    y_train,\n    subsample=None  # show all predictions\n)"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#data-cleaning",
    "href": "block_3/573_model_sel/573_model_sel.html#data-cleaning",
    "title": "Model Selection",
    "section": "",
    "text": "Need to have good data to build a good model.\nBad quality data:\n\nMissing values\nFew observations overall or in a specific group\nBiased sample (not representative of the population)\nNon-independent observations\nInaccurate measurements\nFabricated data\nOut of bounds values\nObsure column names\nTypos (spelling differences in categorical variables)\nUsing multiple values to represent same thing (e.g. NA, NONE, NULL, NaN)\nIncorrect data types\n\n\n\nBetter features help more than a better model.\n\nGood features would ideally:\n\ncapture the most important information\nallow learning with few examples\ngeneralize to new scenarios\n\nTrade-off for simple and expressive features:\n\nsimple features: overfitting risk is low but low score\nexpressive features: overfitting risk is high but high score\n\n\n\n\n\n\n\n\n\n\n\n\nPreprocessing\nEngineering\n\n\n\n\nstep of cleaning and preparing the data for analysis\ncreating new features from existing data\n\n\ngenerally HAVE to do, or error\noptional but can improve model performance\n\n\ne.g. scaling, normalization, imputation\ne.g. one-hot encoding, binning, get more data, group-wise normalization, transformation\n\n\n\n\nFeature Selection: removing irrelevant features, normally done after feature engineering"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#feature-engineering",
    "href": "block_3/573_model_sel/573_model_sel.html#feature-engineering",
    "title": "Model Selection",
    "section": "",
    "text": "Common guidelines:\n\nIs not unique or random\nHas a variance (not constant)\nAdds unique variance (is not constant transformation)\n\nchanging units (e.g. from meters to feet) is not feature engineering\n\nIs ideally interpretable\n\nExamples:\n\nLooking up and getting additional data\nDiscretization (binning): e.g. age -&gt; age group\nGroup-wise normalization: express feature relative to group mean/median\nTransformation: e.g. height, weight -&gt; BMI\n\n\n\n\nparameter: degree\n\nhigher degree: more expressive features =&gt; more overfitting\n\nTry to capture non-linear relationships\n\nLinear regression can only capture lines, planes, hyperplanes\n\ne.g. add a squared feature (feat1^2)\n\n\n\n\n\nDo polynomial then StandardScaler\n\navoids polynomials being very large\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2)\n# degree: max degree of polynomial features\nX_enc = poly_feats.fit_transform(X_toy)\npd.DataFrame(X_enc, columns=poly_feats.get_feature_names_out()).head()\nDegree example:\n\n\n\nDimension\nDegree\nExamples\n\n\n\n\n1\n2\n1, x, x^2\n\n\n2\n2\n1, x, y, x^2, xy, y^2\n\n\n\n\n\n\n\\[ \\hat{y} = Xw \\]\n\nX: design matrix (n x d)\nw: weights (d x 1)\ny: target (n x 1)\n\nFor polynomial features:\n\\(Z = [1, x, x^2]\\)\n\\[ \\hat{y} = Zw = \\begin{bmatrix} 1 & x_1 & x_1^2 \\\\ 1 & x_2 & x_2^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x_n^2 \\end{bmatrix} \\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\end{bmatrix} = \\begin{bmatrix} w_0 + w_1x_1 + w_2x_1^2 \\\\ w_0 + w_1x_2 + w_2x_2^2 \\\\ \\vdots \\\\ w_0 + w_1x_n + w_2x_n^2 \\end{bmatrix} \\]\n\n\n\n\nPolynomial features can be expensive to compute\nKernel trick: compute dot product between two vectors in a higher dimensional space without computing the transformation explicitly"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#feature-importance",
    "href": "block_3/573_model_sel/573_model_sel.html#feature-importance",
    "title": "Model Selection",
    "section": "",
    "text": "import seaborn as sns\n\nsns.heatmap(df.corr(), annot=True)\n\nShows correlation between features\nExtremely simplistic:\n\nOnly looks at linear correlation\nOnly looks at pairwise correlation (isolates features)\n\n\n\n\n\n\nCorrelation among features might make coefficients completely uninterpretable.\nFairly straightforward to interpret coefficients of ordinal features.\nIn categorical features, it’s often helpful to consider one category as a reference point and think about relative importance.\nFor numeric features, relative importance is meaningful after scaling.\nYou have to be careful about the scale of the feature when interpreting the coefficients.\nRemember that explaining the model explaining the data or explaining how the world works.\nThe coefficients tell us only about the model and they might not accurately reflect the data."
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#feature-selection",
    "href": "block_3/573_model_sel/573_model_sel.html#feature-selection",
    "title": "Model Selection",
    "section": "",
    "text": "More features =&gt; more complex model =&gt; overfitting\nFeature selection find features that are important for prediction and remove the rest.\n\nWhy?\n\nInterpretability: easier to explain with fewer features\nComputation: faster to train simpler models (curse of dimensionality)\nData collection: cheaper to collect fewer features\nStorage: less space to store fewer features\nFundamental Tradeoff: reduce overfitting by reducing complexity\n\nHow?\n\ndomain knowledge\nautomatic feature selection\n\nmodel-based selection\nrecursive feature elimination\nforward/backward selection\n\n\n\n\n\nFast and easy\nArbitrary, does not take into account feature combinations\ne.g. correlation threshold, variance threshold\n\n\n\n\n\nUse a supervised model to judge the importance of each feature, and keep only the most important ones.\nCan use different model than the final estimator.\nUse a model that provides some measure of feature importance. (e.g. decision trees, linear models, etc.)\n\nGeneral Steps:\n\nSelect a threshold for which features to keep.\nDiscard features below the threshold.\n\nfrom sklearn.feature_selection import SelectFromModel\n\nselect_lr = SelectFromModel(Ridge(), threshold=\"median\")\n\n# Add feature selection to pipeline (after preprocessing)\npipe = make_pipeline(preprocessor, select_lr, Ridge())\n\n# access selected features\npipe.named_steps[\"selectfrommodel\"]\n# can do: .n_features_in_, .threshold_, .get_support()\n\n\n\n\nRecursively remove features, build a model on those features that remain, and then repeatedly construct a model and remove the weakest feature until a specified number of features remain.\nIteratively eliminates unimportant features.\nComputationally expensive\n\n\n\n\nDecide the number of features to select.\nAssign importances to features, e.g. by fitting a model and looking at coef_ (Linear models) or feature_importances_ (RandomForestClassifier).\nRemove the least important feature.\nRepeat steps 2-3 until only \\(k\\) features are remaining.\n\nfrom sklearn.feature_selection import RFE\n\nrfe = RFE(Ridge(), n_features_to_select=120)\n\npipe_rf_rfe = make_pipeline(preprocessor, rfe, RandomForestRegressor(random_state=42))\n\nresults[\"rfe + rfe\"] = mean_std_cross_val_scores(\n    pipe_rf_rfe, X_train, y_train, return_train_score=True\n)\nNote: for categorical features, RFE might remove some OHE features. This is an unresolved issue.\n\n\n\n\nRFE but using CV to find the optimal number of features to keep.\nvery slow since there is CV within CV.\n\nfrom sklearn.feature_selection import RFECV\n\nrfecv = RFECV(Ridge())\n\npipe_rf_rfecv = make_pipeline(\n    preprocessor, rfecv, RandomForestRegressor(random_state=42)\n)\npipe_rf_rfecv.fit(X_train, y_train);\n\n# optimal number of features\npipe_rfe_ridgecv.named_steps[\"rfecv\"].n_features_\n# which features were selected\npipe_rfe_ridgecv.named_steps[\"rfecv\"].support_\n\n\n\n\n\nNot based on feature importance.\nDefine a scoring function \\(f(S)\\) that measures the quality of a subset of features \\(S\\).\nSearch through all possible subsets of features and pick the subset with the best score.\n\ne.g. for A, B, C search through {}, {A}, {B}, {C}, {A, B}, {A, C}, {B, C}, {A, B, C}…\n# subsets = \\(2^p\\) where \\(p\\) is the number of features\n\nForward selection: start with no features and add one at a time.\n\nadd the feature that results in the best score.\n\nBackward selection: start with all features and remove one at a time.\n\nremove the feature that results in the best score.\n\n\nfrom sklearn.feature_selection import SequentialFeatureSelector\n\npipe_forward = make_pipeline(\n    preprocessor,\n    SequentialFeatureSelector(Ridge(), direction=\"forward\"),\n    RandomForestRegressor(random_state=42),\n)\n\n# results['rf_forward_fs'] = mean_std_cross_val_scores(pipe_forward, X_train, y_train, return_train_score=True)\n\npipe_forward.fit(X_train, y_train)\n\n\n\n\nA feature relevance is only defines in the context of other features.\n\nAdding/removing features can change the importance of other features.\n\nRelevance != causality\nThe methods don’t always work."
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#loss-function",
    "href": "block_3/573_model_sel/573_model_sel.html#loss-function",
    "title": "Model Selection",
    "section": "",
    "text": "A function that measures how well a model fits the data\nThe goal is to minimize the loss function\n\nsmaller loss \\(\\rightarrow\\) better model\n\nCaptures what is important to minimize\nCommon loss functions:\n\nLeast squares loss\nAbsolute error loss\nHuber loss\n\n\n\n\n\n\n\n\n\n\n\n\nScoring Metric\nLoss Function\n\n\n\n\nReport the results (often several metrics)\nUsed to train/ fit the model\n\n\nFind best model hyperparams\nFind best model parameters\n\n\nCan pick those suitable for the task\nConstrainted (need to be differentiable)"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#regression-loss-functions",
    "href": "block_3/573_model_sel/573_model_sel.html#regression-loss-functions",
    "title": "Model Selection",
    "section": "",
    "text": "\\[J(\\omega) = \\sum_{i=1}^{n} (y_i - \\omega^T x_i)^2 \\]\n\nDefine loss as sum of squared errors (difference between prediction and actual value)\nPenalizes heavily for large errors\n\nsensitive to outliers\n\nFor a well-defined OLS, there is a unique solution (not always the case)\n\n\\(\\omega = (X^T X)^{-1} X^T y\\)\n\n\\(X^T X\\) must be invertible\n\n\n\n\n\n\n\\[J(\\omega) = \\sum_{i=1}^{n} |y_i - \\omega^T x_i| \\]\n\nLess sensitive to outliers\nBut minimization is harder\n\nnot differentiable at 0\nno closed form solution\n\nAlternative: Huber Loss\n\nbehaves like OLS for small errors"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#classification-loss-functions",
    "href": "block_3/573_model_sel/573_model_sel.html#classification-loss-functions",
    "title": "Model Selection",
    "section": "",
    "text": "\\[ y_i w^T x_i = \\begin{cases} \\text{correct} & y_i w^T x_i &gt; 0 \\\\ \\text{incorrect} & y_i w^T x_i &lt; 0 \\end{cases} \\]\n\n\\(y_i\\): positive\n\ncorrect: \\(w^T x_i &gt; 0 \\rightarrow y_i w^T x_i &gt; 0\\)\nincorrect: \\(w^T x_i &lt; 0 \\rightarrow y_i w^T x_i &lt; 0\\)\n\n\\(y_i\\): negative\n\ncorrect: \\(w^T x_i &lt; 0 \\rightarrow y_i w^T x_i &gt; 0\\)\nincorrect: \\(w^T x_i &gt; 0 \\rightarrow y_i w^T x_i &lt; 0\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0-1 Loss\n Exponential Loss \n Hinge Loss \n Logistic Loss \n\n\n\n\ndescription\nOnly cares about whether the prediction is correct or not\nPunishes the wrong predictions exponentially, value gets smaller as more confident for correct predictions\n- Confident and correct predictions are not penalized- Grows linearly for incorrect predictions\n- Used in logistic regression- Smooths the degenerate 0-1 loss with log-sum-exp\n\n\nfunction\n\\[\\mathbb{1}\\{y \\neq w^T x\\}\\]\n\\[e^{-y w^T x}\\]\n\\[\\max(0, 1 - y w^T x)\\]\n\\[\\log(1 + e^{-y w^T x})\\]\n\n\nloss\n- when incorrect, loss is 1- when correct, loss is 0- Total loss is the sum of all losses (number of incorrect predictions)\n- when incorrect, loss is large- when correct, loss is small\n- if correct \\(y w^T x &gt; 1\\), \\(1-y w^T x &lt;0\\) - if incorrect \\(y w^T x &lt; 1\\), \\(1-y w^T x &gt;0\\)\n- Convex, differentiable, and smooth\n\n\n\n\nHinge loss + L2 regularization = SVM\n\n\n\n\nLet \\(z = y_i w^T x_i\\)\n\nSigmoid loss: \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)\n\nmaps to [0, 1] (probability)\nfor predict_proba\n\nLogistic loss: \\(\\log(1 + e^{-z})\\)\n\nmaps \\(y_i w^T x_i\\) to \\([0, \\infty]\\) = loss contribution for a single example\nimportant for fit\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nmodel.predict(X_test)\nmodel.predict_proba(X_test)\n\nhyperparameters:\n\nC: inverse of regularization strength\n\nsmaller C \\(\\rightarrow\\) stronger regularization \\(\\rightarrow\\) smaller weights \\(\\rightarrow\\) simpler model \\(\\rightarrow\\) underfitting"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#final-cost-function",
    "href": "block_3/573_model_sel/573_model_sel.html#final-cost-function",
    "title": "Model Selection",
    "section": "",
    "text": "\\[J(w) = \\text{loss} + \\lambda \\cdot \\text{regularization} \\]\n\nmore complex model =&gt; higher regularization =&gt; higher cost"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#regularization",
    "href": "block_3/573_model_sel/573_model_sel.html#regularization",
    "title": "Model Selection",
    "section": "",
    "text": "adding penalty on model complexity (to reduce overfitting)\nBenefits:\n\nreduce overfitting\nless prone to outliers\n\nKey Idea: Pick the line/ hyperplane with the smalles slope (simplest model)\n\n\n\n\n\n\n\n\n\nL0 Norm\nL1 Norm\nL2 Norm\n\n\n\n\nNumber of non-zero elements\nSum of absolute values\nSquare root of the sum of squared values\n\n\n\\(\\|\\|w\\|\\|_0\\)\n\\(\\|\\|w\\|\\|_1\\)\n\\(\\|\\|w\\|\\|_2\\)\n\n\nN/A\n\\(\\sum_{i=1}^{n} \\|x_i\\|\\)\n\\(\\sqrt{\\sum_{i=1}^{n} x_i^2}\\)"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#l0-regularization",
    "href": "block_3/573_model_sel/573_model_sel.html#l0-regularization",
    "title": "Model Selection",
    "section": "",
    "text": "\\[J(\\omega) = ||X\\omega - y||_2^2 + \\lambda ||\\omega||_0\\]\n\nWhere \\(||\\omega||_0\\) is the L0-norm: number of non-zero elements in \\(\\omega\\)\nTo increase the degrees of freedom by 1, need to decrease the error by \\(\\lambda\\)\n\ndegrees of freedom: number of parameters in the model\nsmaller DOF is preferred\n\nHard to optimize since it is non-convex and non-differentiable"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#l2-regularization",
    "href": "block_3/573_model_sel/573_model_sel.html#l2-regularization",
    "title": "Model Selection",
    "section": "",
    "text": "\\[J(\\omega) = ||X\\omega - y||_2^2 + \\lambda ||\\omega||_2^2\\]\n\nWhere \\(||\\omega||_2^2\\) is the L2-norm: sum of squared values of \\(\\omega\\)\nWeights decrease, but DO NOT become zero\nBig \\(\\lambda\\) =&gt; more regularization =&gt; lower weights =&gt; simpler model =&gt; underfitting\n\n\\(\\lambda=0\\) same as OLS\n\nAs \\(\\lambda\\) increases (simpler model):\n\n\\(||X\\omega - y||_2^2\\) increases (less accurate)\n\\(||\\omega||_2^2\\) decreases (smaller weights)\n\n\n\n\n\nTheory: As n grows, \\(\\lambda\\) should be in the range of \\([0,\\sqrt{n}]\\)\nDo this by optimizing validation error or CV error\n\n\n\n\nfrom sklearn.linear_model import Ridge\nridge = Ridge(alpha=1.0) # alpha is lambda\nridge.fit(X_train, y_train)"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#l1-regularization",
    "href": "block_3/573_model_sel/573_model_sel.html#l1-regularization",
    "title": "Model Selection",
    "section": "",
    "text": "\\[J(\\omega) = ||X\\omega - y||_2^2 + \\lambda ||\\omega||_1\\]\n\nSimilar to L2 but uses L1-norm instead.\n\nboth shrink weights and result in simpler models\n\nWeights can become zero\n\nsome features are completely ignored\nkinda like feature selection\n\n\n\n\n\nsparsity: linear function with many zero-valued coefficients\nL0 and L1 regularization =&gt; sparse models\n\n\n\n\nfrom sklearn.linear_model import Lasso\nlasso = Lasso(alpha=1.0) # alpha is lambda\nlasso.fit(X_train, y_train)"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#regularization-and-scaling",
    "href": "block_3/573_model_sel/573_model_sel.html#regularization-and-scaling",
    "title": "Model Selection",
    "section": "",
    "text": "Scaling does not matter:\n\nDecision trees/ Naive Bayes/ Random Forests: only look at 1 feature at a time\nLeast Squares Loss: just change weights\n\nScaling matters:\n\nKNN: distance will be affected by large values\nRegularized Least Squares: regularization term will be affected by large values"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#regularization-and-collinearity",
    "href": "block_3/573_model_sel/573_model_sel.html#regularization-and-collinearity",
    "title": "Model Selection",
    "section": "",
    "text": "L2 regularization: only 1 unique solution\n\ne.g. 3 collinear features =&gt; all equal weights (1/3)\n\nL1 regularization: many solutions\n\ne.g. 3 collinear features =&gt; (1,0,0), (0,1,0), (0,0,1), (1/2,1/2,0), etc."
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#sklearn-elastic-net",
    "href": "block_3/573_model_sel/573_model_sel.html#sklearn-elastic-net",
    "title": "Model Selection",
    "section": "",
    "text": "\\[J(\\omega) = ||X\\omega - y||_2^2 + \\lambda(\\alpha ||\\omega||_1 + (1-\\alpha)||\\omega||_2^2)\\]\n\n\\(\\lambda\\) is the regularization parameter\n\\(\\alpha\\) promotes:\n\nsparcity in L1\nsmoothness in L2\n\nFunction is strictly convex, so there is a unique solution (no collinearity problem)\n\nfrom sklearn.linear_model import ElasticNet\nelastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5) # alpha is lambda, l1_ratio is alpha\nelastic_net.fit(X_train, y_train)"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#ensemble-methods",
    "href": "block_3/573_model_sel/573_model_sel.html#ensemble-methods",
    "title": "Model Selection",
    "section": "",
    "text": "Key Idea: Groups make better decisions than individuals, especially when the group members are diverse.\nTree-based is the most successful ensemble method\n\nRandom Forest\nGradient Boosted Trees\n\nWays to combine models:\n\nAveraging\nStacking\n\n\n\n\n\nDoes not make the individual models better but adds diversity =&gt; better ensemble\nThink of it visually as high variance (not precise) and low bias (accurate on average)\n((insert image from slides))\n\n\n\n\nRF\nBoosting\n\n\n\n\nMany independent trees\nTrees are dependent on each other\n\n\nrandom diversity\nnon-random diversity\n\n\ncan be parallelized\nsequential\n\n\n\n\n\n\nTrain each model on a different subset of the data\nBootstrap Aggregation: Sampling with replacement.\n\n\n\n\n\n\nBagging + Random set of features at each node.\nDeep trees, normally full depth.\nGeneral Idea:\n\nSingle tree is likely to overfit\nUse a collection of diverse trees\nEach tree overfits on some part of the data but reduce overfitting by averaging\n\nAt each node:\n\nrandomly select a subset of features (independednt from each node)\nfind best split among the subset of features\ngrow tree to full depth\n\nPrediction:\n\nVote the trees\n\n\n\n\n\nuses soft prediction (avg predict_proba)\neasy to fit in parallel (each tree is independent)\nSome important hyperparameters:\n\nn_estimators: number of trees (higher = more complex)\nmax_depth: max depth of each tree (higher = more complex)\nmax_features: max number of features to consider at each split (higher = more complex)\n\nDefaults:\n\nClassification: sqrt(# features)\nRegression: # features / 3 or # features\n\n\n\nmax_features and n_estimators get better scores as values increase but more computationally expensive\n\ndifferent from standard “hyperparameter tuning”\n\n\n\n\n\n\n\nRandom Forest + Random Thresholds (instead of the best threshold)\nIndividually, each tree is not that good\nEach tree is grown to full depth\nIn sklearn: ExtraTreesClassifier\n((insert image about the variance comparison))\n\n\n\n\n\nBoost performance through reweighting of data\n\n\nFit initial model on training data\nreweight the training data so that samples that the initial model got wrong became more important\nFit the next model on the reweighted data\n\nwill prioritize the samples that the previous model performed poorly on\n\nRepeat until you have a set of models\nCombine the models into a single model\n\n\n\n\n\nbetter than AdaBoost\nKey Idea: Trying to decrease the residuals of the previous model by creating new tree that can predict the residuals (then use it as correction)\n\nNo randomization\ncombine multiple shallow (depth 1-5) trees\nbuilt sequentially\n\nWas computationally expensive but now it is not:\n\nXGBoost, LightGBM, CatBoost\n\n\n\nConstruct a base tree (root node), no splits\nBuild next tree based on errors of previous tree\n\nthe errors are the residuals\n\nCombine the tree from step 1 with the tree from step 2\nRepeat until max number of estimators is reached\nCombine the trees into a single model\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightbgm.sklearn import LGBMClassifier\n\nImportant Hyperparameters:\n\nn_estimators: number of boosting rounds (higher = more complex)\nlearning_rate: controls how strongly each tree tries to correct the mistakes of the previous trees (higher = more corrections = more complex)\nmax_depth: max depth of each tree (higher = more complex)\nscale_pos_weight: balancing of positive and negative weights\n\n\n\n\n\n\n\n\nTake vote of all models\n\n\n\n\n\nvoting='soft' uses predict_proba and averages the probabilities\n\ngood if trust predict_proba\n\nvoting='hard' uses predict and takes the majority vote\n\nfrom sklearn.ensemble import VotingClassifier\n\naveraging_model = VotingClassifier(\n    list(classifiers.items()), voting=\"soft\"\n)  # need the list() here for cross-validation to work!\n\n\n\n\nKey idea: Weighted voting via simple model\n\nuse outputs as inputs to a new model\n\nUse a simpler model like logistic regression as “meta-model”\nEach model is a feature for the meta-model"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#model-interpretability",
    "href": "block_3/573_model_sel/573_model_sel.html#model-interpretability",
    "title": "Model Selection",
    "section": "",
    "text": "Looking at feature importance can help us understand the model\nWhy?\n\nDebugging: find systematic errors\nDiagnosing bias: find underlying biases\nFeature engineering: find new features\nFeature selection: find unimportant features\nModel interpretability can be leveraged by domain experts to diagnose systematic errors and underlying biases of complex ML systems.\n\n\n\n\n\nKey idea: How much does each feature contribute to the model?\nUse a tree-based model (e.g. random forest) to measure feature importance\n\nDo not use single decision tree because it is not stable\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nEasy to interpret\nNot stable (can change a lot with small changes in data)\n\n\nCan be used for feature selection\nCan be biased towards high cardinality* features (e.g. ID)\n\n\nCan be used for feature engineering (e.g. interaction features)\nCan be biased towards numerical features (e.g. age)\n\n\nna\nCan only be used for tree-based models (e.g. random forest)\n\n\n\n*high cardinality: a feature with a lot of unique values\n\n\n\n\nRandomly shuffle a feature and measure how much the model performance changes\nNo issue with high cardinality features\nCan add a random noise column to measure feature importance\n\nCan be used as a threshold to remove features\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nCan be used for any model (not just tree-based models)\nCorrelated features are still an issue\n\n\nno issue with high cardinality features\nWe do not know if they contribute positively or negatively to the model\n\n\nSimple concept, easy to understand\n\n\n\nUse validation data, so not picking up features that overfit\n\n\n\n\n\n\nfrom sklearn.inspection import permutation_importance\n\nperm_importance = permutation_importance(\n    model,\n    X_test,\n    y_test,\n    n_repeats=10,\n    random_state=42,\n    n_jobs=-1\n)\n\ncan be used on non sklearn models\ndo not have a sign\nonly tell us importance not direction of importance\n\n\n\n\n\n\nWhat is the contribution of each feature to the prediction?\nThe order of the features matters due to interactions\nCould use backward/ forward selection to find the best subset of features but very expensive\nInstead, see how the prediction changes when we add a feature\nShapley values: the average marginal contribution of a feature value over all possible coalitions\n\nMarginal contribution: the difference in the prediction with and without the feature\nCoalition: a subset of features\n\n\nread: https://towardsdatascience.com/using-shap-values-to-explain-how-your-machine-learning-model-works-732b3f40e137\n\n\n\nIndicate the contribution of each feature to the prediction\nSame units as the prediction (for classification, it is probability)\n\n\n\n\nimport shap\n\nX_test_enc = pd.DataFrame(\n    data = pipe[:-1].transform(X_test),\n    columns = feature_names,\n    index = X_test.index\n)\n\nexplainer = shap.Explainer(pipe[-1])\nexplanation = explainer(X_test_enc)\n\n# [sample, feature, class], class 1 is the positive class\nexplanation[:,:,1].values\n\n# visualize\n# also [sample, feature, class]\nshap.plots.waterfall(explanation[2,:,1])\n\n# or force plot: compresses the waterfall plot\nshap.plots.force(explanation[2,:,1])\n\n# summary plit to show impact of each feature\nshap.plots.beeswarm(explanation[:,:,1])\nshap.summary_plot(explanation[:,:,1])"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html",
    "href": "block_3/561_regression/561_regression.html",
    "title": "Regression I",
    "section": "",
    "text": "Basic idea: fit a line to data\nCan help with:\n\nEstimation: how to estimate the true (but unknown) relation between the response and the input variables\nInference: how to use the model to infer information about the unknown relation between variables\nPrediction: how to use the model to predict the value of the response for new observations\n\nBefore starting, normally do some exploratory data analysis (EDA) to get a sense of the data\n\nfind out size\ndistribution of variables\nmissing values/ outliers\nrelationships between variables\n\n\n\n\n\n\nSimple linear regression: one response variable and one explanatory variable\n\nLet \\((X_i, Y_i)\\) for \\(i = 1, \\dots, n\\) random sample of size \\(n\\) from a population.\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\]\nWhere:\n\n\\(Y\\) is the response variable\n\\(X\\) is the explanatory/ input variable\n\\(\\epsilon_i \\sim N(0, \\sigma^2)\\) and \\(\\epsilon_1, \\dots, \\epsilon_n\\) are independent.\n\nIt represents other factors not taken into account in the model\n\n\n\nImage from the book: “Beyond Multiple Linear Regression”, from Paul Roback and Julie Legler\nAssumptions:\n\nconditional expectation of \\(Y\\) is linearly related to \\(X\\) \\[E(Y|X) = \\beta_0 + \\beta_1 X\\]\niid\nall random error \\(\\epsilon_i\\) is normally distributed with mean 0 and variance \\(\\sigma^2\\)\n\n\\[E(\\epsilon | X) = 0 \\\\ and \\\\ Var(\\epsilon | X) = \\sigma^2\\]\n\n\n\nLine that minimizes the sum of squared errors (SSE) or distance between the line and the data points\n\n\\[SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 = \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i)^2\\] [[fact check the equation above]]\n\n\n\n# fit a linear model\nlm(Y ~ X, data = data)\n\n# plot linear line using ggplot2\nggplot(data, aes(x = X, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\nTurn categorical into dummy variables (numerical)\n\ne.g. X = Fireplace in a house, Y = Price of the house\nMake dummy variable: \\(X_2 = 1\\) if house has fireplace, \\(X_2 = 0\\) if house doesn’t have fireplace\n\\(E(Y|X_2) = \\beta_0+ \\beta_2 X_2\\)\n\nSome cool thing:\n\n\\(\\beta_2 = E(Y|X_2 = 1) - E(Y|X_2 = 0) = \\mu_1 - \\mu_0\\)\n\nSAME as null hypothesis for t-test\n\n\\(\\beta_0 = E(Y|X_2 = 0)\\)\n\n\n\n\n\n\\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\]\n\nFor Linear Regression, we are estimating:\n\n\\(\\beta_0\\) and \\(\\beta_1\\)\n\n\n\n\n\nlm &lt;- lm(Y ~ X, data = data) |&gt;\n    tidy() |&gt;\n    mutate_if(is.numeric, round, 3) # round to 3 decimal places\n\n# Returns:\n# term  estimate    std.error   statistic   p.value\n# &lt;chr&gt; &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;             &lt;dbl&gt;\n\n\n\nIf take LR for new samples, we will get different estimates\n\nStandard Error is the standard deviation of the sampling distribution of the estimate\n\nMethods to measure uncertainty in estimation:\n\ncompute SE after taking multiple samples\nrarely take multiple samples, just take it theoretically (what lm does), see third col above\ncan also bootstrap from sample to get distribution of estimates -&gt; get standard error\n\n\n\n\n\nTake multiple samples and compute the variance of the estimates\n\n\n\n\n\nAssumption: conditional distribution of \\(Y_i\\) given \\(X_i\\) is normal with mean \\(\\beta_0 + \\beta_1 X_i\\) and variance \\(\\sigma^2\\), the \\(\\epsilon_i\\) term.\nUse t test with \\(df=n-p\\)\n\nn = sample size\np = number of parameters in the model\n\nUsed by lm to get standard error\nIf conditional dist is normal =&gt; error term is normal =&gt; sampling distribution of \\(\\hat{\\beta_1}\\) is normal\n\nOr if the sample size is large enough, sampling distribution of \\(\\hat{\\beta_1}\\) is approximately normal (CLT)\n\n\n\n\n\n\nBootstrap Normal Theory\n\\[\\beta \\pm z_{\\alpha/2} SE(\\hat{\\beta})\\]\n\nSE = stdev of bootstrap distribution\n\nBootstrap Percentile Method\n\nbasically take the 2.5th and 97.5th percentile of the bootstrap distribution (95% confidence interval)\n\n\\[\\hat{b}_{\\alpha/2} , \\hat{b}_{1-\\alpha/2}\\]\n\n\n\n\n\ntidy(lm_s, conf.int = TRUE)\n\nIf 0 is in the confidence interval, then we can’t reject the null hypothesis (\\(\\beta_1 = 0\\))\n\n\n\n\n\nCan do:\n\n\\(H_0: \\beta_0 = 0\\) vs \\(H_a: \\beta_0 \\neq 0\\)\n\nUsually not interesting\n\n\\(H_0: \\beta_1 = 0\\) vs \\(H_a: \\beta_1 \\neq 0\\)\n\nAnswers: is there a linear relationship between \\(X\\) and \\(Y\\)?\n\n\nRecall, test statistic under null hypothesis:\n\\[ t = \\frac{\\hat{\\beta_1}}{SE(\\hat{\\beta_1})} \\]\nWhich has a t-distribution with \\(n-p\\) degrees of freedom\n\n\n\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ip} + \\epsilon_i\\]\ne.g. \\[ Y_i = \\beta_0 + \\beta_1 height_i + \\beta_2 weight_i + \\epsilon_i \\]\n\nLinear regression models the conditional expectation as linear combination of the predictors\n\n\\[ E(Y|X_1, X_2, \\dots, X_p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p \\]\n\n\n# visualize the relationship between variables\n# using ggpairs from GGally package\nggpairs(data)\n\n# fit the model\nlm_s &lt;- lm(Y ~ X1 + X2 + X3, data = data)\n\n\n\n\nFor 2 categories (e.g. Fireplace exits or not) we only need 1 dummy variable\n\ncorresponding to 0 or 1\n\nFor 3 categories (e.g. old, modern, new) we need 2 dummy variables\n\nM corresponds to 1 if house is modern, 0 otherwise\nN corresponds to 1 if house is new, 0 otherwise\n\nFor \\(k\\) categories, we need \\(k-1\\) dummy variables\n\nlm(Y ~ cat_var, data = data) # cat_var is a categorical variable\n\n\n\n\n\nNormally when you have categorical and continuous variables, the linear model will:\n\nfit different intercepts for each category\nfit the same slope for each category (simpler)*\n\nInteraction terms allow us to fit different slopes for each category\n\n*Occam’s razor: if can explain with simpler model, do so\ne.g.\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 Z_i + (\\beta_3 X_i Z_i) + \\epsilon_i\\]\n\n\\(X_i\\) is a continuous variable\n\\(Z_i\\) is a categorical variable\n\\(\\beta_3\\) is the interaction term\n\nfit &lt;- lm(score ~ age*sex, data = dat)\ntidy(fit) |&gt; mutate_if(is.numeric, round, 3)\n# Returns columns:(intercept), age, sexmale, age:sexmale\n\nIntercept: average teaching score of female instructors of zero age\nage: slope for female instructors. Increase in age by 1 year will increase teaching score by age for female instructors\nsexmale: holding age constant, the difference in teaching score between male and female\nage:sexmale is the offset slope. Increase in age by one year is associated with the increase in teaching score of age + age:sexmale for male instructors.\n\n\n\n\n\nIs this better than nothing/ null model (only intercept)?\n\nIn other words is \\(E(Y|X)\\) better than \\(E(Y)\\)?\n\n\n\n\n\nTotal sum of squares (TSS):\n\n\\(TSS = \\sum (y_i - \\bar{y})^2\\)\nSum of squares from the null (intercept only) model\n\nExplained sum of squares (ESS):\n\n\\(ESS = \\sum (\\hat{y}_i - \\bar{y})^2\\)\nmeasures how mich it is explained by the model\n\nbetter model = larger ESS\n\n\nResidual sum of squares (RSS):\n\n\\(RSS = \\sum (y_i - \\hat{y}_i)^2\\)\nRSS == SSE (sum of squared errors) == n * MSE (mean squared error)\nEstimated parameters minimizes RSS (objective function)\n\nWhere:\n\n\\(y_i\\) observed ith value of y\n\\(\\hat{y}_i\\) predicted value of y with a model (of sample)\n\\(\\bar{y}_i\\) mean of y\n\n\nIf using linear regression using least squares:\n\\[ TSS = ESS + RSS \\]\nRecall: Residuals is the difference between the observed value and the predicted value. \\(r_i = y_i - \\hat{y}_i\\)\n\n\n\n\\[ R^2 = \\frac{ESS}{TSS} = 1 - \\frac{RSS}{TSS} \\]\n\n\\(R^2\\) is the proportion of variance explained by the model\n\nTells us how well the regression model explains the variation in the data\n\n\\(R^2\\) is between -inf and 1 (1 being perfect fit)\n\ngenerally between 0 and 1 since expect TSS &gt; RSS\n\nONLY IF the model has an intercept and is estimated by LS\n\nonly less than 0 if model is worse than null model\n\n\nglance(fit)\n\n\n\n\\(R^2\\) increases with number of predictors\n\n\\(R^2\\) will never decrease when adding predictors\n\nComputed based on “in-sample” predictions\n\nWe do not know how well the model will perform on new data\nBasically only on train data, dunno performance on test data\n\nIs it useful?\n\nYes, to compare size of residuals of fitted model to null model\nCannot be used to test any hypothesis since its distribution is unknown\n\n\n\n\n\n\nIs no longer coefficient of determination\nMeasures correlation between the true and predicted values\n\\(R^2 = Cor(Y, \\hat{Y})^2\\) if \\(\\hat{Y}\\) is a prediction obtained from a LR with an intercept estimated by LS.\n\nThis is ued to assess the prediction performance of a model\n\n\n\n\n\n\n\nCompare two models\n\n\\(reduced\\): intercept + some predictors\n\\(full\\): intercept + predictors\n\nIs the full model better than the reduced model? Simultaneously testing if many parameters are 0\nF-test is a global test\n\ntests if all parameters are 0\nif F-test is significant, then at least one parameter is not 0\n\n\n\\[F \\propto \\frac{(RSS_{reduced} - RSS_{full}) / k}{RSS_{full} / (n - p)}\\]\n\nparams:\n\nk = number of parameters tested (difference between models)\np = number of predictors in the full model (s + 1)\n\n\nlm_red &lt;- lm(score~1, data=dat) # intercept only\nlm_full &lt;- lm(score~ age + sex, data=dat)\n\nanova(lm_red,lm_full)\n# F-test tests H0: coef_age = coef_sex = 0\n\nglance(lm_full)\n# also includes F-statistic in \"statistic\" column + p-value\n# compares it to null model (intercept only)\n\nF-test can test multiple parameters at once\n\ne.g. \\(\\beta_1 = \\beta_2 = \\beta_3 = 0\\)\n\nFor one parameter, F-test is equivalent to t-test\n\ne.g. \\(\\beta_1 = 0\\)\nFor one parameter, \\(t^2 = F\\)\n\nwhere t is the t-statistic and F is the F-statistic\n\n\n\n\n\n\nif F-statistic is large (F &gt; 1), then the full model is better than the reduced model\nif F-statistic is small (F &lt; 1), then the full model is not better than the reduced model\n\n\n\n\n\nBoth depend on RSS and TSS, there is a formula to convert between them\nF-test has a known F-distribution (under certain assumptions) so we can use it to make probabilistic statements\n\n\n\n\n\n\n\n\n\nWhat is your goal?\n\nInference: understand the relationship between the response and the predictors\nPrediction: predict the response for future observations\n\n\n\n\n\n\nMean squared error (MSE): average squared error on new data\n\n\\(MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\)\nTrain MSE: MSE on training data\nTest MSE: MSE on test data\n\nResidual Sum of Squares (RSS): n * MSE\n\n\\(\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\)\nsmall is good\nresiduals are measured in training data\n\nResidual Standard Error (RSE): estimates the standard deviation of the residuals\n\n\\(\\sqrt{\\frac{1}{n-p} RSS}\\)\n\np = number of parameters in the model\n\nBased on training data to evaluate fit of the model (small is good)\n\nCoefficient of Determination (\\(R^2\\)):\n\n\\(R^2 = \\frac{ESS}{TSS} = 1 - \\frac{RSS}{TSS}\\)\n*See Interaction Terms &gt; Coefficient of Determination for more info\n\n\n\n\n\n\n\n\n\nUse F-test to test to compare and test nested models\n\nanova(lm_red,lm_full)\n\nt-test to test contribution of individual predictors\n\ntidy(lm_full, conf.int = TRUE)\n\nF-test is equivalent to t-test when there is only one predictor\n\\(R^2\\) is the proportion of variance explained by the model\n\n\\(R^2\\) increases with number of predictors (RSS decreases)\n\nAdjusted \\(R^2\\) penalizes for number of predictors\n\nAdjusted \\(R^2\\) increases only if the new predictor improves the model more than expected by chance\nAdjusted \\(R^2\\) decreases if the new predictor does not improve the model more than expected by chance \\[R^2_{adj} = 1 - \\frac{RSS/(n-p)}{TSS/(n-1)}\\]\nCam be used to compare models with different number of parameters/ predictors\n\n\nNested models: one model is a special case of the other\n\ne.g. lm(y ~ x1 + x2) is nested in lm(y ~ x1 + x2 + x3)\n\n\n\n\n\n\n\nAverage squared error on new data\nSelection criteria:\n\nThe Mallows Cp\nAkaike information criterion (AIC)\nBayesian information criterion (BIC)\n\nThese add different penalties to the training RSS to adjust for the fact that the training error tends to underestimate the test error\n\n\n\n\n\nForward selection\n\nStart with null model\nAdd predictors one at a time (select the best feats at that number of feats)\nStop when no more predictors improve the model\nleaps::regsubsets\n\nBackward selection: start with full model and remove predictors one at a time\nHybrid: after adding a predictor, check if any predictors can be removed\n\ndat_forward &lt;- regsubsets(\n  assess_val ~ age + FIREPLACE + GARAGE + BASEMENT,\n  data = dat_s,\n  nvmax = 5, # max number of feats\n  method = \"forward\", # backward, exhaustive, etc.\n)\n\n# view\nfws_summary &lt;- summary(dat_forward)\n# will have rsq, adj r2, rss, cp, bic, etc.\n\n\n\n\nOffers an alternative to the above methods\ne.g. Ridge, Lasso, Elastic Net\n\n\n\n\n\n\n\nConditional expectation of predictor \\(Y\\) given \\(X\\): \\[E(Y|X)\\]\nAssumes linear form of ^\n\n\n\nTerms:\n\n\\(\\hat{y}_i\\) is the predicted value of the LR model (sample)\n\\(E(Y_i|X_i)\\) is the conditional expectation of \\(Y\\) given \\(X_i\\)\n\nor the average value of \\(Y_i\\) for a given \\(X_i\\)\nor LR model of population\n\n\nWith our model \\(\\hat{y}\\) we can predict:\n\n\\(E(Y|X_0)\\)\n\\(Y_i\\) (Actual value of \\(Y\\) for \\(X_i\\))\n\nA lot harder to predict (individual prediction)\n\n\n2 types of intervals:\n\nConfidence intervals for prediction (CIP)\nPrediction intervals (PI)\n\n\n\n\n\n\n\n\n\nCIP\nPI\n\n\n\n\nUncertainty of the mean of the prediction\nUncertainty of the individual prediction\n\n\nUncertainty of \\(E(Y_i\\|X_i)\\)\nUncertainty of \\(Y_i\\)\n\n\nError from estimation of \\(\\beta\\)\nError from estimation of \\(\\beta\\) and \\(\\epsilon\\)\n\n\nSmaller than PI\nWider than CIP\n\n\nCentered around \\(\\hat{y}_i\\)\nCentered around \\(\\hat{y}_i\\)\n\n\n\n\n\n\nWe are interested in the mean of the prediction, not the individual prediction \\[E(Y_i|X_i)\\]\nUncertainty only comes from the estimation (1 source of uncertainty)\n\nThe model: \\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\]\napproximates, with uncertainty, the conditional expectation: \\[E(Y_i|X_i) = \\beta_0 + \\beta_1 x_i\\]\ni.e. Estimated coeffs (\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)) are estimates of true population coeffs (\\(\\beta_0\\) and \\(\\beta_1\\))\n\n95% CIP is a range where we have 95% confidence that it contains the average value of \\(Y_i\\) for a given \\(X_i\\)\n\nsmaller CIP means smaller confidence interval =&gt; less confidence it contains the true values\n\n\n\n\nmodel_sample &lt;- lm(y ~ x, data = data_sample)\n\ndata_sample |&gt;\n  select(x, y) |&gt;\n  cbind(\n    predict(\n      model_sample,\n      interval = \"confidence\",\n      level = 0.95, # 95% CIP is default\n      se.fit=TRUE # standard error of the fit,\n    )$fit\n  )\n# or\npredict(\n  model_sample,\n  interval = \"confidence\",\n  newdata = tibble(x1 = 1, x2 = 2),\n)\n\n\n\n\n\nWe are interested in the individual prediction: \\(Y_i\\)\nWider than CIP\n\nThe model: \\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i + \\epsilon_i\\] approximates, with uncertainty, an actual observation \\(Y_i\\).\n\nUncertainty comes from 2 sources:\n\nEstimation of the coeffs (\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\))\nVariability of the error term \\(\\epsilon_i\\)\n\nActual observation \\(Y_i\\) differs from the average (population) by \\(\\epsilon_i\\)\n\n\n95% PI is a range where we have 95% confidence that it contains the actual value of \\(Y_i\\) for a given \\(X_i\\)\n\n\n\ndata_sample |&gt;\n  select(x, y) |&gt;\n  cbind(\n    predict(\n      model_sample,\n      interval = \"prediction\", # change from \"confidence\"\n      level = 0.95, # 95% CIP is default\n      se.fit=TRUE # standard error of the fit\n    )$fit\n  )\n\n\n\n\n\n\n\n\nLinearity in regression != linearity in math\nLinearity in regression means that the model is linear in the parameters\nExamples of linear regression models:\n\n$ y = _0 + _1 x_1 + _2 x_2 + _3 x_3 $\n$ y = _0 + _1 x_1 + _2 x_2 +_3 x_1^2 + _4 x_1 x_2$\n$ y = _0 + _1 x_1 + _2 e^{x_1} + _3 (x_2)$\n\nNon-linear examples:\n\n$ y = _0 + _1 x_1 + _2 x_1^{_3} $\n\n\n\n\n\n\nLS estimation do not depend on any normality assumption\nBut if small sample size, normality assumption is needed for inference (since CLT does not apply)\nOLS (Ordinary Least Squares) do not require assumptions of normality. BUT we need to assume it in order to do inference (e.g. CIP, PI, hypothesis testing).\n\n\n\n\nQ-Q plot is a graphical method for assessing whether or not a data set follows a given distribution such as normal distribution\nPoints in the Q-Q plot follow a straight line if the data are distributed normally\n\nmodel &lt;- lm(y ~ x, data = data_sample)\n\nplot(model, which = 2) # which = 2 for Q-Q plot\n\n\n\n\n\n\nWe assumed that \\(\\epsilon_i\\) are iid with mean 0 and variance \\(\\sigma^2\\)\nEstimated using RSE (residual standard error)\nHow to check this assumption?\n\nPlot residuals vs fitted values\n\nNeed to see evenly spread points around 0\nDo not want to see funnel shape\n\n\nheteroscedasticity: variance of residuals is not constant\nhomoscedasticity: variance of residuals is constant\n\nplot(model, which = 1) # which = 1 for residuals vs fitted values\n\n\n\n\n\nSome of the explanatory variables are linearly related\nWhen this happens, the LS are very “unstable”\n\ncontribution of each variable to the model is hard to assess\ncan inflate the standard errors of the coefficients\n\n\n\n\n\nCorrelation matrix\nVariance Inflation Factor (VIF)\n\nonly works for linear models with &gt; 1 explanatory variables\nVIF of \\(x_j\\) is \\(VIF_j = \\frac{1}{1-R_j^2}\\)\n\\(R_j^2\\) is the \\(R^2\\) from the regression of \\(x_j\\) on all other explanatory variables\n\nHow much of the observed variation of \\(X_j\\) is explained by the other explanatory variables\n\nIf \\(VIF_j &gt;&gt; 1\\), then multicollinearity is a problem, remove \\(x_j\\) from the model\n\nRidge deals with multicollinearity\n\nshrinks the coefficients of correlated variables towards each other\n\n\n\n\n\n\n\nCofounding factors are variables, not in model, that are related to both the response and the explanatory variables\nConfounding refers to a situation in which a variable, not included in the model, is related with both the response and at least one covariate in the model.\ne.g. Job hunting, create a LR model to predict salaries based on programming languages\n\nCofounding factor: years of experience, education level, etc.\n\n\n\n\n\n\n\n\n\nWant to use LR to predict a binary outcome (e.g. whether a person will buy a product or not)\nLinear model will predict the “probability” of buying a product\n\nBUT it will predict values outside of the [0, 1] range\n\nIt also violates:\n\nthe assumption of normality (residuals are not normally distributed)\nequal variance (qq plot shows that residuals are not equal).\n\n\n\n\n\nUsing MLE and assume Bernoulli distribution for \\(Y_i\\), we can predict probabolities between 0 and 1.\n\n\\[ Y \\sim Bernoulli(p) \\] \\[ Bernoulli(p) = p^y (1-p)^{1-y} \\]\n\\[ E(Y) = p \\]\n\n\n\n\n\n\n\n\nLogit function is the inverse of the logistic function\nProperties:\n\nRange: \\((-\\infty, \\infty)\\)\nMonotonic: always increasing or always decreasing\nDifferentiable: can be differentiated\n\n\n\\[ logit(p) = log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p \\]\n\\[ p_i = \\frac{e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p}}{1 + e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p}} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p)}} \\]\n\n\n\n\nProbability of success \\[p = \\frac{Odds}{1 + Odds}\\]\nOdds of success: \\[\\frac{p}{1-p}\\]\n\n\n\n\nlogistic_model &lt;- glm(as.factor(y) ~ x1 + x2 + x3,\n    data = data,\n    family = binomial # use binomial distribution\n)\n\n# default is to predict the log odds of success\npredict(logistic_model) # type = \"link\"\n\n# to predict the odds of success\npredict(logistic_model, type = \"response\")\n\n\n\nIntercept: log odds of success when all predictors are 0\nCoefficients: log odds ratio of success for a 1 unit increase in the predictor\n\nTo get the odds ratio, we need to exponentiate the coefficients.\n# returns the exp(log(odds ratio)) = odds ratio\ntidy(logistic_model, exponentiate = TRUE)\nexample:\n\nif \\(\\beta_1 = 0.4\\), then the odds ratio is \\(e^{0.4} = 1.49\\).\nThis means that for a 1 unit increase in \\(x_1\\), it is more likely to be a success by a factor of 1.49.\nCannot say anything about probability of success increasing as \\(x_1\\) increases.\n\n\n\n\n\nWe can determine whether a regressor is statistically associated with the logarithm of the response’s odds through hypothesis testing for \\(\\beta_i\\).\n\ni.e. To determine whether a coefficient is significant\n\nDo the Walad Statistics test \\[ z_i = \\frac{\\hat{\\beta_i}}{SE(\\hat{\\beta_i})} \\]\n\n\\(H_0\\): \\(\\beta_i = 0\\)\n\\(H_a\\): \\(\\beta_i \\neq 0\\)\n\n\ntidy(logistic_model, conf.int = TRUE)\n# then check the p-value to see if the coefficient is significant"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#introduction",
    "href": "block_3/561_regression/561_regression.html#introduction",
    "title": "Regression I",
    "section": "",
    "text": "Basic idea: fit a line to data\nCan help with:\n\nEstimation: how to estimate the true (but unknown) relation between the response and the input variables\nInference: how to use the model to infer information about the unknown relation between variables\nPrediction: how to use the model to predict the value of the response for new observations\n\nBefore starting, normally do some exploratory data analysis (EDA) to get a sense of the data\n\nfind out size\ndistribution of variables\nmissing values/ outliers\nrelationships between variables"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#simple-linear-regression",
    "href": "block_3/561_regression/561_regression.html#simple-linear-regression",
    "title": "Regression I",
    "section": "",
    "text": "Simple linear regression: one response variable and one explanatory variable\n\nLet \\((X_i, Y_i)\\) for \\(i = 1, \\dots, n\\) random sample of size \\(n\\) from a population.\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\]\nWhere:\n\n\\(Y\\) is the response variable\n\\(X\\) is the explanatory/ input variable\n\\(\\epsilon_i \\sim N(0, \\sigma^2)\\) and \\(\\epsilon_1, \\dots, \\epsilon_n\\) are independent.\n\nIt represents other factors not taken into account in the model\n\n\n\nImage from the book: “Beyond Multiple Linear Regression”, from Paul Roback and Julie Legler\nAssumptions:\n\nconditional expectation of \\(Y\\) is linearly related to \\(X\\) \\[E(Y|X) = \\beta_0 + \\beta_1 X\\]\niid\nall random error \\(\\epsilon_i\\) is normally distributed with mean 0 and variance \\(\\sigma^2\\)\n\n\\[E(\\epsilon | X) = 0 \\\\ and \\\\ Var(\\epsilon | X) = \\sigma^2\\]\n\n\n\nLine that minimizes the sum of squared errors (SSE) or distance between the line and the data points\n\n\\[SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 = \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i)^2\\] [[fact check the equation above]]\n\n\n\n# fit a linear model\nlm(Y ~ X, data = data)\n\n# plot linear line using ggplot2\nggplot(data, aes(x = X, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#slr-with-catagorical-covariates",
    "href": "block_3/561_regression/561_regression.html#slr-with-catagorical-covariates",
    "title": "Regression I",
    "section": "",
    "text": "Turn categorical into dummy variables (numerical)\n\ne.g. X = Fireplace in a house, Y = Price of the house\nMake dummy variable: \\(X_2 = 1\\) if house has fireplace, \\(X_2 = 0\\) if house doesn’t have fireplace\n\\(E(Y|X_2) = \\beta_0+ \\beta_2 X_2\\)\n\nSome cool thing:\n\n\\(\\beta_2 = E(Y|X_2 = 1) - E(Y|X_2 = 0) = \\mu_1 - \\mu_0\\)\n\nSAME as null hypothesis for t-test\n\n\\(\\beta_0 = E(Y|X_2 = 0)\\)"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#estimation-with-slr",
    "href": "block_3/561_regression/561_regression.html#estimation-with-slr",
    "title": "Regression I",
    "section": "",
    "text": "\\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\]\n\nFor Linear Regression, we are estimating:\n\n\\(\\beta_0\\) and \\(\\beta_1\\)"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#uncertainty-in-estimation",
    "href": "block_3/561_regression/561_regression.html#uncertainty-in-estimation",
    "title": "Regression I",
    "section": "",
    "text": "lm &lt;- lm(Y ~ X, data = data) |&gt;\n    tidy() |&gt;\n    mutate_if(is.numeric, round, 3) # round to 3 decimal places\n\n# Returns:\n# term  estimate    std.error   statistic   p.value\n# &lt;chr&gt; &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;             &lt;dbl&gt;\n\n\n\nIf take LR for new samples, we will get different estimates\n\nStandard Error is the standard deviation of the sampling distribution of the estimate\n\nMethods to measure uncertainty in estimation:\n\ncompute SE after taking multiple samples\nrarely take multiple samples, just take it theoretically (what lm does), see third col above\ncan also bootstrap from sample to get distribution of estimates -&gt; get standard error\n\n\n\n\n\nTake multiple samples and compute the variance of the estimates\n\n\n\n\n\nAssumption: conditional distribution of \\(Y_i\\) given \\(X_i\\) is normal with mean \\(\\beta_0 + \\beta_1 X_i\\) and variance \\(\\sigma^2\\), the \\(\\epsilon_i\\) term.\nUse t test with \\(df=n-p\\)\n\nn = sample size\np = number of parameters in the model\n\nUsed by lm to get standard error\nIf conditional dist is normal =&gt; error term is normal =&gt; sampling distribution of \\(\\hat{\\beta_1}\\) is normal\n\nOr if the sample size is large enough, sampling distribution of \\(\\hat{\\beta_1}\\) is approximately normal (CLT)\n\n\n\n\n\n\nBootstrap Normal Theory\n\\[\\beta \\pm z_{\\alpha/2} SE(\\hat{\\beta})\\]\n\nSE = stdev of bootstrap distribution\n\nBootstrap Percentile Method\n\nbasically take the 2.5th and 97.5th percentile of the bootstrap distribution (95% confidence interval)\n\n\\[\\hat{b}_{\\alpha/2} , \\hat{b}_{1-\\alpha/2}\\]\n\n\n\n\n\ntidy(lm_s, conf.int = TRUE)\n\nIf 0 is in the confidence interval, then we can’t reject the null hypothesis (\\(\\beta_1 = 0\\))"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#hypothesis-tests-for-slr",
    "href": "block_3/561_regression/561_regression.html#hypothesis-tests-for-slr",
    "title": "Regression I",
    "section": "",
    "text": "Can do:\n\n\\(H_0: \\beta_0 = 0\\) vs \\(H_a: \\beta_0 \\neq 0\\)\n\nUsually not interesting\n\n\\(H_0: \\beta_1 = 0\\) vs \\(H_a: \\beta_1 \\neq 0\\)\n\nAnswers: is there a linear relationship between \\(X\\) and \\(Y\\)?\n\n\nRecall, test statistic under null hypothesis:\n\\[ t = \\frac{\\hat{\\beta_1}}{SE(\\hat{\\beta_1})} \\]\nWhich has a t-distribution with \\(n-p\\) degrees of freedom"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#multiple-linear-regression",
    "href": "block_3/561_regression/561_regression.html#multiple-linear-regression",
    "title": "Regression I",
    "section": "",
    "text": "\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ip} + \\epsilon_i\\]\ne.g. \\[ Y_i = \\beta_0 + \\beta_1 height_i + \\beta_2 weight_i + \\epsilon_i \\]\n\nLinear regression models the conditional expectation as linear combination of the predictors\n\n\\[ E(Y|X_1, X_2, \\dots, X_p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p \\]\n\n\n# visualize the relationship between variables\n# using ggpairs from GGally package\nggpairs(data)\n\n# fit the model\nlm_s &lt;- lm(Y ~ X1 + X2 + X3, data = data)\n\n\n\n\nFor 2 categories (e.g. Fireplace exits or not) we only need 1 dummy variable\n\ncorresponding to 0 or 1\n\nFor 3 categories (e.g. old, modern, new) we need 2 dummy variables\n\nM corresponds to 1 if house is modern, 0 otherwise\nN corresponds to 1 if house is new, 0 otherwise\n\nFor \\(k\\) categories, we need \\(k-1\\) dummy variables\n\nlm(Y ~ cat_var, data = data) # cat_var is a categorical variable"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#interaction-terms",
    "href": "block_3/561_regression/561_regression.html#interaction-terms",
    "title": "Regression I",
    "section": "",
    "text": "Normally when you have categorical and continuous variables, the linear model will:\n\nfit different intercepts for each category\nfit the same slope for each category (simpler)*\n\nInteraction terms allow us to fit different slopes for each category\n\n*Occam’s razor: if can explain with simpler model, do so\ne.g.\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 Z_i + (\\beta_3 X_i Z_i) + \\epsilon_i\\]\n\n\\(X_i\\) is a continuous variable\n\\(Z_i\\) is a categorical variable\n\\(\\beta_3\\) is the interaction term\n\nfit &lt;- lm(score ~ age*sex, data = dat)\ntidy(fit) |&gt; mutate_if(is.numeric, round, 3)\n# Returns columns:(intercept), age, sexmale, age:sexmale\n\nIntercept: average teaching score of female instructors of zero age\nage: slope for female instructors. Increase in age by 1 year will increase teaching score by age for female instructors\nsexmale: holding age constant, the difference in teaching score between male and female\nage:sexmale is the offset slope. Increase in age by one year is associated with the increase in teaching score of age + age:sexmale for male instructors."
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#goodness-of-fit",
    "href": "block_3/561_regression/561_regression.html#goodness-of-fit",
    "title": "Regression I",
    "section": "",
    "text": "Is this better than nothing/ null model (only intercept)?\n\nIn other words is \\(E(Y|X)\\) better than \\(E(Y)\\)?\n\n\n\n\n\nTotal sum of squares (TSS):\n\n\\(TSS = \\sum (y_i - \\bar{y})^2\\)\nSum of squares from the null (intercept only) model\n\nExplained sum of squares (ESS):\n\n\\(ESS = \\sum (\\hat{y}_i - \\bar{y})^2\\)\nmeasures how mich it is explained by the model\n\nbetter model = larger ESS\n\n\nResidual sum of squares (RSS):\n\n\\(RSS = \\sum (y_i - \\hat{y}_i)^2\\)\nRSS == SSE (sum of squared errors) == n * MSE (mean squared error)\nEstimated parameters minimizes RSS (objective function)\n\nWhere:\n\n\\(y_i\\) observed ith value of y\n\\(\\hat{y}_i\\) predicted value of y with a model (of sample)\n\\(\\bar{y}_i\\) mean of y\n\n\nIf using linear regression using least squares:\n\\[ TSS = ESS + RSS \\]\nRecall: Residuals is the difference between the observed value and the predicted value. \\(r_i = y_i - \\hat{y}_i\\)\n\n\n\n\\[ R^2 = \\frac{ESS}{TSS} = 1 - \\frac{RSS}{TSS} \\]\n\n\\(R^2\\) is the proportion of variance explained by the model\n\nTells us how well the regression model explains the variation in the data\n\n\\(R^2\\) is between -inf and 1 (1 being perfect fit)\n\ngenerally between 0 and 1 since expect TSS &gt; RSS\n\nONLY IF the model has an intercept and is estimated by LS\n\nonly less than 0 if model is worse than null model\n\n\nglance(fit)\n\n\n\n\\(R^2\\) increases with number of predictors\n\n\\(R^2\\) will never decrease when adding predictors\n\nComputed based on “in-sample” predictions\n\nWe do not know how well the model will perform on new data\nBasically only on train data, dunno performance on test data\n\nIs it useful?\n\nYes, to compare size of residuals of fitted model to null model\nCannot be used to test any hypothesis since its distribution is unknown\n\n\n\n\n\n\nIs no longer coefficient of determination\nMeasures correlation between the true and predicted values\n\\(R^2 = Cor(Y, \\hat{Y})^2\\) if \\(\\hat{Y}\\) is a prediction obtained from a LR with an intercept estimated by LS.\n\nThis is ued to assess the prediction performance of a model\n\n\n\n\n\n\n\nCompare two models\n\n\\(reduced\\): intercept + some predictors\n\\(full\\): intercept + predictors\n\nIs the full model better than the reduced model? Simultaneously testing if many parameters are 0\nF-test is a global test\n\ntests if all parameters are 0\nif F-test is significant, then at least one parameter is not 0\n\n\n\\[F \\propto \\frac{(RSS_{reduced} - RSS_{full}) / k}{RSS_{full} / (n - p)}\\]\n\nparams:\n\nk = number of parameters tested (difference between models)\np = number of predictors in the full model (s + 1)\n\n\nlm_red &lt;- lm(score~1, data=dat) # intercept only\nlm_full &lt;- lm(score~ age + sex, data=dat)\n\nanova(lm_red,lm_full)\n# F-test tests H0: coef_age = coef_sex = 0\n\nglance(lm_full)\n# also includes F-statistic in \"statistic\" column + p-value\n# compares it to null model (intercept only)\n\nF-test can test multiple parameters at once\n\ne.g. \\(\\beta_1 = \\beta_2 = \\beta_3 = 0\\)\n\nFor one parameter, F-test is equivalent to t-test\n\ne.g. \\(\\beta_1 = 0\\)\nFor one parameter, \\(t^2 = F\\)\n\nwhere t is the t-statistic and F is the F-statistic\n\n\n\n\n\n\nif F-statistic is large (F &gt; 1), then the full model is better than the reduced model\nif F-statistic is small (F &lt; 1), then the full model is not better than the reduced model\n\n\n\n\n\nBoth depend on RSS and TSS, there is a formula to convert between them\nF-test has a known F-distribution (under certain assumptions) so we can use it to make probabilistic statements"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#model-evaluation",
    "href": "block_3/561_regression/561_regression.html#model-evaluation",
    "title": "Regression I",
    "section": "",
    "text": "What is your goal?\n\nInference: understand the relationship between the response and the predictors\nPrediction: predict the response for future observations\n\n\n\n\n\n\nMean squared error (MSE): average squared error on new data\n\n\\(MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\)\nTrain MSE: MSE on training data\nTest MSE: MSE on test data\n\nResidual Sum of Squares (RSS): n * MSE\n\n\\(\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\)\nsmall is good\nresiduals are measured in training data\n\nResidual Standard Error (RSE): estimates the standard deviation of the residuals\n\n\\(\\sqrt{\\frac{1}{n-p} RSS}\\)\n\np = number of parameters in the model\n\nBased on training data to evaluate fit of the model (small is good)\n\nCoefficient of Determination (\\(R^2\\)):\n\n\\(R^2 = \\frac{ESS}{TSS} = 1 - \\frac{RSS}{TSS}\\)\n*See Interaction Terms &gt; Coefficient of Determination for more info"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#variable-selection",
    "href": "block_3/561_regression/561_regression.html#variable-selection",
    "title": "Regression I",
    "section": "",
    "text": "Use F-test to test to compare and test nested models\n\nanova(lm_red,lm_full)\n\nt-test to test contribution of individual predictors\n\ntidy(lm_full, conf.int = TRUE)\n\nF-test is equivalent to t-test when there is only one predictor\n\\(R^2\\) is the proportion of variance explained by the model\n\n\\(R^2\\) increases with number of predictors (RSS decreases)\n\nAdjusted \\(R^2\\) penalizes for number of predictors\n\nAdjusted \\(R^2\\) increases only if the new predictor improves the model more than expected by chance\nAdjusted \\(R^2\\) decreases if the new predictor does not improve the model more than expected by chance \\[R^2_{adj} = 1 - \\frac{RSS/(n-p)}{TSS/(n-1)}\\]\nCam be used to compare models with different number of parameters/ predictors\n\n\nNested models: one model is a special case of the other\n\ne.g. lm(y ~ x1 + x2) is nested in lm(y ~ x1 + x2 + x3)\n\n\n\n\n\n\n\nAverage squared error on new data\nSelection criteria:\n\nThe Mallows Cp\nAkaike information criterion (AIC)\nBayesian information criterion (BIC)\n\nThese add different penalties to the training RSS to adjust for the fact that the training error tends to underestimate the test error\n\n\n\n\n\nForward selection\n\nStart with null model\nAdd predictors one at a time (select the best feats at that number of feats)\nStop when no more predictors improve the model\nleaps::regsubsets\n\nBackward selection: start with full model and remove predictors one at a time\nHybrid: after adding a predictor, check if any predictors can be removed\n\ndat_forward &lt;- regsubsets(\n  assess_val ~ age + FIREPLACE + GARAGE + BASEMENT,\n  data = dat_s,\n  nvmax = 5, # max number of feats\n  method = \"forward\", # backward, exhaustive, etc.\n)\n\n# view\nfws_summary &lt;- summary(dat_forward)\n# will have rsq, adj r2, rss, cp, bic, etc.\n\n\n\n\nOffers an alternative to the above methods\ne.g. Ridge, Lasso, Elastic Net"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#prediction-intervals-vs-confidence-intervals-for-prediction",
    "href": "block_3/561_regression/561_regression.html#prediction-intervals-vs-confidence-intervals-for-prediction",
    "title": "Regression I",
    "section": "",
    "text": "Conditional expectation of predictor \\(Y\\) given \\(X\\): \\[E(Y|X)\\]\nAssumes linear form of ^\n\n\n\nTerms:\n\n\\(\\hat{y}_i\\) is the predicted value of the LR model (sample)\n\\(E(Y_i|X_i)\\) is the conditional expectation of \\(Y\\) given \\(X_i\\)\n\nor the average value of \\(Y_i\\) for a given \\(X_i\\)\nor LR model of population\n\n\nWith our model \\(\\hat{y}\\) we can predict:\n\n\\(E(Y|X_0)\\)\n\\(Y_i\\) (Actual value of \\(Y\\) for \\(X_i\\))\n\nA lot harder to predict (individual prediction)\n\n\n2 types of intervals:\n\nConfidence intervals for prediction (CIP)\nPrediction intervals (PI)\n\n\n\n\n\n\n\n\n\nCIP\nPI\n\n\n\n\nUncertainty of the mean of the prediction\nUncertainty of the individual prediction\n\n\nUncertainty of \\(E(Y_i\\|X_i)\\)\nUncertainty of \\(Y_i\\)\n\n\nError from estimation of \\(\\beta\\)\nError from estimation of \\(\\beta\\) and \\(\\epsilon\\)\n\n\nSmaller than PI\nWider than CIP\n\n\nCentered around \\(\\hat{y}_i\\)\nCentered around \\(\\hat{y}_i\\)\n\n\n\n\n\n\nWe are interested in the mean of the prediction, not the individual prediction \\[E(Y_i|X_i)\\]\nUncertainty only comes from the estimation (1 source of uncertainty)\n\nThe model: \\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\]\napproximates, with uncertainty, the conditional expectation: \\[E(Y_i|X_i) = \\beta_0 + \\beta_1 x_i\\]\ni.e. Estimated coeffs (\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)) are estimates of true population coeffs (\\(\\beta_0\\) and \\(\\beta_1\\))\n\n95% CIP is a range where we have 95% confidence that it contains the average value of \\(Y_i\\) for a given \\(X_i\\)\n\nsmaller CIP means smaller confidence interval =&gt; less confidence it contains the true values\n\n\n\n\nmodel_sample &lt;- lm(y ~ x, data = data_sample)\n\ndata_sample |&gt;\n  select(x, y) |&gt;\n  cbind(\n    predict(\n      model_sample,\n      interval = \"confidence\",\n      level = 0.95, # 95% CIP is default\n      se.fit=TRUE # standard error of the fit,\n    )$fit\n  )\n# or\npredict(\n  model_sample,\n  interval = \"confidence\",\n  newdata = tibble(x1 = 1, x2 = 2),\n)\n\n\n\n\n\nWe are interested in the individual prediction: \\(Y_i\\)\nWider than CIP\n\nThe model: \\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i + \\epsilon_i\\] approximates, with uncertainty, an actual observation \\(Y_i\\).\n\nUncertainty comes from 2 sources:\n\nEstimation of the coeffs (\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\))\nVariability of the error term \\(\\epsilon_i\\)\n\nActual observation \\(Y_i\\) differs from the average (population) by \\(\\epsilon_i\\)\n\n\n95% PI is a range where we have 95% confidence that it contains the actual value of \\(Y_i\\) for a given \\(X_i\\)\n\n\n\ndata_sample |&gt;\n  select(x, y) |&gt;\n  cbind(\n    predict(\n      model_sample,\n      interval = \"prediction\", # change from \"confidence\"\n      level = 0.95, # 95% CIP is default\n      se.fit=TRUE # standard error of the fit\n    )$fit\n  )"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#potential-problems-in-lr",
    "href": "block_3/561_regression/561_regression.html#potential-problems-in-lr",
    "title": "Regression I",
    "section": "",
    "text": "Linearity in regression != linearity in math\nLinearity in regression means that the model is linear in the parameters\nExamples of linear regression models:\n\n$ y = _0 + _1 x_1 + _2 x_2 + _3 x_3 $\n$ y = _0 + _1 x_1 + _2 x_2 +_3 x_1^2 + _4 x_1 x_2$\n$ y = _0 + _1 x_1 + _2 e^{x_1} + _3 (x_2)$\n\nNon-linear examples:\n\n$ y = _0 + _1 x_1 + _2 x_1^{_3} $\n\n\n\n\n\n\nLS estimation do not depend on any normality assumption\nBut if small sample size, normality assumption is needed for inference (since CLT does not apply)\nOLS (Ordinary Least Squares) do not require assumptions of normality. BUT we need to assume it in order to do inference (e.g. CIP, PI, hypothesis testing).\n\n\n\n\nQ-Q plot is a graphical method for assessing whether or not a data set follows a given distribution such as normal distribution\nPoints in the Q-Q plot follow a straight line if the data are distributed normally\n\nmodel &lt;- lm(y ~ x, data = data_sample)\n\nplot(model, which = 2) # which = 2 for Q-Q plot\n\n\n\n\n\n\nWe assumed that \\(\\epsilon_i\\) are iid with mean 0 and variance \\(\\sigma^2\\)\nEstimated using RSE (residual standard error)\nHow to check this assumption?\n\nPlot residuals vs fitted values\n\nNeed to see evenly spread points around 0\nDo not want to see funnel shape\n\n\nheteroscedasticity: variance of residuals is not constant\nhomoscedasticity: variance of residuals is constant\n\nplot(model, which = 1) # which = 1 for residuals vs fitted values\n\n\n\n\n\nSome of the explanatory variables are linearly related\nWhen this happens, the LS are very “unstable”\n\ncontribution of each variable to the model is hard to assess\ncan inflate the standard errors of the coefficients\n\n\n\n\n\nCorrelation matrix\nVariance Inflation Factor (VIF)\n\nonly works for linear models with &gt; 1 explanatory variables\nVIF of \\(x_j\\) is \\(VIF_j = \\frac{1}{1-R_j^2}\\)\n\\(R_j^2\\) is the \\(R^2\\) from the regression of \\(x_j\\) on all other explanatory variables\n\nHow much of the observed variation of \\(X_j\\) is explained by the other explanatory variables\n\nIf \\(VIF_j &gt;&gt; 1\\), then multicollinearity is a problem, remove \\(x_j\\) from the model\n\nRidge deals with multicollinearity\n\nshrinks the coefficients of correlated variables towards each other\n\n\n\n\n\n\n\nCofounding factors are variables, not in model, that are related to both the response and the explanatory variables\nConfounding refers to a situation in which a variable, not included in the model, is related with both the response and at least one covariate in the model.\ne.g. Job hunting, create a LR model to predict salaries based on programming languages\n\nCofounding factor: years of experience, education level, etc."
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#maximum-likelihood-estimation-mle",
    "href": "block_3/561_regression/561_regression.html#maximum-likelihood-estimation-mle",
    "title": "Regression I",
    "section": "",
    "text": "Want to use LR to predict a binary outcome (e.g. whether a person will buy a product or not)\nLinear model will predict the “probability” of buying a product\n\nBUT it will predict values outside of the [0, 1] range\n\nIt also violates:\n\nthe assumption of normality (residuals are not normally distributed)\nequal variance (qq plot shows that residuals are not equal).\n\n\n\n\n\nUsing MLE and assume Bernoulli distribution for \\(Y_i\\), we can predict probabolities between 0 and 1.\n\n\\[ Y \\sim Bernoulli(p) \\] \\[ Bernoulli(p) = p^y (1-p)^{1-y} \\]\n\\[ E(Y) = p \\]"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#logistic-regression",
    "href": "block_3/561_regression/561_regression.html#logistic-regression",
    "title": "Regression I",
    "section": "",
    "text": "Logit function is the inverse of the logistic function\nProperties:\n\nRange: \\((-\\infty, \\infty)\\)\nMonotonic: always increasing or always decreasing\nDifferentiable: can be differentiated\n\n\n\\[ logit(p) = log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p \\]\n\\[ p_i = \\frac{e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p}}{1 + e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p}} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p)}} \\]\n\n\n\n\nProbability of success \\[p = \\frac{Odds}{1 + Odds}\\]\nOdds of success: \\[\\frac{p}{1-p}\\]\n\n\n\n\nlogistic_model &lt;- glm(as.factor(y) ~ x1 + x2 + x3,\n    data = data,\n    family = binomial # use binomial distribution\n)\n\n# default is to predict the log odds of success\npredict(logistic_model) # type = \"link\"\n\n# to predict the odds of success\npredict(logistic_model, type = \"response\")\n\n\n\nIntercept: log odds of success when all predictors are 0\nCoefficients: log odds ratio of success for a 1 unit increase in the predictor\n\nTo get the odds ratio, we need to exponentiate the coefficients.\n# returns the exp(log(odds ratio)) = odds ratio\ntidy(logistic_model, exponentiate = TRUE)\nexample:\n\nif \\(\\beta_1 = 0.4\\), then the odds ratio is \\(e^{0.4} = 1.49\\).\nThis means that for a 1 unit increase in \\(x_1\\), it is more likely to be a success by a factor of 1.49.\nCannot say anything about probability of success increasing as \\(x_1\\) increases.\n\n\n\n\n\nWe can determine whether a regressor is statistically associated with the logarithm of the response’s odds through hypothesis testing for \\(\\beta_i\\).\n\ni.e. To determine whether a coefficient is significant\n\nDo the Walad Statistics test \\[ z_i = \\frac{\\hat{\\beta_i}}{SE(\\hat{\\beta_i})} \\]\n\n\\(H_0\\): \\(\\beta_i = 0\\)\n\\(H_a\\): \\(\\beta_i \\neq 0\\)\n\n\ntidy(logistic_model, conf.int = TRUE)\n# then check the p-value to see if the coefficient is significant"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html",
    "href": "block_4/562_regression_2/562_regression_2.html",
    "title": "Regression II",
    "section": "",
    "text": "General stages of the workflow:\n\nStudy design\nData collection and wrangling\nExploratory data analysis (EDA)\nData modeling\nEstimation\nResults\nStorytelling\n\n\n\nFrom: https://pages.github.ubc.ca/MDS-2023-24/DSCI_562_regr-2_students/notes/lecture1-glm-link-functions-and-count-regression.html\n\nChoose a proper workflow according to either:\n\nInferential\nPredictive\n\nAlso choose correct regression model\n\n\n\n\n\n\n\nResponse of continuous nature (hence the “ordinary”)\nResponse is subject to regressors (or explanatory variables/ features/ independent variables)\n\nMore than 1 regressor =&gt; multiple linear regression\n\n\n\\[Response = Systematic + Random\\]\n\\[Y_i = \\beta_0 + \\beta_1 g_1(x_{i1}) + \\beta_2 g_2(x_{i2}) + \\dots + \\beta_p g_p(x_{ip}) + \\epsilon_i\\]\n\nrandom is the \\(\\epsilon_i\\) term\n\n\n\n\nLinearity: the relationship between the response and functions of the regressors is linear\nErrors are independent of each other and are normally distributed with mean 0 and variance \\(\\sigma^2\\)\n\nHence, each \\(Y_i\\) is assumed to be independent and normally distributed.\n\n\n\n\nTo fit will need \\(k+2\\) parameters: \\(\\beta_0, \\beta_1, \\dots, \\beta_k, \\sigma^2\\)\nMinimize the sum of squared errors (SSE) OR maximize the likelihood of the observed data\nMaximum Likelihood Estimation (MLE): find the parameters that maximize the likelihood of the observed data\n\nLikelihood: the probability of observing the data given the parameters\nLog-likelihood: the log of the likelihood\n\n\n\n\n\n\nDo a t-test on the parameters to see if they are statistically significant\n\n\n\n\n\n\nOLS allows response to take any real number.\nExamples of non-suitable responses:\n\nNon-negative values\nBinary values (success/failure)\nCount data\n\n\n\n\n\n\n\nRecall: OLS models a continuous response via its conditional mean\n\n\\[\\mu_i = E(Y_i | X_i) = \\beta_0 + \\beta_1 g_1(x_{i1}) + \\beta_2 g_2(x_{i2}) + \\dots + \\beta_p g_p(x_{ip}) \\]\n\nBUT this is not suitable for non-continuous responses (e.g. binary, count, non-negative).\nSolution: use a link function \\(h(\\mu_i)\\) to map the conditional mean to the real line\n\n\n\nLink function: relate the systematic component, \\(\\eta_i\\), with the response’s mean\n\n\\[h(\\mu_i) = \\eta_i = \\beta_0 + \\beta_1 g_1(x_{i1}) + \\beta_2 g_2(x_{i2}) + \\dots + \\beta_p g_p(x_{ip}) \\]\n\nMonotonic: allows for a one-to-one mapping between the mean of the response variable and the linear predictor\n\n\\[\\mu_i = h^{-1}(\\eta_i)\\]\n\nDifferentiable: to allow for maximum likelihood estimation (MLE), used to obtain \\(\\hat{\\beta}\\) $$\n\n\n\n\n\nGeneralized Linear Models (GLM): a generalization of OLS regression that allows for non-continuous responses\n\nGLM = link function + error distribution\n\n\n\n\n\nPoisson regression: a GLM for count data (Equidispersed)\n\nEquidispersed: the variance of the response is equal to its mean (i.e. \\(Var(Y_i) = E(Y_i) = \\lambda_i\\))\n\nIt assumes a random sample of \\(n\\) count observations \\(Y_i\\)s\n\nIndependent\nNot Identically Distributed: Each \\(Y_i\\) has its own mean \\(E(Y_i) = \\lambda_i &gt; 0\\) and variance \\(Var(Y_i) = \\lambda_i &gt; 0\\)\n\n\n\\[Y_i \\sim Poisson(\\lambda_i)\\]\n\n\\(\\lambda_i\\) is the risk of event occurance in a given timeframe or area (definition of Poisson distribution)\n\n\n\n\nLog link function: the log of the mean of the response variable is linearly related to the regressors\n\n\\[h(\\mu_i) = log(\\mu_i) = \\eta_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ik} \\]\nHence,\n\\[\\lambda_i = e^{\\eta_i} = e^{\\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ik}}\\]\n\nThis is good since \\(\\lambda_i\\) (mean count) is always positive\n\n\n\n\nglm(Y ~ X, family = poisson, data = dataset)\n\n# view each regression coefficient\ntidy(glm_model)\ntidy(glm_model, conf.int = TRUE) # for 95% confidence interval\n\n# view model summary\nglance(glm_model)\n\n\ne.g. \\(\\beta_1 = 0.5\\)\n\n\\(\\beta_1\\) is the expected change in the log of the mean count for a one-unit increase in \\(X_1\\) holding all other variables constant\na one-unit increase in \\(X_1\\) will increase the mean count by \\(e^{0.5} = 1.65\\) times.\n\n\n\n\n\n\nTo determine the significance of the parameters \\(\\beta_1, \\beta_2, \\dots, \\beta_p\\), we can do a Wald statistic\n\n\\[z_j = \\frac{\\hat{\\beta_j}}{SE(\\hat{\\beta_j})}\\]\n\nTo test the hypothesis:\n\n\\(H_0: \\beta_j = 0\\)\n\\(H_1: \\beta_j \\neq 0\\)\n\n\n\n\n\n\n\nNegative Binomial Regression: a GLM for count data (Overdispersed)\n\nOverdispersed: the variance of the response is greater than its mean (i.e. \\(Var(Y_i) &gt; E(Y_i) = \\lambda_i\\))\n\n\n\nCheck for Overdispersion\ndispersiontest(glm_model)\n\nif p-value &lt; 0.05, then there is overdispersion (reject null hypothesis)\n\nIf use Poisson regression on overdispersed data, then the standard errors will be underestimated =&gt; Type I error (false positive) increases\n\n\nRecall PMF of Negative Binomial Distribution:\n\n\\[P(Y_i | m,p_i) = (^{y_i + m - 1}_{y_i}) p_i^{y_i} (1 - p_i)^m\\]\n\n\\(y_i\\) is the number of failures before experiencing \\(m\\) successes where probability of success is \\(p_i\\)\n\n\\[E(Y_i) = \\frac{m(1-p_i)}{p_i}\\]\n\\[Var(Y_i) = \\frac{m(1-p_i)}{p_i^2}\\]\n\nRearranging the above equations, we get:\n\n\\[E(Y_i) = \\lambda_i\\]\n\\[Var(Y_i) = \\lambda_i (1 + \\frac{\\lambda_i}{m})\\]\n\nInteresting information:\n\n\\[X \\sim Poisson(\\lambda) = lim_{m \\to \\infty} Negative Binomial(m, p_i)\\]\n\n\nglm.nb(Y ~ X, data = dataset)\nSince negative binomial has the same link function as Poisson, we can interpret the coefficients the same way.\n\n\n\n\n\n\n\nThe deviance (\\(D_k\\)) is used to compare a given model with k regressors (\\(l_k\\)) with the baseline/ saturated model (\\(l_0\\)).\n\nThe baseline model is the “perfect” fit to the data (overfitted), it has a distinct poisson mean (\\(\\lambda_i\\)) for each \\(i\\)th observation.\n\n\n\\[D_k = 2 log \\frac{\\hat{l}_k}{\\hat{l}_0}\\]\n\nInterpretation of \\(D_k\\)\n\nLarge value of \\(D_k\\) =&gt; poor fit compared to baseline model\nSmall value of \\(D_k\\) =&gt; good fit compared to baseline model\n\n\n\n\n\\[D_k = 2 \\sum_{i=1}^n \\left[ y_i log \\left( \\frac{y_i}{\\hat{\\lambda}_i} \\right) - (y_i - \\hat{\\lambda}_i) \\right]\\]\n*note: when \\(y_i = 0\\), log term is defined to be 0.\n\nHypothesises are as follows (opposite of normal hypothesis):\n\n\\(H_0\\): Our model with k regressors fits the data better than the saturated model.\n\\(H_A\\): Otherwise\n\n\nglance(model) # D_k is \"deviance\" col\n\n# to get p-value\npchisq(summary_poisson_model_2$deviance,\n  df = summary_poisson_model_2$df.residual,\n  lower.tail = FALSE\n)\n\nFormally deviance is residual deviance, this is a test statistic.\nAsmptomatically, it has a null distribution of:\n\n\\[D_k \\sim \\chi^2_{n-k-1}\\]\n\ndof: \\(n-k-1\\)\n\n\\(n\\) is the number of observations\n\\(k\\) is the number of regressors (including intercept)\n\n\n\n\n\nanova(model_1, model_2, test = \"Chisq\")\n# deviance column is \\delta D_k\n\nmodel_1 is nested in model_2\n\\(H_0\\): model_1 fits the data better as model_2\n\\(H_A\\): model_2 fits the data better as model_1\n\n\\[\\Delta D_k = D_{k_1} - D_{k_2} \\sim \\chi^2_{k_2 - k_1}\\]\n\n\n\n\n\\[AIC_k = D_k + 2k\\]\n\nAIC can be used to compare models that are not nested.\nSmaller AIC is better (means better fit)\nCan get from glance() function\n\n\n\n\n\\[BIC_k = D_k + k log(n)\\]\n\nBIC tends to select models with fewer regressors than AIC.\nsmaller BIC is better (means better fit)\nCan get from glance() function\n\n\n\n\n\n\nIs a MLE-based GLM for when the response is categorical and nominal.\n\nNominal: unordered categories\n\ne.g. red, green, blue\n\nOrdinal: ordered categories\n\ne.g. low, medium, high\n\n\nSimilar to binomial logistic regression, but with more than 2 categories.\nLink function is the logit function.\n\nneed more than 1 logit function to model the probabilities of each category.\nOne category is the baseline category, the other categories are compared to the baseline category.\n\n\n\\[\\eta_i^{(model 2, model 1)} = \\log\\left[\\frac{P(Y_i = \\texttt{model 2} \\mid X_{i, 1}, X_{i,2}, X_{i,3})}{P(Y_i = \\texttt{model 1} \\mid X_{i, 1}, X_{i,2}, X_{i,3})}\\right]\\]\n\\[= \\beta_0^{(\\texttt{model 2},\\texttt{model 1})} + \\beta_1^{(\\texttt{model 2},\\texttt{model 1})} X_{i, 1} + \\beta_2^{(\\texttt{model 2},\\texttt{model 1})} X_{i, 2} + \\beta_3^{(\\texttt{model 2},\\texttt{model 1})} X_{i, 3}\\] \\[\\eta_i^{(model 3, model 1)} = \\log\\left[\\frac{P(Y_i = \\texttt{model 3} \\mid X_{i, 1}, X_{i,2}, X_{i,3})}{P(Y_i = \\texttt{model 1} \\mid X_{i, 1}, X_{i,2}, X_{i,3})}\\right]\\]\n\\[=\\beta_0^{(\\texttt{model 3},\\texttt{model 1})} + \\beta_1^{(\\texttt{model 3},\\texttt{model 1})} X_{i, 1} + \\beta_2^{(\\texttt{model 3},\\texttt{model 1})} X_{i, 2} + \\beta_3^{(\\texttt{model 3},\\texttt{model 1})} X_{i, 3}\\]\nWith some algebra, we can get the following (For m categories):\n\\[p_{i, \\texttt{model 1}} = \\frac{1}{1 + \\sum_{j=2}^m e^{\\eta_i^{(\\texttt{model j}, \\texttt{model 1})}}}\\]\n\\[p_{i, \\texttt{model 2}} = \\frac{e^{\\eta_i^{(\\texttt{model 2}, \\texttt{model 1})}}}{1 + \\sum_{j=2}^m e^{\\eta_i^{(\\texttt{model j}, \\texttt{model 1})}}}\\]\n\nAll probabilities sum to 1.\n\n\n\n\nThe baseline level is the level that is not included in the model.\n\ncan find using levels() function, the first level is the baseline level.\n\n\nlevels(data$response) # to check levels\n\n# to change levels\ndata$response &lt;- recode_factor(data$response,\n  \"0\" = \"new_level_0\",\n  \"1\" = \"new_level_1\",\n)\n\n\n\nmodel &lt;- multinom(response ~ regressor_1 + regressor_2 + regressor_3,\n  data = data)\n\n# to get test statistics\nmlr_output &lt;- tidy(model,\n  conf.int = TRUE, # to get confidence intervals (default is 95%)\n  exponentiate = TRUE) # to get odds ratios\n# default result is log odds ratios\n\n# can filter p-values\nmlr_output |&gt; filter(p.value &lt; 0.05)\n\n# predict\npredict(model, newdata = data, type = \"probs\")\n# sum of all probabilities is 1\n\n\n\n\nCheck if regressor is significant using Wald test.\n\n\\[z_j^{(u,v)} = \\frac{\\hat{\\beta}_j^{(u,v)}}{SE(\\hat{\\beta}_j^{(u,v)})}\\]\n\nFor large sample sizes, \\(z_j^{(u,v)} \\sim N(0,1)\\)\nTo test the hypothesis:\n\n\\(H_0\\): \\(\\beta_j^{(u,v)} = 0\\)\n\\(H_A\\): \\(\\beta_j^{(u,v)} \\neq 0\\)\n\n\n\n\n\ne.g. \\(\\beta_1^{(b,a)} = 0.5\\)\n\nFor a 1 unit increase in \\(X_1\\), the odds of being in category \\(b\\) is \\(e^{0.5} = 1.65\\) times the odds of being in category \\(a\\).\n\ne.g. \\(\\beta_2^{(c,a)} = -0.5\\)\n\nFor a 1 unit increase in \\(X_2\\), the odds of being in category \\(c\\) decrease by \\(39\\%\\) [$ 1 - (e^{-0.5}) = 1 - 0.61 = 0.39$] less than being in category \\(a\\).\n\n\n\n\n\n\nOrdinal: has a natural ordering\nThere might be loss of information when using MLR for ordinal data\nWe are going to use the proportional odds model for ordinal data\n\nIt is a cumulative logit model\n\n\n\n\n\nReorder the levels of the response variable\n\ndata$response &lt;- as.ordered(data$response)\ndata$response &lt;- fct_relevel(\n  data$response,\n  c(\"unlikely\", \"somewhat likely\", \"very likely\")\n)\nlevels(data$response)\n\n\n\n\nFor a response with \\(m\\) responses and \\(k\\) regressors, the model is:\nWe will have:\n\n\\(m-1\\) equations (link functions: logit)\n\\(m-1\\) intercepts\n\\(k\\) regression coefficients\n\n\n\n\n\\[\n\\begin{gather*}\n\\text{Level } m - 1 \\text{ or any lesser degree versus level } m\\\\\n\\text{Level } m - 2 \\text{ or any lesser degree versus level } m - 1 \\text{ or any higher degree}\\\\\n\\vdots \\\\\n\\text{Level } 1 \\text{ versus level } 2 \\text{ or any higher degree}\\\\\n\\end{gather*}\n\\]\n\\[\n\\begin{gather*}\n\\eta_i^{(m - 1)} = \\log\\left[\\frac{P(Y_i \\leq m - 1 \\mid X_{i,1}, \\ldots, X_{i,k})}{P(Y_i = m \\mid X_{i,1}, \\ldots, X_{i,k})}\\right] = \\beta_0^{(m - 1)} - \\beta_1 X_{i, 1} - \\beta_2 X_{i, 2} - \\ldots - \\beta_k X_{i, k} \\\\\n\\eta_i^{(m - 2)} = \\log\\left[\\frac{P(Y_i \\leq m - 2 \\mid X_{i,1}, \\ldots, X_{i,k})}{P(Y_i &gt; m - 2 \\mid X_{i,1}, \\ldots, X_{i,k})}\\right] = \\beta_0^{(m - 2)} - \\beta_1 X_{i, 1} - \\beta_2 X_{i, 2} - \\ldots - \\beta_k X_{i, k} \\\\\n\\vdots \\\\\n\\eta_i^{(1)} = \\log\\left[\\frac{P(Y_i = 1 \\mid X_{i,1}, \\ldots, X_{i,k})}{P(Y_i &gt; 1 \\mid X_{i,1}, \\ldots, X_{i,k})}\\right] = \\beta_0^{(1)} - \\beta_1 X_{i, 1} - \\beta_2 X_{i, 2} - \\ldots - \\beta_k X_{i, k}.\n\\end{gather*}\n\\]\n\n\n\n\\[p_{i,j} = P(Y_i = j \\mid X_{i,1}, \\ldots, X_{i,k}) = P(Y_i \\leq j \\mid ...) - P(Y_i \\leq j - 1 \\mid ...)\\]\n\n\\(i\\) is the index of the observation\n\\(j\\) is the level of the response variable\n\n\\[\\sum_{j = 1}^{m} p_{i,j} = 1\\]\n\n\n\n\n\nuse MASS::polr function\n\nordinal_model &lt;- polr(\n  formula = response ~ regressor_1 + regressor_2,\n  data = data,\n  Hess = TRUE # Hessian matrix of log-likelihood\n)\n\n\n\n\nSimilar to MLR using Wald test\n\ncbind(\n  tidy(ordinal_model),\n  p.value = pnorm(abs(tidy(ordinal_model)$statistic),\n    lower.tail = FALSE\n  ) * 2\n)\n# confidence intervals\n\nconfint(ordinal_model) # default is 95%\n\n\n\n\ne.g. \\(\\beta_1 = 0.6\\)\n\nFor a one unit increase in \\(X_1\\), the odds of being in a higher category is \\(e^{0.6} = 1.82\\) times the odds of being in a lower category, holding all other variables constant.\n\n\n\n\n\npredict(ordinal_model, newdata = data, type = \"probs\")\n# returns probabilities for each level\n\nTo get the corresponding predicted cumulative odds for a new observation, use VGAM::vglm function\n\nolr &lt;- vglm(\n  response ~ regressor_1 + regressor_2,\n  propodds, # for proportional odds model\n  data,\n)\n\n# can also predict using this model, same as code block above\npredict(olr, newdata = data, type = \"response\")\n\n# get predicted cumulative odds\npredict(olr, newdata = data, type = \"link\") |&gt;\n  exp() # to get odds instead of log odds\n\nInterpret the predicted cumulative odds as:\n\ne.g. \\(logitlink(P[Y_i \\geq j]) = 2.68\\)\n\nA student with [data for \\(X_i\\)] is 2.68 times more likely to be in \\(j\\) or higher category than in category \\(j - 1\\) or lower, holding all other variables constant.\n\ne.g. \\(logitlink(P[Y_i \\geq 2]) = 0.33\\)\n\nA student with [data for \\(X_i\\)] is 3.03 (1/0.33) times more likely to be in \\(j\\) category or lower than in category j or higher, holding all other variables constant.\n\n\n\n\n\n\n\nIf the proportional odds assumption is not met, we can use the partial proportional odds model.\nTest for proportional odds assumption using the Brant-Wald test.\n\n\\(H_0\\): Our OLR model globally fulfills the proportional odds assumption.\n\\(H_A\\): Our OLR model does not globally fulfill the proportional odds assumption.\n\n\nbrant(ordinal_model)\n\nIf the proportional odds assumption is not met, we can use the generalized ordinal logistic regression model.\n\nBasically all \\(\\beta\\)’s are allowed to vary across the different levels of the response variable.\n\n\n\n\n\n\n\nLinear Fixed Effects Model (LFE) is a generalization of the linear regression model\nFixed Effects: the parameters of the model\n\nconstant for all observations\n\n\n\n\n\nData Hierarchy: the data is organized in a hierarchy\n\nCan be due to sampling levels\ne.g. investmests in different firms, students in different schools (sampling schemes may be different in different schools)\n\nMight have some correlation between datapoints in firms/ schools\n\nviolates the independence assumption (i.i.d. observations)\n\n\n\n\n\nGoal: assessing the association of gross investment with market_value and capital in the population of American firms.\nData: 11 firms, 20 observations per firm\n\n2 heirachical levels: firm and observation\n\n\n\nTrial 1: ignore firm\n\nordinary_model &lt;- lm(\n  formula = investment ~ market_value + capital,\n  data = Grunfeld)\n\nTrial 2: Different intercepts for different firms\n\nmodel_varying_intercept &lt;- lm(\n  # -1: so that baseline is not included as first intercept\n    formula = investment ~ market_value + capital + firm - 1,\n    data = Grunfeld)\n\nTrial 3: OLS regeression for each firm\n\n\nThis does NOT solve our goal.\nWe want to find out among all firms, not one specific firm.\n\nmodel_by_firm &lt;- lm(\n  investment ~ market_value * firm + capital * firm,\n  data = Grunfeld)\n\n\n\n\n\n\nFundamental idea:\n\ndata subsets of elements share a correlation structure\ni.e. all n rows of training data are not independent\n\n\n\\[ \\text{mixed effect} = \\text{fixed effect} + \\text{random effect} \\]\n\\[\\beta_{0j} = \\beta_0 + b_{0j}\\]\n\n\\(\\beta_{0j}\\)/ mixed effect: the intercept for the \\(j\\)th school/ firm\n\\(\\beta_0\\)/ fixed effect: the average intercept\n\\(b_{0j}\\)/ random effect: the deviation of the \\(j\\)th school/ firm from the average intercept\n\n\\(b_{0j} \\sim N(0, \\sigma^2_{0})\\)\nindependent of the error term \\(\\epsilon\\)\n\nVariance of the \\(i\\)th observation:\n\n\\(\\sigma^2_{0} + \\sigma^2_{\\epsilon}\\)\n\n\n\n\n\\[\ny_{ij} = \\beta_{0j} + \\beta_{1j}x_{1ij} + \\beta_{2j}x_{2ij} + \\epsilon_{ij} \\\\ = (\\beta_0 + b_{0j}) + (\\beta_1 + b_{1j})x_{1ij} + (\\beta_2 + b_{2j})x_{2ij} + \\epsilon_{ij}\n\\]\nFor \\(i\\) in \\(1, 2, \\ldots, n_j\\) and \\(j\\) in \\(1, 2, \\ldots, J\\)\nNote: \\((b_{0j}, b_{1j}, b_{2j}) \\sim N(\\textbf{0}, \\textbf{D})\\)\n\n\\(\\textbf{0}\\): vector of zero, e.g. \\((0, 0, 0)^T\\)\n\\(\\textbf{D}\\): generic covariance matrix\n\n\\[\\textbf{D} = \\begin{bmatrix} \\sigma^2_{0} & \\sigma_{01} & \\sigma_{02} \\\\ \\sigma_{10} & \\sigma^2_{1} & \\sigma_{12} \\\\ \\sigma_{20} & \\sigma_{21} & \\sigma^2_{2} \\end{bmatrix} = \\begin{bmatrix} \\sigma^2_{0} & \\rho_{01}\\sigma_{0}\\sigma_{1} & \\rho_{02}\\sigma_{0}\\sigma_{2} \\\\ \\rho_{10}\\sigma_{0}\\sigma_{1} & \\sigma^2_{1} & \\rho_{12}\\sigma_{1}\\sigma_{2} \\\\ \\rho_{20}\\sigma_{0}\\sigma_{2} & \\rho_{21}\\sigma_{1}\\sigma_{2} & \\sigma^2_{2} \\end{bmatrix}\\]\n\n\\(\\rho_{uv}\\): pearson correlation between uth and vth random effects\n\n\n\n\n\nuse the lmer function from the lme4 package\n\nmixed_intercept_model &lt;- lmer(\n  response ~ regressor_1 + regressor_2 +\n    (1 | school), # random intercept by firm\n  data\n)\n\nfull_model &lt;- lmer(\n  response ~ regressor_1 + regressor_2 +\n    (regressor_1 + regressor_2| school),\n    # random intercept and slope by firm\n  data\n)\n\nEquation for mixed intercept model:\n\n\\[y_{ij} = (\\beta_0 + b_{0j}) + \\beta_1x_{1ij} + \\beta_2x_{2ij} + \\epsilon_{ij}\\]\n\nEquation for full model:\n\n\\[y_{ij} = (\\beta_0 + b_{0j}) + (\\beta_1 + b_{1j})x_{1ij} + (\\beta_2 + b_{2j})x_{2ij} + \\epsilon_{ij}\\]\n\n\n\n\nCannot do inference using normal t-test\n\nsummary(mixed_intercept_model)\nsummary(full_model)\n\n# obtain coefficients\ncoef(mixed_intercept_model)$firm\ncoef(full_mixed_model)$firm\n\n\n\n\nPredict on existing group\nPredict on new group\n\npredict(full_model,\n  newdata = tibble(school = \"new_school\", regressor_1 = 1, regressor_2 = 2))\n\n\n\n\n\nRecall that classical linear regression (parametric) favours interpreatbility when aiming to make inference\nIf goal is accurate prediction, then we can use local regression (non-linear)\n\n\n\n\nUse step function to fit a piecewise constant function\n\n\\[\nC_0(X_i) = I(X_i &lt; c_1) = \\begin{cases} 1 & \\text{if } X_i &lt; c_1 \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\n\\[\nC_1(X_i) = I(c_1 \\leq X_i &lt; c_2) = \\begin{cases} 1 & \\text{if } c_1 \\leq X_i &lt; c_2 \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\n\\[\n\\vdots\n\\]\n\\[\nC_{k-1}(X_i) = I(c_{k-1} \\leq X_i &lt; c_k) = \\begin{cases} 1 & \\text{if } c_{k-1} \\leq X_i &lt; c_k \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\n\\[\nC_k(X_i) = I(X_i \\leq c_k) = \\begin{cases} 1 & \\text{if } X_i \\leq c_k \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\n\\[Y_i = \\beta_0 + \\beta_1 C_1(X_i) + \\beta_2 C_2(X_i) + \\cdots + \\beta_k C_k(X_i) + \\epsilon_i\\]\n\nNo need \\(C_0\\) just by definition of \\(C_0\\)\n\nbreakpoints &lt;- c(10, 20, 30, 40, 50) # or 5 (number of breakpoints)\n\n# create steps\ndata &lt;- data |&gt; mutate(\n    steps = cut(data$var_to_split,\n                breaks = breakpoints,\n                right = FALSE))\nlevels(data$steps) # check levels\n\nmodel &lt;- lm(Y ~ steps, data = data)\n\n\n\n\nAdd interaction terms to the model\n\n\\[Y_i = \\beta_0 + \\beta_1C_1(X_i) + \\cdots + \\beta_kC_k(X_i) \\\\ + \\beta_{k+1}X_i + \\beta_{k+2}X_iC_1(X_i) + \\cdots + \\beta_{2k}X_iC_k(X_i) + \\epsilon_i\\]\nmodel_piecewise_linear &lt;- lm(Y ~ steps * var_to_split, data = data)\n\n\n\n\\[Y_i = \\beta_0 + \\beta_1X_i + \\beta_2(X_i - c_1)_+ + \\cdots + \\beta_k(X_i - c_{k-1})_+ + \\epsilon_i\\]\nWhere:\n\\[(X_i - c_j)_+ = \\begin{cases} X_i - c_j & \\text{if } X_i &gt; c_j \\\\ 0 & \\text{otherwise} \\end{cases}\\]\nmodel_piecewise_cont_linear &lt;- lm(Y ~ var_to_split +\n    I(var_to_split - breakpoint[2]) * I(var_to_split &gt;= breakpoint[2]) +\n    I(var_to_split - breakpoint[3]) * I(var_to_split &gt;= breakpoint[3]) +\n    I(var_to_split - breakpoint[4]) * I(var_to_split &gt;= breakpoint[4]) +\n    I(var_to_split - breakpoint[5]) * I(var_to_split &gt;= breakpoint[5]),\n    data = data)\n\n\n\n\n\nIn this section:\n\n\\(k\\): number of neighbours\n\\(p\\): number of regressors\n\nkNN is a non-parametric method\nno training phase (lazy learner)\nfinds \\(k\\) closest neighbours to the query point \\(x_0\\) and predicts the average of the neighbours’ responses\n\\(k=1\\) means no training error but overfitting\n\nmodel_knn &lt;- knnreg(Y ~ X, data = data, k = 5)\n\n\n\n\nIdea:\n\nFind closest points to \\(x_i\\) (query point)\nassign weights based on distance\n\ncloser -&gt; more weight\n\nuse weighted least squares for second degree polynomial fit\n\nMinimize sum of squares of weighted residuals\n\n\\[\\sum_{i=1}^n w_i(y_i - \\beta_0 - \\beta_1x_i - \\beta_2x_i^2)^2\\]\n\nThis model can deal with heteroscedasticity (non-constant variance)\n\nWeighted least squares allows different variance for each observation\n\nThings to consider:\n\nspan: between 0 and 1, specifies the proportion of points considered as neighbours (more neighbours -&gt; smoother fit)\ndegree: degree of polynomial to fit\n\n\nmodel_lowess &lt;- lowess(Y ~ X, data = data, span = 0.5, degree = 2)\n\n\n\n\n\n\nThe quantile \\(Q(\\tau)\\) is the observed value of \\(X\\) such that \\(\\tau * 100\\%\\) of the data is less than or equal to \\(X\\).\n\ni.e. on the left side of the distribution\n\n\n \n\n\n\n\nQuantile regression is a form of regression analysis used to estimate the conditional median or other quantiles of a response variable.\nTypes of questions that can be answered with quantile regression:\n\nFor baseball teams in the upper 75% threshold in runs, are these runs largely associated with a large number of hits?\nFor any given team that scores 1000 hits in future tournaments, how many runs can this team score with 50% chance?\n\n\n\n\n\n\nRecall OLS: \\[\n\\mathbb{E}(Y_i \\mid X_{i,j} = x_{i,j}) = \\beta_0 + \\beta_1 x_{i,1} + \\ldots + \\beta_k x_{i,k};\n\\]\n\n\\[\n\\text{loss} = \\sum_{i = 1}^n (y_i - \\beta_0 - \\beta_1 x_{i,1} - \\ldots - \\beta_k x_{i,k})^2.\n\\]\n\nParametric Quantile Regression:\n\n\\[\nQ_i( \\tau \\mid X_{i,j} = x_{i,j}) = \\beta_0(\\tau) + \\beta_1(\\tau) x_{i,1} + \\ldots + \\beta_k(\\tau) x_{i,k}\n\\]\n(notice the \\(\\beta\\)s are now functions of \\(\\tau\\))\n\nError term: Fidelity\n\n\\[\n\\text{loss} = \\sum_{i} e_i[\\tau - I(e_i &lt; 0)] = \\sum_{i: e_i \\geq 0} \\tau|e_i|+\\sum_{i: e_i &lt; 0}(1-\\tau)|e_i|\n\\]\nWhere\n\\[I(e_i &lt; 0) = \\begin{cases} 1 & \\text{if } e_i &lt; 0 \\\\ 0 & \\text{otherwise} \\end{cases}\\]\n\n\n# Plotting the quantiles\nggplot(data, aes(x, y)) +\n    geom_point() +\n    geom_quantile(quantiles = c(0.25, 0.5, 0.75),\n                    formula = y ~ x)\n\n# fit the model\npqr_model &lt;- rq(y ~ x,\n                tau = c(0.25, 0.5, 0.75),\n                data = data)\n\n\n\n\nTo justify the relationship b/w the \\(\\tau\\)th quantile in response and regressors.\nUse test statistics to test the null hypothesis that the \\(\\tau\\)th quantile is not related to the regressors.\n\n\\(t\\)-value with \\(n - k - 1\\) degrees of freedom:\n\n\n\\[t_j = \\frac{\\hat{\\beta}_j(\\tau)}{\\text{SE}(\\hat{\\beta}_j(\\tau))}\\]\n\nNull hypothesis: \\(H_0 : \\beta_j(\\tau) = 0\\).\nCheck p-value: summary(pqr_model)[1] for \\(\\tau = 0.25\\), summary(pqr_model)[2] for \\(\\tau = 0.5\\), summary(pqr_model)[3] for \\(\\tau = 0.75\\).\n\n\n\n\n\nSimilar to OLS\n\\(\\beta_1(\\tau)\\) is the change in the \\(\\tau\\)th quantile of \\(Y\\) for a unit increase in \\(X_1\\).\ne.g. \\(\\beta_1(0.75) = 0.5\\) means that for a unit increase in \\(X_1\\), the 75th quantile of \\(Y\\) increases by 0.5.\n\n\n\n\npredict(pqr_model, newdata = data.frame(...))\n\n\n\n\n\nImplicates no distributional assumptions and no model function specification.\n\\(\\lambda\\) is the penalty parameter.\n\nChoosing how local the estimation is.\nsmall \\(\\lambda\\): better approx, but more variance (model not smooth)\nlarge \\(\\lambda\\): lose local info, favouring smoothness (global info)\n\n\nmedian_rqss &lt;- rqss(y ~ qss(x, lambda = 0.5),\n            tau = 0.5, # cannot do multiple quantiles\n            data = data)\n\nsummary(median_rqss)\n\npredict(median_rqss, newdata = data.frame(...))\n\n\n\n\n\n\n\n\n\nThe probability of missing data is the same for all observations\nMissingness is independent of data (Ideal case because there is no pattern)\nNo systematic differences between missing and non-missing data\n\n\n\n\n\nThe probability of missing data depends on observed data\nCan use imputation to fill in missing data:\n\nHot deck: Replace missing value with a value from the same dataset\nCold deck: Replace missing value with a value from a different dataset\n\n\n\n\n\n\nThe probability of missing data depends on unobservable quantities\nE.g. missing data on income for people who are unemployed\n\n\n\n\n\n\n\n\nRemove all observations with missing data\nIf data is MCAR, this is unbiased\n\nAlso increases standard errors since we’re using less data\n\nIf data is MAR or MNAR, this is biased (CAREFUL)\n\ne.g. if missing data is related to income, lower income will omit telling us their income, so removing them will bias our data to higher income.\n\n\n\n\n\n\nReplace missing data with the mean of the observed data\n\nCan only be used on continuous/ count data\n\nArtificially reduces standard errors (drawback)\n\nAlso reduces variance, which is not good\n\n\nlibrary(mice)\n\ndata &lt;- mice(data, seed = 1, method = \"mean\")\ncomplete(data)\n\n\n\n\nUse a regression model to predict missing data\nUse the predicted value as the imputed value\nWe will reinforce the relationship between the predictor and the variable with missing data\n\nThis is not good if the relationship is not strong\nWill change inference results\n\n\ndata &lt;- mice(data, seed = 1, method = \"norm.predict\")\ncomplete(data)\n\n\n\n\nIdea: Impute missing data multiple times to account for uncertainty\nUse mice package in R. Stands for Multivariate Imputation by Chained Equations\nSteps:\n\nCreate m copies of the dataset\nIn each copy, impute missing data (different values)\nCarry out analysis on each dataset\nCombine models to one pooled model\n\n\nimp_data &lt;- mice(data, seed = 1, m = 15, printFlag = FALSE)\n\ncomplete(data, 3) # Get the third imputed dataset\n\n# estimate OLS regression on each dataset\nmodels &lt;- with(imp_data, lm(y ~ x1 + x2))\n\n# get third model\nmodels$analyses[[3]]\n\n# combine models\npooled_model &lt;- pool(models)\n\nsummary(pooled_model) # remember to exp if using log model"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#data-science-workflow",
    "href": "block_4/562_regression_2/562_regression_2.html#data-science-workflow",
    "title": "Regression II",
    "section": "",
    "text": "General stages of the workflow:\n\nStudy design\nData collection and wrangling\nExploratory data analysis (EDA)\nData modeling\nEstimation\nResults\nStorytelling\n\n\n\nFrom: https://pages.github.ubc.ca/MDS-2023-24/DSCI_562_regr-2_students/notes/lecture1-glm-link-functions-and-count-regression.html\n\nChoose a proper workflow according to either:\n\nInferential\nPredictive\n\nAlso choose correct regression model"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#review-of-regression-i",
    "href": "block_4/562_regression_2/562_regression_2.html#review-of-regression-i",
    "title": "Regression II",
    "section": "",
    "text": "Response of continuous nature (hence the “ordinary”)\nResponse is subject to regressors (or explanatory variables/ features/ independent variables)\n\nMore than 1 regressor =&gt; multiple linear regression\n\n\n\\[Response = Systematic + Random\\]\n\\[Y_i = \\beta_0 + \\beta_1 g_1(x_{i1}) + \\beta_2 g_2(x_{i2}) + \\dots + \\beta_p g_p(x_{ip}) + \\epsilon_i\\]\n\nrandom is the \\(\\epsilon_i\\) term\n\n\n\n\nLinearity: the relationship between the response and functions of the regressors is linear\nErrors are independent of each other and are normally distributed with mean 0 and variance \\(\\sigma^2\\)\n\nHence, each \\(Y_i\\) is assumed to be independent and normally distributed.\n\n\n\n\nTo fit will need \\(k+2\\) parameters: \\(\\beta_0, \\beta_1, \\dots, \\beta_k, \\sigma^2\\)\nMinimize the sum of squared errors (SSE) OR maximize the likelihood of the observed data\nMaximum Likelihood Estimation (MLE): find the parameters that maximize the likelihood of the observed data\n\nLikelihood: the probability of observing the data given the parameters\nLog-likelihood: the log of the likelihood\n\n\n\n\n\n\nDo a t-test on the parameters to see if they are statistically significant\n\n\n\n\n\n\nOLS allows response to take any real number.\nExamples of non-suitable responses:\n\nNon-negative values\nBinary values (success/failure)\nCount data"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#link-function",
    "href": "block_4/562_regression_2/562_regression_2.html#link-function",
    "title": "Regression II",
    "section": "",
    "text": "Recall: OLS models a continuous response via its conditional mean\n\n\\[\\mu_i = E(Y_i | X_i) = \\beta_0 + \\beta_1 g_1(x_{i1}) + \\beta_2 g_2(x_{i2}) + \\dots + \\beta_p g_p(x_{ip}) \\]\n\nBUT this is not suitable for non-continuous responses (e.g. binary, count, non-negative).\nSolution: use a link function \\(h(\\mu_i)\\) to map the conditional mean to the real line\n\n\n\nLink function: relate the systematic component, \\(\\eta_i\\), with the response’s mean\n\n\\[h(\\mu_i) = \\eta_i = \\beta_0 + \\beta_1 g_1(x_{i1}) + \\beta_2 g_2(x_{i2}) + \\dots + \\beta_p g_p(x_{ip}) \\]\n\nMonotonic: allows for a one-to-one mapping between the mean of the response variable and the linear predictor\n\n\\[\\mu_i = h^{-1}(\\eta_i)\\]\n\nDifferentiable: to allow for maximum likelihood estimation (MLE), used to obtain \\(\\hat{\\beta}\\) $$"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#generalized-linear-models-glm",
    "href": "block_4/562_regression_2/562_regression_2.html#generalized-linear-models-glm",
    "title": "Regression II",
    "section": "",
    "text": "Generalized Linear Models (GLM): a generalization of OLS regression that allows for non-continuous responses\n\nGLM = link function + error distribution"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#poisson-regression",
    "href": "block_4/562_regression_2/562_regression_2.html#poisson-regression",
    "title": "Regression II",
    "section": "",
    "text": "Poisson regression: a GLM for count data (Equidispersed)\n\nEquidispersed: the variance of the response is equal to its mean (i.e. \\(Var(Y_i) = E(Y_i) = \\lambda_i\\))\n\nIt assumes a random sample of \\(n\\) count observations \\(Y_i\\)s\n\nIndependent\nNot Identically Distributed: Each \\(Y_i\\) has its own mean \\(E(Y_i) = \\lambda_i &gt; 0\\) and variance \\(Var(Y_i) = \\lambda_i &gt; 0\\)\n\n\n\\[Y_i \\sim Poisson(\\lambda_i)\\]\n\n\\(\\lambda_i\\) is the risk of event occurance in a given timeframe or area (definition of Poisson distribution)\n\n\n\n\nLog link function: the log of the mean of the response variable is linearly related to the regressors\n\n\\[h(\\mu_i) = log(\\mu_i) = \\eta_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ik} \\]\nHence,\n\\[\\lambda_i = e^{\\eta_i} = e^{\\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ik}}\\]\n\nThis is good since \\(\\lambda_i\\) (mean count) is always positive\n\n\n\n\nglm(Y ~ X, family = poisson, data = dataset)\n\n# view each regression coefficient\ntidy(glm_model)\ntidy(glm_model, conf.int = TRUE) # for 95% confidence interval\n\n# view model summary\nglance(glm_model)\n\n\ne.g. \\(\\beta_1 = 0.5\\)\n\n\\(\\beta_1\\) is the expected change in the log of the mean count for a one-unit increase in \\(X_1\\) holding all other variables constant\na one-unit increase in \\(X_1\\) will increase the mean count by \\(e^{0.5} = 1.65\\) times.\n\n\n\n\n\n\nTo determine the significance of the parameters \\(\\beta_1, \\beta_2, \\dots, \\beta_p\\), we can do a Wald statistic\n\n\\[z_j = \\frac{\\hat{\\beta_j}}{SE(\\hat{\\beta_j})}\\]\n\nTo test the hypothesis:\n\n\\(H_0: \\beta_j = 0\\)\n\\(H_1: \\beta_j \\neq 0\\)"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#negative-binomial-regression",
    "href": "block_4/562_regression_2/562_regression_2.html#negative-binomial-regression",
    "title": "Regression II",
    "section": "",
    "text": "Negative Binomial Regression: a GLM for count data (Overdispersed)\n\nOverdispersed: the variance of the response is greater than its mean (i.e. \\(Var(Y_i) &gt; E(Y_i) = \\lambda_i\\))\n\n\n\nCheck for Overdispersion\ndispersiontest(glm_model)\n\nif p-value &lt; 0.05, then there is overdispersion (reject null hypothesis)\n\nIf use Poisson regression on overdispersed data, then the standard errors will be underestimated =&gt; Type I error (false positive) increases\n\n\nRecall PMF of Negative Binomial Distribution:\n\n\\[P(Y_i | m,p_i) = (^{y_i + m - 1}_{y_i}) p_i^{y_i} (1 - p_i)^m\\]\n\n\\(y_i\\) is the number of failures before experiencing \\(m\\) successes where probability of success is \\(p_i\\)\n\n\\[E(Y_i) = \\frac{m(1-p_i)}{p_i}\\]\n\\[Var(Y_i) = \\frac{m(1-p_i)}{p_i^2}\\]\n\nRearranging the above equations, we get:\n\n\\[E(Y_i) = \\lambda_i\\]\n\\[Var(Y_i) = \\lambda_i (1 + \\frac{\\lambda_i}{m})\\]\n\nInteresting information:\n\n\\[X \\sim Poisson(\\lambda) = lim_{m \\to \\infty} Negative Binomial(m, p_i)\\]\n\n\nglm.nb(Y ~ X, data = dataset)\nSince negative binomial has the same link function as Poisson, we can interpret the coefficients the same way."
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#likelihood-based-model-selection",
    "href": "block_4/562_regression_2/562_regression_2.html#likelihood-based-model-selection",
    "title": "Regression II",
    "section": "",
    "text": "The deviance (\\(D_k\\)) is used to compare a given model with k regressors (\\(l_k\\)) with the baseline/ saturated model (\\(l_0\\)).\n\nThe baseline model is the “perfect” fit to the data (overfitted), it has a distinct poisson mean (\\(\\lambda_i\\)) for each \\(i\\)th observation.\n\n\n\\[D_k = 2 log \\frac{\\hat{l}_k}{\\hat{l}_0}\\]\n\nInterpretation of \\(D_k\\)\n\nLarge value of \\(D_k\\) =&gt; poor fit compared to baseline model\nSmall value of \\(D_k\\) =&gt; good fit compared to baseline model\n\n\n\n\n\\[D_k = 2 \\sum_{i=1}^n \\left[ y_i log \\left( \\frac{y_i}{\\hat{\\lambda}_i} \\right) - (y_i - \\hat{\\lambda}_i) \\right]\\]\n*note: when \\(y_i = 0\\), log term is defined to be 0.\n\nHypothesises are as follows (opposite of normal hypothesis):\n\n\\(H_0\\): Our model with k regressors fits the data better than the saturated model.\n\\(H_A\\): Otherwise\n\n\nglance(model) # D_k is \"deviance\" col\n\n# to get p-value\npchisq(summary_poisson_model_2$deviance,\n  df = summary_poisson_model_2$df.residual,\n  lower.tail = FALSE\n)\n\nFormally deviance is residual deviance, this is a test statistic.\nAsmptomatically, it has a null distribution of:\n\n\\[D_k \\sim \\chi^2_{n-k-1}\\]\n\ndof: \\(n-k-1\\)\n\n\\(n\\) is the number of observations\n\\(k\\) is the number of regressors (including intercept)\n\n\n\n\n\nanova(model_1, model_2, test = \"Chisq\")\n# deviance column is \\delta D_k\n\nmodel_1 is nested in model_2\n\\(H_0\\): model_1 fits the data better as model_2\n\\(H_A\\): model_2 fits the data better as model_1\n\n\\[\\Delta D_k = D_{k_1} - D_{k_2} \\sim \\chi^2_{k_2 - k_1}\\]\n\n\n\n\n\\[AIC_k = D_k + 2k\\]\n\nAIC can be used to compare models that are not nested.\nSmaller AIC is better (means better fit)\nCan get from glance() function\n\n\n\n\n\\[BIC_k = D_k + k log(n)\\]\n\nBIC tends to select models with fewer regressors than AIC.\nsmaller BIC is better (means better fit)\nCan get from glance() function"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#multinomial-logistic-regression",
    "href": "block_4/562_regression_2/562_regression_2.html#multinomial-logistic-regression",
    "title": "Regression II",
    "section": "",
    "text": "Is a MLE-based GLM for when the response is categorical and nominal.\n\nNominal: unordered categories\n\ne.g. red, green, blue\n\nOrdinal: ordered categories\n\ne.g. low, medium, high\n\n\nSimilar to binomial logistic regression, but with more than 2 categories.\nLink function is the logit function.\n\nneed more than 1 logit function to model the probabilities of each category.\nOne category is the baseline category, the other categories are compared to the baseline category.\n\n\n\\[\\eta_i^{(model 2, model 1)} = \\log\\left[\\frac{P(Y_i = \\texttt{model 2} \\mid X_{i, 1}, X_{i,2}, X_{i,3})}{P(Y_i = \\texttt{model 1} \\mid X_{i, 1}, X_{i,2}, X_{i,3})}\\right]\\]\n\\[= \\beta_0^{(\\texttt{model 2},\\texttt{model 1})} + \\beta_1^{(\\texttt{model 2},\\texttt{model 1})} X_{i, 1} + \\beta_2^{(\\texttt{model 2},\\texttt{model 1})} X_{i, 2} + \\beta_3^{(\\texttt{model 2},\\texttt{model 1})} X_{i, 3}\\] \\[\\eta_i^{(model 3, model 1)} = \\log\\left[\\frac{P(Y_i = \\texttt{model 3} \\mid X_{i, 1}, X_{i,2}, X_{i,3})}{P(Y_i = \\texttt{model 1} \\mid X_{i, 1}, X_{i,2}, X_{i,3})}\\right]\\]\n\\[=\\beta_0^{(\\texttt{model 3},\\texttt{model 1})} + \\beta_1^{(\\texttt{model 3},\\texttt{model 1})} X_{i, 1} + \\beta_2^{(\\texttt{model 3},\\texttt{model 1})} X_{i, 2} + \\beta_3^{(\\texttt{model 3},\\texttt{model 1})} X_{i, 3}\\]\nWith some algebra, we can get the following (For m categories):\n\\[p_{i, \\texttt{model 1}} = \\frac{1}{1 + \\sum_{j=2}^m e^{\\eta_i^{(\\texttt{model j}, \\texttt{model 1})}}}\\]\n\\[p_{i, \\texttt{model 2}} = \\frac{e^{\\eta_i^{(\\texttt{model 2}, \\texttt{model 1})}}}{1 + \\sum_{j=2}^m e^{\\eta_i^{(\\texttt{model j}, \\texttt{model 1})}}}\\]\n\nAll probabilities sum to 1.\n\n\n\n\nThe baseline level is the level that is not included in the model.\n\ncan find using levels() function, the first level is the baseline level.\n\n\nlevels(data$response) # to check levels\n\n# to change levels\ndata$response &lt;- recode_factor(data$response,\n  \"0\" = \"new_level_0\",\n  \"1\" = \"new_level_1\",\n)\n\n\n\nmodel &lt;- multinom(response ~ regressor_1 + regressor_2 + regressor_3,\n  data = data)\n\n# to get test statistics\nmlr_output &lt;- tidy(model,\n  conf.int = TRUE, # to get confidence intervals (default is 95%)\n  exponentiate = TRUE) # to get odds ratios\n# default result is log odds ratios\n\n# can filter p-values\nmlr_output |&gt; filter(p.value &lt; 0.05)\n\n# predict\npredict(model, newdata = data, type = \"probs\")\n# sum of all probabilities is 1\n\n\n\n\nCheck if regressor is significant using Wald test.\n\n\\[z_j^{(u,v)} = \\frac{\\hat{\\beta}_j^{(u,v)}}{SE(\\hat{\\beta}_j^{(u,v)})}\\]\n\nFor large sample sizes, \\(z_j^{(u,v)} \\sim N(0,1)\\)\nTo test the hypothesis:\n\n\\(H_0\\): \\(\\beta_j^{(u,v)} = 0\\)\n\\(H_A\\): \\(\\beta_j^{(u,v)} \\neq 0\\)\n\n\n\n\n\ne.g. \\(\\beta_1^{(b,a)} = 0.5\\)\n\nFor a 1 unit increase in \\(X_1\\), the odds of being in category \\(b\\) is \\(e^{0.5} = 1.65\\) times the odds of being in category \\(a\\).\n\ne.g. \\(\\beta_2^{(c,a)} = -0.5\\)\n\nFor a 1 unit increase in \\(X_2\\), the odds of being in category \\(c\\) decrease by \\(39\\%\\) [$ 1 - (e^{-0.5}) = 1 - 0.61 = 0.39$] less than being in category \\(a\\)."
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#ordinal-logistic-regression",
    "href": "block_4/562_regression_2/562_regression_2.html#ordinal-logistic-regression",
    "title": "Regression II",
    "section": "",
    "text": "Ordinal: has a natural ordering\nThere might be loss of information when using MLR for ordinal data\nWe are going to use the proportional odds model for ordinal data\n\nIt is a cumulative logit model\n\n\n\n\n\nReorder the levels of the response variable\n\ndata$response &lt;- as.ordered(data$response)\ndata$response &lt;- fct_relevel(\n  data$response,\n  c(\"unlikely\", \"somewhat likely\", \"very likely\")\n)\nlevels(data$response)\n\n\n\n\nFor a response with \\(m\\) responses and \\(k\\) regressors, the model is:\nWe will have:\n\n\\(m-1\\) equations (link functions: logit)\n\\(m-1\\) intercepts\n\\(k\\) regression coefficients\n\n\n\n\n\\[\n\\begin{gather*}\n\\text{Level } m - 1 \\text{ or any lesser degree versus level } m\\\\\n\\text{Level } m - 2 \\text{ or any lesser degree versus level } m - 1 \\text{ or any higher degree}\\\\\n\\vdots \\\\\n\\text{Level } 1 \\text{ versus level } 2 \\text{ or any higher degree}\\\\\n\\end{gather*}\n\\]\n\\[\n\\begin{gather*}\n\\eta_i^{(m - 1)} = \\log\\left[\\frac{P(Y_i \\leq m - 1 \\mid X_{i,1}, \\ldots, X_{i,k})}{P(Y_i = m \\mid X_{i,1}, \\ldots, X_{i,k})}\\right] = \\beta_0^{(m - 1)} - \\beta_1 X_{i, 1} - \\beta_2 X_{i, 2} - \\ldots - \\beta_k X_{i, k} \\\\\n\\eta_i^{(m - 2)} = \\log\\left[\\frac{P(Y_i \\leq m - 2 \\mid X_{i,1}, \\ldots, X_{i,k})}{P(Y_i &gt; m - 2 \\mid X_{i,1}, \\ldots, X_{i,k})}\\right] = \\beta_0^{(m - 2)} - \\beta_1 X_{i, 1} - \\beta_2 X_{i, 2} - \\ldots - \\beta_k X_{i, k} \\\\\n\\vdots \\\\\n\\eta_i^{(1)} = \\log\\left[\\frac{P(Y_i = 1 \\mid X_{i,1}, \\ldots, X_{i,k})}{P(Y_i &gt; 1 \\mid X_{i,1}, \\ldots, X_{i,k})}\\right] = \\beta_0^{(1)} - \\beta_1 X_{i, 1} - \\beta_2 X_{i, 2} - \\ldots - \\beta_k X_{i, k}.\n\\end{gather*}\n\\]\n\n\n\n\\[p_{i,j} = P(Y_i = j \\mid X_{i,1}, \\ldots, X_{i,k}) = P(Y_i \\leq j \\mid ...) - P(Y_i \\leq j - 1 \\mid ...)\\]\n\n\\(i\\) is the index of the observation\n\\(j\\) is the level of the response variable\n\n\\[\\sum_{j = 1}^{m} p_{i,j} = 1\\]\n\n\n\n\n\nuse MASS::polr function\n\nordinal_model &lt;- polr(\n  formula = response ~ regressor_1 + regressor_2,\n  data = data,\n  Hess = TRUE # Hessian matrix of log-likelihood\n)\n\n\n\n\nSimilar to MLR using Wald test\n\ncbind(\n  tidy(ordinal_model),\n  p.value = pnorm(abs(tidy(ordinal_model)$statistic),\n    lower.tail = FALSE\n  ) * 2\n)\n# confidence intervals\n\nconfint(ordinal_model) # default is 95%\n\n\n\n\ne.g. \\(\\beta_1 = 0.6\\)\n\nFor a one unit increase in \\(X_1\\), the odds of being in a higher category is \\(e^{0.6} = 1.82\\) times the odds of being in a lower category, holding all other variables constant.\n\n\n\n\n\npredict(ordinal_model, newdata = data, type = \"probs\")\n# returns probabilities for each level\n\nTo get the corresponding predicted cumulative odds for a new observation, use VGAM::vglm function\n\nolr &lt;- vglm(\n  response ~ regressor_1 + regressor_2,\n  propodds, # for proportional odds model\n  data,\n)\n\n# can also predict using this model, same as code block above\npredict(olr, newdata = data, type = \"response\")\n\n# get predicted cumulative odds\npredict(olr, newdata = data, type = \"link\") |&gt;\n  exp() # to get odds instead of log odds\n\nInterpret the predicted cumulative odds as:\n\ne.g. \\(logitlink(P[Y_i \\geq j]) = 2.68\\)\n\nA student with [data for \\(X_i\\)] is 2.68 times more likely to be in \\(j\\) or higher category than in category \\(j - 1\\) or lower, holding all other variables constant.\n\ne.g. \\(logitlink(P[Y_i \\geq 2]) = 0.33\\)\n\nA student with [data for \\(X_i\\)] is 3.03 (1/0.33) times more likely to be in \\(j\\) category or lower than in category j or higher, holding all other variables constant.\n\n\n\n\n\n\n\nIf the proportional odds assumption is not met, we can use the partial proportional odds model.\nTest for proportional odds assumption using the Brant-Wald test.\n\n\\(H_0\\): Our OLR model globally fulfills the proportional odds assumption.\n\\(H_A\\): Our OLR model does not globally fulfill the proportional odds assumption.\n\n\nbrant(ordinal_model)\n\nIf the proportional odds assumption is not met, we can use the generalized ordinal logistic regression model.\n\nBasically all \\(\\beta\\)’s are allowed to vary across the different levels of the response variable."
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#linear-fixed-effects-model",
    "href": "block_4/562_regression_2/562_regression_2.html#linear-fixed-effects-model",
    "title": "Regression II",
    "section": "",
    "text": "Linear Fixed Effects Model (LFE) is a generalization of the linear regression model\nFixed Effects: the parameters of the model\n\nconstant for all observations\n\n\n\n\n\nData Hierarchy: the data is organized in a hierarchy\n\nCan be due to sampling levels\ne.g. investmests in different firms, students in different schools (sampling schemes may be different in different schools)\n\nMight have some correlation between datapoints in firms/ schools\n\nviolates the independence assumption (i.i.d. observations)\n\n\n\n\n\nGoal: assessing the association of gross investment with market_value and capital in the population of American firms.\nData: 11 firms, 20 observations per firm\n\n2 heirachical levels: firm and observation\n\n\n\nTrial 1: ignore firm\n\nordinary_model &lt;- lm(\n  formula = investment ~ market_value + capital,\n  data = Grunfeld)\n\nTrial 2: Different intercepts for different firms\n\nmodel_varying_intercept &lt;- lm(\n  # -1: so that baseline is not included as first intercept\n    formula = investment ~ market_value + capital + firm - 1,\n    data = Grunfeld)\n\nTrial 3: OLS regeression for each firm\n\n\nThis does NOT solve our goal.\nWe want to find out among all firms, not one specific firm.\n\nmodel_by_firm &lt;- lm(\n  investment ~ market_value * firm + capital * firm,\n  data = Grunfeld)"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#linear-mixed-effects-model",
    "href": "block_4/562_regression_2/562_regression_2.html#linear-mixed-effects-model",
    "title": "Regression II",
    "section": "",
    "text": "Fundamental idea:\n\ndata subsets of elements share a correlation structure\ni.e. all n rows of training data are not independent\n\n\n\\[ \\text{mixed effect} = \\text{fixed effect} + \\text{random effect} \\]\n\\[\\beta_{0j} = \\beta_0 + b_{0j}\\]\n\n\\(\\beta_{0j}\\)/ mixed effect: the intercept for the \\(j\\)th school/ firm\n\\(\\beta_0\\)/ fixed effect: the average intercept\n\\(b_{0j}\\)/ random effect: the deviation of the \\(j\\)th school/ firm from the average intercept\n\n\\(b_{0j} \\sim N(0, \\sigma^2_{0})\\)\nindependent of the error term \\(\\epsilon\\)\n\nVariance of the \\(i\\)th observation:\n\n\\(\\sigma^2_{0} + \\sigma^2_{\\epsilon}\\)\n\n\n\n\n\\[\ny_{ij} = \\beta_{0j} + \\beta_{1j}x_{1ij} + \\beta_{2j}x_{2ij} + \\epsilon_{ij} \\\\ = (\\beta_0 + b_{0j}) + (\\beta_1 + b_{1j})x_{1ij} + (\\beta_2 + b_{2j})x_{2ij} + \\epsilon_{ij}\n\\]\nFor \\(i\\) in \\(1, 2, \\ldots, n_j\\) and \\(j\\) in \\(1, 2, \\ldots, J\\)\nNote: \\((b_{0j}, b_{1j}, b_{2j}) \\sim N(\\textbf{0}, \\textbf{D})\\)\n\n\\(\\textbf{0}\\): vector of zero, e.g. \\((0, 0, 0)^T\\)\n\\(\\textbf{D}\\): generic covariance matrix\n\n\\[\\textbf{D} = \\begin{bmatrix} \\sigma^2_{0} & \\sigma_{01} & \\sigma_{02} \\\\ \\sigma_{10} & \\sigma^2_{1} & \\sigma_{12} \\\\ \\sigma_{20} & \\sigma_{21} & \\sigma^2_{2} \\end{bmatrix} = \\begin{bmatrix} \\sigma^2_{0} & \\rho_{01}\\sigma_{0}\\sigma_{1} & \\rho_{02}\\sigma_{0}\\sigma_{2} \\\\ \\rho_{10}\\sigma_{0}\\sigma_{1} & \\sigma^2_{1} & \\rho_{12}\\sigma_{1}\\sigma_{2} \\\\ \\rho_{20}\\sigma_{0}\\sigma_{2} & \\rho_{21}\\sigma_{1}\\sigma_{2} & \\sigma^2_{2} \\end{bmatrix}\\]\n\n\\(\\rho_{uv}\\): pearson correlation between uth and vth random effects\n\n\n\n\n\nuse the lmer function from the lme4 package\n\nmixed_intercept_model &lt;- lmer(\n  response ~ regressor_1 + regressor_2 +\n    (1 | school), # random intercept by firm\n  data\n)\n\nfull_model &lt;- lmer(\n  response ~ regressor_1 + regressor_2 +\n    (regressor_1 + regressor_2| school),\n    # random intercept and slope by firm\n  data\n)\n\nEquation for mixed intercept model:\n\n\\[y_{ij} = (\\beta_0 + b_{0j}) + \\beta_1x_{1ij} + \\beta_2x_{2ij} + \\epsilon_{ij}\\]\n\nEquation for full model:\n\n\\[y_{ij} = (\\beta_0 + b_{0j}) + (\\beta_1 + b_{1j})x_{1ij} + (\\beta_2 + b_{2j})x_{2ij} + \\epsilon_{ij}\\]\n\n\n\n\nCannot do inference using normal t-test\n\nsummary(mixed_intercept_model)\nsummary(full_model)\n\n# obtain coefficients\ncoef(mixed_intercept_model)$firm\ncoef(full_mixed_model)$firm\n\n\n\n\nPredict on existing group\nPredict on new group\n\npredict(full_model,\n  newdata = tibble(school = \"new_school\", regressor_1 = 1, regressor_2 = 2))"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#piecewise-local-regression",
    "href": "block_4/562_regression_2/562_regression_2.html#piecewise-local-regression",
    "title": "Regression II",
    "section": "",
    "text": "Recall that classical linear regression (parametric) favours interpreatbility when aiming to make inference\nIf goal is accurate prediction, then we can use local regression (non-linear)\n\n\n\n\nUse step function to fit a piecewise constant function\n\n\\[\nC_0(X_i) = I(X_i &lt; c_1) = \\begin{cases} 1 & \\text{if } X_i &lt; c_1 \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\n\\[\nC_1(X_i) = I(c_1 \\leq X_i &lt; c_2) = \\begin{cases} 1 & \\text{if } c_1 \\leq X_i &lt; c_2 \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\n\\[\n\\vdots\n\\]\n\\[\nC_{k-1}(X_i) = I(c_{k-1} \\leq X_i &lt; c_k) = \\begin{cases} 1 & \\text{if } c_{k-1} \\leq X_i &lt; c_k \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\n\\[\nC_k(X_i) = I(X_i \\leq c_k) = \\begin{cases} 1 & \\text{if } X_i \\leq c_k \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\n\\[Y_i = \\beta_0 + \\beta_1 C_1(X_i) + \\beta_2 C_2(X_i) + \\cdots + \\beta_k C_k(X_i) + \\epsilon_i\\]\n\nNo need \\(C_0\\) just by definition of \\(C_0\\)\n\nbreakpoints &lt;- c(10, 20, 30, 40, 50) # or 5 (number of breakpoints)\n\n# create steps\ndata &lt;- data |&gt; mutate(\n    steps = cut(data$var_to_split,\n                breaks = breakpoints,\n                right = FALSE))\nlevels(data$steps) # check levels\n\nmodel &lt;- lm(Y ~ steps, data = data)\n\n\n\n\nAdd interaction terms to the model\n\n\\[Y_i = \\beta_0 + \\beta_1C_1(X_i) + \\cdots + \\beta_kC_k(X_i) \\\\ + \\beta_{k+1}X_i + \\beta_{k+2}X_iC_1(X_i) + \\cdots + \\beta_{2k}X_iC_k(X_i) + \\epsilon_i\\]\nmodel_piecewise_linear &lt;- lm(Y ~ steps * var_to_split, data = data)\n\n\n\n\\[Y_i = \\beta_0 + \\beta_1X_i + \\beta_2(X_i - c_1)_+ + \\cdots + \\beta_k(X_i - c_{k-1})_+ + \\epsilon_i\\]\nWhere:\n\\[(X_i - c_j)_+ = \\begin{cases} X_i - c_j & \\text{if } X_i &gt; c_j \\\\ 0 & \\text{otherwise} \\end{cases}\\]\nmodel_piecewise_cont_linear &lt;- lm(Y ~ var_to_split +\n    I(var_to_split - breakpoint[2]) * I(var_to_split &gt;= breakpoint[2]) +\n    I(var_to_split - breakpoint[3]) * I(var_to_split &gt;= breakpoint[3]) +\n    I(var_to_split - breakpoint[4]) * I(var_to_split &gt;= breakpoint[4]) +\n    I(var_to_split - breakpoint[5]) * I(var_to_split &gt;= breakpoint[5]),\n    data = data)"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#knn-regression",
    "href": "block_4/562_regression_2/562_regression_2.html#knn-regression",
    "title": "Regression II",
    "section": "",
    "text": "In this section:\n\n\\(k\\): number of neighbours\n\\(p\\): number of regressors\n\nkNN is a non-parametric method\nno training phase (lazy learner)\nfinds \\(k\\) closest neighbours to the query point \\(x_0\\) and predicts the average of the neighbours’ responses\n\\(k=1\\) means no training error but overfitting\n\nmodel_knn &lt;- knnreg(Y ~ X, data = data, k = 5)"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#locally-weighted-scatterplot-smoothing-lowess",
    "href": "block_4/562_regression_2/562_regression_2.html#locally-weighted-scatterplot-smoothing-lowess",
    "title": "Regression II",
    "section": "",
    "text": "Idea:\n\nFind closest points to \\(x_i\\) (query point)\nassign weights based on distance\n\ncloser -&gt; more weight\n\nuse weighted least squares for second degree polynomial fit\n\nMinimize sum of squares of weighted residuals\n\n\\[\\sum_{i=1}^n w_i(y_i - \\beta_0 - \\beta_1x_i - \\beta_2x_i^2)^2\\]\n\nThis model can deal with heteroscedasticity (non-constant variance)\n\nWeighted least squares allows different variance for each observation\n\nThings to consider:\n\nspan: between 0 and 1, specifies the proportion of points considered as neighbours (more neighbours -&gt; smoother fit)\ndegree: degree of polynomial to fit\n\n\nmodel_lowess &lt;- lowess(Y ~ X, data = data, span = 0.5, degree = 2)"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#quantile-regression",
    "href": "block_4/562_regression_2/562_regression_2.html#quantile-regression",
    "title": "Regression II",
    "section": "",
    "text": "The quantile \\(Q(\\tau)\\) is the observed value of \\(X\\) such that \\(\\tau * 100\\%\\) of the data is less than or equal to \\(X\\).\n\ni.e. on the left side of the distribution\n\n\n \n\n\n\n\nQuantile regression is a form of regression analysis used to estimate the conditional median or other quantiles of a response variable.\nTypes of questions that can be answered with quantile regression:\n\nFor baseball teams in the upper 75% threshold in runs, are these runs largely associated with a large number of hits?\nFor any given team that scores 1000 hits in future tournaments, how many runs can this team score with 50% chance?\n\n\n\n\n\n\nRecall OLS: \\[\n\\mathbb{E}(Y_i \\mid X_{i,j} = x_{i,j}) = \\beta_0 + \\beta_1 x_{i,1} + \\ldots + \\beta_k x_{i,k};\n\\]\n\n\\[\n\\text{loss} = \\sum_{i = 1}^n (y_i - \\beta_0 - \\beta_1 x_{i,1} - \\ldots - \\beta_k x_{i,k})^2.\n\\]\n\nParametric Quantile Regression:\n\n\\[\nQ_i( \\tau \\mid X_{i,j} = x_{i,j}) = \\beta_0(\\tau) + \\beta_1(\\tau) x_{i,1} + \\ldots + \\beta_k(\\tau) x_{i,k}\n\\]\n(notice the \\(\\beta\\)s are now functions of \\(\\tau\\))\n\nError term: Fidelity\n\n\\[\n\\text{loss} = \\sum_{i} e_i[\\tau - I(e_i &lt; 0)] = \\sum_{i: e_i \\geq 0} \\tau|e_i|+\\sum_{i: e_i &lt; 0}(1-\\tau)|e_i|\n\\]\nWhere\n\\[I(e_i &lt; 0) = \\begin{cases} 1 & \\text{if } e_i &lt; 0 \\\\ 0 & \\text{otherwise} \\end{cases}\\]\n\n\n# Plotting the quantiles\nggplot(data, aes(x, y)) +\n    geom_point() +\n    geom_quantile(quantiles = c(0.25, 0.5, 0.75),\n                    formula = y ~ x)\n\n# fit the model\npqr_model &lt;- rq(y ~ x,\n                tau = c(0.25, 0.5, 0.75),\n                data = data)\n\n\n\n\nTo justify the relationship b/w the \\(\\tau\\)th quantile in response and regressors.\nUse test statistics to test the null hypothesis that the \\(\\tau\\)th quantile is not related to the regressors.\n\n\\(t\\)-value with \\(n - k - 1\\) degrees of freedom:\n\n\n\\[t_j = \\frac{\\hat{\\beta}_j(\\tau)}{\\text{SE}(\\hat{\\beta}_j(\\tau))}\\]\n\nNull hypothesis: \\(H_0 : \\beta_j(\\tau) = 0\\).\nCheck p-value: summary(pqr_model)[1] for \\(\\tau = 0.25\\), summary(pqr_model)[2] for \\(\\tau = 0.5\\), summary(pqr_model)[3] for \\(\\tau = 0.75\\).\n\n\n\n\n\nSimilar to OLS\n\\(\\beta_1(\\tau)\\) is the change in the \\(\\tau\\)th quantile of \\(Y\\) for a unit increase in \\(X_1\\).\ne.g. \\(\\beta_1(0.75) = 0.5\\) means that for a unit increase in \\(X_1\\), the 75th quantile of \\(Y\\) increases by 0.5.\n\n\n\n\npredict(pqr_model, newdata = data.frame(...))\n\n\n\n\n\nImplicates no distributional assumptions and no model function specification.\n\\(\\lambda\\) is the penalty parameter.\n\nChoosing how local the estimation is.\nsmall \\(\\lambda\\): better approx, but more variance (model not smooth)\nlarge \\(\\lambda\\): lose local info, favouring smoothness (global info)\n\n\nmedian_rqss &lt;- rqss(y ~ qss(x, lambda = 0.5),\n            tau = 0.5, # cannot do multiple quantiles\n            data = data)\n\nsummary(median_rqss)\n\npredict(median_rqss, newdata = data.frame(...))"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#missing-data",
    "href": "block_4/562_regression_2/562_regression_2.html#missing-data",
    "title": "Regression II",
    "section": "",
    "text": "The probability of missing data is the same for all observations\nMissingness is independent of data (Ideal case because there is no pattern)\nNo systematic differences between missing and non-missing data\n\n\n\n\n\nThe probability of missing data depends on observed data\nCan use imputation to fill in missing data:\n\nHot deck: Replace missing value with a value from the same dataset\nCold deck: Replace missing value with a value from a different dataset\n\n\n\n\n\n\nThe probability of missing data depends on unobservable quantities\nE.g. missing data on income for people who are unemployed\n\n\n\n\n\n\n\n\nRemove all observations with missing data\nIf data is MCAR, this is unbiased\n\nAlso increases standard errors since we’re using less data\n\nIf data is MAR or MNAR, this is biased (CAREFUL)\n\ne.g. if missing data is related to income, lower income will omit telling us their income, so removing them will bias our data to higher income.\n\n\n\n\n\n\nReplace missing data with the mean of the observed data\n\nCan only be used on continuous/ count data\n\nArtificially reduces standard errors (drawback)\n\nAlso reduces variance, which is not good\n\n\nlibrary(mice)\n\ndata &lt;- mice(data, seed = 1, method = \"mean\")\ncomplete(data)\n\n\n\n\nUse a regression model to predict missing data\nUse the predicted value as the imputed value\nWe will reinforce the relationship between the predictor and the variable with missing data\n\nThis is not good if the relationship is not strong\nWill change inference results\n\n\ndata &lt;- mice(data, seed = 1, method = \"norm.predict\")\ncomplete(data)\n\n\n\n\nIdea: Impute missing data multiple times to account for uncertainty\nUse mice package in R. Stands for Multivariate Imputation by Chained Equations\nSteps:\n\nCreate m copies of the dataset\nIn each copy, impute missing data (different values)\nCarry out analysis on each dataset\nCombine models to one pooled model\n\n\nimp_data &lt;- mice(data, seed = 1, m = 15, printFlag = FALSE)\n\ncomplete(data, 3) # Get the third imputed dataset\n\n# estimate OLS regression on each dataset\nmodels &lt;- with(imp_data, lm(y ~ x1 + x2))\n\n# get third model\nmodels$analyses[[3]]\n\n# combine models\npooled_model &lt;- pool(models)\n\nsummary(pooled_model) # remember to exp if using log model"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html",
    "title": "Supervised Learning II",
    "section": "",
    "text": "Infinite amount of numbers but finite amount of bits to represent them\nThese small errors will accumulate and cause problems\n\n\n\n\nlarge datasets with millions of params\nsmall errors can accumulate and cause problems\n\n\n\n\n\nBinary numbers are represented as a sum of powers of 2\ne.g. 104 in binary is 1101000 = \\(1(2^6) + 1(2^5) + 0(2^4) + 1(2^3) + 0(2^2) + 0(2^1) + 0(2^0) = 64 + 32 + 8 = 104\\)\nUnsigned Integers: \\(2^n - 1\\) is the largest number that can be represented with n bits\n\ne.g. 8 bits can represent 0 to 255\nnp.iinfo(np.uint8) gives the min and max values\n\nSigned Integers: \\(2^{n-1} - 1\\) is the largest positive number that can be represented with n bits\n\n\\(-2^{n-1}\\) is the smallest negative number that can be represented with n bits\ne.g. 8 bits can represent -128 to 127 (0 is included in the positive numbers)\n1 bit is used to represent the sign\nnp.iinfo(np.int8) gives the min and max values\n\n\n\n\n\n\n14.75 in binary is 1110.11\n\n\n\n\n2^3\n2^2\n2^1\n2^0\n2^-1\n2^-2\n\n\n\n\n1\n1\n1\n0\n1\n1\n\n\n8\n4\n2\n0\n0.5\n0.25\n\n\n\n$ 8 + 4 + 2 + 0 + 0.5 + 0.25 = 14.75 $\n\n\n\n\nWe typically have a fixed number of bits to represent the fractional part\ne.g. 8 bits total, 4 bits for the integer part and 4 bits for the fractional part\n\nmax value is 15.9375 (\\(2^3 + 2^2 + 2^1 + 2^0 + 2^{-1} + 2^{-2} + 2^{-3} + 2^{-4}\\))\n\noverflow if try a higher value\n\nmin value (bigger than 0) is 0.0625 (\\(2^{-4}\\))\n\nor precision of 0.0625 (any less =&gt; underflow)\n\n\n\n\n\n\n\nRather than having a fixed location for the binary point, we let it “float” around.\n\nlike how we write 0.1234 as 1.234 x 10^-1\n\nFormat: \\[(-1)^S \\times 1. M \\times 2^E\\]\n\nS is the sign bit\nM is the mantissa, always between 1 and 2 (1.0 is implied)\nE is the exponent\n\n\nFloat 64 (double precision) \nFloat 32 (single precision) \n\n\n\n\n\n\nThe spacing changes depending on the floating point number (because of the exponent)\n\n\n\nimport numpy as np\n\nnp.spacing(1e16) # 1.0\n\nnp.nextafter(1e16, 2e16) - 1e16 # 1.0\n\n\n\n\n\n\n1.0 + 2.0 + 3.0 == 6.0 True\n0.1 + 0.2 == 0.3 False\n\n0.1, 0.2, and 0.3 are not exactly representable in binary\n\n1e16 + 1 == 1e16 True\n\n1 is less than the spacing, so it is rounded back\n\n1e16 + 2.0 == 1e16 False\n\n2.0 is greater than the spacing, so it is rounded up\n\n1e16 + 1.0 + 1.0  == 1e16 True\n\n1.0 is less than the spacing, so it is rounded back, then 1.0 is added, which is less than the spacing, so it is rounded back again\n\n\n\n\n\n\n\nIn ML, we want to minimize a loss function\n\ntypically a sum of losses over the training set\n\nCan think of ML as a 3 step process:\n\nChoose model: controls space of possible functions that map X to y\nChoose loss function: measures how well the model fits the data\nChoose optimization algorithm: finds the best model\n\n\n\n\n\nOptimization: process to min/max a function\nObjective Function: function to be optimized\nDomain: set to search for optimal value\nMinimizer: value that minimizes the objective function\n\n\n\n\nCommon loss function is MSE (mean squared error):\n\\[L(w) = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2\\]\nUsing a simple linear regression model \\(y = w_0 + w_1x\\), we can rewrite the loss function as:\n\\[L(w) = \\frac{1}{n} \\sum_{i=1}^n ((w_0 + w_1x_i) - y_i)^2\\]\nSo optimization is finding the values of \\(w_0\\) and \\(w_1\\) that minimize the loss function, \\(L(w)\\).\nIn vector format:\n\\[\\text{MSE} = \\mathcal{L}(\\mathbf{w}) = \\frac{1}{n}\\sum^{n}_{i=1}(\\mathbf{x}_i \\mathbf{w} - y_i)^2\\]\nIn full-matrix format\n\\[\\text{MSE} = \\mathcal{L}(\\mathbf{w}) = \\frac{1}{n}(\\mathbf{X} \\mathbf{w} - \\mathbf{y})^T (\\mathbf{X} \\mathbf{w} - \\mathbf{y}) \\]\n\n\n\n\\[\n\\mathbf{y}=\n\\left[\n\\begin{array}{c} y_1 \\\\\n\\vdots \\\\\ny_i \\\\\n\\vdots\\\\\ny_n\n\\end{array}\n\\right]_{n \\times 1}, \\quad\n\\mathbf{X}=\n\\left[\n\\begin{array}{c} \\mathbf{x}_1 \\\\\n\\vdots \\\\\n\\mathbf{x}_i \\\\\n\\vdots\\\\\n\\mathbf{x}_n\n\\end{array}\n\\right]_{n \\times d}\n= \\left[\\begin{array}{cccc}\nx_{11} & x_{12} & \\cdots & x_{1 d} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{i 1} & x_{i 2} & \\cdots & x_{i d}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n 1} & x_{n 2} & \\cdots & x_{n d}\n\\end{array}\\right]_{n \\times d},\n\\quad\n\\mathbf{w}=\n\\left[\n\\begin{array}{c} w_1 \\\\\n\\vdots\\\\\nw_d\n\\end{array}\n\\right]_{d \\times 1}\n\\]\n\n\\(n\\): number of examples\n\\(d\\): number of input features/dimensions\n\nThe goal is to find the weights \\(\\mathbf{w}\\) that minimize the loss function.\nFormulas:\n\\[\\mathbf{y} = \\mathbf{X} \\mathbf{w}\\]\n\\[\\hat{\\mathbf{y}_i} = \\mathbf{w}^T \\mathbf{x}_i\\]\n\n\n\n\n\nOne of the most important optimization algorithms in ML\nIterative optimization algorithm\nSteps:\n\nstart with some arbitrary \\(\\mathbf{w}\\)\ncalculate the gradient using all training examples\nuse the gradient to adjust \\(\\mathbf{w}\\)\nrepeat for \\(I\\) iterations or until the step-size is sufficiently small\n\nCost: \\(O(ndt)\\) for t iterations, better than brute force search \\(O(nd^2 + d^3)\\)\n\n\\[w_{t+1} = w_t - \\alpha \\nabla= L(w_t)\\]\n\n\\(w_t\\): current value of the weights\n\\(\\alpha\\): learning rate\n\\(\\nabla L(w_t)\\): gradient of the loss function at \\(w_t\\)\n\n\n\n\nLoss function: \\(L(w) = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2\\)\nGradient: \\(\\nabla L(w) = \\frac{d}{dw} L(w) = \\frac{2}{n} \\sum_{i=1}^n x_{i1} (x_{i1} w_1 - y_i)\\)\n\nOr in Matrix form: \\(\\nabla L(w) = \\frac{2}{n} \\mathbf{X}^T (\\mathbf{X} \\mathbf{w} - \\mathbf{y})\\)\n\n\n\n\n\n\nNeed to scale for the contour plot to be more “round”\n\nbetter for gradient descent\n\n\n\n\nIn real life, contour plots are not so nice\n\n\n\n\n\n\nInitialization: Start with an initial set of parameters, often randomly chosen.\nForward pass: Generate predictions using the current values of the parameters. (E.g., \\(\\hat{y_i} = x_{1}w_1 + Bias\\) in the toy example above)\nLoss calculation: Evaluate the loss, which quantifies the discrepancy between the model’s predictions and the actual target values.\nGradient calculation: Compute the gradient of the loss function with respect to each parameter either on a batch or the full dataset. This gradient indicates the direction in which the loss is increasing and its magnitude.\nParameter Update: Adjust the parameters in the opposite direction of the calculated gradient, scaled by the learning rate. This step aims to reduce the loss by moving the parameters toward values that minimize it.\n\n\n\n\n\nUse minimize function from scipy.optimize\n\nfrom scipy.optimize import minimize\n\ndef mse(w, X, y):\n    \"\"\"Mean squared error.\"\"\"\n    return np.mean((X @ w - y) ** 2)\n\ndef mse_grad(w, X, y):\n    \"\"\"Gradient of mean squared error.\"\"\"\n    n = len(y)\n    return (2/n) * X.T @ (X @ w - y)\n\nout = minimize(mse, w, jac=mse_grad, args=(X_scaled_ones, toy_y), method=\"BFGS\")\n# jac: function to compute the gradient (optional)\n# - will use finite difference approximation if not provided\n\nOther methods:\n\nBFGS: Broyden–Fletcher–Goldfarb–Shanno algorithm\nCG: Conjugate gradient algorithm\nL-BFGS-B: Limited-memory BFGS with bounds on the variables\nSLSQP: Sequential Least SQuares Programming\nTNC: Truncated Newton algorithm\n\n\n\n\n\n\n\nInstead of updating our parameters based on a gradient calculated using all training data, we simply use one of our data points (the \\(i\\)-th one)\n\nGradient Descent\nLoss function:\n\\[\\text{MSE} = \\mathcal{L}(\\mathbf{w}) = \\frac{1}{n}\\sum^{n}_{i=1} (\\mathbf{x}_i \\mathbf{w} - y_i)^2\\]\nUpdate procedure:\n\\[\\mathbf{w}^{j+1} = \\mathbf{w}^{j} - \\alpha \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}^{j})\\]\nStochastic Gradient Descent\nLoss function:\n\\[\\text{MSE}_i = \\mathcal{L}_i(\\mathbf{w}) = (\\mathbf{x}_i \\mathbf{w} - y_i)^2\\]\nUpdate procedure: \\[\\mathbf{w}^{j+1} = \\mathbf{w}^{j} - \\alpha \\nabla_\\mathbf{w} \\mathcal{L}_i(\\mathbf{w}^{j})\\]\n\n\n\n\n\nGradient Descent\nStochastic Gradient Descent\n\n\n\n\nUse all data points\nUse one data point\n\n\nSlow\nFast\n\n\nAccurate\nLess Accurate\n\n\n\n\nMini-batch Gradient Descent is a (in-between) compromise between the two\nInstead of using a single data point, we use a small batch of data points d\n\n\n\n\nShuffle and divide all data into \\(k\\) batches, every example is used once\n\nDefault in PyTorch\nAn example will only show up in one batch\n\nChoose some examples for each batch without replacement\n\nAn example may show up in multiple batches\nThe same example cannot show up in the same batch more than once\n\nChoose some examples for each batch with replacement\n\nAn example may show up in multiple batches\nThe same example may show up in the same batch more than once\n\n\n\n\n\n\nAssume we have a dataset of \\(n\\) observations (also known as rows, samples, examples, data points, or points)\n\nIteration: each time you update model weights\nBatch: a subset of data used in an iteration\nEpoch: One full pass through the dataset to look at all \\(n\\) observations\n\nIn other words,\n\nIn GD, each iteration involves computing the gradient over all examples, so\n\n\\[1 \\: \\text{iteration} = 1 \\: \\text{epoch}\\]\n\nIn SGD, each iteration involves one data point, so\n\n\\[n \\text{ iterations} = 1 \\: \\text{epoch}\\]\n\nIn MGD, each iteration involves a batch of data, so\n\n\\[\n\\begin{align}\n\\frac{n}{\\text{batch size}} \\text{iterations} &= 1 \\text{ epoch}\\\\\n\\end{align}\n\\]\n*Note: nobody really says “minibatch SGD”, we just say SGD: in SGD you can specify a batch size of anything between 1 and \\(n\\)\n\n\n\n\n\nModels that does a good job of approximating complex non-linear functions\nIt is a sequence of layers, each of which is a linear transformation followed by a non-linear transformation\n\n\n\n\nNode (or neuron): a single unit in a layer\nInput layer: the features of the data\nHidden layer: the layer(s) between the input and output layers\nOutput layer: the prediction(s) of the model\nWeights: the parameters of the model\nActivation function: the non-linear transformation (e.g. ReLU, Sigmoid, Tanh, etc.)\n\n\nX : (n x d), W : (h x d), b : (n x h), where h is the number of hidden nodes b is actually 1 x hs, but we can think of it as n x hs because it is broadcasted\n\\[\\mathbf{H}^{(1)} = \\phi^{(1)} (\\mathbf{X}\\mathbf{W}^{(1)\\text{T}} + \\mathbf{b}^{(1)})\\]\n\\[\\mathbf{H}^{(2)} = \\phi^{(2)} (\\mathbf{H}^{(1)}\\mathbf{W}^{(2)\\text{T}} + \\mathbf{b}^{(2)})\\]\n\\[\\mathbf{Y} = (\\mathbf{H}^{(2)}\\mathbf{W}^{(3)\\text{T}} + \\mathbf{b}^{(3)})\\]\n\nIn a layer, \\[\\text{ num of weights} = \\text{num of nodes in previous layer} \\times \\text{num of nodes in current layer}\\]\n\n\\[\\text{num of biases} = \\text{num of nodes in current layer}\\]\n\\[\\text{num of parameters} = \\text{num of weights} + \\text{num of biases}\\]\n\n\n\n\n\n\n\nBackpropagation: a method to calculate the gradient of the loss function with respect to the weights\nChain rule: a method to calculate the gradient of a function composed of multiple functions\nIt is pretty complicated, but PyTorch does it for us\n\n\n\n\n\n\nNeural networks with &gt; 1 hidden layer\n\nNN with 1 hidden layer: shallow neural network\n\n\n\n\n\n\n\nPyTorch is a popular open-source machine learning library by Facebook based on Torch\nIt is a Python package that provides two high-level features:\n\nTensor computation (like NumPy) with strong GPU acceleration\nGradient computation through automatic differentiation\n\n\n\n\n\nSimilar to ndarray in NumPy\n\nimport torch\n\n# Create a tensor\nx = torch.tensor([1, 2, 3, 4, 5]) # int\nx = torch.tensor([1, 2, 3, 4, 5.]) # float\nx = torch.tensor([[1, 2], [3, 4], [5, 6]])\n\ny = torch.zeros(3, 2)\ny = torch.ones(3, 2)\ny = torch.rand(3, 2)\n\n# Check the shape, dimensions, and data type\nx.shape\nx.ndim\nx.dtype\n\n# Operations\na = torch.rand(1, 3)\nb = torch.rand(3, 1)\n\na + b # broadcasting\na * b # element-wise multiplication\na @ b # matrix multiplication\na.mean()\na.sum()\n\n# Indexing\na[0,:] # first row\na[0] # first row\na[:,0] # first column\n\n# Convert to NumPy\nx.numpy()\n\n\n\n# Check if GPU is available\ntorch.backends.mps.is_available() # mac M chips\ntorch.cuda.is_available() # Nvidia GPU\n\n# To activate GPU\ndevice = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nx.to('cpu') # move tensor to cpu\n\n\n\nuse backward() to compute the gradient, backpropagation\n\nX = torch.tensor([1.0, 2.0, 3.0], requires_grad=False)\nw = torch.tensor([1.0], requires_grad=True)  # Random initial weight\ny = torch.tensor([2.0, 4.0, 6.0], requires_grad=False)  # Target values\nmse = ((X * w - y)**2).mean()\nmse.backward()\nw.grad\n\n\n\n\n\nEvery NN model has to inherit from torch.nn.Module\n\nfrom torch import nn\n\nclass linearRegression(nn.Module):  # inherit from nn.Module\n\n    def __init__(self, input_size, output_size):\n        super().__init__()  # call the constructor of the parent class\n\n        self.linear = nn.Linear(input_size, output_size,)  # wX + b\n\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\n# Create a model\nmodel = linearRegression(1, 1) # input size, output size\n\n# View model\nsummary(model)\n\n## Train the model\nLEARNING_RATE = 0.02\ncriterion = nn.MSELoss()  # loss function\noptimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)  # optimization algorithm is SGD\n\n# DataLoader for mini-batch\nfrom torch.utils.data import DataLoader, TensorDataset\n\nBATCH_SIZE = 50\ndataset = TensorDataset(X_t, y_t)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n# Training\ndef trainer(model, criterion, optimizer, dataloader, epochs=5, verbose=True):\n    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n\n    for epoch in range(epochs):\n        losses = 0\n\n        for X, y in dataloader:\n\n            optimizer.zero_grad()       # Clear gradients w.r.t. parameters\n            y_hat = model(X).flatten()  # Forward pass to get output\n            loss = criterion(y_hat, y)  # Calculate loss\n            loss.backward()             # Getting gradients w.r.t. parameters\n            optimizer.step()            # Update parameters\n            losses += loss.item()       # Add loss for this batch to running total\n\n        if verbose: print(f\"epoch: {epoch + 1}, loss: {losses / len(dataloader):.4f}\")\n\ntrainer(model, criterion, optimizer, dataloader, epochs=30, verbose=True)\n\n\n\n\nuse torch.nn.Sequential to create a model\n\nclass nonlinRegression(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n\n        self.main = torch.nn.Sequential(\n            nn.Linear(input_size, hidden_size),  # input -&gt; hidden layer\n            nn.Sigmoid(),                        # sigmoid activation function in hidden layer\n            nn.Linear(hidden_size, output_size)  # hidden -&gt; output layer\n        )\n\n    def forward(self, x):\n        x = self.main(x)\n        return x\n\n\n\n\n\n\nTask\nCriterion (Loss)\nOptimizer\n\n\n\n\nRegression\nMSELoss\nSGD\n\n\nBinary Classification\nBCELoss\nAdam\n\n\nMulti-class Classification\nCrossEntropyLoss\nAdam\n\n\n\n\nInput of CrossEntropyLoss doesn’t need to be normalized (i.e. no need to sum to 1/ no need to use nn.Softmax)\n\n# criterions\nfrom torch import nn\nreg_criterion = torch.nn.MSELoss()\nbc_criterion = torch.nn.BCEWithLogitsLoss()\nmse_criterion = torch.nn.CrossEntropyLoss()\n\n# optimizers\nfrom torch import optim\nreg_optim = torch.optim.SGD(model.parameters(), lr=0.2)\nclass_optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n\n\n\n\n\n\nIt is to calculate the gradient of the loss function with respect to the weights\nIt is a special case of the chain rule of calculus\nProcess:\n\nDo “forward pass” to calculate the output of the network (prediction and loss)\n\n\n\nDo “backward pass” to calculate the gradients of the loss function with respect to the weights. Below is an example of reverse-mode autmatic differentiation (backpropagation):\n\n\n\n\n\n\n\ntorch.autograd is PyTorch’s automatic differentiation engine that powers neural network training\n\nimport torch\n\n# Create model\nclass network(torch.nn.Module):\n    def __init__(self):\n        super(network, self).__init__()\n        self.layer1 = torch.nn.Linear(1, 6)\n        self.dropout = torch.nn.Dropout(0.2) # dropout layer\n        ...\n\n    def forward(self, x):\n        x = self.layer1(x)\n        ...\n        return x\n\nmodel = network()\ncriterion = torch.nn.MSELoss()\n\n# Forward pass\nloss = criterion(model(x), y)\n# Backward pass\nloss.backward()\n\n# Access gradients\nprint(model.layer1.weight.grad) # or model.layer1.weight.bias.grad\n\n# Update weights\nmodel.state_dict() # get the current weights\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\noptimizer.step() # update weights\n\n\n\n\nBackpropagation can suffer from two problems because of multiple chain rule applications:\n\nVanishing gradients: the gradients of the loss function with respect to the weights become very small\n\n0 gradients because of underflow\n\nExploding gradients: the gradients of the loss function with respect to the weights become very large\n\nPossible solutions:\n\nUse ReLU activation function: but it can also suffer from the dying ReLU problem (gradients are 0)\nWeight initialization: initialize the weights with small values\nBatch normalization: normalize the input layer by adjusting and scaling the activations\nSkip connections: add connections that skip one or more layers\nGradient clipping: clip the gradients during backpropagation\n\n\n\n\n\n\n\n\nAdd validation loss to the training loop\nEarly stopping: if we see the validation loss is increasing, we stop training\n\nDefine a patience parameter: if the validation loss increases for patience epochs, we stop training\n\nRegularization: add a penalty term to the loss function to prevent overfitting\n\nSee 573 notes for more details\nweight_decay parameter in the optimizer\n\nDropout: randomly set some neurons to 0 during training\n\nIt prevents overfitting by reducing the complexity of the model\ntorch.nn.Dropout(0.2)\n\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\ndef trainer(model, criterion, optimizer, trainloader, validloader, epochs=5, patience=5):\n    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n\n    train_loss = []\n    valid_loss = []\n\n    for epoch in range(epochs):  # for each epoch\n        train_batch_loss = 0\n        valid_batch_loss = 0\n\n        # Training\n        for X, y in trainloader:\n\n            optimizer.zero_grad()       # Zero all the gradients w.r.t. parameters\n\n            y_hat = model(X).flatten()  # Forward pass to get output\n            loss = criterion(y_hat, y)  # Calculate loss based on output\n            loss.backward()             # Calculate gradients w.r.t. parameters\n            optimizer.step()            # Update parameters\n\n            train_batch_loss += loss.item()  # Add loss for this batch to running total\n\n        train_loss.append(train_batch_loss / len(trainloader))\n\n        # Validation\n        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n\n            for X_valid, y_valid in validloader:\n\n                y_hat = model(X_valid).flatten()  # Forward pass to get output\n                loss = criterion(y_hat, y_valid)  # Calculate loss based on output\n\n                valid_batch_loss += loss.item()\n\n        valid_loss.append(valid_batch_loss / len(validloader))\n\n        # Early stopping\n        if epoch &gt; 0 and valid_loss[-1] &gt; valid_loss[-2]:\n            consec_increases += 1\n        else:\n            consec_increases = 0\n        if consec_increases == patience:\n            print(f\"Stopped early at epoch {epoch + 1} - val loss increased for {consec_increases} consecutive epochs!\")\n            break\n\n    return train_loss, valid_loss\n\nUsing the trainer function:\n\nimport torch\nimport torch.nn\nimport torch.optim\n\ntorch.manual_seed(1)\n\nmodel = network(1, 6, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05) # weight_decay=0.01 for L2 regularization\ntrain_loss, valid_loss = trainer(model, criterion, optimizer, trainloader, validloader, epochs=201, patience=3)\n\nplot_loss(train_loss, valid_loss)\n\n\n\n\nAny continuous function can be approximated arbitrarily well by a neural network with a single hidden layer\n\nIn other words, NN are universal function approximators\n\n\n\n\n\n\n\n\nDrastically reduces the number of params (compared to NN):\n\nhave activations depend on small number of inputs\nsame parameters (convolutional filter) are used for different parts of the image\n\nCan capture spatial information (preserves the structure of the image)\n\n\n\n\nIdea: use a small filter/kernel to extract features from the image\n\nFilter: a small matrix of weights (normally odd dimensioned -&gt; for symmetry)\n\n\n\n\nNotice that the filter results in a smaller output image\n\nThis is because we are not padding the image\nWe can add padding to the image to keep the same size\n\nPadding: add zeros around the image\n\nCan also add stride to move the filter more than 1 pixel at a time\n\n\n\n\n\n\nimg src\n\n\n\n\n\nconv_1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=(3,3))\n\nArguments:\n\nin_channels: number of input channels (gray scale image has 1 channel, RGB has 3)\nout_channels: number of output channels (similar to hidden nodes in NN)\nkernel_size: size of the filter\nstride: how many pixels to move the filter each time\npadding: how many pixels to add around the image\n\n\n\n\nSize of input image (e.g. 256x256) doesn’t matter, what matters is: in_channels, out_channels, kernel_size\n\n\\[\\text{total params} = (\\text{out channels} \\times \\text{in channels} \\times \\text{kernel size}^2) + \\text{out channels}\\]\n\\[\\text{output size} = \\frac{\\text{input size} - \\text{kernel size} + 2 \\times \\text{padding}}{\\text{stride}} + 1\\]\n\n\n\nImages: [batch_size, channels, height, width]\nKernel: [out_channels, in_channels, kernel_height, kernel_width]\n\nNote: before passing the image to the convolutional layer, we need to reshape it to the correct dimensions. Also if you want to plt.imshow() the image, you need to reshape it back to [height, width, channels].\n\n\n\n\n\nfeature learning -&gt; classification\nUse torch.nn.Flatten() to flatten the image\nAt the end need to either do regression or classification\n\n\n\n\n\nIdea: reduce the size of the image\n\nless params\nless overfitting\n\nCommon types:\n\nMax pooling: take the max value in each region\n\nWorks well since it takes the sharpest features\n\nAverage pooling: take the average value in each region\n\n\n\n\n\nclass CNN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.main = torch.nn.Sequential(\n\n            torch.nn.Conv2d(in_channels=1,\n                out_channels=3,\n                kernel_size=(3, 3),\n                padding=1),\n            torch.nn.ReLU(), # activation function\n            torch.nn.MaxPool2d((2, 2)),\n\n            torch.nn.Conv2d(in_channels=3,\n                out_channels=2,\n                kernel_size=(3, 3),\n                padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d((2, 2)),\n\n            torch.nn.Flatten(),\n            torch.nn.Linear(1250, 1)\n        )\n\n    def forward(self, x):\n        out = self.main(x)\n        return out\n# Trainer code\ndef trainer(\n    model, criterion, optimizer, trainloader, validloader, epochs=5, verbose=True\n):\n    train_loss, train_accuracy, valid_loss, valid_accuracy = [], [], [], []\n    for epoch in range(epochs):  # for each epoch\n        train_batch_loss = 0\n        train_batch_acc = 0\n        valid_batch_loss = 0\n        valid_batch_acc = 0\n\n        # Training\n        for X, y in trainloader:\n            if device.type in ['cuda', 'mps']:\n                    X, y = X.to(device), y.to(device)\n            optimizer.zero_grad()  # Zero all the gradients w.r.t. parameters\n            y_hat = model(X)  # Forward pass to get output\n            idx = torch.softmax(y_hat, dim=1).argmax(dim=1) # Multiclass classification\n            loss = criterion(y_hat, y)\n            loss.backward()  # Calculate gradients w.r.t. parameters\n            optimizer.step()  # Update parameters\n            train_batch_loss += loss.item()  # Add loss for this batch to running total\n            train_batch_acc += (\n                    (idx.squeeze() == y).type(\n                        torch.float32).mean().item()\n                )\n        train_loss.append(train_batch_loss / len(trainloader))\n        train_accuracy.append(train_batch_acc / len(trainloader))\n\n        # Validation\n        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n            for X, y in validloader:\n                if device.type in ['cuda', 'mps']:\n                    X, y = X.to(device), y.to(device)\n                y_hat = model(X)\n                idx = torch.softmax(y_hat, dim=1).argmax(dim=1)\n                loss = criterion(y_hat, y)\n                valid_batch_loss += loss.item()\n                valid_batch_acc += (\n                    (idx.squeeze() == y).type(\n                        torch.float32).mean().item()\n                )\n        valid_loss.append(valid_batch_loss / len(validloader))\n        valid_accuracy.append(valid_batch_acc / len(validloader))  # accuracy\n\n        # Print progress\n        if verbose:\n            print(\n                f\"Epoch {epoch + 1}:\",\n                f\"Train Loss: {train_loss[-1]:.3f}.\",\n                f\"Valid Loss: {valid_loss[-1]:.3f}.\",\n                f\"Train Accuracy: {train_accuracy[-1]:.2f}.\",\n                f\"Valid Accuracy: {valid_accuracy[-1]:.2f}.\",\n            )\n\n    results = {\n        \"train_loss\": train_loss,\n        \"train_accuracy\": train_accuracy,\n        \"valid_loss\": valid_loss,\n        \"valid_accuracy\": valid_accuracy,\n    }\n    return results\n\n\n\n\nTo get a summary of the model\n\nNo need to manually calculate the output size of each layer\n\n\nfrom torchsummary import summary\n\nmodel = CNN()\nsummary(model, (1, 256, 256))\n\n\n\n\n\n\n\nNormally there are 2 steps:\n\ncreate a dataset object: the raw data\ncreate a dataloader object: batches the data, shuffles, etc.\n\nUse torchvision to load the data\n\ntorchvision.datasets.ImageFolder: loads images from folders\nAssumes structure: root/class_1/xxx.png, root/class_2/xxx.png, …\n\n\nimport torch\nfrom torchvision import datasets, transforms\n\nIMAGE_SIZE = (256, 256)\nBATCH_SIZE = 32\n\n# create transform object\ndata_transforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.ToTensor()\n])\n\n# create dataset object\ntrain_dataset = datasets.ImageFolder(root='path/to/data', transform=data_transforms)\n\n# check out the data\ntrain_dataset.classes # list of classes\ntrain_dataset.targets # list of labels\ntrain_dataset.samples # list of (path, label) tuples\n\n# create dataloader object\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,          # our raw data\n    batch_size=BATCH_SIZE,  # the size of batches we want the dataloader to return\n    shuffle=True,           # shuffle our data before batching\n    drop_last=False         # don't drop the last batch even if it's smaller than batch_size\n)\n\n# get a batch of data\nimages, labels = next(iter(train_loader))\n\n\n\n\nPyTorch documentation\nConvention: .pt or .pth file extension\n\nPATH = \"models/my_cnn.pt\"\n\n# load model\nmodel = bitmoji_CNN() # must have defined the model class\nmodel.load_state_dict(torch.load(PATH))\nmodel.eval() # set model to evaluation mode (not training mode)\n\n# save model\ntorch.save(model.state_dict(), PATH)\n\n\n\n\nTo make CNN more robust to different images + increase the size of the dataset\nCommon augmentations:\n\nCrop\nRotate\nFlip\nColor jitter\n\n\ndata_transforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.RandomVerticalFlip(p=0.5), # p=0.5 means 50% chance of applying this augmentation\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor()\n])\n\n\n\n\n\nNN has a lot of hyperparameters\n\nGrid search will take a long time\nNeed a smarter approach: Optimization Algorithms\n\nExamples: Ax (we will use this), Raytune, Neptune, skorch.\n\n\n\n\n\nIdea: use a pre-trained model and fine-tune it to our specific task\nInstall from torchvision.models\n\nAll models have been trained on ImageNet dataset (224x224 images)\n\nSee here for code\n\n\n\ndensenet = models.densenet121(weights='DenseNet121_Weights.DEFAULT')\n\nfor param in densenet.parameters():  # Freeze parameters so we don't update them\n    param.requires_grad = False\n# can fine-tune to freeze only some layers\n\nlist(densenet.named_children())[-1] # check the last layer\n\n# update the last layer\nnew_layers = nn.Sequential(\n    nn.Linear(1024, 500),\n    nn.ReLU(),\n    nn.Linear(500, 1)\n)\ndensenet.classifier = new_layers\nThen train the model as usual.\ndensenet.to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(densenet.parameters(), lr=2e-3)\nresults = trainer(densenet, criterion, optimizer, train_loader, valid_loader, device, epochs=10)\n\n\n\n\nIdea:\n\nTake output from pre-trained model\nFeed output to a new model\n\n\ndef get_features(model, train_loader, valid_loader):\n    \"\"\"\n    Extract features from both training and validation datasets using the provided model.\n\n    This function passes data through a given neural network model to extract features. It's designed\n    to work with datasets loaded using PyTorch's DataLoader. The function operates under the assumption\n    that gradients are not required, optimizing memory and computation for inference tasks.\n    \"\"\"\n\n    # Disable gradient computation for efficiency during inference\n    with torch.no_grad():\n        # Initialize empty tensors for training features and labels\n        Z_train = torch.empty((0, 1024))  # Assuming each feature vector has 1024 elements\n        y_train = torch.empty((0))\n\n        # Initialize empty tensors for validation features and labels\n        Z_valid = torch.empty((0, 1024))\n        y_valid = torch.empty((0))\n\n        # Process training data\n        for X, y in train_loader:\n            # Extract features and concatenate them to the corresponding tensors\n            Z_train = torch.cat((Z_train, model(X)), dim=0)\n            y_train = torch.cat((y_train, y))\n\n        # Process validation data\n        for X, y in valid_loader:\n            # Extract features and concatenate them to the corresponding tensors\n            Z_valid = torch.cat((Z_valid, model(X)), dim=0)\n            y_valid = torch.cat((y_valid, y))\n\n    # Return the feature and label tensors\n    return Z_train, y_train, Z_valid, y_valid\nNow we can use the extracted features to train a new model.\n# Extract features from the pre-trained model\ndensenet = models.densenet121(weights='DenseNet121_Weights.DEFAULT')\ndensenet.classifier = nn.Identity()  # remove that last \"classification\" layer\nZ_train, y_train, Z_valid, y_valid = get_features(densenet, train_loader, valid_loader)\n\n# Train a new model using the extracted features\n# Let's scale our data\nscaler = StandardScaler()\nZ_train = scaler.fit_transform(Z_train)\nZ_valid = scaler.transform(Z_valid)\n\n# Fit a model\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(Z_train, y_train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative Models\nDiscriminative Models\n\n\n\n\nDirectly model the joint probability distribution of the input and output\nModel the conditional probability of the output given the input\n\n\nDirectly model \\(P(y\\|x)\\)\nEstimate \\(P(x\\|y)\\) to then deduce \\(P(y\\|x)\\)\n\n\nBuild model for each class\nMake boundary between classes\n\n\n“Generate or draw a cat”\n“Distinquish between cats and dogs”\n\n\nExamples: Naibe bayes, ChatGPT\nExamples: Logistic Regression, SVM, Tree based models, CNN\n\n\n\n\n\n\n\n\nDesigned to reconstruct the input\nEncoder and a decoder\nWhy do we need autoencoders?\n\nDimensionality reduction\nDenoising\n\n\n\n\n\nMaybe the z axis is unimportant in the input space for classification\n\nfrom torch import nn\n\nclass autoencoder(torch.nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, 2),\n            nn.Sigmoid()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(2, input_size),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n# Set up the training\nBATCH_SIZE = 100\ntorch.manual_seed(1)\nX_tensor = torch.tensor(X, dtype=torch.float32)\ndataloader = DataLoader(X_tensor,\n                        batch_size=BATCH_SIZE)\nmodel = autoencoder(3, 2)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters())\n\n# Train the model\nEPOCHS = 5\n\nfor epoch in range(EPOCHS):\n    for batch in dataloader:\n        optimizer.zero_grad()           # Clear gradients w.r.t. parameters\n        y_hat = model(batch)            # Forward pass to get output\n        loss = criterion(y_hat, batch)  # Calculate loss\n        loss.backward()                 # Getting gradients w.r.t. parameters\n        optimizer.step()                # Update parameters\n\n# Use encoder\nmodel.eval()\nX_encoded = model.encoder(X_tensor)\n\n\n\n\nRemove noise from the input\nUse Transposed Convolution Layers to upsample the input\n\nNormal convolution: downsample (output is smaller than input)\nTransposed convolution: upsample (output is larger than input)\n\n\ndef conv_block(input_channels, output_channels):\n    return nn.Sequential(\n        nn.Conv2d(input_channels, output_channels, 3, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(2)  # reduce x-y dims by two; window and stride of 2\n    )\n\ndef deconv_block(input_channels, output_channels, kernel_size):\n    return nn.Sequential(\n        nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride=2),\n        nn.ReLU()\n    )\n\nclass autoencoder(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            conv_block(1, 32),\n            conv_block(32, 16),\n            conv_block(16, 8)\n        )\n        self.decoder = nn.Sequential(\n            deconv_block(8, 8, 3),\n            deconv_block(8, 16, 2),\n            deconv_block(16, 32, 2),\n            nn.Conv2d(32, 1, 3, padding=1)  # final conv layer to decrease channel back to 1\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        x = torch.sigmoid(x)  # get pixels between 0 and 1\n        return x\n# Set up the training\nEPOCHS = 20\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters())\nimg_list = []\n\nfor epoch in range(EPOCHS):\n    losses = 0\n    for batch, _ in trainloader:\n        noisy_batch = batch + noise * torch.randn(*batch.shape)\n        noisy_batch = torch.clip(noisy_batch, 0.0, 1.0)\n        optimizer.zero_grad()\n        y_hat = model(noisy_batch)\n        loss = criterion(y_hat, batch)\n        loss.backward()\n        optimizer.step()\n        losses += loss.item()\n    print(f\"epoch: {epoch + 1}, loss: {losses / len(trainloader):.4f}\")\n    # Save example results each epoch so we can see what's going on\n    with torch.no_grad():\n        noisy_8 = noisy_batch[:1, :1, :, :]\n        model_8 = model(input_8)\n        real_8 = batch[:1, :1, :, :]\n    img_list.append(utils.make_grid([noisy_8[0], model_8[0], real_8[0]], padding=1))```\n\n\n\n\n\n\nModel used to generate new data (indistinguishable from real data)\nNo need for labels (unsupervised learning)\nSee here\n\n\n\nTwo networks:\n\nGenerator: creates new data\nDiscriminator: tries to distinguish between real and fake data\n\nBoth are battling each other:\n\nGenerator tries to create data that the discriminator can’t distinguish from real data\nDiscriminator tries to distinguish between real and fake data\n\n\n\n\n\nTrain the discriminator (simple binary classification)\n\nTrain the discriminator on real data\nTrain the discriminator on fake data (generated by the generator)\n\nTrain the generator\n\nGenerate fake images with the generator and label them as real\nPass to discriminator and ask it to classify them (real or fake)\nPass judgement to a loss function (see how far it is from the ideal output)\n\nideal output: all fake images are classified as real\n\nDo backpropagation and update the generator\n\nRepeat\n\n\n\n\n\nCreating the data loader\nDATA_DIR = \"../input/face-recognition-dataset/Extracted Faces\"\n\nBATCH_SIZE = 64\nIMAGE_SIZE = (128, 128)\n\ndata_transforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE), # uses CPU (bottleneck)\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ndataset = datasets.ImageFolder(root=DATA_DIR, transform=data_transforms)\ndata_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\nCreating the generator\n\nclass Generator(nn.Module):\n\n def __init__(self, LATENT_SIZE):\n     super(Generator, self).__init__()\n\n     self.main = nn.Sequential(\n         nn.ConvTranspose2d(LATENT_SIZE, 1024, kernel_size=4, stride=1, padding=0, bias=False),\n         nn.BatchNorm2d(1024),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(512),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(128),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(3),\n\n         nn.Tanh()\n     )\n\n def forward(self, input):\n     return self.main(input)\nCreating the discriminator\n\nclass Discriminator(nn.Module):\n\n def __init__(self):\n     super(Discriminator, self).__init__()\n\n     self.main = nn.Sequential(\n         nn.Conv2d(3, 128, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(128),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.Conv2d(128, 512, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(512),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n         nn.Flatten(),\n         nn.Sigmoid()\n     )\n\n def forward(self, input):\n     return self.main(input)\nInstantiating the models\ndevice = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n\nLATENT_SIZE = 100\ngenerator = Generator(LATENT_SIZE).to(device)\ndiscriminator = Discriminator().to(device)\n\ncriterion = nn.BCELoss()\n\noptimizerG = optim.Adam(generator.parameters(), lr=0.001, betas=(0.5, 0.999))\noptimizerD = optim.Adam(discriminator.parameters(), lr=0.001, betas=(0.5, 0.999))\n\n def weights_init(m):\n     if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n         nn.init.normal_(m.weight.data, 0.0, 0.02)\n     elif isinstance(m, nn.BatchNorm2d):\n         nn.init.normal_(m.weight.data, 1.0, 0.02)\n         nn.init.constant_(m.bias.data, 0)\n\n\n generator.apply(weights_init)\n discriminator.apply(weights_init);\nTraining the GAN\n img_list = []\nfixed_noise = torch.randn(BATCH_SIZE, LATENT_SIZE, 1, 1).to(device)\n\n NUM_EPOCHS = 50\nfrom statistics import mean\nprint('Training started:\\n')\n\n D_real_epoch, D_fake_epoch, loss_dis_epoch, loss_gen_epoch = [], [], [], []\n\n for epoch in range(NUM_EPOCHS):\n     D_real_iter, D_fake_iter, loss_dis_iter, loss_gen_iter = [], [], [], []\n\n     for real_batch, _ in data_loader:\n\n         # STEP 1: train discriminator\n         # ==================================\n         optimizerD.zero_grad()\n\n         real_batch = real_batch.to(device)\n         real_labels = torch.ones((real_batch.shape[0],), dtype=torch.float).to(device)\n\n         output = discriminator(real_batch).view(-1)\n         loss_real = criterion(output, real_labels)\n\n         # Iteration book-keeping\n         D_real_iter.append(output.mean().item())\n\n         # Train with fake data\n         noise = torch.randn(real_batch.shape[0], LATENT_SIZE, 1, 1).to(device)\n\n         fake_batch = generator(noise)\n         fake_labels = torch.zeros_like(real_labels)\n\n         output = discriminator(fake_batch.detach()).view(-1)\n         loss_fake = criterion(output, fake_labels)\n\n         # Update discriminator weights\n         loss_dis = loss_real + loss_fake\n         loss_dis.backward()\n         optimizerD.step()\n\n         # Iteration book-keeping\n         loss_dis_iter.append(loss_dis.mean().item())\n         D_fake_iter.append(output.mean().item())\n\n         # STEP 2: train generator\n         # ==================================\n         optimizerG.zero_grad()\n\n         # Calculate the output with the updated weights of the discriminator\n         output = discriminator(fake_batch).view(-1)\n         loss_gen = criterion(output, real_labels)\n         loss_gen.backward()\n\n         # Book-keeping\n         loss_gen_iter.append(loss_gen.mean().item())\n\n         # Update generator weights and store loss\n         optimizerG.step()\n\n     print(f\"Epoch ({epoch + 1}/{NUM_EPOCHS})\\t\",\n         f\"Loss_G: {mean(loss_gen_iter):.4f}\",\n         f\"Loss_D: {mean(loss_dis_iter):.4f}\\t\",\n         f\"D_real: {mean(D_real_iter):.4f}\",\n         f\"D_fake: {mean(D_fake_iter):.4f}\")\n\n     # Epoch book-keeping\n     loss_gen_epoch.append(mean(loss_gen_iter))\n     loss_dis_epoch.append(mean(loss_dis_iter))\n     D_real_epoch.append(mean(D_real_iter))\n     D_fake_epoch.append(mean(D_fake_iter))\n\n     # Keeping track of the evolution of a fixed noise latent vector\n     with torch.no_grad():\n         fake_images = generator(fixed_noise).detach().cpu()\n         #img_list.append(utils.make_grid(fake_images, normalize=True, nrows=10))\n\n print(\"\\nTraining ended.\")\nVisualize training process\n plt.plot(np.array(loss_gen_epoch), label='loss_gen')\n plt.plot(np.array(loss_dis_epoch), label='loss_dis')\n plt.xlabel(\"Epoch\")\n plt.ylabel(\"Loss\")\n plt.legend();\n plt.plot(np.array(D_real_epoch), label='D_real')\n plt.plot(np.array(D_fake_epoch), label='D_fake')\n plt.xlabel(\"Epoch\")\n plt.ylabel(\"Probability\")\n plt.legend();\n\n\n\n\nclass multiModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        ...\n\n    def forward(self, image, data):\n        x_cnn = self.cnn(image) # 1st model: CNN\n        x_fc = self.fc(data) # 2nd model: Fully connected\n        return torch.cat((x_cnn, x_fc), dim=1) # concatenate the two outputs"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#rounding-errors-in-programming",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#rounding-errors-in-programming",
    "title": "Supervised Learning II",
    "section": "",
    "text": "Infinite amount of numbers but finite amount of bits to represent them\nThese small errors will accumulate and cause problems\n\n\n\n\nlarge datasets with millions of params\nsmall errors can accumulate and cause problems\n\n\n\n\n\nBinary numbers are represented as a sum of powers of 2\ne.g. 104 in binary is 1101000 = \\(1(2^6) + 1(2^5) + 0(2^4) + 1(2^3) + 0(2^2) + 0(2^1) + 0(2^0) = 64 + 32 + 8 = 104\\)\nUnsigned Integers: \\(2^n - 1\\) is the largest number that can be represented with n bits\n\ne.g. 8 bits can represent 0 to 255\nnp.iinfo(np.uint8) gives the min and max values\n\nSigned Integers: \\(2^{n-1} - 1\\) is the largest positive number that can be represented with n bits\n\n\\(-2^{n-1}\\) is the smallest negative number that can be represented with n bits\ne.g. 8 bits can represent -128 to 127 (0 is included in the positive numbers)\n1 bit is used to represent the sign\nnp.iinfo(np.int8) gives the min and max values\n\n\n\n\n\n\n14.75 in binary is 1110.11\n\n\n\n\n2^3\n2^2\n2^1\n2^0\n2^-1\n2^-2\n\n\n\n\n1\n1\n1\n0\n1\n1\n\n\n8\n4\n2\n0\n0.5\n0.25\n\n\n\n$ 8 + 4 + 2 + 0 + 0.5 + 0.25 = 14.75 $\n\n\n\n\nWe typically have a fixed number of bits to represent the fractional part\ne.g. 8 bits total, 4 bits for the integer part and 4 bits for the fractional part\n\nmax value is 15.9375 (\\(2^3 + 2^2 + 2^1 + 2^0 + 2^{-1} + 2^{-2} + 2^{-3} + 2^{-4}\\))\n\noverflow if try a higher value\n\nmin value (bigger than 0) is 0.0625 (\\(2^{-4}\\))\n\nor precision of 0.0625 (any less =&gt; underflow)\n\n\n\n\n\n\n\nRather than having a fixed location for the binary point, we let it “float” around.\n\nlike how we write 0.1234 as 1.234 x 10^-1\n\nFormat: \\[(-1)^S \\times 1. M \\times 2^E\\]\n\nS is the sign bit\nM is the mantissa, always between 1 and 2 (1.0 is implied)\nE is the exponent\n\n\nFloat 64 (double precision) \nFloat 32 (single precision) \n\n\n\n\n\n\nThe spacing changes depending on the floating point number (because of the exponent)\n\n\n\nimport numpy as np\n\nnp.spacing(1e16) # 1.0\n\nnp.nextafter(1e16, 2e16) - 1e16 # 1.0\n\n\n\n\n\n\n1.0 + 2.0 + 3.0 == 6.0 True\n0.1 + 0.2 == 0.3 False\n\n0.1, 0.2, and 0.3 are not exactly representable in binary\n\n1e16 + 1 == 1e16 True\n\n1 is less than the spacing, so it is rounded back\n\n1e16 + 2.0 == 1e16 False\n\n2.0 is greater than the spacing, so it is rounded up\n\n1e16 + 1.0 + 1.0  == 1e16 True\n\n1.0 is less than the spacing, so it is rounded back, then 1.0 is added, which is less than the spacing, so it is rounded back again"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#optimization",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#optimization",
    "title": "Supervised Learning II",
    "section": "",
    "text": "In ML, we want to minimize a loss function\n\ntypically a sum of losses over the training set\n\nCan think of ML as a 3 step process:\n\nChoose model: controls space of possible functions that map X to y\nChoose loss function: measures how well the model fits the data\nChoose optimization algorithm: finds the best model\n\n\n\n\n\nOptimization: process to min/max a function\nObjective Function: function to be optimized\nDomain: set to search for optimal value\nMinimizer: value that minimizes the objective function\n\n\n\n\nCommon loss function is MSE (mean squared error):\n\\[L(w) = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2\\]\nUsing a simple linear regression model \\(y = w_0 + w_1x\\), we can rewrite the loss function as:\n\\[L(w) = \\frac{1}{n} \\sum_{i=1}^n ((w_0 + w_1x_i) - y_i)^2\\]\nSo optimization is finding the values of \\(w_0\\) and \\(w_1\\) that minimize the loss function, \\(L(w)\\).\nIn vector format:\n\\[\\text{MSE} = \\mathcal{L}(\\mathbf{w}) = \\frac{1}{n}\\sum^{n}_{i=1}(\\mathbf{x}_i \\mathbf{w} - y_i)^2\\]\nIn full-matrix format\n\\[\\text{MSE} = \\mathcal{L}(\\mathbf{w}) = \\frac{1}{n}(\\mathbf{X} \\mathbf{w} - \\mathbf{y})^T (\\mathbf{X} \\mathbf{w} - \\mathbf{y}) \\]\n\n\n\n\\[\n\\mathbf{y}=\n\\left[\n\\begin{array}{c} y_1 \\\\\n\\vdots \\\\\ny_i \\\\\n\\vdots\\\\\ny_n\n\\end{array}\n\\right]_{n \\times 1}, \\quad\n\\mathbf{X}=\n\\left[\n\\begin{array}{c} \\mathbf{x}_1 \\\\\n\\vdots \\\\\n\\mathbf{x}_i \\\\\n\\vdots\\\\\n\\mathbf{x}_n\n\\end{array}\n\\right]_{n \\times d}\n= \\left[\\begin{array}{cccc}\nx_{11} & x_{12} & \\cdots & x_{1 d} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{i 1} & x_{i 2} & \\cdots & x_{i d}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n 1} & x_{n 2} & \\cdots & x_{n d}\n\\end{array}\\right]_{n \\times d},\n\\quad\n\\mathbf{w}=\n\\left[\n\\begin{array}{c} w_1 \\\\\n\\vdots\\\\\nw_d\n\\end{array}\n\\right]_{d \\times 1}\n\\]\n\n\\(n\\): number of examples\n\\(d\\): number of input features/dimensions\n\nThe goal is to find the weights \\(\\mathbf{w}\\) that minimize the loss function.\nFormulas:\n\\[\\mathbf{y} = \\mathbf{X} \\mathbf{w}\\]\n\\[\\hat{\\mathbf{y}_i} = \\mathbf{w}^T \\mathbf{x}_i\\]"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#gradient-descent",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#gradient-descent",
    "title": "Supervised Learning II",
    "section": "",
    "text": "One of the most important optimization algorithms in ML\nIterative optimization algorithm\nSteps:\n\nstart with some arbitrary \\(\\mathbf{w}\\)\ncalculate the gradient using all training examples\nuse the gradient to adjust \\(\\mathbf{w}\\)\nrepeat for \\(I\\) iterations or until the step-size is sufficiently small\n\nCost: \\(O(ndt)\\) for t iterations, better than brute force search \\(O(nd^2 + d^3)\\)\n\n\\[w_{t+1} = w_t - \\alpha \\nabla= L(w_t)\\]\n\n\\(w_t\\): current value of the weights\n\\(\\alpha\\): learning rate\n\\(\\nabla L(w_t)\\): gradient of the loss function at \\(w_t\\)\n\n\n\n\nLoss function: \\(L(w) = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2\\)\nGradient: \\(\\nabla L(w) = \\frac{d}{dw} L(w) = \\frac{2}{n} \\sum_{i=1}^n x_{i1} (x_{i1} w_1 - y_i)\\)\n\nOr in Matrix form: \\(\\nabla L(w) = \\frac{2}{n} \\mathbf{X}^T (\\mathbf{X} \\mathbf{w} - \\mathbf{y})\\)\n\n\n\n\n\n\nNeed to scale for the contour plot to be more “round”\n\nbetter for gradient descent\n\n\n\n\nIn real life, contour plots are not so nice\n\n\n\n\n\n\nInitialization: Start with an initial set of parameters, often randomly chosen.\nForward pass: Generate predictions using the current values of the parameters. (E.g., \\(\\hat{y_i} = x_{1}w_1 + Bias\\) in the toy example above)\nLoss calculation: Evaluate the loss, which quantifies the discrepancy between the model’s predictions and the actual target values.\nGradient calculation: Compute the gradient of the loss function with respect to each parameter either on a batch or the full dataset. This gradient indicates the direction in which the loss is increasing and its magnitude.\nParameter Update: Adjust the parameters in the opposite direction of the calculated gradient, scaled by the learning rate. This step aims to reduce the loss by moving the parameters toward values that minimize it.\n\n\n\n\n\nUse minimize function from scipy.optimize\n\nfrom scipy.optimize import minimize\n\ndef mse(w, X, y):\n    \"\"\"Mean squared error.\"\"\"\n    return np.mean((X @ w - y) ** 2)\n\ndef mse_grad(w, X, y):\n    \"\"\"Gradient of mean squared error.\"\"\"\n    n = len(y)\n    return (2/n) * X.T @ (X @ w - y)\n\nout = minimize(mse, w, jac=mse_grad, args=(X_scaled_ones, toy_y), method=\"BFGS\")\n# jac: function to compute the gradient (optional)\n# - will use finite difference approximation if not provided\n\nOther methods:\n\nBFGS: Broyden–Fletcher–Goldfarb–Shanno algorithm\nCG: Conjugate gradient algorithm\nL-BFGS-B: Limited-memory BFGS with bounds on the variables\nSLSQP: Sequential Least SQuares Programming\nTNC: Truncated Newton algorithm"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#stochastic-gradient-descent",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#stochastic-gradient-descent",
    "title": "Supervised Learning II",
    "section": "",
    "text": "Instead of updating our parameters based on a gradient calculated using all training data, we simply use one of our data points (the \\(i\\)-th one)\n\nGradient Descent\nLoss function:\n\\[\\text{MSE} = \\mathcal{L}(\\mathbf{w}) = \\frac{1}{n}\\sum^{n}_{i=1} (\\mathbf{x}_i \\mathbf{w} - y_i)^2\\]\nUpdate procedure:\n\\[\\mathbf{w}^{j+1} = \\mathbf{w}^{j} - \\alpha \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}^{j})\\]\nStochastic Gradient Descent\nLoss function:\n\\[\\text{MSE}_i = \\mathcal{L}_i(\\mathbf{w}) = (\\mathbf{x}_i \\mathbf{w} - y_i)^2\\]\nUpdate procedure: \\[\\mathbf{w}^{j+1} = \\mathbf{w}^{j} - \\alpha \\nabla_\\mathbf{w} \\mathcal{L}_i(\\mathbf{w}^{j})\\]\n\n\n\n\n\nGradient Descent\nStochastic Gradient Descent\n\n\n\n\nUse all data points\nUse one data point\n\n\nSlow\nFast\n\n\nAccurate\nLess Accurate\n\n\n\n\nMini-batch Gradient Descent is a (in-between) compromise between the two\nInstead of using a single data point, we use a small batch of data points d\n\n\n\n\nShuffle and divide all data into \\(k\\) batches, every example is used once\n\nDefault in PyTorch\nAn example will only show up in one batch\n\nChoose some examples for each batch without replacement\n\nAn example may show up in multiple batches\nThe same example cannot show up in the same batch more than once\n\nChoose some examples for each batch with replacement\n\nAn example may show up in multiple batches\nThe same example may show up in the same batch more than once\n\n\n\n\n\n\nAssume we have a dataset of \\(n\\) observations (also known as rows, samples, examples, data points, or points)\n\nIteration: each time you update model weights\nBatch: a subset of data used in an iteration\nEpoch: One full pass through the dataset to look at all \\(n\\) observations\n\nIn other words,\n\nIn GD, each iteration involves computing the gradient over all examples, so\n\n\\[1 \\: \\text{iteration} = 1 \\: \\text{epoch}\\]\n\nIn SGD, each iteration involves one data point, so\n\n\\[n \\text{ iterations} = 1 \\: \\text{epoch}\\]\n\nIn MGD, each iteration involves a batch of data, so\n\n\\[\n\\begin{align}\n\\frac{n}{\\text{batch size}} \\text{iterations} &= 1 \\text{ epoch}\\\\\n\\end{align}\n\\]\n*Note: nobody really says “minibatch SGD”, we just say SGD: in SGD you can specify a batch size of anything between 1 and \\(n\\)"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#neural-networks",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#neural-networks",
    "title": "Supervised Learning II",
    "section": "",
    "text": "Models that does a good job of approximating complex non-linear functions\nIt is a sequence of layers, each of which is a linear transformation followed by a non-linear transformation\n\n\n\n\nNode (or neuron): a single unit in a layer\nInput layer: the features of the data\nHidden layer: the layer(s) between the input and output layers\nOutput layer: the prediction(s) of the model\nWeights: the parameters of the model\nActivation function: the non-linear transformation (e.g. ReLU, Sigmoid, Tanh, etc.)\n\n\nX : (n x d), W : (h x d), b : (n x h), where h is the number of hidden nodes b is actually 1 x hs, but we can think of it as n x hs because it is broadcasted\n\\[\\mathbf{H}^{(1)} = \\phi^{(1)} (\\mathbf{X}\\mathbf{W}^{(1)\\text{T}} + \\mathbf{b}^{(1)})\\]\n\\[\\mathbf{H}^{(2)} = \\phi^{(2)} (\\mathbf{H}^{(1)}\\mathbf{W}^{(2)\\text{T}} + \\mathbf{b}^{(2)})\\]\n\\[\\mathbf{Y} = (\\mathbf{H}^{(2)}\\mathbf{W}^{(3)\\text{T}} + \\mathbf{b}^{(3)})\\]\n\nIn a layer, \\[\\text{ num of weights} = \\text{num of nodes in previous layer} \\times \\text{num of nodes in current layer}\\]\n\n\\[\\text{num of biases} = \\text{num of nodes in current layer}\\]\n\\[\\text{num of parameters} = \\text{num of weights} + \\text{num of biases}\\]\n\n\n\n\n\n\n\nBackpropagation: a method to calculate the gradient of the loss function with respect to the weights\nChain rule: a method to calculate the gradient of a function composed of multiple functions\nIt is pretty complicated, but PyTorch does it for us\n\n\n\n\n\n\nNeural networks with &gt; 1 hidden layer\n\nNN with 1 hidden layer: shallow neural network"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#pytorch-for-neural-networks",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#pytorch-for-neural-networks",
    "title": "Supervised Learning II",
    "section": "",
    "text": "PyTorch is a popular open-source machine learning library by Facebook based on Torch\nIt is a Python package that provides two high-level features:\n\nTensor computation (like NumPy) with strong GPU acceleration\nGradient computation through automatic differentiation\n\n\n\n\n\nSimilar to ndarray in NumPy\n\nimport torch\n\n# Create a tensor\nx = torch.tensor([1, 2, 3, 4, 5]) # int\nx = torch.tensor([1, 2, 3, 4, 5.]) # float\nx = torch.tensor([[1, 2], [3, 4], [5, 6]])\n\ny = torch.zeros(3, 2)\ny = torch.ones(3, 2)\ny = torch.rand(3, 2)\n\n# Check the shape, dimensions, and data type\nx.shape\nx.ndim\nx.dtype\n\n# Operations\na = torch.rand(1, 3)\nb = torch.rand(3, 1)\n\na + b # broadcasting\na * b # element-wise multiplication\na @ b # matrix multiplication\na.mean()\na.sum()\n\n# Indexing\na[0,:] # first row\na[0] # first row\na[:,0] # first column\n\n# Convert to NumPy\nx.numpy()\n\n\n\n# Check if GPU is available\ntorch.backends.mps.is_available() # mac M chips\ntorch.cuda.is_available() # Nvidia GPU\n\n# To activate GPU\ndevice = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nx.to('cpu') # move tensor to cpu\n\n\n\nuse backward() to compute the gradient, backpropagation\n\nX = torch.tensor([1.0, 2.0, 3.0], requires_grad=False)\nw = torch.tensor([1.0], requires_grad=True)  # Random initial weight\ny = torch.tensor([2.0, 4.0, 6.0], requires_grad=False)  # Target values\nmse = ((X * w - y)**2).mean()\nmse.backward()\nw.grad\n\n\n\n\n\nEvery NN model has to inherit from torch.nn.Module\n\nfrom torch import nn\n\nclass linearRegression(nn.Module):  # inherit from nn.Module\n\n    def __init__(self, input_size, output_size):\n        super().__init__()  # call the constructor of the parent class\n\n        self.linear = nn.Linear(input_size, output_size,)  # wX + b\n\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\n# Create a model\nmodel = linearRegression(1, 1) # input size, output size\n\n# View model\nsummary(model)\n\n## Train the model\nLEARNING_RATE = 0.02\ncriterion = nn.MSELoss()  # loss function\noptimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)  # optimization algorithm is SGD\n\n# DataLoader for mini-batch\nfrom torch.utils.data import DataLoader, TensorDataset\n\nBATCH_SIZE = 50\ndataset = TensorDataset(X_t, y_t)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n# Training\ndef trainer(model, criterion, optimizer, dataloader, epochs=5, verbose=True):\n    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n\n    for epoch in range(epochs):\n        losses = 0\n\n        for X, y in dataloader:\n\n            optimizer.zero_grad()       # Clear gradients w.r.t. parameters\n            y_hat = model(X).flatten()  # Forward pass to get output\n            loss = criterion(y_hat, y)  # Calculate loss\n            loss.backward()             # Getting gradients w.r.t. parameters\n            optimizer.step()            # Update parameters\n            losses += loss.item()       # Add loss for this batch to running total\n\n        if verbose: print(f\"epoch: {epoch + 1}, loss: {losses / len(dataloader):.4f}\")\n\ntrainer(model, criterion, optimizer, dataloader, epochs=30, verbose=True)\n\n\n\n\nuse torch.nn.Sequential to create a model\n\nclass nonlinRegression(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n\n        self.main = torch.nn.Sequential(\n            nn.Linear(input_size, hidden_size),  # input -&gt; hidden layer\n            nn.Sigmoid(),                        # sigmoid activation function in hidden layer\n            nn.Linear(hidden_size, output_size)  # hidden -&gt; output layer\n        )\n\n    def forward(self, x):\n        x = self.main(x)\n        return x\n\n\n\n\n\n\nTask\nCriterion (Loss)\nOptimizer\n\n\n\n\nRegression\nMSELoss\nSGD\n\n\nBinary Classification\nBCELoss\nAdam\n\n\nMulti-class Classification\nCrossEntropyLoss\nAdam\n\n\n\n\nInput of CrossEntropyLoss doesn’t need to be normalized (i.e. no need to sum to 1/ no need to use nn.Softmax)\n\n# criterions\nfrom torch import nn\nreg_criterion = torch.nn.MSELoss()\nbc_criterion = torch.nn.BCEWithLogitsLoss()\nmse_criterion = torch.nn.CrossEntropyLoss()\n\n# optimizers\nfrom torch import optim\nreg_optim = torch.optim.SGD(model.parameters(), lr=0.2)\nclass_optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#backpropagation",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#backpropagation",
    "title": "Supervised Learning II",
    "section": "",
    "text": "It is to calculate the gradient of the loss function with respect to the weights\nIt is a special case of the chain rule of calculus\nProcess:\n\nDo “forward pass” to calculate the output of the network (prediction and loss)\n\n\n\nDo “backward pass” to calculate the gradients of the loss function with respect to the weights. Below is an example of reverse-mode autmatic differentiation (backpropagation):\n\n\n\n\n\n\n\ntorch.autograd is PyTorch’s automatic differentiation engine that powers neural network training\n\nimport torch\n\n# Create model\nclass network(torch.nn.Module):\n    def __init__(self):\n        super(network, self).__init__()\n        self.layer1 = torch.nn.Linear(1, 6)\n        self.dropout = torch.nn.Dropout(0.2) # dropout layer\n        ...\n\n    def forward(self, x):\n        x = self.layer1(x)\n        ...\n        return x\n\nmodel = network()\ncriterion = torch.nn.MSELoss()\n\n# Forward pass\nloss = criterion(model(x), y)\n# Backward pass\nloss.backward()\n\n# Access gradients\nprint(model.layer1.weight.grad) # or model.layer1.weight.bias.grad\n\n# Update weights\nmodel.state_dict() # get the current weights\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\noptimizer.step() # update weights\n\n\n\n\nBackpropagation can suffer from two problems because of multiple chain rule applications:\n\nVanishing gradients: the gradients of the loss function with respect to the weights become very small\n\n0 gradients because of underflow\n\nExploding gradients: the gradients of the loss function with respect to the weights become very large\n\nPossible solutions:\n\nUse ReLU activation function: but it can also suffer from the dying ReLU problem (gradients are 0)\nWeight initialization: initialize the weights with small values\nBatch normalization: normalize the input layer by adjusting and scaling the activations\nSkip connections: add connections that skip one or more layers\nGradient clipping: clip the gradients during backpropagation\n\n\n\n\n\n\n\n\nAdd validation loss to the training loop\nEarly stopping: if we see the validation loss is increasing, we stop training\n\nDefine a patience parameter: if the validation loss increases for patience epochs, we stop training\n\nRegularization: add a penalty term to the loss function to prevent overfitting\n\nSee 573 notes for more details\nweight_decay parameter in the optimizer\n\nDropout: randomly set some neurons to 0 during training\n\nIt prevents overfitting by reducing the complexity of the model\ntorch.nn.Dropout(0.2)\n\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\ndef trainer(model, criterion, optimizer, trainloader, validloader, epochs=5, patience=5):\n    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n\n    train_loss = []\n    valid_loss = []\n\n    for epoch in range(epochs):  # for each epoch\n        train_batch_loss = 0\n        valid_batch_loss = 0\n\n        # Training\n        for X, y in trainloader:\n\n            optimizer.zero_grad()       # Zero all the gradients w.r.t. parameters\n\n            y_hat = model(X).flatten()  # Forward pass to get output\n            loss = criterion(y_hat, y)  # Calculate loss based on output\n            loss.backward()             # Calculate gradients w.r.t. parameters\n            optimizer.step()            # Update parameters\n\n            train_batch_loss += loss.item()  # Add loss for this batch to running total\n\n        train_loss.append(train_batch_loss / len(trainloader))\n\n        # Validation\n        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n\n            for X_valid, y_valid in validloader:\n\n                y_hat = model(X_valid).flatten()  # Forward pass to get output\n                loss = criterion(y_hat, y_valid)  # Calculate loss based on output\n\n                valid_batch_loss += loss.item()\n\n        valid_loss.append(valid_batch_loss / len(validloader))\n\n        # Early stopping\n        if epoch &gt; 0 and valid_loss[-1] &gt; valid_loss[-2]:\n            consec_increases += 1\n        else:\n            consec_increases = 0\n        if consec_increases == patience:\n            print(f\"Stopped early at epoch {epoch + 1} - val loss increased for {consec_increases} consecutive epochs!\")\n            break\n\n    return train_loss, valid_loss\n\nUsing the trainer function:\n\nimport torch\nimport torch.nn\nimport torch.optim\n\ntorch.manual_seed(1)\n\nmodel = network(1, 6, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05) # weight_decay=0.01 for L2 regularization\ntrain_loss, valid_loss = trainer(model, criterion, optimizer, trainloader, validloader, epochs=201, patience=3)\n\nplot_loss(train_loss, valid_loss)\n\n\n\n\nAny continuous function can be approximated arbitrarily well by a neural network with a single hidden layer\n\nIn other words, NN are universal function approximators"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#convolutional-neural-networks-cnn",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#convolutional-neural-networks-cnn",
    "title": "Supervised Learning II",
    "section": "",
    "text": "Drastically reduces the number of params (compared to NN):\n\nhave activations depend on small number of inputs\nsame parameters (convolutional filter) are used for different parts of the image\n\nCan capture spatial information (preserves the structure of the image)\n\n\n\n\nIdea: use a small filter/kernel to extract features from the image\n\nFilter: a small matrix of weights (normally odd dimensioned -&gt; for symmetry)\n\n\n\n\nNotice that the filter results in a smaller output image\n\nThis is because we are not padding the image\nWe can add padding to the image to keep the same size\n\nPadding: add zeros around the image\n\nCan also add stride to move the filter more than 1 pixel at a time\n\n\n\n\n\n\nimg src\n\n\n\n\n\nconv_1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=(3,3))\n\nArguments:\n\nin_channels: number of input channels (gray scale image has 1 channel, RGB has 3)\nout_channels: number of output channels (similar to hidden nodes in NN)\nkernel_size: size of the filter\nstride: how many pixels to move the filter each time\npadding: how many pixels to add around the image\n\n\n\n\nSize of input image (e.g. 256x256) doesn’t matter, what matters is: in_channels, out_channels, kernel_size\n\n\\[\\text{total params} = (\\text{out channels} \\times \\text{in channels} \\times \\text{kernel size}^2) + \\text{out channels}\\]\n\\[\\text{output size} = \\frac{\\text{input size} - \\text{kernel size} + 2 \\times \\text{padding}}{\\text{stride}} + 1\\]\n\n\n\nImages: [batch_size, channels, height, width]\nKernel: [out_channels, in_channels, kernel_height, kernel_width]\n\nNote: before passing the image to the convolutional layer, we need to reshape it to the correct dimensions. Also if you want to plt.imshow() the image, you need to reshape it back to [height, width, channels].\n\n\n\n\n\nfeature learning -&gt; classification\nUse torch.nn.Flatten() to flatten the image\nAt the end need to either do regression or classification\n\n\n\n\n\nIdea: reduce the size of the image\n\nless params\nless overfitting\n\nCommon types:\n\nMax pooling: take the max value in each region\n\nWorks well since it takes the sharpest features\n\nAverage pooling: take the average value in each region\n\n\n\n\n\nclass CNN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.main = torch.nn.Sequential(\n\n            torch.nn.Conv2d(in_channels=1,\n                out_channels=3,\n                kernel_size=(3, 3),\n                padding=1),\n            torch.nn.ReLU(), # activation function\n            torch.nn.MaxPool2d((2, 2)),\n\n            torch.nn.Conv2d(in_channels=3,\n                out_channels=2,\n                kernel_size=(3, 3),\n                padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d((2, 2)),\n\n            torch.nn.Flatten(),\n            torch.nn.Linear(1250, 1)\n        )\n\n    def forward(self, x):\n        out = self.main(x)\n        return out\n# Trainer code\ndef trainer(\n    model, criterion, optimizer, trainloader, validloader, epochs=5, verbose=True\n):\n    train_loss, train_accuracy, valid_loss, valid_accuracy = [], [], [], []\n    for epoch in range(epochs):  # for each epoch\n        train_batch_loss = 0\n        train_batch_acc = 0\n        valid_batch_loss = 0\n        valid_batch_acc = 0\n\n        # Training\n        for X, y in trainloader:\n            if device.type in ['cuda', 'mps']:\n                    X, y = X.to(device), y.to(device)\n            optimizer.zero_grad()  # Zero all the gradients w.r.t. parameters\n            y_hat = model(X)  # Forward pass to get output\n            idx = torch.softmax(y_hat, dim=1).argmax(dim=1) # Multiclass classification\n            loss = criterion(y_hat, y)\n            loss.backward()  # Calculate gradients w.r.t. parameters\n            optimizer.step()  # Update parameters\n            train_batch_loss += loss.item()  # Add loss for this batch to running total\n            train_batch_acc += (\n                    (idx.squeeze() == y).type(\n                        torch.float32).mean().item()\n                )\n        train_loss.append(train_batch_loss / len(trainloader))\n        train_accuracy.append(train_batch_acc / len(trainloader))\n\n        # Validation\n        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n            for X, y in validloader:\n                if device.type in ['cuda', 'mps']:\n                    X, y = X.to(device), y.to(device)\n                y_hat = model(X)\n                idx = torch.softmax(y_hat, dim=1).argmax(dim=1)\n                loss = criterion(y_hat, y)\n                valid_batch_loss += loss.item()\n                valid_batch_acc += (\n                    (idx.squeeze() == y).type(\n                        torch.float32).mean().item()\n                )\n        valid_loss.append(valid_batch_loss / len(validloader))\n        valid_accuracy.append(valid_batch_acc / len(validloader))  # accuracy\n\n        # Print progress\n        if verbose:\n            print(\n                f\"Epoch {epoch + 1}:\",\n                f\"Train Loss: {train_loss[-1]:.3f}.\",\n                f\"Valid Loss: {valid_loss[-1]:.3f}.\",\n                f\"Train Accuracy: {train_accuracy[-1]:.2f}.\",\n                f\"Valid Accuracy: {valid_accuracy[-1]:.2f}.\",\n            )\n\n    results = {\n        \"train_loss\": train_loss,\n        \"train_accuracy\": train_accuracy,\n        \"valid_loss\": valid_loss,\n        \"valid_accuracy\": valid_accuracy,\n    }\n    return results\n\n\n\n\nTo get a summary of the model\n\nNo need to manually calculate the output size of each layer\n\n\nfrom torchsummary import summary\n\nmodel = CNN()\nsummary(model, (1, 256, 256))\n\n\n\n\n\n\n\nNormally there are 2 steps:\n\ncreate a dataset object: the raw data\ncreate a dataloader object: batches the data, shuffles, etc.\n\nUse torchvision to load the data\n\ntorchvision.datasets.ImageFolder: loads images from folders\nAssumes structure: root/class_1/xxx.png, root/class_2/xxx.png, …\n\n\nimport torch\nfrom torchvision import datasets, transforms\n\nIMAGE_SIZE = (256, 256)\nBATCH_SIZE = 32\n\n# create transform object\ndata_transforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.ToTensor()\n])\n\n# create dataset object\ntrain_dataset = datasets.ImageFolder(root='path/to/data', transform=data_transforms)\n\n# check out the data\ntrain_dataset.classes # list of classes\ntrain_dataset.targets # list of labels\ntrain_dataset.samples # list of (path, label) tuples\n\n# create dataloader object\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,          # our raw data\n    batch_size=BATCH_SIZE,  # the size of batches we want the dataloader to return\n    shuffle=True,           # shuffle our data before batching\n    drop_last=False         # don't drop the last batch even if it's smaller than batch_size\n)\n\n# get a batch of data\nimages, labels = next(iter(train_loader))\n\n\n\n\nPyTorch documentation\nConvention: .pt or .pth file extension\n\nPATH = \"models/my_cnn.pt\"\n\n# load model\nmodel = bitmoji_CNN() # must have defined the model class\nmodel.load_state_dict(torch.load(PATH))\nmodel.eval() # set model to evaluation mode (not training mode)\n\n# save model\ntorch.save(model.state_dict(), PATH)\n\n\n\n\nTo make CNN more robust to different images + increase the size of the dataset\nCommon augmentations:\n\nCrop\nRotate\nFlip\nColor jitter\n\n\ndata_transforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.RandomVerticalFlip(p=0.5), # p=0.5 means 50% chance of applying this augmentation\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor()\n])\n\n\n\n\n\nNN has a lot of hyperparameters\n\nGrid search will take a long time\nNeed a smarter approach: Optimization Algorithms\n\nExamples: Ax (we will use this), Raytune, Neptune, skorch.\n\n\n\n\n\nIdea: use a pre-trained model and fine-tune it to our specific task\nInstall from torchvision.models\n\nAll models have been trained on ImageNet dataset (224x224 images)\n\nSee here for code\n\n\n\ndensenet = models.densenet121(weights='DenseNet121_Weights.DEFAULT')\n\nfor param in densenet.parameters():  # Freeze parameters so we don't update them\n    param.requires_grad = False\n# can fine-tune to freeze only some layers\n\nlist(densenet.named_children())[-1] # check the last layer\n\n# update the last layer\nnew_layers = nn.Sequential(\n    nn.Linear(1024, 500),\n    nn.ReLU(),\n    nn.Linear(500, 1)\n)\ndensenet.classifier = new_layers\nThen train the model as usual.\ndensenet.to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(densenet.parameters(), lr=2e-3)\nresults = trainer(densenet, criterion, optimizer, train_loader, valid_loader, device, epochs=10)\n\n\n\n\nIdea:\n\nTake output from pre-trained model\nFeed output to a new model\n\n\ndef get_features(model, train_loader, valid_loader):\n    \"\"\"\n    Extract features from both training and validation datasets using the provided model.\n\n    This function passes data through a given neural network model to extract features. It's designed\n    to work with datasets loaded using PyTorch's DataLoader. The function operates under the assumption\n    that gradients are not required, optimizing memory and computation for inference tasks.\n    \"\"\"\n\n    # Disable gradient computation for efficiency during inference\n    with torch.no_grad():\n        # Initialize empty tensors for training features and labels\n        Z_train = torch.empty((0, 1024))  # Assuming each feature vector has 1024 elements\n        y_train = torch.empty((0))\n\n        # Initialize empty tensors for validation features and labels\n        Z_valid = torch.empty((0, 1024))\n        y_valid = torch.empty((0))\n\n        # Process training data\n        for X, y in train_loader:\n            # Extract features and concatenate them to the corresponding tensors\n            Z_train = torch.cat((Z_train, model(X)), dim=0)\n            y_train = torch.cat((y_train, y))\n\n        # Process validation data\n        for X, y in valid_loader:\n            # Extract features and concatenate them to the corresponding tensors\n            Z_valid = torch.cat((Z_valid, model(X)), dim=0)\n            y_valid = torch.cat((y_valid, y))\n\n    # Return the feature and label tensors\n    return Z_train, y_train, Z_valid, y_valid\nNow we can use the extracted features to train a new model.\n# Extract features from the pre-trained model\ndensenet = models.densenet121(weights='DenseNet121_Weights.DEFAULT')\ndensenet.classifier = nn.Identity()  # remove that last \"classification\" layer\nZ_train, y_train, Z_valid, y_valid = get_features(densenet, train_loader, valid_loader)\n\n# Train a new model using the extracted features\n# Let's scale our data\nscaler = StandardScaler()\nZ_train = scaler.fit_transform(Z_train)\nZ_valid = scaler.transform(Z_valid)\n\n# Fit a model\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(Z_train, y_train)"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#advanced-cnn",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#advanced-cnn",
    "title": "Supervised Learning II",
    "section": "",
    "text": "Generative Models\nDiscriminative Models\n\n\n\n\nDirectly model the joint probability distribution of the input and output\nModel the conditional probability of the output given the input\n\n\nDirectly model \\(P(y\\|x)\\)\nEstimate \\(P(x\\|y)\\) to then deduce \\(P(y\\|x)\\)\n\n\nBuild model for each class\nMake boundary between classes\n\n\n“Generate or draw a cat”\n“Distinquish between cats and dogs”\n\n\nExamples: Naibe bayes, ChatGPT\nExamples: Logistic Regression, SVM, Tree based models, CNN\n\n\n\n\n\n\n\n\nDesigned to reconstruct the input\nEncoder and a decoder\nWhy do we need autoencoders?\n\nDimensionality reduction\nDenoising\n\n\n\n\n\nMaybe the z axis is unimportant in the input space for classification\n\nfrom torch import nn\n\nclass autoencoder(torch.nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, 2),\n            nn.Sigmoid()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(2, input_size),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n# Set up the training\nBATCH_SIZE = 100\ntorch.manual_seed(1)\nX_tensor = torch.tensor(X, dtype=torch.float32)\ndataloader = DataLoader(X_tensor,\n                        batch_size=BATCH_SIZE)\nmodel = autoencoder(3, 2)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters())\n\n# Train the model\nEPOCHS = 5\n\nfor epoch in range(EPOCHS):\n    for batch in dataloader:\n        optimizer.zero_grad()           # Clear gradients w.r.t. parameters\n        y_hat = model(batch)            # Forward pass to get output\n        loss = criterion(y_hat, batch)  # Calculate loss\n        loss.backward()                 # Getting gradients w.r.t. parameters\n        optimizer.step()                # Update parameters\n\n# Use encoder\nmodel.eval()\nX_encoded = model.encoder(X_tensor)\n\n\n\n\nRemove noise from the input\nUse Transposed Convolution Layers to upsample the input\n\nNormal convolution: downsample (output is smaller than input)\nTransposed convolution: upsample (output is larger than input)\n\n\ndef conv_block(input_channels, output_channels):\n    return nn.Sequential(\n        nn.Conv2d(input_channels, output_channels, 3, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(2)  # reduce x-y dims by two; window and stride of 2\n    )\n\ndef deconv_block(input_channels, output_channels, kernel_size):\n    return nn.Sequential(\n        nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride=2),\n        nn.ReLU()\n    )\n\nclass autoencoder(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            conv_block(1, 32),\n            conv_block(32, 16),\n            conv_block(16, 8)\n        )\n        self.decoder = nn.Sequential(\n            deconv_block(8, 8, 3),\n            deconv_block(8, 16, 2),\n            deconv_block(16, 32, 2),\n            nn.Conv2d(32, 1, 3, padding=1)  # final conv layer to decrease channel back to 1\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        x = torch.sigmoid(x)  # get pixels between 0 and 1\n        return x\n# Set up the training\nEPOCHS = 20\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters())\nimg_list = []\n\nfor epoch in range(EPOCHS):\n    losses = 0\n    for batch, _ in trainloader:\n        noisy_batch = batch + noise * torch.randn(*batch.shape)\n        noisy_batch = torch.clip(noisy_batch, 0.0, 1.0)\n        optimizer.zero_grad()\n        y_hat = model(noisy_batch)\n        loss = criterion(y_hat, batch)\n        loss.backward()\n        optimizer.step()\n        losses += loss.item()\n    print(f\"epoch: {epoch + 1}, loss: {losses / len(trainloader):.4f}\")\n    # Save example results each epoch so we can see what's going on\n    with torch.no_grad():\n        noisy_8 = noisy_batch[:1, :1, :, :]\n        model_8 = model(input_8)\n        real_8 = batch[:1, :1, :, :]\n    img_list.append(utils.make_grid([noisy_8[0], model_8[0], real_8[0]], padding=1))```"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#generative-adversarial-networks-gans",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#generative-adversarial-networks-gans",
    "title": "Supervised Learning II",
    "section": "",
    "text": "Model used to generate new data (indistinguishable from real data)\nNo need for labels (unsupervised learning)\nSee here\n\n\n\nTwo networks:\n\nGenerator: creates new data\nDiscriminator: tries to distinguish between real and fake data\n\nBoth are battling each other:\n\nGenerator tries to create data that the discriminator can’t distinguish from real data\nDiscriminator tries to distinguish between real and fake data\n\n\n\n\n\nTrain the discriminator (simple binary classification)\n\nTrain the discriminator on real data\nTrain the discriminator on fake data (generated by the generator)\n\nTrain the generator\n\nGenerate fake images with the generator and label them as real\nPass to discriminator and ask it to classify them (real or fake)\nPass judgement to a loss function (see how far it is from the ideal output)\n\nideal output: all fake images are classified as real\n\nDo backpropagation and update the generator\n\nRepeat\n\n\n\n\n\nCreating the data loader\nDATA_DIR = \"../input/face-recognition-dataset/Extracted Faces\"\n\nBATCH_SIZE = 64\nIMAGE_SIZE = (128, 128)\n\ndata_transforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE), # uses CPU (bottleneck)\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ndataset = datasets.ImageFolder(root=DATA_DIR, transform=data_transforms)\ndata_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\nCreating the generator\n\nclass Generator(nn.Module):\n\n def __init__(self, LATENT_SIZE):\n     super(Generator, self).__init__()\n\n     self.main = nn.Sequential(\n         nn.ConvTranspose2d(LATENT_SIZE, 1024, kernel_size=4, stride=1, padding=0, bias=False),\n         nn.BatchNorm2d(1024),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(512),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(128),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(3),\n\n         nn.Tanh()\n     )\n\n def forward(self, input):\n     return self.main(input)\nCreating the discriminator\n\nclass Discriminator(nn.Module):\n\n def __init__(self):\n     super(Discriminator, self).__init__()\n\n     self.main = nn.Sequential(\n         nn.Conv2d(3, 128, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(128),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.Conv2d(128, 512, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(512),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n         nn.Flatten(),\n         nn.Sigmoid()\n     )\n\n def forward(self, input):\n     return self.main(input)\nInstantiating the models\ndevice = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n\nLATENT_SIZE = 100\ngenerator = Generator(LATENT_SIZE).to(device)\ndiscriminator = Discriminator().to(device)\n\ncriterion = nn.BCELoss()\n\noptimizerG = optim.Adam(generator.parameters(), lr=0.001, betas=(0.5, 0.999))\noptimizerD = optim.Adam(discriminator.parameters(), lr=0.001, betas=(0.5, 0.999))\n\n def weights_init(m):\n     if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n         nn.init.normal_(m.weight.data, 0.0, 0.02)\n     elif isinstance(m, nn.BatchNorm2d):\n         nn.init.normal_(m.weight.data, 1.0, 0.02)\n         nn.init.constant_(m.bias.data, 0)\n\n\n generator.apply(weights_init)\n discriminator.apply(weights_init);\nTraining the GAN\n img_list = []\nfixed_noise = torch.randn(BATCH_SIZE, LATENT_SIZE, 1, 1).to(device)\n\n NUM_EPOCHS = 50\nfrom statistics import mean\nprint('Training started:\\n')\n\n D_real_epoch, D_fake_epoch, loss_dis_epoch, loss_gen_epoch = [], [], [], []\n\n for epoch in range(NUM_EPOCHS):\n     D_real_iter, D_fake_iter, loss_dis_iter, loss_gen_iter = [], [], [], []\n\n     for real_batch, _ in data_loader:\n\n         # STEP 1: train discriminator\n         # ==================================\n         optimizerD.zero_grad()\n\n         real_batch = real_batch.to(device)\n         real_labels = torch.ones((real_batch.shape[0],), dtype=torch.float).to(device)\n\n         output = discriminator(real_batch).view(-1)\n         loss_real = criterion(output, real_labels)\n\n         # Iteration book-keeping\n         D_real_iter.append(output.mean().item())\n\n         # Train with fake data\n         noise = torch.randn(real_batch.shape[0], LATENT_SIZE, 1, 1).to(device)\n\n         fake_batch = generator(noise)\n         fake_labels = torch.zeros_like(real_labels)\n\n         output = discriminator(fake_batch.detach()).view(-1)\n         loss_fake = criterion(output, fake_labels)\n\n         # Update discriminator weights\n         loss_dis = loss_real + loss_fake\n         loss_dis.backward()\n         optimizerD.step()\n\n         # Iteration book-keeping\n         loss_dis_iter.append(loss_dis.mean().item())\n         D_fake_iter.append(output.mean().item())\n\n         # STEP 2: train generator\n         # ==================================\n         optimizerG.zero_grad()\n\n         # Calculate the output with the updated weights of the discriminator\n         output = discriminator(fake_batch).view(-1)\n         loss_gen = criterion(output, real_labels)\n         loss_gen.backward()\n\n         # Book-keeping\n         loss_gen_iter.append(loss_gen.mean().item())\n\n         # Update generator weights and store loss\n         optimizerG.step()\n\n     print(f\"Epoch ({epoch + 1}/{NUM_EPOCHS})\\t\",\n         f\"Loss_G: {mean(loss_gen_iter):.4f}\",\n         f\"Loss_D: {mean(loss_dis_iter):.4f}\\t\",\n         f\"D_real: {mean(D_real_iter):.4f}\",\n         f\"D_fake: {mean(D_fake_iter):.4f}\")\n\n     # Epoch book-keeping\n     loss_gen_epoch.append(mean(loss_gen_iter))\n     loss_dis_epoch.append(mean(loss_dis_iter))\n     D_real_epoch.append(mean(D_real_iter))\n     D_fake_epoch.append(mean(D_fake_iter))\n\n     # Keeping track of the evolution of a fixed noise latent vector\n     with torch.no_grad():\n         fake_images = generator(fixed_noise).detach().cpu()\n         #img_list.append(utils.make_grid(fake_images, normalize=True, nrows=10))\n\n print(\"\\nTraining ended.\")\nVisualize training process\n plt.plot(np.array(loss_gen_epoch), label='loss_gen')\n plt.plot(np.array(loss_dis_epoch), label='loss_dis')\n plt.xlabel(\"Epoch\")\n plt.ylabel(\"Loss\")\n plt.legend();\n plt.plot(np.array(D_real_epoch), label='D_real')\n plt.plot(np.array(D_fake_epoch), label='D_fake')\n plt.xlabel(\"Epoch\")\n plt.ylabel(\"Probability\")\n plt.legend();\n\n\n\n\nclass multiModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        ...\n\n    def forward(self, image, data):\n        x_cnn = self.cnn(image) # 1st model: CNN\n        x_fc = self.fc(data) # 2nd model: Fully connected\n        return torch.cat((x_cnn, x_fc), dim=1) # concatenate the two outputs"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html",
    "href": "block_3/513_databases/513_databases.html",
    "title": "Relational Databases",
    "section": "",
    "text": "What is a database?\n\nOrganized collection of related data\n\nWhart is a database management system (DBMS)?\n\nCollection of programs that enables users to create and maintain a database\nallows users to create, query, modify, and manage\n\n\nDATABASE != DBMS\n\n\n\nEfficiency: Data is stored efficiently.\nIntegrity: Data is consistent and correct.\nSecurity: Data is safe from unauthorized access.\nConcurrent Access: Multiple users can access the same data at the same time.\nCrash Recovery: Data is safe from crashes.\nIndependence: Data is independent of the programs that use it.\n\n\n\n\n\n\nWorks with entities and relationships\nEntity: a thing or object in the real world that is distinguishable from other objects\n\ne.g. students in a school\n\nRelationship: an association among entities\n\ne.g. a student is enrolled in a course\n\n\n\n\n\n\nCollection of relations\nA relation is an instance of a relation schema (similar to object = instance of a class)\nRelation Schema specifies:\n\nName of relation\nName and domain of each attribute\n\nDomain: set of constraints that determines the type, length, format, range, uniqueness and nullability of values stored for an attribute\n\n\n\n\n\n\nQuery: a request for information from a database. Results in a relation.\nStructured Query Language (SQL): standard programming language for managing and manipulating databases.\nCan be categorized into:\n\nData Definition Language (DDL): used to define the database structure or schema.\nData Manipulation Language (DML): used to read, insert, delete, and modify data.\nData Query Language (DQL): used to retrieve data from a database.\nData Control Language (DCL): used to control access to data stored in a database.\n\n\n\n\n\n\nSpecific flavor of SQL and DBMS\nopen-source, cross-platform DBMS that implements the relational model\nreliable, robust, and feature-rich\n\n\n\n\n\n\nCommand\nUsage\n\n\n\n\n\\l\nList all databases\n\n\n\\c\nConnect to a database\n\n\n\\d\nDescribe tables and views\n\n\n\\dt\nList tables\n\n\n\\dt+\nList tables with additional info\n\n\n\\d+\nList tables and views with additional info\n\n\n\\!\nExecute shell commands\n\n\n\\cd\nChange directory\n\n\n\\i\nExecute commands from a file\n\n\n\\h\nView help on SQL commands\n\n\n\\?\nView help on psql meta commands\n\n\n\\q\nQuit interactive shell\n\n\n\n\n\n\n\nTo install: pip install ipython-sql\nTo load: %load_ext sql\n\n# Login to database\nimport json\nimport urllib.parse\n\n# use credentials.json to login (not included in repo)\nwith open('data/credentials.json') as f:\n    login = json.load(f)\n\nusername = login['user']\npassword = urllib.parse.quote(login['password'])\nhost = login['host']\nport = login['port']\n\nEstablish connection:\n\n%sql postgresql://{username}:{password}@{host}:{port}/world\n\nRun queries:\n\noutput = %sql SELECT name, population FROM country;\nor\n%%sql output &lt;&lt; # Not a pandas dataframe\nSELECT\n  name, population\nFROM\n  country\n;\n\n# convert to pandas dataframe\ndf = output.DataFrame()\n\nSet configurations:\n\n%config SqlMagic.displaylimit = 20\n\n\n\n\n\ntaken from https://www.sqltutorial.org/sql-cheat-sheet/*\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription / Example\n\n\n\n\nSELECT c1, c2FROM t;\nQuery data in columns c1, c2 from a table\n\n\nSELECT *FROM t;\nQuery all rows and columns from a table\n\n\nSELECT c1, c2FROM tWHERE condition;\nQuery data and filter rows with a condition\n\n\nSELECT DISTINCT c1FROM tWHERE condition;\nQuery distinct rows from a table\n\n\nSELECT COUNT(DISTINCT (c1, c2))FROM t;\nCount distinct rows in a table\n\n\nSELECT c1, c2FROM tORDER BY c1 ASC [DESC];\nSort the result set in ascending or descending order\n\n\nSELECT c1, c2FROM tORDER BY c1LIMIT n OFFSET offset;\nSkip offset of rows and return the next n rows\n\n\nSELECT c1, aggregate(c2)FROM tGROUP BY c1;\nGroup rows using an aggregate function\n\n\nSELECT c1, aggregate(c2)FROM tGROUP BY c1HAVING condition;\nFilter groups using HAVING clause\n\n\nSELECT CONCAT(c1, c2)FROM t;\nConcatenate two or more strings\n\n\n\nNotes:\n\nConditions: =, &gt;=, &lt;=, IN ('a','b'), IS NULL, …\nStrings must be enclosed in single quotes '...'\nAggregate functions: AVG, COUNT, MAX, MIN, SUM, ROUND(value, decimal_places)\nNeed decimal point to prevent integer division\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription / Example\n\n\n\n\nSELECT c1, c2FROM t1INNER JOIN t2 ON condition;\nInner join t1 and t2\n\n\nSELECT c1, c2FROM t1LEFT JOIN t2 ON condition;\nLeft join t1 and t2\n\n\nSELECT c1, c2FROM t1RIGHT JOIN t2 ON condition;\nRight join t1 and t2\n\n\nSELECT c1, c2FROM t1FULL OUTER JOIN t2 ON condition;\nPerform full outer join\n\n\nSELECT c1, c2FROM t1CROSS JOIN t2;\nProduce a Cartesian product of rows in tables\n\n\nSELECT c1, c2FROM t1 AINNER JOIN t2 B ON condition;\nJoin t1 to itself using INNER JOIN clause\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription / Example\n\n\n\n\nSELECT c1, c2FROM t1UNION [ALL]SELECT c1, c2 FROM t2;\nCombine rows from two queries\n\n\nSELECT c1, c2FROM t1INTERSECTSELECT c1, c2 FROM t2;\nReturn the intersection of two queries\n\n\nSELECT c1, c2FROM t1MINUSSELECT c1, c2 FROM t2;\nSubtract a result set from another result set\n\n\nSELECT c1, c2FROM tWHERE c1 [NOT] LIKE pattern;\nQuery rows using pattern matching % _\n\n\nSELECT c1, c2FROM tWHERE c1 [NOT] IN value_list;\nQuery rows in a list\n\n\nSELECT c1, c2FROM tWHERE c1 BETWEEN low AND high;\nQuery rows between two values\n\n\nSELECT c1, c2FROM tWHERE c1 IS [NOT] NULL;\nCheck if values in a table is NULL or not\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription / Example\n\n\n\n\nCREATE TABLE t (id INT PRIMARY KEY,name VARCHAR NOT NULL,price INT DEFAULT 0);\nCreate a new table with three columns\n\n\nDROP TABLE t;\nDelete the table from the database\n\n\nALTER TABLE t ADD column;\nAdd a new column to the table\n\n\nALTER TABLE t DROP COLUMN c;\nDrop column c from the\n\n\n\n\n\n\n\nPostgreSQL supports many data types. The most common ones are:\n\nBoolean, BOOLEAN or BOOL\n\nTrue: TRUE, t, true, y, yes, on, 1\n\nCharacters\n\nCHAR(n): string of n characters (pad with spaces)\nVARCHAR(n): string of up to n characters\nTEXT: PostgreSQL-specific type for storing strings of any length\n\nDateTime: date and time\n\nDATE: date (YYYY-MM-DD)\n\nCURRENT_DATE: current date\n\nTIME: time\nTIMESTAMP: date and time\nTIMESTAMPTZ: date and time with timezone\n\nBinary: binary data\nNumbers\n\n\n\n\n\n\" is used for identifiers (e.g. table names, column names)\n' is used for strings.\n\n\n\n\n\n\n\n\n\n\n\n\nName\nStorage Size\nDescription\nRange\n\n\n\n\nsmallint\n2 bytes\nsmall-range integer\n-32,768 to +32,767\n\n\ninteger\n4 bytes\ntypical choice for integer\n-2,147,483,648 to +2,147,483,647\n\n\nbigint\n8 bytes\nlarge-range integer\n-9,223,372,036,854,775,808 to +9,223,372,036,854,775,807\n\n\nserial\n4 bytes\nauto-incrementing integer\n1 to 2,147,483,647\n\n\nbigserial\n8 bytes\nlarge auto-incrementing integer\n1 to 9,223,372,036,854,775,807\n\n\n\nNote: serial and bigserial are not true types, but merely a notational convenience for creating unique identifier columns.\nFloating-Point Numbers\n\n\n\n\n\n\n\n\n\nName\nStorage Size\nDescription\nRange\n\n\n\n\nreal\n4 bytes\nvariable-precision, inexact\nat least 6 decimal digits (implementation dependent)\n\n\ndouble precision\n8 bytes\nvariable-precision, inexact\nat least 15 decimal digits (implementation dependent)\n\n\n\nArbitrary Precision Numbers\n\n\n\n\n\n\n\n\n\nName\nStorage Size\nDescription\nRange\n\n\n\n\nnumeric or decimal\nvariable\nuser-specified precision, exact\n131072 digits before and 16383 digits after the decimal point\n\n\n\nNote: numeric and decimal are the same type in Postgres.\n\n\n\n\nCan do : &lt;col&gt;::&lt;data_type&gt; or CAST(&lt;col&gt; AS &lt;data_type&gt;)\ne.g. SELECT '123'::integer;\ne.g. SELECT CAST('123' AS integer);\n\n\n\n\nSELECT *\nFROM table_name\nWHERE [NOT]\n    condition\n    AND/OR\n    condition;\n\n\n\n\n\n\n\nCondition Example\nDescription\n\n\n\n\nWHERE column = value\nEquals; returns true if the column equals the value.\n\n\nWHERE column &lt;&gt; value\nNot equals; true if the column is not equal to value.\n\n\nWHERE column &gt; value\nGreater than; true if the column is more than value.\n\n\nWHERE column &lt; value\nLess than; true if the column is less than value.\n\n\nWHERE column BETWEEN value1 AND value2\nTrue if the column is within the range of value1 and value2.\n\n\nWHERE column [NOT] IN (value1, value2, ...)\nTrue if the column is equal to any of multiple values.\n\n\nWHERE column [NOT] LIKE pattern\nTrue if the column matches the SQL pattern.\n\n\nWHERE column IS [NOT] NULL\nTrue if the column is NULL.\n\n\n\n\n\n\nLIKE is case-sensitive\nILIKE is case-insensitive\nWildcards:\n\n% (any string of zero or more characters)\n_ (any single character)\n\ne.g. WHERE column LIKE 'abc%'\n\nOther nuances:\n\nusing ESCAPE to identify escape character\n\ne.g. WHERE column LIKE '%$%%' ESCAPE '$': matches strings that contains %\n\n\n\n\n\n\nSELECT column AS alias\nFROM table_name;\n\nCannot use alias in WHERE clause because order of execution is FROM then WHERE\n\n\n\n\n\nFROM and JOIN\nWHERE\nGROUP BY, then HAVING\nSELECT , then DISTINCT\nORDER BY\nLIMIT and OFFSET\n\n\n\n\n\n\n\n\n\nShort Description\nExample/Syntax\n\n\n\n\nAddition\ncol1 + col2\n\n\nSubtraction\ncol1 - col2\n\n\nMultiplication\ncol1 * col2\n\n\nDivision\ncol1 / col2\n\n\nModulus\ncol1 % col2\n\n\nAbsolute Value\nABS(col)\n\n\nRound to n decimal places\nROUND(col, n)\n\n\nRound up\nCEILING(col)\n\n\nRound down\nFLOOR(col)\n\n\nPower of n\nPOWER(col, n)\n\n\nSquare Root\nSQRT(col)\n\n\nTruncate to n decimal places\nTRUNCATE(col, n)\n\n\nGenerate random number\nRAND()\n\n\n\n\n\n\n\n\n\n\n\n\n\nShort Description\nExample/Syntax\n\n\n\n\nConcatenate strings\nCONCAT(str1, str2, ...) or str1 \\|\\| str2\n\n\nLength of string\nCHAR_LENGTH(str)\n\n\nConvert to lower case\nLOWER(str)\n\n\nConvert to upper case\nUPPER(str)\n\n\nExtract substring\nSUBSTRING(str, start, length)\n\n\nTrim spaces\nTRIM(str)\n\n\nReplace substring\nREPLACE(str, from_str, to_str)\n\n\nPosition of substring\nPOSITION(substring IN str)\n\n\n\n\n\n\n\n\n\nShort Description\nExample/Syntax\n\n\n\n\nCurrent date\nCURRENT_DATE\n\n\nCurrent time\nCURRENT_TIME\n\n\nCurrent date and time\nCURRENT_TIMESTAMP\n\n\nExtract part of date/time\nEXTRACT(part FROM date/time)\n\n\nAdd interval to date/time\ndate/time + INTERVAL\n\n\nSubtract interval from date/time\ndate/time - INTERVAL\n\n\nDifference between dates/times\nDATEDIFF(date1, date2)\n\n\nFormat date/time\nFORMAT(date/time, format)\n\n\n\ne.g. SELECT EXTRACT(YEAR FROM CURRENT_DATE);\n\n\n\n\n\n\n\n\n\n\nShort Description\nExample/Syntax\n\n\n\n\nCheck for NULL\ncol IS NULL\n\n\nCheck for non-NULL\ncol IS NOT NULL\n\n\nReplace NULL with specified value\nCOALESCE(col, replace_value)\n\n\nNull-safe equal to operator\ncol1 &lt;=&gt; col2\n\n\nCase statement with NULL handling\nCASE WHEN col IS NULL THEN result ELSE other_result END\n\n\nNull if expression is NULL\nNULLIF(expression, NULL)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nAVG()\nReturns the average value.\n\n\nCOUNT()\nReturns the number of rows.\n\n\nMAX()\nReturns the maximum value.\n\n\nMIN()\nReturns the minimum value.\n\n\nSUM()\nReturns the sum of all or distinct values.\n\n\nVAR()\nReturns the variance of all or distinct values.\n\n\nSTDDEV\nReturns the standard deviation of all or distinct values.\n\n\n\nImportant notes:\n\nCannot use aggregate function with normal column (in SELECT clause) without GROUP BY clause\nAll aggregate functions ignore NULL values except COUNT(*)\nCannot use aggregate function in WHERE clause because order of execution is FROM then WHERE\n\n\n\n\n\nGROUP BY clause is used to group rows with the same values\n\n-- Formal syntax\nSELECT\n    grouping_columns, aggregated_columns\nFROM\n    table1\nWHERE -- filter rows before grouping\n    condition\nGROUP BY -- must be between WHERE and ORDER BY\n    grouping_columns\nHAVING -- Used to filter groups (after grouping)\n    group_condition\nORDER BY\n    grouping_columns\n\n\n\n\nJOIN clause is used to combine rows from two or more tables based on a related column between them\n\nDEFAULT: INNER JOIN\n\n\nSELECT -- columns from both tables\n    t1.column1, t2.column2\nFROM\n    table1 AS t1 -- after using alias, use alias instead of table name\njointype\n    table2 AS t2\nON -- condition to join tables\n    t1.column = t2.column\n\n\n\n\n\n\n\nJoin Type\nDescription\n\n\n\n\nCROSS\nReturns the Cartesian product of the sets of rows from the joined tables (all possible combinations)\n\n\nINNER\nReturns rows when there is a match in both tables. (intersect)\n\n\nNATURAL\nReturns all rows without specifying ON, names have to be the same in both tables.\n\n\nLEFT OUTER\nReturns all rows from the left-hand table, plus any rows in the right-hand table that match the left-hand side.\n\n\nRIGHT OUTER\nReturns all rows from the right-hand table, plus any rows in the left-hand table that match the right-hand side.\n\n\nFULL OUTER\nReturns all rows from both tables, with nulls in place of those rows that have no match in the other table.\n\n\n\n\n\n\n\n\n\nAdd new rows to table, by:\n\ncolumn position:\n    INSERT INTO table_name\n    VALUES\n        (value1, value2, ...),\n        (value1, value2, ...), -- can insert multiple rows at once\n        ...\n\nvalues need to be in the same order as the columns\ndon’t need to specify column names\n\ncolumn name:\nINSERT INTO table_name(column1, column2, ...)\nVALUES (value1, value2, ...)\n\nvalues can be in any order\nneed to specify column names\n\nfrom another table:\nINSERT INTO\n    table_name(column1, column2, ...)\nSELECT *\nFROM other_table\n\n\n\n\n\n\nModify existing rows in table\n\nUPDATE table_name\nSET column1 = value1,\n    column2 = value2,\n    ...\nWHERE condition\n\n\n\n\nRemove rows from table\nremoves all rows but keeps table if no WHERE clause\n\nDELETE FROM table_name\nWHERE condition\n\n\n\n\nRemove all rows from table\nfaster than DELETE because it doesn’t scan every row\n\nalso does not log each row deletion\n\n\nTRUNCATE TABLE table_name\n\n\n\n\n\n\nCREATE TABLE table_name (\n    column1 datatype [constraint] PRIMARY KEY, -- for simple primary key\n    column2 datatype UNIQUE, -- unique constraint\n    column3 TEXT DEFAULT 'default value', -- default value\n    column4 datatype\n        [CONSTRAINT constraint_name] CHECK (condition), -- check constraint\n    -- e.g.\n    name TEXT NOT NULL,\n    phone CHAR(12) CHECK (phone LIKE '___-___-____')\n    ...\n\n    -- table constraints\n    -- if set composite primary key, also can simple\n    [CONSTRAINT constraint_name] PRIMARY KEY (column1, column2, ...),\n);\n\n\n\n\nSimple key: a single column\nComposite key: multiple columns\n\n\n\n\nCan uniquely identify a table row\nMust be minimal: no subset of the candidate key can be a candidate key\nCan have multiple in a table (e.g. id and email)\n\n\n\n\n\nA candidate key that is chosen to be the main identifier of a table\nAutomaticaly unique and not null\nMust be unique and not null\nMust be minimal\n\ngenerally the candidate key with the fewest columns\n\n\n\n\n\n\nA column that references a primary key in another table\nPrevents invalid data from being inserted\nCan be null\nChild table: table with foreign key\nParent table: table with primary key\n\n-- parent table\nCREATE TABLE instructor (\n    id INTEGER PRIMARY KEY,\n    name TEXT NOT NULL,\n    email TEXT UNIQUE,\n);\n\n-- child table\nCREATE TABLE instructor_course (\n    id INTEGER,\n    course_id TEXT,\n    PRIMARY KEY (id, course_id), -- composite primary key\n    FOREIGN KEY (id) REFERENCES instructor (id)\n        ON DELETE CASCADE,\n\n    -- can also specify column name\n    FOREIGN KEY (course_id) REFERENCES instructor (id)\n        ON DELETE CASCADE -- delete child rows when parent row is deleted\n        ON UPDATE CASCADE -- update child rows when child row is updated\n);\n\nForeign key constraint ensures there are no orphaned rows\n\ni.e. all foreign key in child table must exist in parent table\n\n\n\n\n\n\n\n\n\nReferential Action\nDescription\n\n\n\n\nNO ACTION\nDefault. Rejects update or delete of parent row\n\n\nSET NULL\nSets foreign key to null\n\n\nCASCADE\nDeletes or updates child rows when parent row is deleted or updated\n\n\nSET DEFAULT\nSets foreign key to default value\n\n\n\n\n\n\n\n\nUsed to store temporary data\nAutomatically dropped at end of session\nPrivate to current session\nPhysically stored in temp tablespace\n\nCREATE TEMPORARY TABLE table_name (\n    ...\n);\n\n-- can also create from another table\nCREATE TEMPORARY TABLE\n    temp_table_name\nAS\n    SELECT name, department FROM other_table\n;\n\n\n\n\n\nRemove table and all its data\nPostgres does not allow dropping a table if it has a foreign key constraint\n\nneed to drop the foreign key constraint first\nor use CASCADE to drop the table and all its foreign key constraints\n\n\nDROP TABLE table_name [CASCADE];\n\n\n\n\nA transaction is a sequence of SQL statements that are treated as a unit, so either all of them are executed, or none of them are executed.\n\n[{BEGIN|START} [TRANSACTION]]; -- BEGIN more common, can just start  with `BEGIN`\n\n&lt;SQL statements&gt;\n\n{COMMIT|ROLLBACK};\n\nCOMMIT: make all changes made by the transaction permanent\n\nwithout COMMIT, changes are not visible to other users\n\nROLLBACK: undo all changes made by the transaction\n\n\n\nProperties of a transaction:\n\nAtomicity: all or nothing (transaction either completes or is aborted)\nConsistency: database must be in a consistent state before and after the transaction\n\ne.g. no scores &gt; 100, transaction will not be committed if it violates this\n\nIsolation: if two transactions are executed concurrently, the result should be the same as if they were executed one after the other\nDurability: changes made by a committed transaction must be permanent (even if there is a system failure), achieved using transaction log\n\n\n\n\n\n\nA subquery is a query within a query\nNormally used to mix aggregate and non-aggregate queries\nCan only be used if subquery returns ONLY 1 value.\n\n-- Selects countries with a population greater than the average population of all countries\nSELECT\n    name\nFROM\n    country\nWHERE\n    surfacearea &gt; (\n        SELECT AVG(surfacearea) FROM country\n    )\n;\n\n\n\nA correlated subquery is a subquery that uses values from the outer query\n\n-- Selects countries with the largest population in their continent\nSELECT\n    c1.name, c1.continent\nFROM\n    country c1\nWHERE\n    c1.population = (\n        SELECT\n            MAX(c2.population)\n        FROM\n            country c2\n        WHERE\n            c2.continent = c1.continent\n    )\n;\n\n\n\n\nANY and ALL are used to compare a value to a list or subquery\n\n\n\n\nANY\nALL\n\n\n\n\nif one of values true =&gt; true\nif all values true =&gt; true\n\n\n\ne.g.\n-- Find the names of all countries that have a\n-- population greater than all European countries.\nSELECT\n    name\nFROM\n    country\nWHERE\n    continent &lt;&gt; 'Europe'\n    AND\n    population &gt; ALL (\n        SELECT\n            population\n        FROM\n            country\n        WHERE\n            continent = 'Europe'\n    )\n;\n\nEXISTS is used to check if a subquery returns any rows\n\nfaster than IN because it stops as soon as it finds a match\n\n\nSELECT co.name\nFROM country co\nWHERE EXISTS (SELECT * FROM city ci\n        WHERE co.code = ci.countrycode AND ci.population &gt; 5000000);\n        ```\n\n\n\n\n\nIs a named result of a SELECT query\n\nBehaves like a table (called virtual table sometimes)\nCurrent and Dynamic: whenever you query a view, you get the most up-to-date data\nViews persists: they are kept\nNot materialized: they are not stored in the database\nViews can hide complexity\nManage access to data\nDo not support constraints\n\n\nCREATE VIEW view_name AS\n    select_statment\n;\n\nDROP VIEW [IF EXISTS] view_name;\n\n\n\nMaterialized views are stored in the database\nThey are updated periodically\nThey are used for performance reasons (a lot faster)\n\nCREATE MATERIALIZED VIEW my_mat_view AS\n    select_statement\n;\n\nREFRESH MATERIALIZED VIEW [CONCURRENTLY] my_mat_view;\n-- CONCURRENTLY: allows you to query the view while it is being refreshed, no guarantee it is up-to-date\n\nDROP MATERIALIZED VIEW my_mat_view;\n\n\n\n\nDROP TABLE IF EXISTS temp_table_name;\n\nCREATE TEMPORARY TABLE temp_table_name AS (\n    ...\n);\n\n\n\n\nCommon Table Expressions: temporary named result of a query\n\nWITH\n    expression_name [(column_names, ...)]\nAS (\n    query\n)\nquery\n-- SELECT * FROM expression_name;\n-- This query needs to use the column names defined in the CTE after the expression name\n;\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nTemporary Tables\nViews\nMaterialized Views\nCTEs\n\n\n\n\nNature\nPhysical table, session-scoped\nVirtual table\nPhysical storage of query results\nTemporary result set\n\n\nData Storage\nStores data temporarily\nDoes not store data, only stores the query itself\nStores results of a query\nDoes not store data, used within query execution\n\n\nScope\nExists only during a database session\nPermanent, unless dropped\nPermanent, unless dropped\nExists only during the execution of a query\n\n\nAccess\nCan only be accessed by 1 user\nCan be accessed by multiple users\nCan be accessed by multiple users\nCan be accessed by multiple users\n\n\nUsage\nIntermediate data storage, complex queries\nSimplifying access to complex queries, data security\nPerformance improvement for complex queries\nBreaking down complex queries, recursive queries  can manipulate default processing order of SQL clauses\n\n\nPerformance\nDependent on data size and indexes\nExecutes underlying query each time\nFaster access, as data is pre-calculated\nDependent on the complexity of the query\n\n\nUpdates\nData persists for the session, can be updated\nReflects real-time data from base tables\nRequires refresh to update data\nNot applicable (recomputed with each query execution)\n\n\nIndexing\nCan have indexes\nCannot have indexes\nCan have indexes\nNot applicable\n\n\n\n\n\n\n\nWindow functions are used to compute aggregate values over a group of rows\nOnly allowed in SELECT and ORDER BY clauses\nProcessed:\n\nafter WHERE, GROUP BY, and HAVING\nbefore SELECT and ORDER BY\n\n\nSELECT\n    column,\n    window_function_name(expression) OVER (\n        PARTITION BY column\n        ORDER BY column\n        frame_clause\n    )\n    -- e.g.\n    continent,\n    MAX(population)\n        OVER (PARTITION BY continent)\nFROM\n    table\n;\n\n\n\nAggregate functions\n\nAVG, COUNT, MAX, MIN, SUM\n\nRanking functions\n\nCUME_DIST(): returns the cumulative distribution, i.e. the percentage of values less than or equal to the current value.\nNTILE(): given a specified number of buckets, it tells us in which bucket each row goes among other rows in the partition.\nPERCENT_RANK(): similar to CUME_DIST(), but considers only the percentage of values less than (and not equal to) the current value.\nDENSE_RANK(): returns the rank of a row within a partition without jumps after duplicate ranks (e.g. 1, 2, 2, 3, …)\nRANK(): returns the rank of row within a partition but with jumps after duplicates ranks (e.g. 1, 2, 2, 4, …)\nROW_NUMBER(): returns simply the number of a row in a partition, regardless of duplicate values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElements\nRelational\nNon-relational\n\n\n\n\nData Model\ntables\nkey-value pairs, documents, graphs, etc.\n\n\nSchema\nfixed (updates are complicated and time-consuming)\ndynamic (schema-free)\n\n\nQuerying\nsemi standard (SQL)\nproprietary\n\n\nPerformance\nstrong data consistency and integrity\nfaster performance for specific usecases\n\n\nEfficiency\nwrite times and table locks reduce efficiency\nfaster read and write times\n\n\nScalability\nvertical scaling\nhorizontal scaling\n\n\nDevelopment\nrequire more effort\neasier to develop and require fewer resources\n\n\nPrinciples\nACID (Atomicity, Consistency, Isolation, Durability)\nBASE (Basically Available, Soft state, Eventual consistency)\n\n\n\n\n\n\n\nMain goals:\n\nreduce the dependence on fixed schema\nreduce the dependence on a single point of truth\nscaling\n\n\n\n\n\nSchema-free\nAccepting Basic Availability\n\nusually see all tweets, but sometimes a refresh shows new tweets\n\nBASE:\n\nBasically Available\nSoft state\nEventual consistent\n\n\n\n\n\n\nKey-value stores: like a dictionary\n\nRedis, Memcached, Amazon DynamoDB\n\nDocument stores: type of key-value store but with an internal searchable structure.\n\nDocument: contains all the relevant data and has a unique key\nMongoDB, CouchDB\n\nColumn stores: based on Google’s BigTable\n\nCassandra, HBase, Google BigTable\n\nGraph databases: based on graph theory\n\nNeo4j, OrientDB, Amazon Neptune\n\n\n\n\n\n\n\nBased on JSON-like (JavaScript Object Notation) documents for storage:\n\nActually it is BSON (Binary JSON)\nMax BSON document size: 16MB\n\nStructure:\n\nDatabase: client.get_database_names()\nCollection: client[db_name].get_collection_names()\nDocument: client[db_name][collection_name].find_one() or client[db_name][collection_name].find()\n\n\n\n\n\nclient[db_name][collection_name].find()\n\nreturns a cursor\nlist(client[db_name][collection_name].find()) returns a list of documents\nnext(client[db_name][collection_name].find()) returns the next document\n\n\nclient = MongoClient()\nclient[db_name][collection_name].find(\n    filter={...},\n    projection={...},\n    sort=[...],\n    skip=...,\n    limit=...,\n)\n\n\n\nfilter is a dictionary\nfilter = {\"key\": \"value\"} returns all documents where key is value\n\n\n\n\n\n\n\n\n\nOperator\nsymbol\nnotes\n\n\n\n\n$eq or $neq\n= or ~=\n\n\n\n$gt or $gte\n&gt; or &gt;=\n\n\n\n$lt or $lte\n&lt; or &lt;=\n\n\n\n$ne\n!=\n\n\n\n$in\nin\nif any of the values in the list matches the value of the key, the document is returned\n\n\n$nin\nnot in\n\n\n\n$all\nall\nall values in the list must match the value of the keys\n\n\n$exists\nexists\n\n\n\n$regex\nregex\n\n\n\n\n\nAlso can use $not to negate the operator\nusing $or, $nor and $and:\n\nfilter = {\n    \"$or\": [\n        {\"key_1\": \"value_1\"},\n        {\"key_2\": \"value_2\"},\n    ],\n    \"$and\": [\n        {\"key_3\": \"value_3\"},\n        {\"key_4\": \"value_4\"},\n    ],\n    \"key_5\": {\"$gte\": 10, \"$lte\": 20}, # and is implied\n}\n\nfilter = {\n  \"key_1\": \"value_1\",\n  \"key_2\": \"value_2\", # and is implied\n}\n\n\n\n\nprojection is a dictionary\nprojection = {\"key_1\": 1, \"key_2\": 0} returns all documents with only key_1 and without key_2\n\n\n\n\n\nsort is a list of tuples\n\n1: ascending order\n-1: descending order\n\nsort = [(\"key_1\", 1), (\"key_2\", -1)] sorts by key_1 in ascending order and then by key_2 in descending order\n\n\n\n\n\nlimit is an integer\nlimit = 10 returns the first 10 documents\n\n\n\n\n\nskip is an integer\nskip = 10 skips the first 10 documents\n\n\n\n\n\ncount_documents is a method\nclient[db_name][collection_name].count_documents(filter={...}) returns the number of documents that match the filter\n\n\n\n\n\ndistinct is a method\nclient[db_name][collection_name].distinct(\"key_1\") returns a list of distinct values for key_1"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#introduction",
    "href": "block_3/513_databases/513_databases.html#introduction",
    "title": "Relational Databases",
    "section": "",
    "text": "What is a database?\n\nOrganized collection of related data\n\nWhart is a database management system (DBMS)?\n\nCollection of programs that enables users to create and maintain a database\nallows users to create, query, modify, and manage\n\n\nDATABASE != DBMS\n\n\n\nEfficiency: Data is stored efficiently.\nIntegrity: Data is consistent and correct.\nSecurity: Data is safe from unauthorized access.\nConcurrent Access: Multiple users can access the same data at the same time.\nCrash Recovery: Data is safe from crashes.\nIndependence: Data is independent of the programs that use it."
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#relational-data-model",
    "href": "block_3/513_databases/513_databases.html#relational-data-model",
    "title": "Relational Databases",
    "section": "",
    "text": "Works with entities and relationships\nEntity: a thing or object in the real world that is distinguishable from other objects\n\ne.g. students in a school\n\nRelationship: an association among entities\n\ne.g. a student is enrolled in a course"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#relational-database",
    "href": "block_3/513_databases/513_databases.html#relational-database",
    "title": "Relational Databases",
    "section": "",
    "text": "Collection of relations\nA relation is an instance of a relation schema (similar to object = instance of a class)\nRelation Schema specifies:\n\nName of relation\nName and domain of each attribute\n\nDomain: set of constraints that determines the type, length, format, range, uniqueness and nullability of values stored for an attribute"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#query-language-in-a-dbms",
    "href": "block_3/513_databases/513_databases.html#query-language-in-a-dbms",
    "title": "Relational Databases",
    "section": "",
    "text": "Query: a request for information from a database. Results in a relation.\nStructured Query Language (SQL): standard programming language for managing and manipulating databases.\nCan be categorized into:\n\nData Definition Language (DDL): used to define the database structure or schema.\nData Manipulation Language (DML): used to read, insert, delete, and modify data.\nData Query Language (DQL): used to retrieve data from a database.\nData Control Language (DCL): used to control access to data stored in a database."
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#postgresql",
    "href": "block_3/513_databases/513_databases.html#postgresql",
    "title": "Relational Databases",
    "section": "",
    "text": "Specific flavor of SQL and DBMS\nopen-source, cross-platform DBMS that implements the relational model\nreliable, robust, and feature-rich\n\n\n\n\n\n\nCommand\nUsage\n\n\n\n\n\\l\nList all databases\n\n\n\\c\nConnect to a database\n\n\n\\d\nDescribe tables and views\n\n\n\\dt\nList tables\n\n\n\\dt+\nList tables with additional info\n\n\n\\d+\nList tables and views with additional info\n\n\n\\!\nExecute shell commands\n\n\n\\cd\nChange directory\n\n\n\\i\nExecute commands from a file\n\n\n\\h\nView help on SQL commands\n\n\n\\?\nView help on psql meta commands\n\n\n\\q\nQuit interactive shell\n\n\n\n\n\n\n\nTo install: pip install ipython-sql\nTo load: %load_ext sql\n\n# Login to database\nimport json\nimport urllib.parse\n\n# use credentials.json to login (not included in repo)\nwith open('data/credentials.json') as f:\n    login = json.load(f)\n\nusername = login['user']\npassword = urllib.parse.quote(login['password'])\nhost = login['host']\nport = login['port']\n\nEstablish connection:\n\n%sql postgresql://{username}:{password}@{host}:{port}/world\n\nRun queries:\n\noutput = %sql SELECT name, population FROM country;\nor\n%%sql output &lt;&lt; # Not a pandas dataframe\nSELECT\n  name, population\nFROM\n  country\n;\n\n# convert to pandas dataframe\ndf = output.DataFrame()\n\nSet configurations:\n\n%config SqlMagic.displaylimit = 20"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#sql-basic-commands",
    "href": "block_3/513_databases/513_databases.html#sql-basic-commands",
    "title": "Relational Databases",
    "section": "",
    "text": "taken from https://www.sqltutorial.org/sql-cheat-sheet/*\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription / Example\n\n\n\n\nSELECT c1, c2FROM t;\nQuery data in columns c1, c2 from a table\n\n\nSELECT *FROM t;\nQuery all rows and columns from a table\n\n\nSELECT c1, c2FROM tWHERE condition;\nQuery data and filter rows with a condition\n\n\nSELECT DISTINCT c1FROM tWHERE condition;\nQuery distinct rows from a table\n\n\nSELECT COUNT(DISTINCT (c1, c2))FROM t;\nCount distinct rows in a table\n\n\nSELECT c1, c2FROM tORDER BY c1 ASC [DESC];\nSort the result set in ascending or descending order\n\n\nSELECT c1, c2FROM tORDER BY c1LIMIT n OFFSET offset;\nSkip offset of rows and return the next n rows\n\n\nSELECT c1, aggregate(c2)FROM tGROUP BY c1;\nGroup rows using an aggregate function\n\n\nSELECT c1, aggregate(c2)FROM tGROUP BY c1HAVING condition;\nFilter groups using HAVING clause\n\n\nSELECT CONCAT(c1, c2)FROM t;\nConcatenate two or more strings\n\n\n\nNotes:\n\nConditions: =, &gt;=, &lt;=, IN ('a','b'), IS NULL, …\nStrings must be enclosed in single quotes '...'\nAggregate functions: AVG, COUNT, MAX, MIN, SUM, ROUND(value, decimal_places)\nNeed decimal point to prevent integer division\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription / Example\n\n\n\n\nSELECT c1, c2FROM t1INNER JOIN t2 ON condition;\nInner join t1 and t2\n\n\nSELECT c1, c2FROM t1LEFT JOIN t2 ON condition;\nLeft join t1 and t2\n\n\nSELECT c1, c2FROM t1RIGHT JOIN t2 ON condition;\nRight join t1 and t2\n\n\nSELECT c1, c2FROM t1FULL OUTER JOIN t2 ON condition;\nPerform full outer join\n\n\nSELECT c1, c2FROM t1CROSS JOIN t2;\nProduce a Cartesian product of rows in tables\n\n\nSELECT c1, c2FROM t1 AINNER JOIN t2 B ON condition;\nJoin t1 to itself using INNER JOIN clause\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription / Example\n\n\n\n\nSELECT c1, c2FROM t1UNION [ALL]SELECT c1, c2 FROM t2;\nCombine rows from two queries\n\n\nSELECT c1, c2FROM t1INTERSECTSELECT c1, c2 FROM t2;\nReturn the intersection of two queries\n\n\nSELECT c1, c2FROM t1MINUSSELECT c1, c2 FROM t2;\nSubtract a result set from another result set\n\n\nSELECT c1, c2FROM tWHERE c1 [NOT] LIKE pattern;\nQuery rows using pattern matching % _\n\n\nSELECT c1, c2FROM tWHERE c1 [NOT] IN value_list;\nQuery rows in a list\n\n\nSELECT c1, c2FROM tWHERE c1 BETWEEN low AND high;\nQuery rows between two values\n\n\nSELECT c1, c2FROM tWHERE c1 IS [NOT] NULL;\nCheck if values in a table is NULL or not\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription / Example\n\n\n\n\nCREATE TABLE t (id INT PRIMARY KEY,name VARCHAR NOT NULL,price INT DEFAULT 0);\nCreate a new table with three columns\n\n\nDROP TABLE t;\nDelete the table from the database\n\n\nALTER TABLE t ADD column;\nAdd a new column to the table\n\n\nALTER TABLE t DROP COLUMN c;\nDrop column c from the"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#database-data-types",
    "href": "block_3/513_databases/513_databases.html#database-data-types",
    "title": "Relational Databases",
    "section": "",
    "text": "PostgreSQL supports many data types. The most common ones are:\n\nBoolean, BOOLEAN or BOOL\n\nTrue: TRUE, t, true, y, yes, on, 1\n\nCharacters\n\nCHAR(n): string of n characters (pad with spaces)\nVARCHAR(n): string of up to n characters\nTEXT: PostgreSQL-specific type for storing strings of any length\n\nDateTime: date and time\n\nDATE: date (YYYY-MM-DD)\n\nCURRENT_DATE: current date\n\nTIME: time\nTIMESTAMP: date and time\nTIMESTAMPTZ: date and time with timezone\n\nBinary: binary data\nNumbers"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#vs",
    "href": "block_3/513_databases/513_databases.html#vs",
    "title": "Relational Databases",
    "section": "",
    "text": "\" is used for identifiers (e.g. table names, column names)\n' is used for strings.\n\n\n\n\n\n\n\n\n\n\n\n\nName\nStorage Size\nDescription\nRange\n\n\n\n\nsmallint\n2 bytes\nsmall-range integer\n-32,768 to +32,767\n\n\ninteger\n4 bytes\ntypical choice for integer\n-2,147,483,648 to +2,147,483,647\n\n\nbigint\n8 bytes\nlarge-range integer\n-9,223,372,036,854,775,808 to +9,223,372,036,854,775,807\n\n\nserial\n4 bytes\nauto-incrementing integer\n1 to 2,147,483,647\n\n\nbigserial\n8 bytes\nlarge auto-incrementing integer\n1 to 9,223,372,036,854,775,807\n\n\n\nNote: serial and bigserial are not true types, but merely a notational convenience for creating unique identifier columns.\nFloating-Point Numbers\n\n\n\n\n\n\n\n\n\nName\nStorage Size\nDescription\nRange\n\n\n\n\nreal\n4 bytes\nvariable-precision, inexact\nat least 6 decimal digits (implementation dependent)\n\n\ndouble precision\n8 bytes\nvariable-precision, inexact\nat least 15 decimal digits (implementation dependent)\n\n\n\nArbitrary Precision Numbers\n\n\n\n\n\n\n\n\n\nName\nStorage Size\nDescription\nRange\n\n\n\n\nnumeric or decimal\nvariable\nuser-specified precision, exact\n131072 digits before and 16383 digits after the decimal point\n\n\n\nNote: numeric and decimal are the same type in Postgres.\n\n\n\n\nCan do : &lt;col&gt;::&lt;data_type&gt; or CAST(&lt;col&gt; AS &lt;data_type&gt;)\ne.g. SELECT '123'::integer;\ne.g. SELECT CAST('123' AS integer);\n\n\n\n\nSELECT *\nFROM table_name\nWHERE [NOT]\n    condition\n    AND/OR\n    condition;\n\n\n\n\n\n\n\nCondition Example\nDescription\n\n\n\n\nWHERE column = value\nEquals; returns true if the column equals the value.\n\n\nWHERE column &lt;&gt; value\nNot equals; true if the column is not equal to value.\n\n\nWHERE column &gt; value\nGreater than; true if the column is more than value.\n\n\nWHERE column &lt; value\nLess than; true if the column is less than value.\n\n\nWHERE column BETWEEN value1 AND value2\nTrue if the column is within the range of value1 and value2.\n\n\nWHERE column [NOT] IN (value1, value2, ...)\nTrue if the column is equal to any of multiple values.\n\n\nWHERE column [NOT] LIKE pattern\nTrue if the column matches the SQL pattern.\n\n\nWHERE column IS [NOT] NULL\nTrue if the column is NULL.\n\n\n\n\n\n\nLIKE is case-sensitive\nILIKE is case-insensitive\nWildcards:\n\n% (any string of zero or more characters)\n_ (any single character)\n\ne.g. WHERE column LIKE 'abc%'\n\nOther nuances:\n\nusing ESCAPE to identify escape character\n\ne.g. WHERE column LIKE '%$%%' ESCAPE '$': matches strings that contains %\n\n\n\n\n\n\nSELECT column AS alias\nFROM table_name;\n\nCannot use alias in WHERE clause because order of execution is FROM then WHERE\n\n\n\n\n\nFROM and JOIN\nWHERE\nGROUP BY, then HAVING\nSELECT , then DISTINCT\nORDER BY\nLIMIT and OFFSET\n\n\n\n\n\n\n\n\n\nShort Description\nExample/Syntax\n\n\n\n\nAddition\ncol1 + col2\n\n\nSubtraction\ncol1 - col2\n\n\nMultiplication\ncol1 * col2\n\n\nDivision\ncol1 / col2\n\n\nModulus\ncol1 % col2\n\n\nAbsolute Value\nABS(col)\n\n\nRound to n decimal places\nROUND(col, n)\n\n\nRound up\nCEILING(col)\n\n\nRound down\nFLOOR(col)\n\n\nPower of n\nPOWER(col, n)\n\n\nSquare Root\nSQRT(col)\n\n\nTruncate to n decimal places\nTRUNCATE(col, n)\n\n\nGenerate random number\nRAND()\n\n\n\n\n\n\n\n\n\n\n\n\n\nShort Description\nExample/Syntax\n\n\n\n\nConcatenate strings\nCONCAT(str1, str2, ...) or str1 \\|\\| str2\n\n\nLength of string\nCHAR_LENGTH(str)\n\n\nConvert to lower case\nLOWER(str)\n\n\nConvert to upper case\nUPPER(str)\n\n\nExtract substring\nSUBSTRING(str, start, length)\n\n\nTrim spaces\nTRIM(str)\n\n\nReplace substring\nREPLACE(str, from_str, to_str)\n\n\nPosition of substring\nPOSITION(substring IN str)\n\n\n\n\n\n\n\n\n\nShort Description\nExample/Syntax\n\n\n\n\nCurrent date\nCURRENT_DATE\n\n\nCurrent time\nCURRENT_TIME\n\n\nCurrent date and time\nCURRENT_TIMESTAMP\n\n\nExtract part of date/time\nEXTRACT(part FROM date/time)\n\n\nAdd interval to date/time\ndate/time + INTERVAL\n\n\nSubtract interval from date/time\ndate/time - INTERVAL\n\n\nDifference between dates/times\nDATEDIFF(date1, date2)\n\n\nFormat date/time\nFORMAT(date/time, format)\n\n\n\ne.g. SELECT EXTRACT(YEAR FROM CURRENT_DATE);\n\n\n\n\n\n\n\n\n\n\nShort Description\nExample/Syntax\n\n\n\n\nCheck for NULL\ncol IS NULL\n\n\nCheck for non-NULL\ncol IS NOT NULL\n\n\nReplace NULL with specified value\nCOALESCE(col, replace_value)\n\n\nNull-safe equal to operator\ncol1 &lt;=&gt; col2\n\n\nCase statement with NULL handling\nCASE WHEN col IS NULL THEN result ELSE other_result END\n\n\nNull if expression is NULL\nNULLIF(expression, NULL)"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#aggregate-functions",
    "href": "block_3/513_databases/513_databases.html#aggregate-functions",
    "title": "Relational Databases",
    "section": "",
    "text": "Function\nDescription\n\n\n\n\nAVG()\nReturns the average value.\n\n\nCOUNT()\nReturns the number of rows.\n\n\nMAX()\nReturns the maximum value.\n\n\nMIN()\nReturns the minimum value.\n\n\nSUM()\nReturns the sum of all or distinct values.\n\n\nVAR()\nReturns the variance of all or distinct values.\n\n\nSTDDEV\nReturns the standard deviation of all or distinct values.\n\n\n\nImportant notes:\n\nCannot use aggregate function with normal column (in SELECT clause) without GROUP BY clause\nAll aggregate functions ignore NULL values except COUNT(*)\nCannot use aggregate function in WHERE clause because order of execution is FROM then WHERE"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#grouping",
    "href": "block_3/513_databases/513_databases.html#grouping",
    "title": "Relational Databases",
    "section": "",
    "text": "GROUP BY clause is used to group rows with the same values\n\n-- Formal syntax\nSELECT\n    grouping_columns, aggregated_columns\nFROM\n    table1\nWHERE -- filter rows before grouping\n    condition\nGROUP BY -- must be between WHERE and ORDER BY\n    grouping_columns\nHAVING -- Used to filter groups (after grouping)\n    group_condition\nORDER BY\n    grouping_columns"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#joining-tables",
    "href": "block_3/513_databases/513_databases.html#joining-tables",
    "title": "Relational Databases",
    "section": "",
    "text": "JOIN clause is used to combine rows from two or more tables based on a related column between them\n\nDEFAULT: INNER JOIN\n\n\nSELECT -- columns from both tables\n    t1.column1, t2.column2\nFROM\n    table1 AS t1 -- after using alias, use alias instead of table name\njointype\n    table2 AS t2\nON -- condition to join tables\n    t1.column = t2.column\n\n\n\n\n\n\n\nJoin Type\nDescription\n\n\n\n\nCROSS\nReturns the Cartesian product of the sets of rows from the joined tables (all possible combinations)\n\n\nINNER\nReturns rows when there is a match in both tables. (intersect)\n\n\nNATURAL\nReturns all rows without specifying ON, names have to be the same in both tables.\n\n\nLEFT OUTER\nReturns all rows from the left-hand table, plus any rows in the right-hand table that match the left-hand side.\n\n\nRIGHT OUTER\nReturns all rows from the right-hand table, plus any rows in the left-hand table that match the right-hand side.\n\n\nFULL OUTER\nReturns all rows from both tables, with nulls in place of those rows that have no match in the other table."
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#data-manipulation",
    "href": "block_3/513_databases/513_databases.html#data-manipulation",
    "title": "Relational Databases",
    "section": "",
    "text": "Add new rows to table, by:\n\ncolumn position:\n    INSERT INTO table_name\n    VALUES\n        (value1, value2, ...),\n        (value1, value2, ...), -- can insert multiple rows at once\n        ...\n\nvalues need to be in the same order as the columns\ndon’t need to specify column names\n\ncolumn name:\nINSERT INTO table_name(column1, column2, ...)\nVALUES (value1, value2, ...)\n\nvalues can be in any order\nneed to specify column names\n\nfrom another table:\nINSERT INTO\n    table_name(column1, column2, ...)\nSELECT *\nFROM other_table\n\n\n\n\n\n\nModify existing rows in table\n\nUPDATE table_name\nSET column1 = value1,\n    column2 = value2,\n    ...\nWHERE condition\n\n\n\n\nRemove rows from table\nremoves all rows but keeps table if no WHERE clause\n\nDELETE FROM table_name\nWHERE condition\n\n\n\n\nRemove all rows from table\nfaster than DELETE because it doesn’t scan every row\n\nalso does not log each row deletion\n\n\nTRUNCATE TABLE table_name"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#creating-altering-and-dropping-tables",
    "href": "block_3/513_databases/513_databases.html#creating-altering-and-dropping-tables",
    "title": "Relational Databases",
    "section": "",
    "text": "CREATE TABLE table_name (\n    column1 datatype [constraint] PRIMARY KEY, -- for simple primary key\n    column2 datatype UNIQUE, -- unique constraint\n    column3 TEXT DEFAULT 'default value', -- default value\n    column4 datatype\n        [CONSTRAINT constraint_name] CHECK (condition), -- check constraint\n    -- e.g.\n    name TEXT NOT NULL,\n    phone CHAR(12) CHECK (phone LIKE '___-___-____')\n    ...\n\n    -- table constraints\n    -- if set composite primary key, also can simple\n    [CONSTRAINT constraint_name] PRIMARY KEY (column1, column2, ...),\n);\n\n\n\n\nSimple key: a single column\nComposite key: multiple columns\n\n\n\n\nCan uniquely identify a table row\nMust be minimal: no subset of the candidate key can be a candidate key\nCan have multiple in a table (e.g. id and email)\n\n\n\n\n\nA candidate key that is chosen to be the main identifier of a table\nAutomaticaly unique and not null\nMust be unique and not null\nMust be minimal\n\ngenerally the candidate key with the fewest columns\n\n\n\n\n\n\nA column that references a primary key in another table\nPrevents invalid data from being inserted\nCan be null\nChild table: table with foreign key\nParent table: table with primary key\n\n-- parent table\nCREATE TABLE instructor (\n    id INTEGER PRIMARY KEY,\n    name TEXT NOT NULL,\n    email TEXT UNIQUE,\n);\n\n-- child table\nCREATE TABLE instructor_course (\n    id INTEGER,\n    course_id TEXT,\n    PRIMARY KEY (id, course_id), -- composite primary key\n    FOREIGN KEY (id) REFERENCES instructor (id)\n        ON DELETE CASCADE,\n\n    -- can also specify column name\n    FOREIGN KEY (course_id) REFERENCES instructor (id)\n        ON DELETE CASCADE -- delete child rows when parent row is deleted\n        ON UPDATE CASCADE -- update child rows when child row is updated\n);\n\nForeign key constraint ensures there are no orphaned rows\n\ni.e. all foreign key in child table must exist in parent table\n\n\n\n\n\n\n\n\n\nReferential Action\nDescription\n\n\n\n\nNO ACTION\nDefault. Rejects update or delete of parent row\n\n\nSET NULL\nSets foreign key to null\n\n\nCASCADE\nDeletes or updates child rows when parent row is deleted or updated\n\n\nSET DEFAULT\nSets foreign key to default value\n\n\n\n\n\n\n\n\nUsed to store temporary data\nAutomatically dropped at end of session\nPrivate to current session\nPhysically stored in temp tablespace\n\nCREATE TEMPORARY TABLE table_name (\n    ...\n);\n\n-- can also create from another table\nCREATE TEMPORARY TABLE\n    temp_table_name\nAS\n    SELECT name, department FROM other_table\n;"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#drop-table",
    "href": "block_3/513_databases/513_databases.html#drop-table",
    "title": "Relational Databases",
    "section": "",
    "text": "Remove table and all its data\nPostgres does not allow dropping a table if it has a foreign key constraint\n\nneed to drop the foreign key constraint first\nor use CASCADE to drop the table and all its foreign key constraints\n\n\nDROP TABLE table_name [CASCADE];"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#transaction",
    "href": "block_3/513_databases/513_databases.html#transaction",
    "title": "Relational Databases",
    "section": "",
    "text": "A transaction is a sequence of SQL statements that are treated as a unit, so either all of them are executed, or none of them are executed.\n\n[{BEGIN|START} [TRANSACTION]]; -- BEGIN more common, can just start  with `BEGIN`\n\n&lt;SQL statements&gt;\n\n{COMMIT|ROLLBACK};\n\nCOMMIT: make all changes made by the transaction permanent\n\nwithout COMMIT, changes are not visible to other users\n\nROLLBACK: undo all changes made by the transaction\n\n\n\nProperties of a transaction:\n\nAtomicity: all or nothing (transaction either completes or is aborted)\nConsistency: database must be in a consistent state before and after the transaction\n\ne.g. no scores &gt; 100, transaction will not be committed if it violates this\n\nIsolation: if two transactions are executed concurrently, the result should be the same as if they were executed one after the other\nDurability: changes made by a committed transaction must be permanent (even if there is a system failure), achieved using transaction log"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#subqueries",
    "href": "block_3/513_databases/513_databases.html#subqueries",
    "title": "Relational Databases",
    "section": "",
    "text": "A subquery is a query within a query\nNormally used to mix aggregate and non-aggregate queries\nCan only be used if subquery returns ONLY 1 value.\n\n-- Selects countries with a population greater than the average population of all countries\nSELECT\n    name\nFROM\n    country\nWHERE\n    surfacearea &gt; (\n        SELECT AVG(surfacearea) FROM country\n    )\n;\n\n\n\nA correlated subquery is a subquery that uses values from the outer query\n\n-- Selects countries with the largest population in their continent\nSELECT\n    c1.name, c1.continent\nFROM\n    country c1\nWHERE\n    c1.population = (\n        SELECT\n            MAX(c2.population)\n        FROM\n            country c2\n        WHERE\n            c2.continent = c1.continent\n    )\n;\n\n\n\n\nANY and ALL are used to compare a value to a list or subquery\n\n\n\n\nANY\nALL\n\n\n\n\nif one of values true =&gt; true\nif all values true =&gt; true\n\n\n\ne.g.\n-- Find the names of all countries that have a\n-- population greater than all European countries.\nSELECT\n    name\nFROM\n    country\nWHERE\n    continent &lt;&gt; 'Europe'\n    AND\n    population &gt; ALL (\n        SELECT\n            population\n        FROM\n            country\n        WHERE\n            continent = 'Europe'\n    )\n;\n\nEXISTS is used to check if a subquery returns any rows\n\nfaster than IN because it stops as soon as it finds a match\n\n\nSELECT co.name\nFROM country co\nWHERE EXISTS (SELECT * FROM city ci\n        WHERE co.code = ci.countrycode AND ci.population &gt; 5000000);\n        ```"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#views",
    "href": "block_3/513_databases/513_databases.html#views",
    "title": "Relational Databases",
    "section": "",
    "text": "Is a named result of a SELECT query\n\nBehaves like a table (called virtual table sometimes)\nCurrent and Dynamic: whenever you query a view, you get the most up-to-date data\nViews persists: they are kept\nNot materialized: they are not stored in the database\nViews can hide complexity\nManage access to data\nDo not support constraints\n\n\nCREATE VIEW view_name AS\n    select_statment\n;\n\nDROP VIEW [IF EXISTS] view_name;\n\n\n\nMaterialized views are stored in the database\nThey are updated periodically\nThey are used for performance reasons (a lot faster)\n\nCREATE MATERIALIZED VIEW my_mat_view AS\n    select_statement\n;\n\nREFRESH MATERIALIZED VIEW [CONCURRENTLY] my_mat_view;\n-- CONCURRENTLY: allows you to query the view while it is being refreshed, no guarantee it is up-to-date\n\nDROP MATERIALIZED VIEW my_mat_view;"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#temporary-tables",
    "href": "block_3/513_databases/513_databases.html#temporary-tables",
    "title": "Relational Databases",
    "section": "",
    "text": "DROP TABLE IF EXISTS temp_table_name;\n\nCREATE TEMPORARY TABLE temp_table_name AS (\n    ...\n);"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#ctes",
    "href": "block_3/513_databases/513_databases.html#ctes",
    "title": "Relational Databases",
    "section": "",
    "text": "Common Table Expressions: temporary named result of a query\n\nWITH\n    expression_name [(column_names, ...)]\nAS (\n    query\n)\nquery\n-- SELECT * FROM expression_name;\n-- This query needs to use the column names defined in the CTE after the expression name\n;"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#summary-tt-vs-views-vs-ctes",
    "href": "block_3/513_databases/513_databases.html#summary-tt-vs-views-vs-ctes",
    "title": "Relational Databases",
    "section": "",
    "text": "Feature\nTemporary Tables\nViews\nMaterialized Views\nCTEs\n\n\n\n\nNature\nPhysical table, session-scoped\nVirtual table\nPhysical storage of query results\nTemporary result set\n\n\nData Storage\nStores data temporarily\nDoes not store data, only stores the query itself\nStores results of a query\nDoes not store data, used within query execution\n\n\nScope\nExists only during a database session\nPermanent, unless dropped\nPermanent, unless dropped\nExists only during the execution of a query\n\n\nAccess\nCan only be accessed by 1 user\nCan be accessed by multiple users\nCan be accessed by multiple users\nCan be accessed by multiple users\n\n\nUsage\nIntermediate data storage, complex queries\nSimplifying access to complex queries, data security\nPerformance improvement for complex queries\nBreaking down complex queries, recursive queries  can manipulate default processing order of SQL clauses\n\n\nPerformance\nDependent on data size and indexes\nExecutes underlying query each time\nFaster access, as data is pre-calculated\nDependent on the complexity of the query\n\n\nUpdates\nData persists for the session, can be updated\nReflects real-time data from base tables\nRequires refresh to update data\nNot applicable (recomputed with each query execution)\n\n\nIndexing\nCan have indexes\nCannot have indexes\nCan have indexes\nNot applicable"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#window-functions",
    "href": "block_3/513_databases/513_databases.html#window-functions",
    "title": "Relational Databases",
    "section": "",
    "text": "Window functions are used to compute aggregate values over a group of rows\nOnly allowed in SELECT and ORDER BY clauses\nProcessed:\n\nafter WHERE, GROUP BY, and HAVING\nbefore SELECT and ORDER BY\n\n\nSELECT\n    column,\n    window_function_name(expression) OVER (\n        PARTITION BY column\n        ORDER BY column\n        frame_clause\n    )\n    -- e.g.\n    continent,\n    MAX(population)\n        OVER (PARTITION BY continent)\nFROM\n    table\n;\n\n\n\nAggregate functions\n\nAVG, COUNT, MAX, MIN, SUM\n\nRanking functions\n\nCUME_DIST(): returns the cumulative distribution, i.e. the percentage of values less than or equal to the current value.\nNTILE(): given a specified number of buckets, it tells us in which bucket each row goes among other rows in the partition.\nPERCENT_RANK(): similar to CUME_DIST(), but considers only the percentage of values less than (and not equal to) the current value.\nDENSE_RANK(): returns the rank of a row within a partition without jumps after duplicate ranks (e.g. 1, 2, 2, 3, …)\nRANK(): returns the rank of row within a partition but with jumps after duplicates ranks (e.g. 1, 2, 2, 4, …)\nROW_NUMBER(): returns simply the number of a row in a partition, regardless of duplicate values"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#relational-vs.-non-relational-databases",
    "href": "block_3/513_databases/513_databases.html#relational-vs.-non-relational-databases",
    "title": "Relational Databases",
    "section": "",
    "text": "Elements\nRelational\nNon-relational\n\n\n\n\nData Model\ntables\nkey-value pairs, documents, graphs, etc.\n\n\nSchema\nfixed (updates are complicated and time-consuming)\ndynamic (schema-free)\n\n\nQuerying\nsemi standard (SQL)\nproprietary\n\n\nPerformance\nstrong data consistency and integrity\nfaster performance for specific usecases\n\n\nEfficiency\nwrite times and table locks reduce efficiency\nfaster read and write times\n\n\nScalability\nvertical scaling\nhorizontal scaling\n\n\nDevelopment\nrequire more effort\neasier to develop and require fewer resources\n\n\nPrinciples\nACID (Atomicity, Consistency, Isolation, Durability)\nBASE (Basically Available, Soft state, Eventual consistency)"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#nosql-not-only-sql",
    "href": "block_3/513_databases/513_databases.html#nosql-not-only-sql",
    "title": "Relational Databases",
    "section": "",
    "text": "Main goals:\n\nreduce the dependence on fixed schema\nreduce the dependence on a single point of truth\nscaling\n\n\n\n\n\nSchema-free\nAccepting Basic Availability\n\nusually see all tweets, but sometimes a refresh shows new tweets\n\nBASE:\n\nBasically Available\nSoft state\nEventual consistent\n\n\n\n\n\n\nKey-value stores: like a dictionary\n\nRedis, Memcached, Amazon DynamoDB\n\nDocument stores: type of key-value store but with an internal searchable structure.\n\nDocument: contains all the relevant data and has a unique key\nMongoDB, CouchDB\n\nColumn stores: based on Google’s BigTable\n\nCassandra, HBase, Google BigTable\n\nGraph databases: based on graph theory\n\nNeo4j, OrientDB, Amazon Neptune"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#mongodb",
    "href": "block_3/513_databases/513_databases.html#mongodb",
    "title": "Relational Databases",
    "section": "",
    "text": "Based on JSON-like (JavaScript Object Notation) documents for storage:\n\nActually it is BSON (Binary JSON)\nMax BSON document size: 16MB\n\nStructure:\n\nDatabase: client.get_database_names()\nCollection: client[db_name].get_collection_names()\nDocument: client[db_name][collection_name].find_one() or client[db_name][collection_name].find()\n\n\n\n\n\nclient[db_name][collection_name].find()\n\nreturns a cursor\nlist(client[db_name][collection_name].find()) returns a list of documents\nnext(client[db_name][collection_name].find()) returns the next document\n\n\nclient = MongoClient()\nclient[db_name][collection_name].find(\n    filter={...},\n    projection={...},\n    sort=[...],\n    skip=...,\n    limit=...,\n)\n\n\n\nfilter is a dictionary\nfilter = {\"key\": \"value\"} returns all documents where key is value\n\n\n\n\n\n\n\n\n\nOperator\nsymbol\nnotes\n\n\n\n\n$eq or $neq\n= or ~=\n\n\n\n$gt or $gte\n&gt; or &gt;=\n\n\n\n$lt or $lte\n&lt; or &lt;=\n\n\n\n$ne\n!=\n\n\n\n$in\nin\nif any of the values in the list matches the value of the key, the document is returned\n\n\n$nin\nnot in\n\n\n\n$all\nall\nall values in the list must match the value of the keys\n\n\n$exists\nexists\n\n\n\n$regex\nregex\n\n\n\n\n\nAlso can use $not to negate the operator\nusing $or, $nor and $and:\n\nfilter = {\n    \"$or\": [\n        {\"key_1\": \"value_1\"},\n        {\"key_2\": \"value_2\"},\n    ],\n    \"$and\": [\n        {\"key_3\": \"value_3\"},\n        {\"key_4\": \"value_4\"},\n    ],\n    \"key_5\": {\"$gte\": 10, \"$lte\": 20}, # and is implied\n}\n\nfilter = {\n  \"key_1\": \"value_1\",\n  \"key_2\": \"value_2\", # and is implied\n}\n\n\n\n\nprojection is a dictionary\nprojection = {\"key_1\": 1, \"key_2\": 0} returns all documents with only key_1 and without key_2\n\n\n\n\n\nsort is a list of tuples\n\n1: ascending order\n-1: descending order\n\nsort = [(\"key_1\", 1), (\"key_2\", -1)] sorts by key_1 in ascending order and then by key_2 in descending order\n\n\n\n\n\nlimit is an integer\nlimit = 10 returns the first 10 documents\n\n\n\n\n\nskip is an integer\nskip = 10 skips the first 10 documents\n\n\n\n\n\ncount_documents is a method\nclient[db_name][collection_name].count_documents(filter={...}) returns the number of documents that match the filter\n\n\n\n\n\ndistinct is a method\nclient[db_name][collection_name].distinct(\"key_1\") returns a list of distinct values for key_1"
  },
  {
    "objectID": "block_3/522_workflows/522_workflows.html",
    "href": "block_3/522_workflows/522_workflows.html",
    "title": "Workflows",
    "section": "",
    "text": "Link: https://github.com/UBC-MDS/english-score-predictor\n\n\n\n\n\nhttps://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008316#sec021\n\nBase Your Image on a Minimal Official Docker Image\n\nUse minimal and official base images for simplicity and security.\n\nTag Images with Explicit Versions\n\nExplicitly tag image versions in FROM instructions for reproducibility.\n\nWrite Instructions in the Right Order\n\nOptimize Dockerfile instructions order for Docker’s layer caching.\n\nDocument the Build Context\n\nClearly document the process to rebuild the image.\n\nSpecify Software Versions\n\nUse version pinning for software installations.\n\nUse Version Control\n\nMaintain Dockerfiles in version control systems.\n\nMount Datasets at Run Time\n\nStore large datasets outside the container and mount them at runtime.\n\nMake the Image One-Click Runnable\n\nEnsure the container is easily runnable with sensible defaults.\n\nOrder the Instructions\n\nOrder Dockerfile instructions from least to most likely to change.\n\nRegularly Use and Rebuild Containers\n\nRegularly use and update containers to identify and fix issues early.\n\n\n\n\n\n\ndocker build -t &lt;image_name&gt; .: build an image from a Dockerfile\ndocker run --rm -it &lt;image_name&gt; bash: run a container from an image\n\n-it: interactive mode\n--rm: remove the container after exiting\n\ndocker run --rm -it -v &lt;host_dir&gt;:&lt;container_dir&gt; &lt;image_name&gt; bash: run a container from an image and mount a host directory to a container directory\nexit: exit a container\n\n\n\ndocker images: list all images\ndocker rmi &lt;image_name&gt;: remove an image\ndocker ps -a: list all containers\ndocker rm &lt;container_name&gt;: remove a container\ndocker compose up: run a container from a docker-compose.yml file\n\nsimilar to conda env create -f environment.yml and conda activate &lt;env_name&gt;\n\n\n\n\n\n\nCommon structure:\n\nFROM &lt;base_image&gt;\nRUN &lt;command&gt;\nCOPY &lt;host_dir&gt; &lt;container_dir&gt;\nWORKDIR &lt;container_dir&gt;\nCMD &lt;command&gt;\n\n\nFROM ubuntu:18.04\n\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    cmake \\\n    git \\\n    wget \\\n    unzip \\\n    yasm \\\n    pkg-config \\\n    libswscale-dev \\\n    libtbb2 \\\n    libtbb-dev \\\n    libjpeg-dev \\\n    libpng-dev \\\n    libtiff-dev \\\n    libavformat-dev \\\n    libpq-dev```\ndocker compose run --rm &lt;service_name&gt; bash: run a container from a docker-compose.yml file and mount a host directory to a container directory"
  },
  {
    "objectID": "block_3/522_workflows/522_workflows.html#project",
    "href": "block_3/522_workflows/522_workflows.html#project",
    "title": "Workflows",
    "section": "",
    "text": "Link: https://github.com/UBC-MDS/english-score-predictor"
  },
  {
    "objectID": "block_3/522_workflows/522_workflows.html#docker",
    "href": "block_3/522_workflows/522_workflows.html#docker",
    "title": "Workflows",
    "section": "",
    "text": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008316#sec021\n\nBase Your Image on a Minimal Official Docker Image\n\nUse minimal and official base images for simplicity and security.\n\nTag Images with Explicit Versions\n\nExplicitly tag image versions in FROM instructions for reproducibility.\n\nWrite Instructions in the Right Order\n\nOptimize Dockerfile instructions order for Docker’s layer caching.\n\nDocument the Build Context\n\nClearly document the process to rebuild the image.\n\nSpecify Software Versions\n\nUse version pinning for software installations.\n\nUse Version Control\n\nMaintain Dockerfiles in version control systems.\n\nMount Datasets at Run Time\n\nStore large datasets outside the container and mount them at runtime.\n\nMake the Image One-Click Runnable\n\nEnsure the container is easily runnable with sensible defaults.\n\nOrder the Instructions\n\nOrder Dockerfile instructions from least to most likely to change.\n\nRegularly Use and Rebuild Containers\n\nRegularly use and update containers to identify and fix issues early.\n\n\n\n\n\n\ndocker build -t &lt;image_name&gt; .: build an image from a Dockerfile\ndocker run --rm -it &lt;image_name&gt; bash: run a container from an image\n\n-it: interactive mode\n--rm: remove the container after exiting\n\ndocker run --rm -it -v &lt;host_dir&gt;:&lt;container_dir&gt; &lt;image_name&gt; bash: run a container from an image and mount a host directory to a container directory\nexit: exit a container\n\n\n\ndocker images: list all images\ndocker rmi &lt;image_name&gt;: remove an image\ndocker ps -a: list all containers\ndocker rm &lt;container_name&gt;: remove a container\ndocker compose up: run a container from a docker-compose.yml file\n\nsimilar to conda env create -f environment.yml and conda activate &lt;env_name&gt;\n\n\n\n\n\n\nCommon structure:\n\nFROM &lt;base_image&gt;\nRUN &lt;command&gt;\nCOPY &lt;host_dir&gt; &lt;container_dir&gt;\nWORKDIR &lt;container_dir&gt;\nCMD &lt;command&gt;\n\n\nFROM ubuntu:18.04\n\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    cmake \\\n    git \\\n    wget \\\n    unzip \\\n    yasm \\\n    pkg-config \\\n    libswscale-dev \\\n    libtbb2 \\\n    libtbb-dev \\\n    libjpeg-dev \\\n    libpng-dev \\\n    libtiff-dev \\\n    libavformat-dev \\\n    libpq-dev```\ndocker compose run --rm &lt;service_name&gt; bash: run a container from a docker-compose.yml file and mount a host directory to a container directory"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html",
    "href": "block_2/512_algs_ds/512_algs_ds.html",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Big O\nname\nchange in runtime if I double \\(n\\)?\n\n\n\n\n\\(O(1)\\)\nconstant\nsame\n\n\n\\(O(\\log n)\\)\nlogarithmic\nincreased by a constant\n\n\n\\(O(\\sqrt{n})\\)\nsquare root\nincreased by roughly 1.4x\n\n\n\\(O(n)\\)\nlinear\n2x\n\n\n\\(O(n \\log n)\\)\nlinearithmic\nroughly 2x\n\n\n\\(O(n^2)\\)\nquadratic\n4x\n\n\n\\(O(n^3)\\)\ncubic\n8x\n\n\n\\(O(n^k)\\)\npolynomial\nincrease by a factor of \\(2^k\\)\n\n\n\\(O(2^n)\\)\nexponential\nsquared\n\n\n\nsorted from fastest to slowest\nIf mult/ div by contant c: \\(log_c(n)\\) e.g. for (int i = 0; i &lt; n; i *= 2) If add/ sub by constant c: \\(n/c\\) e.g. for (int i = 0; i &lt; n; i += 2)\n\nWe write \\(O(f(n))\\) for some function \\(f(n)\\).\nYou get the doubling time by taking \\(f(2n)/f(n)\\).\nE.g. if \\(f(n)=n^3\\), then \\(f(2n)/f(n)=(2n)^3/n^3=8\\).\n\nSo if you double \\(n\\), the running time goes up 8x.\n\nFor \\(O(2^n)\\), increasing \\(n\\) by 1 causes the runtime to double!\n\nNote: these are common cases of big O, but this list is not exhaustive.\n\n\n\n\n\n\nSpace complexity is the amount of memory used by an algorithm.\nWe can use big O notation to describe space complexity.\n\n\n\n\nrange() is a generator, so it doesn’t take up memory\nlist(range()) is a list, so it takes up memory\nnp.arange() is an array, so it takes up memory\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nLinear Search\nBinary Search\n\n\n\n\nPrinciple\nSequentially checks each element until a match is found or end is reached.\nRepeatedly divides in half the portion of the list that could contain the item until you’ve narrowed down the possible locations to just one.\n\n\nBest-case Time Complexity\n(O(1))\n(O(1))\n\n\nSpace Complexity\n(O(1))\n(O(1))\n\n\nWorks on\nUnsorted and sorted lists\nSorted lists only\n\n\n\n\n\ndef linear_search(arr, x):\n    for i in range(len(arr)):\n        if arr[i] == x:\n            return i\n    return -1  # not found\n\n# Example usage:\narr = [10, 20, 80, 30, 60, 50, 110, 100, 130, 170]\nx = 110\nresult = linear_search(arr, x)\nprint(\"Element is present at index\" if result != -1 else \"Element is not present in array\", result)\n\n\n\ndef binary_search(arr, l, r, x):\n    while l &lt;= r:\n        mid = l + (r - l) // 2\n        # Check if x is present at mid\n        if arr[mid] == x:\n            return mid\n        # If x is greater, ignore left half\n        elif arr[mid] &lt; x:\n            l = mid + 1\n        # If x is smaller, ignore right half\n        else:\n            r = mid - 1\n    # Element was not present\n    return -1\n\n# Example usage:\narr = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nx = 70\nresult = binary_search(arr, 0, len(arr)-1, x)\nprint(\"Element is present at index\" if result != -1 else \"Element is not present in array\", result)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nWorst-case Time Complexity\nSpace Complexity\nDescription\nViz\n\n\n\n\nInsertion Sort\n(O(n^2))\n(O(1))\nBuilds the final sorted list one item at a time. It takes one input element per iteration and finds its correct position in the sorted list.\n\n\n\nSelection Sort\n(O(n^2))\n(O(1))\nDivides the input list into two parts: a sorted and an unsorted sublist. It repeatedly selects the smallest (or largest) element from the unsorted sublist and moves it to the end of the sorted sublist.\n\n\n\nBubble Sort\n(O(n^2))\n(O(1))\nRepeatedly steps through the list, compares adjacent elements and swaps them if they are in the wrong order. The process is repeated for each item.\n\n\n\nMerge Sort\n(O(n n))\n(O(n))\nDivides the unsorted list into n sublists, each containing one element, then repeatedly merges sublists to produce new sorted sublists until there is only one sublist remaining.\n\n\n\nHeap Sort\n(O(n n))\n(O(1))\nConverts the input data into a heap data structure. It then extracts the topmost element (max or min) and reconstructs the heap, repeating this process until the heap is empty.\n\n\n\n\ngifs from https://emre.me/algorithms/sorting-algorithms/\n\n\n\n\n\n\nHashing is a technique that is used to uniquely identify a specific object from a group of similar objects.\nin python: hash()\nonly immutable objects can be hashed\n\nlists, sets, and dictionaries are mutable and cannot be hashed\ntuples are immutable and can be hashed\n\n\n\n\n\n\nCreating dictionaries:\n\nx = {}\n\nx = {'a': 1, 'b': 2}\n\nx = dict()\n\nx = dict(a=1, b=2)\nx = dict([('a', 1), ('b', 2)])\nx = dict(zip(['a', 'b'], [1, 2]))\nx = dict({'a': 1, 'b': 2})\n\n\nAccessing values:\n\nx['a']: if key is not found, raises KeyError\nx.get('a', 0): returns 0 if key is not found\n\n\n\n\n\n\ndefaultdict is a subclass of dict that returns a default value when a key is not found\n\nfrom collections import defaultdict\nd = defaultdict(int)\n\nd['a'] returns 0\n\nd = defaultdict(list)\n\nd['a'] returns []\nnot list() because list() is a function that returns an empty list, list is a type\n\nd = defaultdict(set)\n\nd['a'] returns set()\n\nd = defaultdict(lambda: \"hello I am your friendly neighbourhood default value\")\n\nd['a'] returns \"hello I am your friendly neighbourhood default value\"\n\n\n\n\n\n\n\nCounter is a subclass of dict that counts the number of occurrences of an element in a list\n\nfrom collections import Counter\nc = Counter(['a', 'b', 'c', 'a', 'b', 'b'])\n\nc['a'] returns 2\nc['b'] returns 3\nc['c'] returns 1\nc['d'] returns 0\n\nc = Counter({'a': 2, 'b': 3, 'c': 1})\nc = Counter(a=2, b=3, c=1)\n\nother functions:\n\nc.most_common(2) returns the 2 most common elements in the list: [('b', 3), ('a', 2)]\n\n\n\n\n\n\n\ncontains vertices (or nodes) and edges\nuse networkx to create graphs in Python\n\nimport networkx as nx\n\nG = nx.Graph()  # create empty graph\nG.add_node(\"YVR\")  # add node 1\nG.add_node(\"YYZ\")  # add node 2\nG.add_node(\"YUL\")  # add node 3\n\nG.add_edge(\"YVR\", \"YYZ\", weight=4)  # add edge between node 1 and node 2\nG.add_edge(\"YVR\", \"YUL\", weight=5)  # add edge between node 1 and node 3\n\nnx.draw(G, with_labels=True) # draw graph but random layout\nnx.draw(G, with_labels=True, pos=nx.spring_layout(G, seed=5)) # not random layout\n\n\n\ndirected graph: edges have direction\n\nnx.DiGraph()\n\n\n\n\n\n\nDegree: number of edges connected to a node\nPath: sequence of nodes connected by edges\nConnected: graph where there is a path between every pair of nodes\nComponent: connected subgraph\n\n\n\n\n\n\n\ndef bfs(graph, start, search):\n    # graph: networkx graph\n\n    visited = set()\n    queue = [start]\n    while queue:\n        node = queue.pop(0)\n        if node in visited:\n            continue\n        visited.add(node)\n        if node == search:\n            return True\n        for neighbour in g.neighbors(node):\n            queue.append(neighbour)\n\n    return False\n\n\n\nFIFO (first in first out)\n\nqueue = []\nqueue.append(1)  # add to end of queue\nqueue.append(2)  # add to end of queue\nqueue.pop(0)  # remove from front of queue\n\n\n\n\n\n\n\nLIFO (last in first out)\n\nstack = []\nstack.append(1)  # add to end of stack\nstack.append(2)  # add to end of stack\nstack.pop()  # remove from end of stack\n\n\n\n\n\nAdjacency List\n\nlists of all the edges in the graph\nspace complexity: O(E)\nstill need to store all the nodes\n\nAdjacency Matrix\n\nmatrix of 0s and 1s with size V x V\ndense matrix space complexity: O(V^2)\nsparse matrix space complexity: O(E)\n\nCan do both directed and undirected graphs\n\n\nnetworkx uses scipy sparse matrix\n\ncreate sparse matrix: scipy.sparse.csr_matrix(x)\nneed to acces using matrix[row, col]\n\nmatrix[row][col] will not work, in np it will work\n\nto sum all the rows: matrix.sum(axis=1)\n\ncannot do np.sum(matrix, axis=1)\nor do matrix.getnnz(axis=1) to get number of non-zero elements in each row\n\nto find vertex: np.argmax(matrix.getnnz(axis=1))\nto find # max edges: np.max(matrix.getnnz(axis=1))\n\n\n\n\n\n\n\n\n\nLinear programming is a method to achieve the best outcome (e.g. maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships\n\n\n\n\nTo specify an optimization problem, we need to specify a few things:\n\nA specification of the space of possible inputs.\nAn objective function which takes an input and computes a score.\n(Optional) A set of constraints, which take an input and return true/false.\n\ncan only get same or worse score than not having constraints\n\nAre we maximizing or minimizing?\n\n\n\n\n\nPuLP is an LP modeler written in Python\ncontinuous problems are easier to solve than discrete problems\n\ndiscrete problems might not be “optimal”\n\nExample of discrete problem: assigning TAs to courses\n\n# Define the problem\nprob = pulp.LpProblem(\"TA-assignments\", pulp.LpMaximize)\n\n# Define the variables\nx = pulp.LpVariable.dicts(\"x\", (TAs, courses), 0, 1, pulp.LpInteger)\n\n# add constraints\nfor course in courses:\n    prob += pulp.lpSum(x[ta][course] for ta in TAs) == 1 # += adds constraint\n\n# add objective\nprob += pulp.lpSum(x[ta][course] * happiness[ta][course] for ta in TAs for course in courses)\n\n# solve\nprob.solve()\n# prob.solve(pulp.apis.PULP_CBC_CMD(msg=0)) # to suppress output\n\n# check status\npulp.LpStatus[prob.status] # 'Optimal'\n\n# print results\nfor ta in TAs:\n    for course in courses:\n        if x[ta][course].value() == 1.0:\n            print(f\"{ta} is assigned to {course}\")\n\n\n\n\nDynamic programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions using a memory-based data structure (array, map, etc).\nDynamic programming only works on problems with optimal substructure and overlapping subproblems.\n\noptimal substructure: optimal solution can be constructed from optimal solutions of its subproblems\noverlapping subproblems: subproblems recur many times\n\nDynamic programming is usually applied to optimization problems."
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#big-o-notation",
    "href": "block_2/512_algs_ds/512_algs_ds.html#big-o-notation",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Big O\nname\nchange in runtime if I double \\(n\\)?\n\n\n\n\n\\(O(1)\\)\nconstant\nsame\n\n\n\\(O(\\log n)\\)\nlogarithmic\nincreased by a constant\n\n\n\\(O(\\sqrt{n})\\)\nsquare root\nincreased by roughly 1.4x\n\n\n\\(O(n)\\)\nlinear\n2x\n\n\n\\(O(n \\log n)\\)\nlinearithmic\nroughly 2x\n\n\n\\(O(n^2)\\)\nquadratic\n4x\n\n\n\\(O(n^3)\\)\ncubic\n8x\n\n\n\\(O(n^k)\\)\npolynomial\nincrease by a factor of \\(2^k\\)\n\n\n\\(O(2^n)\\)\nexponential\nsquared\n\n\n\nsorted from fastest to slowest\nIf mult/ div by contant c: \\(log_c(n)\\) e.g. for (int i = 0; i &lt; n; i *= 2) If add/ sub by constant c: \\(n/c\\) e.g. for (int i = 0; i &lt; n; i += 2)\n\nWe write \\(O(f(n))\\) for some function \\(f(n)\\).\nYou get the doubling time by taking \\(f(2n)/f(n)\\).\nE.g. if \\(f(n)=n^3\\), then \\(f(2n)/f(n)=(2n)^3/n^3=8\\).\n\nSo if you double \\(n\\), the running time goes up 8x.\n\nFor \\(O(2^n)\\), increasing \\(n\\) by 1 causes the runtime to double!\n\nNote: these are common cases of big O, but this list is not exhaustive."
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#space-complexity",
    "href": "block_2/512_algs_ds/512_algs_ds.html#space-complexity",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Space complexity is the amount of memory used by an algorithm.\nWe can use big O notation to describe space complexity.\n\n\n\n\nrange() is a generator, so it doesn’t take up memory\nlist(range()) is a list, so it takes up memory\nnp.arange() is an array, so it takes up memory"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#searching",
    "href": "block_2/512_algs_ds/512_algs_ds.html#searching",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Feature\nLinear Search\nBinary Search\n\n\n\n\nPrinciple\nSequentially checks each element until a match is found or end is reached.\nRepeatedly divides in half the portion of the list that could contain the item until you’ve narrowed down the possible locations to just one.\n\n\nBest-case Time Complexity\n(O(1))\n(O(1))\n\n\nSpace Complexity\n(O(1))\n(O(1))\n\n\nWorks on\nUnsorted and sorted lists\nSorted lists only\n\n\n\n\n\ndef linear_search(arr, x):\n    for i in range(len(arr)):\n        if arr[i] == x:\n            return i\n    return -1  # not found\n\n# Example usage:\narr = [10, 20, 80, 30, 60, 50, 110, 100, 130, 170]\nx = 110\nresult = linear_search(arr, x)\nprint(\"Element is present at index\" if result != -1 else \"Element is not present in array\", result)\n\n\n\ndef binary_search(arr, l, r, x):\n    while l &lt;= r:\n        mid = l + (r - l) // 2\n        # Check if x is present at mid\n        if arr[mid] == x:\n            return mid\n        # If x is greater, ignore left half\n        elif arr[mid] &lt; x:\n            l = mid + 1\n        # If x is smaller, ignore right half\n        else:\n            r = mid - 1\n    # Element was not present\n    return -1\n\n# Example usage:\narr = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nx = 70\nresult = binary_search(arr, 0, len(arr)-1, x)\nprint(\"Element is present at index\" if result != -1 else \"Element is not present in array\", result)"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#sorting",
    "href": "block_2/512_algs_ds/512_algs_ds.html#sorting",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Algorithm\nWorst-case Time Complexity\nSpace Complexity\nDescription\nViz\n\n\n\n\nInsertion Sort\n(O(n^2))\n(O(1))\nBuilds the final sorted list one item at a time. It takes one input element per iteration and finds its correct position in the sorted list.\n\n\n\nSelection Sort\n(O(n^2))\n(O(1))\nDivides the input list into two parts: a sorted and an unsorted sublist. It repeatedly selects the smallest (or largest) element from the unsorted sublist and moves it to the end of the sorted sublist.\n\n\n\nBubble Sort\n(O(n^2))\n(O(1))\nRepeatedly steps through the list, compares adjacent elements and swaps them if they are in the wrong order. The process is repeated for each item.\n\n\n\nMerge Sort\n(O(n n))\n(O(n))\nDivides the unsorted list into n sublists, each containing one element, then repeatedly merges sublists to produce new sorted sublists until there is only one sublist remaining.\n\n\n\nHeap Sort\n(O(n n))\n(O(1))\nConverts the input data into a heap data structure. It then extracts the topmost element (max or min) and reconstructs the heap, repeating this process until the heap is empty.\n\n\n\n\ngifs from https://emre.me/algorithms/sorting-algorithms/"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#hashmap",
    "href": "block_2/512_algs_ds/512_algs_ds.html#hashmap",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Hashing is a technique that is used to uniquely identify a specific object from a group of similar objects.\nin python: hash()\nonly immutable objects can be hashed\n\nlists, sets, and dictionaries are mutable and cannot be hashed\ntuples are immutable and can be hashed\n\n\n\n\n\n\nCreating dictionaries:\n\nx = {}\n\nx = {'a': 1, 'b': 2}\n\nx = dict()\n\nx = dict(a=1, b=2)\nx = dict([('a', 1), ('b', 2)])\nx = dict(zip(['a', 'b'], [1, 2]))\nx = dict({'a': 1, 'b': 2})\n\n\nAccessing values:\n\nx['a']: if key is not found, raises KeyError\nx.get('a', 0): returns 0 if key is not found\n\n\n\n\n\n\ndefaultdict is a subclass of dict that returns a default value when a key is not found\n\nfrom collections import defaultdict\nd = defaultdict(int)\n\nd['a'] returns 0\n\nd = defaultdict(list)\n\nd['a'] returns []\nnot list() because list() is a function that returns an empty list, list is a type\n\nd = defaultdict(set)\n\nd['a'] returns set()\n\nd = defaultdict(lambda: \"hello I am your friendly neighbourhood default value\")\n\nd['a'] returns \"hello I am your friendly neighbourhood default value\"\n\n\n\n\n\n\n\nCounter is a subclass of dict that counts the number of occurrences of an element in a list\n\nfrom collections import Counter\nc = Counter(['a', 'b', 'c', 'a', 'b', 'b'])\n\nc['a'] returns 2\nc['b'] returns 3\nc['c'] returns 1\nc['d'] returns 0\n\nc = Counter({'a': 2, 'b': 3, 'c': 1})\nc = Counter(a=2, b=3, c=1)\n\nother functions:\n\nc.most_common(2) returns the 2 most common elements in the list: [('b', 3), ('a', 2)]"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#graphs",
    "href": "block_2/512_algs_ds/512_algs_ds.html#graphs",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "contains vertices (or nodes) and edges\nuse networkx to create graphs in Python\n\nimport networkx as nx\n\nG = nx.Graph()  # create empty graph\nG.add_node(\"YVR\")  # add node 1\nG.add_node(\"YYZ\")  # add node 2\nG.add_node(\"YUL\")  # add node 3\n\nG.add_edge(\"YVR\", \"YYZ\", weight=4)  # add edge between node 1 and node 2\nG.add_edge(\"YVR\", \"YUL\", weight=5)  # add edge between node 1 and node 3\n\nnx.draw(G, with_labels=True) # draw graph but random layout\nnx.draw(G, with_labels=True, pos=nx.spring_layout(G, seed=5)) # not random layout\n\n\n\ndirected graph: edges have direction\n\nnx.DiGraph()\n\n\n\n\n\n\nDegree: number of edges connected to a node\nPath: sequence of nodes connected by edges\nConnected: graph where there is a path between every pair of nodes\nComponent: connected subgraph"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#graph-searching",
    "href": "block_2/512_algs_ds/512_algs_ds.html#graph-searching",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "def bfs(graph, start, search):\n    # graph: networkx graph\n\n    visited = set()\n    queue = [start]\n    while queue:\n        node = queue.pop(0)\n        if node in visited:\n            continue\n        visited.add(node)\n        if node == search:\n            return True\n        for neighbour in g.neighbors(node):\n            queue.append(neighbour)\n\n    return False\n\n\n\nFIFO (first in first out)\n\nqueue = []\nqueue.append(1)  # add to end of queue\nqueue.append(2)  # add to end of queue\nqueue.pop(0)  # remove from front of queue\n\n\n\n\n\n\n\nLIFO (last in first out)\n\nstack = []\nstack.append(1)  # add to end of stack\nstack.append(2)  # add to end of stack\nstack.pop()  # remove from end of stack\n\n\n\n\n\nAdjacency List\n\nlists of all the edges in the graph\nspace complexity: O(E)\nstill need to store all the nodes\n\nAdjacency Matrix\n\nmatrix of 0s and 1s with size V x V\ndense matrix space complexity: O(V^2)\nsparse matrix space complexity: O(E)\n\nCan do both directed and undirected graphs\n\n\nnetworkx uses scipy sparse matrix\n\ncreate sparse matrix: scipy.sparse.csr_matrix(x)\nneed to acces using matrix[row, col]\n\nmatrix[row][col] will not work, in np it will work\n\nto sum all the rows: matrix.sum(axis=1)\n\ncannot do np.sum(matrix, axis=1)\nor do matrix.getnnz(axis=1) to get number of non-zero elements in each row\n\nto find vertex: np.argmax(matrix.getnnz(axis=1))\nto find # max edges: np.max(matrix.getnnz(axis=1))"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#linear-programming",
    "href": "block_2/512_algs_ds/512_algs_ds.html#linear-programming",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Linear programming is a method to achieve the best outcome (e.g. maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#defining-the-problem",
    "href": "block_2/512_algs_ds/512_algs_ds.html#defining-the-problem",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "To specify an optimization problem, we need to specify a few things:\n\nA specification of the space of possible inputs.\nAn objective function which takes an input and computes a score.\n(Optional) A set of constraints, which take an input and return true/false.\n\ncan only get same or worse score than not having constraints\n\nAre we maximizing or minimizing?"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#using-python-pulp",
    "href": "block_2/512_algs_ds/512_algs_ds.html#using-python-pulp",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "PuLP is an LP modeler written in Python\ncontinuous problems are easier to solve than discrete problems\n\ndiscrete problems might not be “optimal”\n\nExample of discrete problem: assigning TAs to courses\n\n# Define the problem\nprob = pulp.LpProblem(\"TA-assignments\", pulp.LpMaximize)\n\n# Define the variables\nx = pulp.LpVariable.dicts(\"x\", (TAs, courses), 0, 1, pulp.LpInteger)\n\n# add constraints\nfor course in courses:\n    prob += pulp.lpSum(x[ta][course] for ta in TAs) == 1 # += adds constraint\n\n# add objective\nprob += pulp.lpSum(x[ta][course] * happiness[ta][course] for ta in TAs for course in courses)\n\n# solve\nprob.solve()\n# prob.solve(pulp.apis.PULP_CBC_CMD(msg=0)) # to suppress output\n\n# check status\npulp.LpStatus[prob.status] # 'Optimal'\n\n# print results\nfor ta in TAs:\n    for course in courses:\n        if x[ta][course].value() == 1.0:\n            print(f\"{ta} is assigned to {course}\")"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#dynamic-programming",
    "href": "block_2/512_algs_ds/512_algs_ds.html#dynamic-programming",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Dynamic programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions using a memory-based data structure (array, map, etc).\nDynamic programming only works on problems with optimal substructure and overlapping subproblems.\n\noptimal substructure: optimal solution can be constructed from optimal solutions of its subproblems\noverlapping subproblems: subproblems recur many times\n\nDynamic programming is usually applied to optimization problems."
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html",
    "href": "block_2/571_sup_learn/571_sup_learn.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "Machine Learning: A field of study that gives computers the ability to learn without being explicitly programmed.\n\nauto detect patterns in data and make predictions\nPopular def of supervised learning: input: data + labels, output: model\n\nTraining a model from input data and its corresponding targets to predict targets for new examples.\n\n\nFundamental goal of machine learning: generalize beyond the examples in the training set.\n\n\n\n\n\n\n\n\n\n\n\n\nSupervised Learning\nUnsupervised Learning\n\n\n\n\nDescription\nThe training data includes the desired solutions, called labels.\nThe training data is unlabeled.\n\n\nSubtypes/Goals\n- Predict based on input data and its corresponding targets- Classification: Predict a class label e.g. will someone pass or fail final based on previous scores- Regression: Predict a continuous number e.g. what score will someone get on final based on previous score\n- Clustering: Group similar instances- Anomaly detection: Detect abnormal instances- Visualization and dimensionality reduction: Simplify data- Association rule learning: Discover relations between attributes\n\n\nOther Examples\nDecision tree, naive bayes, kNN, random forest, support vector machine, neural network\nk-means, hierarchical cluster analysis, expectation maximization, t-SNE, apriori, FP-growth\n\n\n\n\n\n\n\nFeatures/ X[n x d]: The input variables used to make predictions. (d = # of features)\nTarget/ y [n x 1]: The output variable we are trying to predict.\nExample: A particular instance of data, usually represented as a vector of features. (n = # of training examples)\nTraining: Fitting the model to examples.\nLabel: The target value for a particular example. (y)\n\n\n\n\n\n\n\n\n\n\n\n\nParameters\nHyperparameters\n\n\n\n\nDescription\nThe coefficients learned by the model during training.\nSettings used to control the training process.\n\n\nLearning/Setting Process\nAre learned automatically during training.\nAre set before training.\n\n\nPurpose or Role\nRule values that are learned from the training data (examples/features).\nControls how complex the model is. Validate using validation score (can overfit if too complex).\n\n\nExamples\ne.g. coefficients in linear regression, weights in neural networks\ne.g. learning rate, number of iterations, number of hidden layers, depth of decision tree, k in kNN and k-means\n\n\n\n\n\n\n\n\nGenerally there are 2 kinds of error:\n\nTraining error (\\(E_{train}\\)): Error on the training data.\nDistribution error/ test error/ generalization error (\\(E_{D}\\)): Error on new data.\n\n\\(E\\_{approx} = E_{D} - E_{train}\\)\n\n\n\n\n\nTraining set: The data used to train the model. Used a lot to set parameters.\nValidation set: The data used to evaluate the model during training. Used a few times to set hyperparameters.\nTest set: The data used to evaluate the model after training. Used once to estimate \\(E_{D}\\).\nDeployment: The model is used in the real world.\n\nfrom sklearn.model_selection import train_test_split\n\n# Used directly on X and y\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=123\n)  # 80%-20% train test split on X and y\n\n# Used on a dataframe\ntrain_df, test_df = train_test_split(\n    df, test_size=0.2, random_state=123\n)  # 80%-20% train test split on df\n\n\n\nA method of estimating \\(E_{D}\\) using the training set.\n\nDivide the training set into \\(k\\) folds.\nFor each fold, train on the other \\(k-1\\) folds and evaluate on the current fold.\n\nBenefits:\n\nMore accurate estimate of \\(E_{D}\\). Sometimes just unlucky with train/test split, this helps.\nMore efficient use of data.\n\n\nfrom sklearn.model_selection import cross_val_score, cross_validate\n\n# cross_val_score not as comprehensive as cross_validate\n# cross_val_score only returns the scores\ncv_scores = cross_val_score(model, X_train, y_train, cv=10)\n\n# using cross_validate\nscores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n# returns a dictionary with keys: ['fit_time', 'score_time', 'test_score', 'train_score']\npd.DataFrame(scores)\ncross-validation does not return a model. It is not a way to build a model that can be applied to new data. The purpose of cross-validation is to evaluate how well the model will generalize to unseen data.\n\n\n\n\n\nAKA the bias/ variance trade-off in supervised learning.\n\nBias: Tendency to consistently learn the same wrong thing (high bias = underfitting).\nVariance: Tenency to learn random things irrespective of the real signal (high variance = overfitting).\n\nAs you increase model complexity, \\(E_{train}\\) goes down but \\(E_{approx} = E_{D} - E_{train}\\) goes up."
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#supervised-vs.-unsupervised-learning",
    "href": "block_2/571_sup_learn/571_sup_learn.html#supervised-vs.-unsupervised-learning",
    "title": "Supervised Learning",
    "section": "",
    "text": "Supervised Learning\nUnsupervised Learning\n\n\n\n\nDescription\nThe training data includes the desired solutions, called labels.\nThe training data is unlabeled.\n\n\nSubtypes/Goals\n- Predict based on input data and its corresponding targets- Classification: Predict a class label e.g. will someone pass or fail final based on previous scores- Regression: Predict a continuous number e.g. what score will someone get on final based on previous score\n- Clustering: Group similar instances- Anomaly detection: Detect abnormal instances- Visualization and dimensionality reduction: Simplify data- Association rule learning: Discover relations between attributes\n\n\nOther Examples\nDecision tree, naive bayes, kNN, random forest, support vector machine, neural network\nk-means, hierarchical cluster analysis, expectation maximization, t-SNE, apriori, FP-growth"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#common-terms",
    "href": "block_2/571_sup_learn/571_sup_learn.html#common-terms",
    "title": "Supervised Learning",
    "section": "",
    "text": "Features/ X[n x d]: The input variables used to make predictions. (d = # of features)\nTarget/ y [n x 1]: The output variable we are trying to predict.\nExample: A particular instance of data, usually represented as a vector of features. (n = # of training examples)\nTraining: Fitting the model to examples.\nLabel: The target value for a particular example. (y)\n\n\n\n\n\n\n\n\n\n\n\n\nParameters\nHyperparameters\n\n\n\n\nDescription\nThe coefficients learned by the model during training.\nSettings used to control the training process.\n\n\nLearning/Setting Process\nAre learned automatically during training.\nAre set before training.\n\n\nPurpose or Role\nRule values that are learned from the training data (examples/features).\nControls how complex the model is. Validate using validation score (can overfit if too complex).\n\n\nExamples\ne.g. coefficients in linear regression, weights in neural networks\ne.g. learning rate, number of iterations, number of hidden layers, depth of decision tree, k in kNN and k-means"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#errors",
    "href": "block_2/571_sup_learn/571_sup_learn.html#errors",
    "title": "Supervised Learning",
    "section": "",
    "text": "Generally there are 2 kinds of error:\n\nTraining error (\\(E_{train}\\)): Error on the training data.\nDistribution error/ test error/ generalization error (\\(E_{D}\\)): Error on new data.\n\n\\(E\\_{approx} = E_{D} - E_{train}\\)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#training-validation-and-test-sets",
    "href": "block_2/571_sup_learn/571_sup_learn.html#training-validation-and-test-sets",
    "title": "Supervised Learning",
    "section": "",
    "text": "Training set: The data used to train the model. Used a lot to set parameters.\nValidation set: The data used to evaluate the model during training. Used a few times to set hyperparameters.\nTest set: The data used to evaluate the model after training. Used once to estimate \\(E_{D}\\).\nDeployment: The model is used in the real world.\n\nfrom sklearn.model_selection import train_test_split\n\n# Used directly on X and y\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=123\n)  # 80%-20% train test split on X and y\n\n# Used on a dataframe\ntrain_df, test_df = train_test_split(\n    df, test_size=0.2, random_state=123\n)  # 80%-20% train test split on df\n\n\n\nA method of estimating \\(E_{D}\\) using the training set.\n\nDivide the training set into \\(k\\) folds.\nFor each fold, train on the other \\(k-1\\) folds and evaluate on the current fold.\n\nBenefits:\n\nMore accurate estimate of \\(E_{D}\\). Sometimes just unlucky with train/test split, this helps.\nMore efficient use of data.\n\n\nfrom sklearn.model_selection import cross_val_score, cross_validate\n\n# cross_val_score not as comprehensive as cross_validate\n# cross_val_score only returns the scores\ncv_scores = cross_val_score(model, X_train, y_train, cv=10)\n\n# using cross_validate\nscores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n# returns a dictionary with keys: ['fit_time', 'score_time', 'test_score', 'train_score']\npd.DataFrame(scores)\ncross-validation does not return a model. It is not a way to build a model that can be applied to new data. The purpose of cross-validation is to evaluate how well the model will generalize to unseen data."
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#fundamental-trade-off",
    "href": "block_2/571_sup_learn/571_sup_learn.html#fundamental-trade-off",
    "title": "Supervised Learning",
    "section": "",
    "text": "AKA the bias/ variance trade-off in supervised learning.\n\nBias: Tendency to consistently learn the same wrong thing (high bias = underfitting).\nVariance: Tenency to learn random things irrespective of the real signal (high variance = overfitting).\n\nAs you increase model complexity, \\(E_{train}\\) goes down but \\(E_{approx} = E_{D} - E_{train}\\) goes up."
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#baseline",
    "href": "block_2/571_sup_learn/571_sup_learn.html#baseline",
    "title": "Supervised Learning",
    "section": "Baseline",
    "text": "Baseline\n\nA simple, fast, and easily explainable model that is used as a starting point for a more sophisticated model.\n\nMost common baseline for classification is majority class classifier.\nMost common baseline for regression is mean predictor.\nin sklearn, DummyClassifier and DummyRegressor are used to create baselines."
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#decision-trees",
    "href": "block_2/571_sup_learn/571_sup_learn.html#decision-trees",
    "title": "Supervised Learning",
    "section": "Decision Trees",
    "text": "Decision Trees\n\nbasic idea: predict using a series of if-then-else questions\ndepth of tree: number of questions asked (hyperparameter)\ndecision boundary: region of feature space where all instances are assigned to the same class\ndecision stump: a decision tree with only one split (depth = 1)\n\n\n\n\n\n\n\n\nAdvantages\nDisadvantages\n\n\n\n\n- Easy to interpret and explain.\n- Biased with imbalanced datasets.\n\n\n- Can handle both numerical and categorical data.\n- Greedy splitting algorithm might not find the globally optimal tree.\n\n\n- Can handle multi-output problems.\n- Hard to learn the true relationship between features and target (can only ask yes/no questions).\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier(max_depth=3)  # create model object\nclf.fit(X_train, y_train)  # fit model on training data\nclf.score(X_test, y_test)  # score model on test data or use clf.predict(X_test)\n\ncan also use DecisionTreeRegressor for regression problems (continuous target)\n\ndifference:\n\nscore: returns R^2 score (1 is best, 0 is worst)\nleaf nodes: returns average of target values in leaf node\nDecisionTreeClassifier uses entropy and DecisionTreeRegressor uses variance"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#knn",
    "href": "block_2/571_sup_learn/571_sup_learn.html#knn",
    "title": "Supervised Learning",
    "section": "kNN",
    "text": "kNN\n\nkNN is a non-parametric model (no parameters to learn and stores all training data)\nkNN is a lazy learner (no training, just prediction)\n\nslow at prediction time\n\nkNN is a supervised model (needs labels)\nhyperparameters:\n\nk: number of neighbors to consider, smaller k means more complex decision boundary\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)\n\n\n\n\n\n\n\nPros of k-NNs for Supervised Learning\nCons of k-NNs for Supervised Learning\n\n\n\n\nEasy to understand, interpret.\nCan be potentially VERY slow during prediction time with a large training set.\n\n\nSimple hyperparameter (n_neighbors) controlling the tradeoff\nOften not great test accuracy compared to modern approaches.\n\n\nCan learn very complex functions given enough data.\nDoesn’t work well on datasets with many features or sparse datasets.\n\n\nLazy learning: Takes no time to fit\nFalls apart when # dimensions increase (curse of dimensionality)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#svm-rbf",
    "href": "block_2/571_sup_learn/571_sup_learn.html#svm-rbf",
    "title": "Supervised Learning",
    "section": "SVM RBF",
    "text": "SVM RBF\n\nSVM is a parametric model (needs to learn parameters)\n\nremembers the support vectors\nuses a kernel function to transform the data (RBF, Radial Basis Func, is the default)\n\nDecision boundary only depends on support vectors (smooth)\nhyperparameters:\n\nC: regularization parameter, larger C means more complex\ngamma: kernel coefficient, larger gamma means more complex\n\n\nfrom sklearn.svm import SVC\n\nsvm = SVC(C=10, gamma=0.1)\nsvm.fit(X_train, y_train)\nsvm.score(X_test, y_test)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#scaling-values-using-standardscaler",
    "href": "block_2/571_sup_learn/571_sup_learn.html#scaling-values-using-standardscaler",
    "title": "Supervised Learning",
    "section": "Scaling values using StandardScaler",
    "text": "Scaling values using StandardScaler\n\nin KNN, we need to scale the data (in classification, we don’t need to scale the data)\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()  # create feature trasformer object\nscaler.fit(X_train)  # fitting the transformer on the train split\nX_train_scaled = scaler.transform(X_train)  # transforming the train split\nX_test_scaled = scaler.transform(X_test)  # transforming the test split"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#address-missing-values-using-simpleimputer",
    "href": "block_2/571_sup_learn/571_sup_learn.html#address-missing-values-using-simpleimputer",
    "title": "Supervised Learning",
    "section": "Address missing values using SimpleImputer",
    "text": "Address missing values using SimpleImputer\n\nreplace all missing values with the mean/ median of the column\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy='median')  # create imputer object\nimputer.fit(X_train_num_only)  # fitting the imputer on the train split\nX_train_num_only_imputed = imputer.transform(X_train_num_only)  # transforming the train split\nX_test_num_only_imputed = imputer.transform(X_test_num_only)  # transforming the test split"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#address-cataegorical-values-using-onehotencoder",
    "href": "block_2/571_sup_learn/571_sup_learn.html#address-cataegorical-values-using-onehotencoder",
    "title": "Supervised Learning",
    "section": "Address cataegorical values using OneHotEncoder",
    "text": "Address cataegorical values using OneHotEncoder\n\nturn categorical values into one-hot encoding\nto get the column names, use get_feature_names(): encoder.get_feature_names().tolist()\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(handle_unknown='ignore')  # create encoder object\nencoder.fit(X_train_cat_only)  # fitting the encoder on the train split\nX_train_cat_only_encoded = encoder.transform(X_train_cat_only)  # transforming the train split\nX_test_cat_only_encoded = encoder.transform(X_test_cat_only)  # transforming the test split\n\nother arguments for OneHotEncoder:\n\nhandle_unknown='ignore' will ignore unknown categories\n\nif you don’t set this, you will get an error if there are unknown categories in the test set\n\nsparse_output=False will return a dense matrix instead of a sparse matrix\n\ndefault is sparse_output=True (returns a sparse matrix - only stores non-zero values)\n\ndrop=\"if_binary\" will drop one of the columns if there are only two categories\n\ndefault is drop=None (no columns are dropped)\ndrop=\"first\" will drop the first column\ndrop=[0, 2] will drop the first and third columns\n\n\n\n\nDiscretizing\n\ne.g turning age into age groups (e.g. child, adult, senior or 0-20, 20-40, 40-60, 60+)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#address-catagorical-values-using-ordinalencoder",
    "href": "block_2/571_sup_learn/571_sup_learn.html#address-catagorical-values-using-ordinalencoder",
    "title": "Supervised Learning",
    "section": "Address catagorical values using OrdinalEncoder",
    "text": "Address catagorical values using OrdinalEncoder\n\nturn categorical values into ordinal encoding (e.g. low, medium, high)\ndtype=int will make the output an integer\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\nordered_categories = ['low', 'medium', 'high']\nencoder = OrdinalEncoder(categories=[ordered_categories], dtype=int)  # create encoder object\nencoder.fit(X_train_cat_only)  # fitting the encoder on the train split\nX_train_cat_only_encoded = encoder.transform(X_train_cat_only)  # transforming the train split\nX_test_cat_only_encoded = encoder.transform(X_test_cat_only)  # transforming the test split"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#address-bag-of-words-using-countvectorizer",
    "href": "block_2/571_sup_learn/571_sup_learn.html#address-bag-of-words-using-countvectorizer",
    "title": "Supervised Learning",
    "section": "Address Bag of Words using CountVectorizer",
    "text": "Address Bag of Words using CountVectorizer\n\nturn a string of words into a vector of word counts (e.g., “white couch” -&gt; [“white”: 1, “couch”: 1])\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(stop_words='english')  # create vectorizer object\nX_train_text_only_vectorized = vectorizer.fit_transform(X_train_text_only)  # fitting and transforming the train split\nX_test_text_only_vectorized = vectorizer.transform(X_test_text_only)  # transforming the test split\n\nParameters:\n\nstop_words='english' will remove common English words (e.g., “the”, “a”, “an”, “and”, “or”, “but”, “not”)\n\ndefault is stop_words=None (no words are removed)\n\nmax_features=100 will only keep the 100 most common words\n\ndefault is max_features=None (all words are kept)\n\n\nhandles unknown words by ignoring them"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#sklearn-summary",
    "href": "block_2/571_sup_learn/571_sup_learn.html#sklearn-summary",
    "title": "Supervised Learning",
    "section": "sklearn summary",
    "text": "sklearn summary\n\n\n\n\n\n\n\n\n\nEstimators\nTransformers\n\n\n\n\nPurpose\nused to fit and predict\nused to change input data\n\n\nUsage\nNeed to fit X_train, y_train\nNeed to fit X_train (no y_train)\n\n\n\nCan score on X_test, y_test\nnothing to score\n\n\nExamples\n- DecisionTreeClassifier\n- StandardScaler\n\n\n\n- KNeighborsClassifier\n- SimpleImputer\n\n\n\n- LogisticRegression\n- OneHotEncoder\n\n\n\n- SVC\n- OrdinalEncoder\n\n\n\n\nDon’t fit with transformer then cross validate with estimator\n\nThis is data leakage (train is influenced by validation)\n\nsolution: Use Sklearn Pipeline!"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#pipeline",
    "href": "block_2/571_sup_learn/571_sup_learn.html#pipeline",
    "title": "Supervised Learning",
    "section": "Pipeline",
    "text": "Pipeline\n\nuse sklearn.pipeline.Pipeline\nmake a pipeline:\n\nfrom sklearn.pipeline import make_pipeline\n\npipe_knn = make_pipeline(\n    SimpleImputer(strategy='median'),\n    StandardScaler(),\n    KNeighborsClassifier(n_neighbors=10)\n)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#column-transformer",
    "href": "block_2/571_sup_learn/571_sup_learn.html#column-transformer",
    "title": "Supervised Learning",
    "section": "Column Transformer",
    "text": "Column Transformer\n\nThis is a transformer that can handle multiple columns\n\nfrom sklearn.compose import make_column_transformer\n\npreprocessor = make_column_transformer(\n    (make_pipeline(SimpleImputer(), StandardScaler()), numeric_feats),  # scaling on numeric features\n    (\"passthrough\", passthrough_feats),  # no transformations on the binary features\n    (OneHotEncoder(), categorical_feats),  # OHE on categorical features\n    # normally OHE is put at the end since it makes new cols\n    (\"drop\", drop_feats),  # drop the drop features\n)\n\nget names of transformers: preprocessor.named_transformers_\nget new column names: preprocessor.named_transformers_[\"onehotencoder\"].get_feature_names()"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#preprocessing-1",
    "href": "block_2/571_sup_learn/571_sup_learn.html#preprocessing-1",
    "title": "Supervised Learning",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nScaling values using StandardScaler\n\nin KNN, we need to scale the data (in classification, we don’t need to scale the data)\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()  # create feature trasformer object\nscaler.fit(X_train)  # fitting the transformer on the train split\nX_train_scaled = scaler.transform(X_train)  # transforming the train split\nX_test_scaled = scaler.transform(X_test)  # transforming the test split\n\n\nAddress missing values using SimpleImputer\n\nreplace all missing values with the mean/ median of the column\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy='median')  # create imputer object\nimputer.fit(X_train_num_only)  # fitting the imputer on the train split\nX_train_num_only_imputed = imputer.transform(X_train_num_only)  # transforming the train split\nX_test_num_only_imputed = imputer.transform(X_test_num_only)  # transforming the test split\n\n\nAddress cataegorical values using OneHotEncoder\n\nturn categorical values into one-hot encoding\nto get the column names, use get_feature_names(): encoder.get_feature_names().tolist()\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(handle_unknown='ignore')  # create encoder object\nencoder.fit(X_train_cat_only)  # fitting the encoder on the train split\nX_train_cat_only_encoded = encoder.transform(X_train_cat_only)  # transforming the train split\nX_test_cat_only_encoded = encoder.transform(X_test_cat_only)  # transforming the test split\n\nother arguments for OneHotEncoder:\n\nhandle_unknown='ignore' will ignore unknown categories\n\nif you don’t set this, you will get an error if there are unknown categories in the test set\n\nsparse_output=False will return a dense matrix instead of a sparse matrix\n\ndefault is sparse_output=True (returns a sparse matrix - only stores non-zero values)\n\ndrop=\"if_binary\" will drop one of the columns if there are only two categories\n\ndefault is drop=None (no columns are dropped)\ndrop=\"first\" will drop the first column\ndrop=[0, 2] will drop the first and third columns\n\n\n\n\nDiscretizing\n\ne.g turning age into age groups (e.g. child, adult, senior or 0-20, 20-40, 40-60, 60+)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#address-catagorical-values-using-ordinalencoder-1",
    "href": "block_2/571_sup_learn/571_sup_learn.html#address-catagorical-values-using-ordinalencoder-1",
    "title": "Supervised Learning",
    "section": "Address catagorical values using OrdinalEncoder",
    "text": "Address catagorical values using OrdinalEncoder\n\nturn categorical values into ordinal encoding (e.g. low, medium, high)\ndtype=int will make the output an integer\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\nordered_categories = ['low', 'medium', 'high']\nencoder = OrdinalEncoder(categories=[ordered_categories], dtype=int)  # create encoder object\nencoder.fit(X_train_cat_only)  # fitting the encoder on the train split\nX_train_cat_only_encoded = encoder.transform(X_train_cat_only)  # transforming the train split\nX_test_cat_only_encoded = encoder.transform(X_test_cat_only)  # transforming the test split\n\nAddress Bag of Words using CountVectorizer\n\nturn a string of words into a vector of word counts (e.g., “white couch” -&gt; [“white”: 1, “couch”: 1])\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(stop_words='english')  # create vectorizer object\nX_train_text_only_vectorized = vectorizer.fit_transform(X_train_text_only)  # fitting and transforming the train split\nX_test_text_only_vectorized = vectorizer.transform(X_test_text_only)  # transforming the test split\n\nParameters:\n\nstop_words='english' will remove common English words (e.g., “the”, “a”, “an”, “and”, “or”, “but”, “not”)\n\ndefault is stop_words=None (no words are removed)\n\nmax_features=100 will only keep the 100 most common words\n\ndefault is max_features=None (all words are kept)\n\nbinary=True will only keep 0 or 1 for each word (instead of the count)\n\ndefault is binary=False (the count is kept)\n\n\nhandles unknown words by ignoring them\n\n\nBreaking Golden Rule\n\nIf we know fixed categories (i.e., provinces in Canada), we can break the golden rule and pass the list of known/possible categories\n\n\n\n\nsklearn summary\n\n\n\n\n\n\n\n\n\nEstimators\nTransformers\n\n\n\n\nPurpose\nused to fit and predict\nused to change input data\n\n\nUsage\nNeed to fit X_train, y_train\nNeed to fit X_train (no y_train)\n\n\n\nCan score on X_test, y_test\nnothing to score\n\n\nExamples\n- DecisionTreeClassifier\n- StandardScaler\n\n\n\n- KNeighborsClassifier\n- SimpleImputer\n\n\n\n- LogisticRegression\n- OneHotEncoder\n\n\n\n- SVC\n- OrdinalEncoder\n\n\n\n\nDon’t fit with transformer then cross validate with estimator\n\nThis is data leakage (train is influenced by validation)\n\nsolution: Use Sklearn Pipeline!\n\n\n\nPipeline\n\nuse sklearn.pipeline.Pipeline\nmake a pipeline:\n\nfrom sklearn.pipeline import make_pipeline\n\npipe_knn = make_pipeline(\n    SimpleImputer(strategy='median'),\n    StandardScaler(),\n    KNeighborsClassifier(n_neighbors=10)\n)\n\nscores = cross_validate(pipe_knn, X_train, y_train, return_train_score=True)\npipe_knn.fit(X_train, y_train)\n\n\nColumn Transformer\n\nThis is a transformer that can handle multiple columns\n\nfrom sklearn.compose import make_column_transformer\n\npreprocessor = make_column_transformer(\n    (make_pipeline(SimpleImputer(), StandardScaler()), numeric_feats),  # scaling on numeric features\n    (\"passthrough\", passthrough_feats),  # no transformations on the binary features\n    (OneHotEncoder(), categorical_feats),  # OHE on categorical features\n    # normally OHE is put at the end since it makes new cols\n    (\"drop\", drop_feats),  # drop the drop features\n)\n\nget names of transformers: preprocessor.named_transformers_\nget new column names: preprocessor.named_transformers_[\"onehotencoder\"].get_feature_names()"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#hyperparamter-optimization",
    "href": "block_2/571_sup_learn/571_sup_learn.html#hyperparamter-optimization",
    "title": "Supervised Learning",
    "section": "Hyperparamter Optimization",
    "text": "Hyperparamter Optimization\n\nMethods\n\nManual\n\nTakes a lot of time\nintuition is not always correct\nsome hyperparameters work together\n\nAutomated\n\nGrid search\n\n\n\n\nGrid Search\n\nExhaustive search over specified parameter values for an estimator\n\nruns \\(n^m\\) CV for m hyperparameters and n values for each parameter\n\nAfter finding best parameter, it trains/fits the model on the whole training set\n\n\nParameters:\n\nGridSearchCV(estimator, param_grid, scoring=None, cv=None, n_jobs=None))\nestimator: estimator object\nparam_grid: dictionary with parameters names as keys and lists of parameter settings to try as values\n\nuses __ syntax to specify parameters of the estimator\ne.g:\n\ncolumntransformer__countvectorizer__max_features: max features of count vectorizer in column transformer\nsvc__gamma: gamma of SVC\n\n\nscoring: scoring method\ncv: cross-validation method\nn_jobs: number of jobs to run in parallel (-1 means use all processors)\n\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\npipe_svm = make_pipeline(preprocessor, SVC())\n\nparam_grid = {\n    \"columntransformer__countvectorizer__max_features\": [100, 200, 400, 800, 1000, 2000],\n    \"svc__gamma\": [0.001, 0.01, 0.1, 1.0, 10, 100],\n    \"svc__C\": [0.001, 0.01, 0.1, 1.0, 10, 100],\n}\n\n# Create a grid search object\ngs = GridSearchCV(pipe_svm, param_grid=param_grid, cv=5, n_jobs=-1)\ngs.fit(X_train, y_train)\n\ngs.best_score_ # returns the best score\ngs.best_params_ # returns the best parameters\n# Returns a dataframe of all the results\npd.DataFrame(random_search.cv_results_)[\n    [\n        \"mean_test_score\",\n        \"param_columntransformer__countvectorizer__max_features\",\n        \"param_svc__gamma\",\n        \"param_svc__C\",\n        \"mean_fit_time\",\n        \"rank_test_score\",\n    ]\n].set_index(\"rank_test_score\").sort_index().T\n\n# Can score on the test set\ngs.score(X_test, y_test)\n\n\n\nRandom Search\n\nPicks random values for the hyperparameters according to a distribution\nonly runs n_iter CV\n\n\nParameters\n\nRandomizedSearchCV(estimator, param_distributions, n_iter=10, scoring=None, cv=None, n_jobs=None))\nestimator: estimator object\nparam_distributions: dictionary with parameters names as keys and distributions or lists of parameters to try\n\ncan also pass param_grid from grid search (but does not exhaustively search)\n\nn_iter: number of parameter settings that are sampled\nscoring: scoring method\ncv: cross-validation method\nn_jobs: number of jobs to run in parallel (-1 means use all processors)\n\n\n\nCode\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {\n    \"columntransformer__countvectorizer__max_features\": randint(100, 2000),\n    \"svc__C\": uniform(0.1, 1e4),  # loguniform(1e-3, 1e3),\n    \"svc__gamma\": loguniform(1e-5, 1e3),\n}\n\nrs = RandomizedSearchCV(\n    pipe_svm,\n    param_distributions=param_dist,\n    n_iter=10,\n    cv=5,\n    n_jobs=-1,\n    random_state=42,\n)\n\n\n\nRandom vs Grid Search\n\nAdvantages of Random Search\n\nFaster\nAdding hyperparameters that do not influence the performance does not decrease the performance\nWorks better when some hyperparameters are more important than others\nrecommended more than grid search"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#naive-bayes",
    "href": "block_2/571_sup_learn/571_sup_learn.html#naive-bayes",
    "title": "Supervised Learning",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nBayes’ Theorem\n\\[ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\]\n\n\nBasic idea\n\nWe have a set of classes (e.g. spam or not spam)\nWe have a set of features (e.g. words in an email)\n\nWe want to find the probability of a class given a set of features.\n\\[ P(C|F_1, F_2, ..., F_n) = \\frac{P(F_1, F_2, ..., F_n|C)P(C)}{P(F_1, F_2, ..., F_n)} \\]\n\n\nNaive Bayes\n\nBag of words model (order of words doesn’t matter)\nAssume that all features are conditionally independent of each other (naive assumption)\nThis allows us to simplify the equation to:\n\n\\[ P(C|F*1, F_2, ..., F_n) \\approx \\frac{P(C)* \\prod P(F_i|C)}{P(F_1, F_2, ..., F_n)} \\]\n\n\nLaplace Smoothing\n\nIf a word is not in the training set, then the probability of that word given a class is 0\nThis will cause the entire probability to be 0\nWe can fix this by adding 1 to the numerator and adding the number of words to the denominator\n\n\\[ P(F_i|C) = \\frac{count(F_i, C) + 1}{count(C) + |V|} \\]\nwhere V is the number of possible word values\n\n\nSklearn Implementation\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\n\npipe_nb = make_pipeline(CountVectorizer(), MultinomialNB())\nresults_dict[\"Naive Bayes\"] = mean_std_cross_val_scores(\n    pipe_nb, X_train, y_train, return_train_score=True\n)\n\nMultinomialNB generally works better than BernoulliNB, especially for large text datasets\n\nBernoulliNB assumes that the features are binary (e.g. 0 or 1)\nMultinomialNB assumes that the features are counts (e.g. 0, 1, 2, 3, …)\n\nParameters:\n\nalpha is the Laplace smoothing parameter (actually hyperparameter), default is alpha=1.0\n\nHigh alpha means more smoothing =&gt; underfitting\nLow alpha means less smoothing =&gt; overfitting\n\n\n\n\n\nContinuous Features\n\nWe can use a Gaussian Naive Bayes model for continuous features\nThis assumes that the features are normally distributed\n\nIf not, can use sklearn.preprocessing.PowerTransformer to transform the data to be more normal\n\n\nfrom sklearn.naive_bayes import GaussianNB\n\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n\n# view\nmodel.theta_  # mean of each feature per class\nmodel.sigma_  # variance of each feature per class\nmodel.var_ # overall variance of each feature\nmodel.class_prior_  # prior probability of each class\n\nmodel.predict_proba(X_test)  # probability of each class"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#linear-models",
    "href": "block_2/571_sup_learn/571_sup_learn.html#linear-models",
    "title": "Supervised Learning",
    "section": "Linear Models",
    "text": "Linear Models\n\nmake predictions using a linear function of the input features\ndecision boundary is a hyperplane\n\nif 2d, decision boundary is a line\nuncertain near the decision boundary\n\nLimitations:\n\ncan only learn linear decision boundaries\ncan only learn linear functions of the input features\n\n\n\nLinear Regression\n\nMain idea: find the line that minimizes the sum of squared errors\nComponents:\n\ninput features (d features)\ncoefficients (d coefficients)\nintercept/ bias (1 intercept)\n\nNormally has d+1 parameters (one for each feature plus the intercept)\nMakes d-1 hyperplanes (separating lines) in d dimensions\nMore complex normally means the coefficients are larger\nRaw output score can be used to calculate probability score for a given prediction\nSCALING IS IMPORTANT\n\nIf features are on different scales, the coefficients will be on different scales\n\n\n\nRidge\n\\[ \\min\\_{w} ||Xw - y||\\_2^2 + \\alpha ||w||\\_2^2 \\]\n\nL2 regularization\nHyperparameters:\n\nalpha: regularization strength\n\nlarger values =&gt; more regularization =&gt; simpler model =&gt; underfitting\nmore regularization =&gt; smaller coefficients =&gt; less sensitive to changes in input features (outliers)\n\n\n\nfrom sklearn.linear_model import Ridge\n\npipe = make_pipeline(StandardScaler(), Ridge())\nscores = cross_validate(pipe, X_train, y_train, return_train_score=True)\n\ncoeffs = pipe_ridge.named_steps[\"ridge\"].coef_ # view coefficients\n# coeffs.shape = (n_features,), one coefficient for each feature\nintercept = pipe_ridge.named_steps[\"ridge\"].intercept_ # view intercept/ bias\n\n\nLaso\n\\[ \\min\\_{w} ||Xw - y||\\_2^2 + \\alpha ||w||\\_1 \\]\n\nL1 regularization\nHyperparameters:\n\nalpha: regularization strength\n\nlarger values =&gt; more regularization =&gt; simpler model =&gt; underfitting\n\n\n\n\n\n\nLogistic Regression\n\nMain idea: use linear regression to predict the probability of an event\nApplies a “threshold” to the raw output score to make a prediction -&gt; decides whether to predict 0/1 or -1/1\nComponents:\n\ninput features (d features)\ncoefficients (d coefficients)\nintercept/ bias (1 intercept)\nthreshold r (1 threshold)\n\nHyperparameters:\n\nC: inverse of regularization strength\n\nlarger values =&gt; less regularization =&gt; more complex model =&gt; overfitting\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nscores = cross_validate(lr, X_train, y_train, return_train_score=True)\n\n# access coefficients and intercept\nlr.coef_ # shape = (n_classes, n_features)\nlr.intercept_ # shape = (n_classes,)\n\nlr.classes_ # array of classes\n\npredict_proba returns the probability of each class\n\nfor binary classification, returns both classes (although one is redundant)\nbased on the order of lr.classes_\nsum of probabilities for each sample is 1\n\npredict returns the class with the highest probability\n\n\nsigmoid function\n\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\]\n\nturns -inf to 0 and inf to 1 \n\n\n\n\nLinear SVM\n\nMain idea: find the line that maximizes the margin between the decision boundary and the closest points\n\nfrom sklearn.svm import SVC\n\nlinear_svc = SVC(kernel=\"linear\")\nscores = cross_validate(linear_svc, X_train, y_train, return_train_score=True)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#multi-class-meta-strategies",
    "href": "block_2/571_sup_learn/571_sup_learn.html#multi-class-meta-strategies",
    "title": "Supervised Learning",
    "section": "Multi-class, meta-strategies",
    "text": "Multi-class, meta-strategies\n\nCan do multiclass naturally: KNN, decision trees\n2 hacky ways to use binary classifiers for multi-class classification:\n\n\nOne-vs-Rest (OVR)\n\nTrain a binary classifier for each class\n\ncreates binary linear classifiers separating each class from the rest (i.e blue vs rest, red vs rest, green vs rest)\n\nClassify by choosing the class with the highest probability\n\n\ne.g. a point on (0, -5) would get:\n\nlr.coef_ would give 3x2 array (3 classes, 2 features)\nlr.intercept_ would give 3x1 array (3 classes, 1 intercept)\nGet score with test_points[4]@lr.coef_.T + lr.intercept_ return array size 3, choose the class with the highest score\n\n\n\nOne-vs-One (OVO)\n\nTrain a binary classifier for each pair of classes\n\ncreates binary linear classifiers separating each class from each other class (i.e blue vs red, blue vs green, red vs green)\ntrains \\(\\frac{n(n-1)}{2}\\) classifiers\n\ncount the number of times each class wins\nClassify by choosing the class with the most wins\n\n\n\nUsing this in Python\nfrom sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n\nmodel = OneVsOneClassifier(LogisticRegression())\n%timeit model.fit(X_train_multi, y_train_multi);\n\nmodel = OneVsRestClassifier(LogisticRegression())\n%timeit model.fit(X_train_multi, y_train_multi);"
  },
  {
    "objectID": "block_5/574_spat_temp_mod/574_spat_temp.html",
    "href": "block_5/574_spat_temp_mod/574_spat_temp.html",
    "title": "Spatial and Temporal Models",
    "section": "",
    "text": "Collection of observations made sequentially in time\nData Types:\n\nUnivariate: Single observation at each time point (e.g. bike sale over time)\nMultivariate: Multiple observations at each time point (e.g. bike sale + profit over time)\nHeirarchical: Multiple time series, each with a hierarchical structure (e.g. bike sale + profit for each store over time)\n\nCommon Tasks:\n\nPrediction/ Forecasting (Supervised Learning)\n\nDifficult since many factors\n\nClustering/ Anomaly Detection (Unsupervised Learning)\n\n\n\n\n\n\n\nTime plot: x-axis = time, y-axis = value\n\n\n\n\n\nObservations close in time are often correlated\n\nCan quanitify using autocorrelation\n\nAutocorrelation: Correlation of a time series with a lagged version of itself\n\nLag: Time difference between two observations\nACF: Autocorrelation function\n\nPlots autocorrelation for different lags\n\nPACF: Partial autocorrelation function\n\nPlots correlation between two observations after removing the effect of other lags\n\ne.g. data[t (lag=1)] = data[t].shift(t)\n\n\n\n\n\nPlot of ACF vs. lag\nHelps identify patterns in time series\nUse statsmodels.graphics.tsaplots.plot_acf()\n\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nfig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(11, 5))\nplot_acf(data, lags=20, title='ACF', ax=ax1)\nax1.set_xlabel(\"Lag (years)\")\nax1.set_ylabel('ACF')\n \n\nShading indicates if correlation is significantly different from 0\n\n\\(CI = \\pm z_{\\alpha/2} SE(r_k)\\), \\(z_{\\alpha/2} \\approx 1.96\\) for 95% CI\n\\(SE(r_k) = \\frac{1}{\\sqrt{T}}\\), where \\(T\\) is the number of observations - Or Bartlett’s formula: \\(SE(r_k) = \\sqrt{\\frac{1 + 2\\sum_{j=1}^{k-1}r_j^2}{T}}\\) \n\nCO2 plot has a trend so ACF for smaller lags tend to be higher\nGeneral Key Observations:\n\nACF almost always decays with lag\nIf a series alternates (oscillates about mean), ACF will alternates too\nIf a series has seasonal or cyclical fluctuations, the ACF will oscillate at the same frequency\nIf there is a trend, ACF will decay slower (due to high correlation of the consecutive observations)\nExperience is required to interpret ACF\n\n\n\n\n\n\n\nTrend: Long-term increase/ decrease\nSeasonality: Regular pattern of up and down fluctuations (fixed interval)\n\ntypically over smaller time frame\n\nCyclic: Fluctuations not of fixed period (unknown and changing interval)\n\ntypically over larger time frame\n\n\n\n\n\n\nTime series with:\n\n0 mean\nConstant variance\nNo autocorrelation\n\nFurther assumed that it is iid and gaussian: \\(N(0, \\sigma^2)\\)\nWhy do we care?\n\nCannot predict white noise\nIf residuals from time series for a forecast should resemble white noise\n\nImplies that the model has captured all the information in the data\n\n\n\n\n\n\n\n\nWhen we decompose, we split the time series into 3 components:\n\nTrend-cycle (T): Long-term increase/ decrease\nSeasonal (S): same as seasonal above\nResidual: Random fluctuations\n\n\n\n\nAdditive Model: \\(Y_t = T_t + S_t + R_t\\)\n\nWhen the magnitude of the seasonal fluctuations does not change with the level of the time series\n\nMultiplicative Model: \\(Y_t = T_t \\times S_t \\times R_t\\)\n\nWhen the magnitude of the seasonal fluctuations does change with the level of the time series\n\n\n\n\n\nCurve Fitting: Fit a polynomial of degree \\(n\\) to the time series\nfrom statsmodels.tsa.tsatools import detrend\n\ndetrended = data - detrend(data, order=2) # order=2 for quadratic\nMoving Average: Smooths out short-term fluctuations and highlights longer-term trends\n# rolling is a pandas function\nrolling_mean = df.rolling(window=5, center=True).mean()\n\n# For even window, common practice to do:\nwindow = 4\ndf.rolling(window).mean().rolling(2).mean().shift(-window//2)\n\nwindow: Number of observations used for calculating the statistic\ncenter: Set the labels at the center of the window\n\nif odd, the label is at the center\nif even, the label is at the right\n\n\n\n\nThe even code does this: \n\n\n\n\n\nSimple steps:\n\nRemove the trend from the data (the detrended data above)\nEstimate the seasonal component by averaging the detrended data over each season\n\n\n\n\n\n\nThe residual is the remainder after removing the trend and seasonal components\nIf additive model: \\(R_t = Y_t - T_t - S_t\\)\nIf multiplicative model: \\(R_t = \\frac{Y_t}{T_t \\times S_t}\\)\n\n\n\n\n\nLuckily, there exists a function to do all of this for us: seasonal_decompose()\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndecomposition = seasonal_decompose(data, model='additive', period=12)\n\ndecomposition.trend # the trend component\ndecomposition.seasonal # the seasonal component\ndecomposition.resid # the residual component\n\n# plot the decomposition\nfig = decomposition.plot()\n\n\n\n\n\n\nForecasting: Predicting future values of a time series\n\n\n\n\nAverage: Use average of all past observations\n\n\n\nNaive: Use the last observation as the forecast\nSeasonally Adjusted Naive: Same as Naive but with seasonally adjusted data (classical decomposition)\n\n\n\nSeasonally Naive: Use the last observation from the same season (only one with seasonality)\n\ndf[\"month\"] = df.index.month\nlast_season = (df.drop_duplicates(\"month\", keep=\"last\")\n                 .sort_values(by=\"month\")\n                 .set_index(\"month\")[\"value\"]\n              )\ndf = df.drop(columns=\"month\")\nlast_season\n\n\nDrift: Linearly extrapolate the trend (only one that is not a straight horizontal line)\n\n\n\n\n\n\n\n\nForecast is a weighted average of all past observations\nRecursively defined: \\(\\hat{y}_{t+1|t} = \\alpha y_t + (1 - \\alpha) \\hat{y}_{t|t-1}\\)\n\\(\\alpha\\): Smoothing parameter\n\nClose to 0: More weight to past observations\nClose to 1: More weight to current observation (closer to Naive forecast)\n\nInitial Forecast:\n\n\\(\\hat{y}_{1|0} = y_1\\)\nHeuristic: linear interpolation of the first few observations\nLearn it by optimizing SSE\n\nForecasts are flat\n\n\n from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n\n SES = SimpleExpSmoothing(data, initialization_method='heuristic')=\n\n # Fit the model\n model = SES.fit(smoothing_level=0.2, optimized=False)\n\n # Forecast\n forecast = model.forecast(steps=5)\n\n\n\n\nExtend SES to include a trend component \\[\\hat{y}_{t+h|t} = \\ell_t + h b_t\\]\n\\[\\ell_t = \\alpha y_t + (1 - \\alpha)(\\ell_{t-1} + b_{t-1})\\]\n\\[b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1 - \\beta)b_{t-1}\\]\n\\(\\ell_t\\): Level\n\\(b_t\\): Smoothness of the trend\n\nClose to 0: Trend is more linear\nClose to 1: Trend changes with each observation\n\n\\(\\alpha\\): Smoothing parameter for level\n\n\n\n\n\nExtend Holt’s method to include a seasonal component \\[\\hat{y}_{t+h|t} = \\ell_t + h b_t + s_{t-m+h_m}\\]\n\\[b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1 - \\beta)b_{t-1}\\]\nFor Additive Seasonal: \\[\\ell_t = \\alpha(y_t - s_{t-m}) + (1 - \\alpha)(\\ell_{t-1} + b_{t-1})\\]\n\\[s_t = \\gamma(y_t - \\ell_{t-1} - b_{t-1}) + (1 - \\gamma)s_{t-m}\\]\nFor Multiplicative Seasonal: \\[\\ell_t = \\alpha\\frac{y_t}{s_{t-m}} + (1 - \\alpha)(\\ell_{t-1} + b_{t-1})\\]\n\\[s_t = \\gamma\\frac{y_t}{\\ell_{t-1} + b_{t-1}} + (1 - \\gamma)s_{t-m}\\]\n\n\n\n\nTrend component\nSeasonal Component\n\n\n\n\nNone (N)\nNone (N)\n\n\nAdditive (A)\nAdditive (A)\n\n\nAdditive Damped (Ad)\nMultiplicative (M)\n\n\n\n\nSimple Exponential Smoothing (N,N)\nHolt’s Method (A,N)\nHolt’s Winter Method (A,A)\n\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\n model = ExponentialSmoothing(data,\n     trend='add',\n     damped_trend=True,\n     seasonal='mul',\n     seasonal_periods=12,\n     initialization_method='estimated'\n ).fit(method=\"least_squares\")\n\n\n\n\nComponents:\n\nError: {A, M}\nTrend: {N, A, Ad}\nSeasonal: {N, A, M}\n\n\nfrom statsmodels.tsa.holtwinters import ETSModel\n\n model = ETSModel(data,\n      error='add',\n      trend='add',\n      damped_trend=True,\n      seasonal='add',\n      seasonal_periods=12\n ).fit()\n\n # Forecast\n model.forecast(steps=5)\n\n # Summary\n  model.summary()\n\nCan generate prediction intervals (confidence intervals):\n\nmodel.get_prediction() (analytical)\nmodel.simulate()\n\n\npred = model.get_prediction(start=df.index[-1] + pd.DateOffset(months=1), end=\"2020\").summary_frame()\n\n# or\nq = 0.975 # 95% CI\nsim = model.simulate(anchor=\"end\", nsimulations=348, repetitions=100, random_errors=\"bootstrap\")\nsimu = pd.DataFrame({\"median\": simulations.median(axis=1),\n                     \"pi_lower\": simulations.quantile((1 - q), axis=1),\n                     \"pi_upper\": simulations.quantile(q, axis=1)},\n                    index=simulations.index)\n\n\n\n\n\nMetrics, Commonly used:\n\nAIC, BIC\nSSE/ MSE/ RMSE\n\n# using ets model from above\nmodel.aic\nmodel.bic\nmodel.mse\nResiduals:\n\nVisual inspection (should be uncorrelated, zero mean, normally distributed)\nRunning diagnostic Portmanteau tests:\n\nLjung-Box Test: \\(H_0\\): Residuals are uncorrelated (white noise)\n\np-value &lt; 0.05: Reject \\(H_0\\) (bad)\n\nJarque-Bera Test: \\(H_0\\): Residuals are normally distributed\n\np-value &lt; 0.05: Reject \\(H_0\\) (bad)\n\n\n\n\n# using ets model from above\nmodel.summary().tables[-1]\n\n# Ljung-Box Test\np = model.test_serial_correlation(method=\"ljungbox\", lags=10)[0,1,-1]\n# Jarque-Bera Test\np = model.test_normality(method=\"jarquebera\")[0,1]\n\nOut-of-sample Forecasting:\n\nSplit data into training and testing\nFit model on training data\nForecast on testing data\nCompare forecast with actuals\n\n\n\n\n\n\n\nARIMA: AutoRegressive Integrated Moving Average\nCommonly used for time series forecasting (other than exponential smoothing)\nBased on autocorrelation of data\nDo not model trend nor seasonality, so it is typically constrained to stationary data\n\n\n\n\nStatistical properties of a time series do not change over time\n\nMean, variance is constant\nIs roughly horizontal (no strong trend)\nDoes not show predictable patterns (no seasonality)\n\nDOES not mean that the time series is constant, just that the way it changes is constant\nIt is one way of modelling dependence structure\n\nCan only be independent in one way but dependent in many ways\n\n\n\n\n\n\n\n\n\n\n\n\nProperty\nStrong Stationarity\nWeak Stationarity\n\n\n\n\nMean, Variance, Autocovariance\nConstant\nConstant\n\n\nHigher order moments (skewness, kurtosis)\nConstant\nNot necessarily constant\n\n\n\n\nWeak stationarity is often sufficient for time series analysis\n\n\n\n\n\n\nVisual Inspection: Plot the time series\n\nLook for trends, seasonality, and variance (none of these should be present)\nMake a correlogram plot (ACF plot should rapidly decay to 0)\n\nSummary Statistics: Calculate mean, variance, and autocovariance\n\nMean and variance should be roughly constant over time\n\nHypothesis Testing: Use statistical tests\n\nAugmented Dickey-Fuller (ADF) test\n\nNull hypothesis: Time series is non-stationary\nsmall p: it is stationary (reject null)\nUse statsmodels.tsa.stattools.adfuller\n\nKwiatkowski-Phillips-Schmidt-Shin (KPSS) test\n\nNull hypothesis: Time series is stationary\nsmall p: it is non-stationary (reject null)\n\n\n\nfrom statsmodels.tsa.stattools import adfuller\n\n# ADF test\nresult = adfuller(data)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\n\n\n\n\nStabilizing the variance using transformations\n\nLog or box-cox transformation\n\n\\[w_t = \\begin{cases} \\frac{y_t^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0 \\\\ \\ln(y_t) & \\text{if } \\lambda = 0 \\end{cases}\\]\nfrom scipy.stats import boxcox\nimport numpy as np\n\ndata = boxcox(data, lmbda=0)\n\n# log transformation\ndata = np.log(data)\nStabilize the mean using differencing\n\nFirst difference: \\(y' = y_t - y_{t-1}\\)\nSecond difference: \\(y'' = y' - y'_{t-1} = y_t - 2y_{t-1} + y_{t-2}\\)\nSeasonal difference: \\(y' = y_t - y_{t-m}\\), where \\(m\\) is the seasonal period\n\n# First difference\ndata1 = data.diff().dropna()\n# Second difference\ndata2 = data.diff().diff().dropna()\n# Seasonal difference, m is the seasonal period\ndata_m = data.diff(m).dropna()\n\n\n\n\n\n\n\n\n\n\n\nAR (AutoRegressive) Model\nMA (Moving Average) Model\n\n\n\n\nRegression of the time series on its own lagged values\nRegression of the time series on past forecast errors\n\n\n\\(y_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + \\epsilon_t\\)\n\\(y_t = \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\ldots + \\theta_q \\epsilon_{t-q}\\)\n\n\n\\(p\\): order of the AR model\n\\(q\\): order of the MA model\n\n\n\\(\\phi\\): AR coefficients\n\\(\\theta\\): MA coefficients\n\n\n\\(\\epsilon_t\\): white noise\n\\(\\epsilon_t\\): white noise\n\n\nLong memory model: \\(y_1\\) has a direct effect on \\(y_t\\) for all \\(t\\)\nShort memory model: \\(y_t\\) is only affected by recent values of \\(\\epsilon\\)\n\n\nGood for modeling time-series with dependency on past values\nGood for modeling time-series with a lot of volatility and noise\n\n\nLess sensitive to choice of lag or window size\nMore sensitive to choice of lag or window size\n\n\n\n\nBoth values are between -1 and 1\nAR value of 1 means that the time series is a random walk\n\n\n\n\n\nARMA: AutoRegressive Moving Average\nCombines AR and MA models\nKey Idea: Parsimony\n\nfit a simpler, mixed model with fewer parameters, than either a pure AR or a pure MA model\n\n\n\\[y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\ldots + \\theta_q \\epsilon_{t-q}\\]\n\n\\(c\\): constant\n\\(\\phi\\): AR coefficients\n\\(\\theta\\): MA coefficients\nUsually write it as ARMA(p, q)\n\n\n\n\n\nARIMA: AutoRegressive Integrated Moving Average\nCombines ARMA with differencing\nARIMA(p, d, q)\n\np: order of the AR model\nd: degree of differencing\nq: order of the MA model\n\nUse statsmodels.tsa.arima.model.ARIMA\n\nfrom statsmodels.tsa.arima.model import ARIMA\n\n# All with first order differencing\nmodel_ar = ARIMA(data[\"col\"], order=(3, 1, 0)).fit() # AR(3)\nmodel_ma = ARIMA(data[\"col\"], order=(0, 1, 1)).fit() # MA(1)\nmodel_arima = ARIMA(data[\"col\"], order=(3, 1, 3)).fit() # ARIMA(3, 3)\n\nmodel_arma = ARIMA(data[\"col\"], order=(3, 0, 3)).fit() # ARMA(3, 3)\n\n\nimport pmdarima as pm\n\nautoarima = pm.auto_arima(data.col,\n                          start_p=0, star_d=1, start_q=0,\n                          max_p=5, max_d=3, max_q=5,\n                          seasonal=False)\n\nautoarima.summary()\n\n\n\n\nSARIMA: Seasonal ARIMA\nSARIMA(p, d, q)(P, D, Q, m)\n\np, d, q: ARIMA parameters\nP, D, Q: Seasonal ARIMA parameters\nm: seasonal period\n\nE.g. In a dataset with years and 12 months\n\n\\(p=2\\) means Jan is affected by Dec and Nov\n\\(P=2\\) means Jan is affected by Jan of the previous 2 years\n\n\nsarima = ARIMA(data[\"col\"], order=(3, 1, 3), seasonal_order=(1, 1, 1, 12)).fit()\n\nAlso have SARIMAX (with exogenous variables)\n\nadds exogenous variables (other time series) to the model\nNot the most effective model\n\n\n\n\n\n\n\nACF and PACF plots\n\nACF: Autocorrelation Function\nPACF: Partial Autocorrelation Function\nUse these to determine the order of the AR and MA models\n\n\n\n\n\n\n\n\n\nACF Plot\nPACF Plot\n\n\n\n\nMeasures correlation between an observation and its lagged values\nsame but removes intermediate correlations (kinda isolates the direct effect)\n\n\nFor MA(q), cuts off after lag q\nFor AR(p), cuts off after lag p\n\n\nElse, tails off (exp or like damped sin)\nElse, tails off (no clear pattern)\n\n\n\n\nSee the cutoff when the peaks are lower than the shaded region\n\n\n\n\n\\(y_t=-0.9y_{t-1}+\\epsilon_t\\) \n\\(y_t=0.3y_{t-1}+\\epsilon_t\\) \n\\(y_t=0.5y_{t-1}-0.8y_{t-2}+\\epsilon_t\\) \n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_process import ArmaProcess\n\n# Simulate 1\ndf = pd.DataFrame(ArmaProcess(ar=[1, -0.9]).generate_sample())\n# Simulate 2\ndf = pd.DataFrame(ArmaProcess(ar=[1, 0.3]).generate_sample())\n# Simulate 3\ndf = pd.DataFrame(ArmaProcess(ar=[1, -0.5, 0.8]).generate_sample())\n\n# Plot\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\nfig.suptitle(\"y_t = 0.9y_{t-1} + e_t\")\ndf.plot(ax=axes[0])\nplot_acf(df, ax=axes[1])\nplot_pacf(df, ax=axes[2]);\n\n\n\n\n\\(y_t = \\epsilon_t + 0.9\\epsilon_{t-1}\\) \n\ndf = pd.DataFrame(ArmaProcess(ma=[1, -0.9]).generate_sample())\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))\nfig.suptitle(\"$y_t = \\epsilon_t - 0.9 \\epsilon_{t-1} $\")\ndf.plot(ax=axes[0])\nplot_acf(df, ax=axes[1])\nplot_pacf(df, ax=axes[2])\n\n\n\n(Based on lab 2 q4)\n\nLoad Data\nimport pandas as pd\n\n# turns first col into index + parses dates\ndf = pd.read_csv('data.csv', index_col=0, parse_dates=True)\nEDA with plot + ACF + PACF (Stationarity check)\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\ndf.plot(ax=axes[0])\nplot_acf(df, ax=axes[1])\nplot_pacf(df, ax=axes[2]);\n\nCan also check with ADF test (p&gt;0.05 means non-stationary)\n\nfrom statsmodels.tsa.stattools import adfuller\n\n# ADF test\nresult = adfuller(data)\nMake the time series stationary\n# Difference\ndata1 = data.diff().dropna()\n\n # Log transformation\n data = np.log(data)\n\nRepeat step 2 and check, also use ACF and PACF to find the AR and MA orders\n\nARIMA Model\nfrom statsmodels.tsa.arima.model import ARIMA\n\nmodel = ARIMA(train, order=(2, 1, 2), seasonal_order=(0, 1, 0, 12)).fit()\n\nmodel.summary()\nmodel.plot_diagnostics()\n\nCan also use auto_arima from pmdarima for hyperparameter tuning\n\nimport pmdarima as pm\n\nautoarima = pm.auto_arima(data.col,\n                          start_p=0, star_d=1, start_q=0,\n                          max_p=5, max_d=3, max_q=5,\n                          seasonal=False)\n\nautoarima.summary()\nautoarima.plot_diagnostics()\nForecast\nforecast = model.forecast(steps=len(valid))\n\nCan also use predict for in-sample prediction\n\npred = model.predict(start=len(train), end=len(train)+len(valid)-1)\n\nfig, ax = plt.subplots()\nax.plot(valid, label='Valid')\nax.plot(pred, label='Prediction')\nax.legend()\n\n\n\n\n\n\n\n\n\n\n\nTraditional ML\nTime Series ML\n\n\n\n\nData is IID\nData is ordered\n\n\nCV is random\nUse sliding window CV\n\n\nUse feature engineering\nUse lags, rolling windows, etc.\n\n\nPredict new data\nPredict future (specify horizon)\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n# turns first col into index + parses dates\ndf = pd.read_csv('data.csv', index_col=0, parse_dates=True)\n\n\n\nfrom sktime.transformations.series.lag import Lag\n# Make a new column with lag\ndf['col-1'] = df['col'].shift(1)\n# or use sktime\nt = Lag(lags=[1,2,3], index_out=\"original\")\npd.concat([df, t.fit_transform(df)], axis=1)\n\n\n\n\nNever use random shuffling\nNeed to keep temporal order\n\nfrom sktime.split import temporal_train_test_split\nfrom sklearn.model_selection import train_test_split\n\ny_train, y_test = temporal_train_test_split(y, test_size=0.2)\n\n# or use sklearn\ndf_train, df_test = train_test_split(df.dropna(), test_size=0.2, shuffle=False)\n\n\n\nExpanding Window: start with small training set and increase it \nFixed/sliding Window: use a fixed window size \n\n\n\n\n\nfrom sktime.forecasting.model_evaluation import evaluate\nfrom sktime.forecasting.naive import NaiveForecaster\nfrom sktime.forecasting.arima import AutoARIMA\n\nforecaster = NaiveForecaster(strategy=\"last\", sp=12) # seasonal naive\n# forecaster = AutoARIMA(sp=12)\n\nresults = evaluate(forecaster=forecaster, y=y_train, cv=cv, strategy=\"refit\", return_data=True)\n\n\n\n\n\nOne-step forecasting: one step ahead\nMulti-step forecasting: multiple steps ahead\n\nRecursive strategy: predict t, then it becomes part of the input for t+1\nDirect strategy: have a model for each step (model for t+1, another for t+2, etc)\nHybrid strategy: is dumb and bad\nMulti-output strategy: 2 different series (e.g. temperature and humidity)\n\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sktime.forecasting.compose import make_reduction\n\nregressor = KNeighborsRegressor()\n\n\nforecaster = make_reduction(regressor,\n   window_length=12,\n   strategy=\"recursive\") # or \"direct\" or \"dirrec\" or \"multioutput\"\n\n\n\n\n\n\nCoerce to stationary (via diff or transforms)\nSmoothing (e.g. moving average)\nImpute missing value (e.g. linear interpolation)\nRemoving outliers\n\n\n\n\n\nLagging features/ responses\nAdding time stamps (e.g. day of week, month, etc) 3, Rolling Statistics (e.g. rolling mean, rolling std)\n\n\n\n\n\n\nMeans time series with multiple variables (e.g. temperature and humidity)\n\n\n\n\n\n\nWe have been dealing with point forecasts (modelling averages)\nWant to estimate the uncertainty of our forecasts\n\nor the extreme (e.g. 90% or 95% quantiles)\n\nexample: find upper quantile of electricity demand so that we can plan for the maximum demand\n\nor predict the variance of the forecast (how volatile a metric will be in the future)\n\n\n\n\n\nAssume distribution of forecasts are normal\n\n\\[\n\\hat{y}_{T+h|T} \\pm c \\hat{\\sigma}_{h}\n\\]\n\n\\(\\hat{\\sigma}_{h}\\) is the standard deviation of the forecast\n\\(c\\): coverage factor (e.g. 1.96 for 95% confidence interval)\n\n\\[\n\\hat{\\sigma}_{h} = \\sqrt{\\frac{1}{T-K}\\sum_{t=1}^{T} e_{t}^{2}}\n\\]\n\nFocus is finding \\(\\hat{\\sigma}_{h}\\)\n\n\\(K\\): number of parameters\n\\(T\\): total length of time series\n\\(e_{t} = y_{t} - \\hat{y}_{t|t-1}\\)\n\nMethods that have been derived mathematically: | Method | Forecast sd | |——–|————–| | Mean | \\(\\hat{\\sigma}_{h} = \\hat{\\sigma_1} \\sqrt{1 + \\frac{h}{T}}\\) | |Naive | \\(\\hat{\\sigma}_{h} = \\hat{\\sigma_1} \\sqrt{h}\\) | | Seasonal Naive | $_{h} = $ | | Drift | \\(\\hat{\\sigma}_{h} = \\hat{\\sigma_1} \\sqrt{h(1+\\frac{h}{T})}\\) |\nRecall: \\(h\\) is the forecast horizon (steps ahead), \\(m\\) is the seasonal period\n\nfrom pandas import pd\n\nc = 1.96 # 95% confidence interval\n\ntrain['pred'] = train['y'].shift(1)\ntrain['residuals'] = train['y'] - train['pred']\nsigma = train['residuals'].std()\n\nh = np.arange(1, len(forecast_index) + 1)\nnaive_forecast = train['y'].iloc[-1]\n\n# create lower and upper bound\nnaive = pd.DataFrame({\"y\": naive_forecast,\n                      \"pi_lower\": naive_forecast - c * sigma * np.sqrt(horizon),\n                      \"pi_upper\": naive_forecast + c * sigma * np.sqrt(horizon),\n                      \"Label\": \"Naive\"},\n                     index=forecast_index)\nplot_prediction_intervals(train[\"y\"], naive, \"y\", valid=valid[\"y\"])\n# ETS\nmodel = ETSModel(train[\"y\"], error=\"add\", trend=\"add\", seasonal=\"add\").fit(disp=0)\n\nets = model.get_prediction(start=forecast_index[0], end=forecast_index[-1]).summary_frame()\nplot_prediction_intervals(train[\"y\"], ets, \"mean\", valid=valid[\"y\"], width=800)\n\n# ARIMA\nmodel = ARIMA(train[\"y\"], order=(3, 1, 0), seasonal_order=(2, 1, 0, 12)).fit()\n\narima = model.get_prediction(start=forecast_index[0], end=forecast_index[-1]).summary_frame()\nplot_prediction_intervals(train[\"y\"], arima, \"mean\", valid=valid[\"y\"], width=800)\n\n\n\n\n\nAssume future errors will be similar to past errors\nDraw from the distribution of past errors to simulate future errors\n\n\\[y_{T+h} = \\hat{y}_{T+h|T} + \\epsilon_{T+h}\\]\n# Fit an ETS model\nmodel = ETSModel(train[\"y\"], error=\"add\", trend=\"add\").fit(disp=0)\n\n# simulate predictions\nets = model.simulate(anchor=\"end\", nsimulations=len(forecast_index),\n                     repetitions=n_simulations,\n                     random_errors=\"bootstrap\")\n# plot\nax = train[\"y\"].plot.line()\nets.plot.line(ax=ax, legend=False, color=\"r\", alpha=0.05,\n              xlabel=\"Time\", ylabel=\"y\", figsize=(8,5));\n\n# get quantiles\nets = pd.DataFrame({\"median\": ets.median(axis=1),\n                    \"pi_lower\": ets.quantile(1-0.975, axis=1),\n                    \"pi_upper\": ets.quantile(0.975, axis=1)},\n                   index=forecast_index)\n\n\n\n\n\nWish to predict particular quantile instead of mean\n\ne.g \\(q=0.9\\) so we expect 90% of the future values to be below the forecast\n\nPinball loss/ Quantile loss: \\[\n\\mathcal{L}=\n     \\left\\{\n\\begin{array}{ll}\n      (1-q)(\\hat{y}_{t,q}-y_t) \\text{,} \\;\\; \\text{ if } y_t &lt; \\hat{y}_{t,q} \\\\\n      q(y_t-\\hat{y}_{t,q}) \\text{,} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\text{ if } y_t \\ge \\hat{y}_{t,q} \\\\\n\\end{array}\n\\right.\n\\]\n\n\n\n\n\n\n\n\nHigh Quantile\nLow Quantile\n\n\n\n\nHigher penalty for predicting OVER\nHigher penalty for predicting UNDER\n\n\n\n\n\nsee here\nQuantile loss is not currently a supported criterion in pytorch but it’s easy to define ourselves. We really have two options:\n\nTrain a network for each quantile we want to predict; or\nTrain a network to output multiple quantiles at once\n\n\n\n\n\n\nThere are 4 main sources of uncertainty:\n\nRandom error term\nUncertainty in model parameter estimates\nUncertainty in model selection\nUncertainty about consistency of data generating process in the future\n\nMost methods only consider the first source of uncertainty\nSimulation tries to consider 2 and 3\n4 is practically impossible to consider\n\n\n\n\n\n\nOutliers are observations that are significantly different from the rest of the data\n\nCan be due to measurement error, data entry error, or just unique observations\n\n\n\n\n\nMethodology:\n\nSubtract the rolling median from data (with suitable window size)\nCalculate standard deviation of the residuals (\\(\\hat{\\sigma_r}\\))\nAssume normally distributed residuals then identify outliers as outside the 95% confidence interval (\\(\\pm 1.96 \\hat{\\sigma_r}\\))\n\n\n\n\n\n\nMethodology:\n\nDecompose time series to find residuals:\n\nNon-seasonal data: use LOESS\nSeasonal data: use STL (Seasonal-Trend decomposition using LOESS)\n\nCalculate \\(q_{0.1}\\) and \\(q_{0.9}\\) of the residuals\nIdentify outliers as \\(\\pm2 \\times (q_{0.9} - q_{0.1})\\)\n\n\n\n\n\n\nMethodology:\n\nFit a model to the data\nIdentify outliers as significant deviations from model predictions (e.g. 95% confidence interval)\n\n\n\n\n\n\nTrain an ML model to predict outliers\nA few common packages: pyod, sklearn, luminaire, sklyline, etc.\n\n\n\n\nBuilt on basis of decision trees\nHigh-level idea:\n\nrandomly select a feature\nrandomly splits that feature into 2 values\nrepeat until all data points are isolated\n\nLess splits to isolate a data point = more likely to be an outlier\nScore = [0, 1] where 1 is an outlier, &gt; 0.5 are normal observations.\n\n\n\nExample of sklearn’s IsolationForest:\n\nfrom sklearn.ensemble import IsolationForest\n\noutliers = IsolationForest(contamination=0.05).fit_predict(df) == -1\n\n\n\n\nFor each data point, calculate the distance to its k-th nearest neighbor\n\nLarge distance = outlier\n\nSupports 3 kNN detectors:\n\nLargest: distance to the k-th neighbor\nMean: average distance to k neighbors\nMedian: median distance to k neighbors\n\npyod’s KNN() outlier detection\n\nfrom pyod.models.knn import KNN\n\noutliers = KNN(contamination=0.05).fit_predict(df).labels_ == 1\n\n\n\n\n\nGlobal outliers: A data point with its value is far outside of the entirety of the data set (e.g., billionaires)\nLocal/Contextual outliers: A data point is considered a contextual outlier if its value significantly deviates from the rest the data points in the same context. (e.g., earning 50K income in a developing countries)\n\n\n\n\n\n\nImputation: Filling in missing values/ outliers in a dataset\nOverarching techniques:\n\nRemove (.dropna())\nFill manually based on some expert-interpreted values (.fillna())\nFill with mean/median/mode (.fillna())\nFill based on rolling statistic, e.g. rolling mean/median\nPolynomial interpolation\nFill based on temporal dependence\n\ni.e. use same value from the same period last season, or average of all periods in the past season\n\nFill with model fitted values\nUse MICE (Multiple Imputation by Chained Equations) from statsmodels or IterativeImputer from sklearn.\n\n\n\n\n\n\nClassical time series (ARIMA, Exponential Smoothing) limitations:\n\nThey are linear models\nInputs must be specified, not learned automatically\nFocus on univariate time series (lack support for multivariate time series)\nFocus on regression (lack support for classification)\nAssumes complete, non-missing data\n\nDeep learning models can address these limitations, as NNs are:\n\nrobust to noise\ncan learn complex, non-linear relationships\nsupport multivariate time series\nfeature learning (learn the inputs)\ntemporal dependencies can be learned\n\n\n\n\n\nNN allows us so that the features do not need to be ordered\n\ne.g. target, lag1. lag2, lag3 is the same as lag2, lag3, target, lag1\n\nNeed to take into account inherent temporal dependencies (approach above does not)\nDo something similar like CNN (retain spatial information) but for time series\n\n\n\n\n\nCan work with 3D data (channels, height, width)\nCan also do 2D data (features, time)\nUse Conv1D from pytorch to work with time series data\n\nLooks at local groups of data (filter of some size)\nMissing memory (does not remember previous values, only looks at local groups of data)\n\ncan be solved with RNN\n\n\n\n\nImage with sequence of 20 values, filtered with 4 kernels of size 3\nimport torch.nn.Conv1d\nimport pandas as pd\nimport plotly.express as px\nfrom sktime.utils.data_processing import lag_df\n\nconv_layer = nn.Conv1d(in_channels=1, out_channels=4, kernel_size=3, padding=1)\n\n# Load data\ntrain = (pd.read_csv(\"data/unemployed_train.csv\", index_col=0, parse_dates=True)\n           .resample(\"1M\").mean()\n           .assign(Label=\"Train\")\n        )\ntest = (pd.read_csv(\"data/unemployed_valid.csv\", index_col=0, parse_dates=True)\n           .resample(\"1M\").mean()\n           .assign(Label=\"Test\")\n        )\n\n# Plot train and test data\npx.line(pd.concat((train, test)), y=\"Unemployed persons\", color=\"Label\", width=600, title=\"Australian Unemployment\")\n\n# Make lag features\nSEQUENCE_LENGTH = 24\nBATCH_SIZE = 16\ncnn_data = lag_df(train, lags=SEQUENCE_LENGTH, dropna=True)\ncnn_data = cnn_data[cnn_data.columns[::-1]]  # Reverse columns to have lag0 at the end\n\nThen make CNN with optimizer Adam and loss function MSE\n\n\n\n\n\nReasonable results with CNN because preserve structure of data\nStructure: - Split up and process one time-step at a time\n \nGreat video on RNNs\nDraw back: Vanishing Gradient Problem\n\nGradient becomes so small that it does not update the weights\nEarly time steps are not updated (long-term dependencies are not learned), they “forget”\nCan be solved with LSTM and GRU\n\n\n\n\n\n\n\nAnother great video on LSTM\nAll the yellow boxes (LSTM cells) are identical (same weights and architecture)\nComponents:\n\nCell State: The horizontal line running through the top of the diagram\n\nIt runs straight down the entire chain, with only some minor linear interactions\n\nForget Gate: Decides what information to throw away from the cell state\nInput Gate: Decides what new information to store in the cell state\nOutput Gate: Decides what to output based on the cell state\n\n\nfrom torch import nn\n\nclass LSTM(nn.Module):\n    def __init__(self, input_size, output_size, hidden_dim):\n        super().__init__()\n\n        self.hidden_dim = hidden_dim\n        self.lstm = nn.LSTM(input_size, hidden_dim, batch_first=True, num_layers=1)\n        self.fc = nn.Linear(hidden_dim, output_size)\n\n    def forward(self, x, hidden):\n        # x :           (batch_size, seq_length, input_size)\n        # hidden :      (short_mem, long_mem), each (n_layers, batch_size, hidden_dim)\n        # output :  (batch_size, seq_length, input_size)\n        # note that the output will have the same shape as the input because\n        # it makes a forecast for each time step in the sequence\n        # but we only care about the last prediction (the forecast after the sequence)\n        # so I'll only take the last value of the output\n\n        prediction, (short_mem, long_mem) = self.lstm(x, hidden)\n        output = self.fc(prediction)\n        return output[:, -1, :], short_mem, long_mem\n\nGRU is similar to LSTM, but has less parameters and is faster to train\n\n\n\n\n\n\n\nFrankenstein of classical time series models (decomposition, regression, exponential smoothing, etc)\n\nimport pandas as pd\nfrom prophet import Prophet\n\n# Sample data\ndata = pd.read_csv('example_data.csv')\ndata['ds'] = pd.to_datetime(data['ds'])\n\n# Initialize Prophet model\nmodel = Prophet(interval_width=0.95, yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True)\n\n# Fit the model\nmodel.fit(data)\n\n# Make future predictions\ntest_dates = model.make_future_dataframe(periods=len(test), freq='M', include_history=False)\nforecast = model.predict(test_dates)\n\n# Visualize the forecast\nfig = model.plot(forecast)\n\n\n\n\nGluonTS: alternative to pytorch.\nPyTorch Forecasting: built on top of pytorch, but with more focus on time series forecasting.\nsktime: scikit-learn for time series data.\nTidyverts: R package for time series forecasting.\n\n\n\n\n\n\nHeirarchical Time Series: Forecasting at different levels of aggregation (e.g. product sales at store level, then at regional level, then at national level)\n\nBottom-up: Forecast at the lowest level and aggregate up\nTop-down: Forecast at the highest level and disaggregate down\n\nMultiple Seasonalities: e.g. daily and weekly seasonality\n\ndecompose independently\ndecompose simultaneously (e.g. propher/ statsmodels)\n\nMultivariate Time Series: e.g. sales and advertising spend\n\nVAR (Vector Auto Regression)\nLSTM with multiple inputs\n\nExplanatory variables:\n\neasy to add features to ML models\nARIMA can but using exog parameter\n\nTime Series Classification:\n\nHidden Markov Models\n\n\n\n\n\n\n\nData with location information\nHas spacial dependence\nMain Tasks:\n\nWrangling\nVisualization\nModelling\n\nRepresentation:\n\nVector\nRaster\n\n\n\n\n\nCollection of discrete locations/ vertices (x, y) to form:\n\nPoints: single location\nLines: series of points\nPolygons: series of lines (closed shape)\n\nStored in .shp (shapefile) format\n\n.shp: geometry\n.shx: index (how geometry relates to one another)\n.dbf: attributes (e.g. population, area)\n\n\n\n\n\nTo read and write vector data\nBuilt off of pandas and shapely\nSimilar to pandas it has:\n\nGeoSeries: series of geometries\nGeoDataFrame: dataframe with geometry column\n\nGeometry column contains vector data\n\n\n\n\nimport geopandas as gpd\n\n# Read data\ngdf = gpd.read_file(“path/to/shp_dir”)\n\n# Plot data\ngdf.plot()\n\n\n\nimport geopandas as gpd\n\nlat = [49.2827, 49.2827, 49.2827, 49.2827]\nlon = [-123.1207, -123.1207, -123.1207, -123.1207]\n\ngdf = gpd.GeoDataFrame(geometry=gpd.points_from_xy(lon, lat))\ngdf.plot()\n\n\n\n\nIt is like the wikipedia of geospacial data\nUse osmnx to get data from OpenStreetMap\n\nimport osmnx as ox\n\nvancouver = ox.geocode_to_gdf(\"Vancouver, Canada\")\nvancouver.plot(edgecolor=\"0.2\")\nplt.title(\"Vancouver\");\n\n# get higher resolution\nvan_bc = gpd.clip(bc, vancouver)\nvan_bc.plot(edgecolor=\"0.2\")\n\n# Plot stanley park in vancouver\nstanley_park = ox.geocode_to_gdf(\"Stanley Park, Vancouver\")\nax = van_bc.plot(edgecolor=\"0.2\")\nstanley_park.plot(ax=ax, edgecolor=\"0.2\", color=\"tomato\")\n\n# Graph bike network in vancouver\nbike_network = ox.graph_from_place(\"Stanley Park, Vancouver\",\n    network_type=\"bike\")\n\nax = stanley_park.plot(edgecolor=\"0.2\")\nbike_network.plot(ax=ax, edgecolor=\"0.2\", color=\"tomato\")\n\n# can be interactive\nbike_network.explore()\n\n\n\n\n\nAdd width to line: gdf.buffer(2) to add a 2m to left and right of the line (4m total)\nGet Length of line: gdf.length.sum() to get the length of the line\n\nNeed to convert to linear meters first\n\nGet Area of polygon: gdf.area.sum() to get the area of the polygon\n\nNeed to convert to linear meters first\n\nJoining: gpd.sjoin(gdf1, gdf2, how=‘left’, predicate=‘intersects’)\n\nhow: left, right, inner, outer\npredicate: intersects, contains, within, touches, crosses, overlaps\n\nGrouping: gdf.groupby(by=‘column’).sum().sort_values(\"length\", ascending=False)\n\n\n\n\n\n\n\nEach pixel has 4 bands: Red, Green, Blue, and Infrared\nResolution: size of each pixel (e.g. 1m x 1m)\n\nsmaller resolution = more detailed\n\nMost common format: GeoTIFF (.tif)\nUse Python library rasterio to read and write raster data\n\nimport rasterio\n\ndataset = rasterio.open(“path/to/raster.tif”)\n\n\n\n\nTypically identified by EPSG (European Petroleum Survey Group) code\nCommon CRS:\n\nAngular units (latitude and longitude): EPSG:4326\nLinear units (meters): Most common is UTM which is divided into zones. For British Columbia, it’s EPSG:32610\nMinimize distortion by choosing the right CRS, for Canada, it’s EPSG:3347 (“Lambert projection”)\n\nChange code in geopandas:gdf.to_crs(“EPSG:3347”)\n\n\n\n\n\n\n\n\n\nEasy to use and quick\n\n\nGet data of UBC buildings from osmnx\n\nimport osmnx as ox\nimport geopandas as gpd\n\nubc = (ox.features.features_from_place(\"University of British Columbia, Canada\",\n                                tags={'building':True}) # Just keep building footprints\n         .loc[:, [\"geometry\"]]                 # just keep the geometry column for now\n         .query(\"geometry.type == 'Polygon'\")  # only what polygons (buidling footprints)\n         .assign(Label=\"Building Footprints\")  # assign a label for later use\n         .reset_index(drop=True)               # reset to 0 integer indexing\n        )\n\nGet the building footprint of a specific building from its coordinates\n\npoint_coord = Point(-123.25203756532703,49.26314716306668)\n\nubc[ubc.contains(point_office)] # get the building that contains the point\n\nubc.loc[47, \"Label\"] = \"Earth Science Building\" # change the label\n\nPlot the GeoDataFrame\n\nax = ubc.plot(figsize=(8, 8), column=\"Label\", legend=True,\nedgecolor=\"0.2\", markersize=200, cmap=\"rainbow\")\nplt.title(\"UBC\");\n\nAdd map to the background\n\nimport contextily as cx\n\nax = (ubc.to_crs(\"EPSG:3857\")\n         .plot(figsize=(10, 8), column=\"Label\", legend=True,\n               edgecolor=\"0.2\", markersize=200, cmap=\"rainbow\")\n     )\n\ncx.add_basemap(ax, source=cx.providers.OpenStreetMap.Mapnik)  # I'm using OSM as the source. See all provides with ctx.providers\nplt.axis(\"off\")\nplt.title(\"UBC\");\n\n\n\n\n\nTo add interactivity to the map\nBacked by MapBox (mapping and location data cloud platform)\n\nimport plotly.express as px\n\n# Does the same thing as the previous cell, but with plotly express (interactive)\nfig = px.choropleth_mapbox(ubc,\n                            geojson=ubc.geometry,\n                            locations=ubc.index,\n                            color=\"Label\",\n                            center={\"lat\": 49.261, \"lon\": -123.246},\n                            zoom=12.5,\n                            mapbox_style=\"open-street-map\")\nfig.update_layout(margin=dict(l=0, r=0, t=30, b=10))\n\nCan also plot the buildings with different colours based on building area\n\n# Calculate area\nubc[\"Area\"] = ubc.to_crs(epsg=3347).area  # (https://epsg.io/3347)\n\n# Make plot\nfig = px.choropleth_mapbox(ubc,\n                            geojson=ubc.geometry,\n                            locations=ubc.index,\n                            color=\"Area\",\n                            center={\"lat\": 49.261, \"lon\": -123.246},\n                            zoom=12.5,\n                            mapbox_style=\"carto-positron\")\nfig.update_layout(margin=dict(l=0, r=0, t=30, b=10))\n\n\n\n\n\nWeb-based geospatial analysis tool\nEven more powerful than Plotly Express\nHow it works:\n\nCreate instance of map with keplergl.KeplerGl()\nAdd as much data with .add_data() method\nCustomize and configure the map using GUI\n\n\nfrom keplergl import KeplerGl\n\nubc_map = keplergl.KeplerGl(height=500)\nubc_map.add_data(data=ubc.copy(), name=\"Building heights\")\n\nubc_map\n\nCan also make a 3d map with building heights\n\nLoad the building heights data\nJoin the data with the building footprints\nPlot the 3d map + addjust the GUI settings\n\n\n\n\n\n\n\nSource: https://www.neonscience.org/resources/learning-hub/tutorials/spatial-interpolation-basics\n\nTwo common ways to model spatial data:\n\nSpatial interpolation: use a set of observations in space to estimate the value of a spatial field\nAreal interpolation: project data from one set of polygons to another set of polygons\n\n“everything is related to everything else, but near things are more related than distant things” (Tobler, 1970)\n\n\n\n\nUse scipy module interpolate to do deterministic interpolation\nCommon Techniques:\n\nInverse Distance Weighting (IDW): estimate the value of a point based on the values of its neighbours (farther neighbours have less weight)\n\n\\[\\hat{z}(x) = \\frac{\\sum_{i=1}^{n} w_i(x)z_i}{\\sum_{i=1}^{n} w_i(x)}\\]\nwhere \\(w_i(x) = \\frac{1}{d_i(x)^p}\\), \\(d_i(x)\\) is the distance between \\(x\\) and \\(i\\), and \\(p\\) is the power parameter\n\nNearest Neighbour: estimate the value of a point based on the value of the nearest point (does not consider weights)\n\n\nLess smooth than IDW (more jagged)\n\n\nPolynomial Interpolation: estimate the value of a point based on the values of its neighbours using a polynomial function\nRadial Basis Function: estimate the value of a point based on the values of its neighbours using a radial basis function\n\n\nfrom scipy.interpolate import NearestNDInterpolator\n\n# Get the geodataframe\ngpm25 = (gpd.GeoDataFrame(\n            pm25,\n            crs=\"EPSG:4326\", # angular CRS\n            geometry=gpd.points_from_xy(pm25[\"Lon\"], pm25[\"Lat\"])) # create geometry from coordinates\n        .to_crs(\"EPSG:3347\") # convert to one for Canada\n        )\n\n# Creates points to interpolate\ngpm25[\"Easting\"], gpm25[\"Northing\"] = gpm25.geometry.x, gpm25.geometry.y\n\n# Create a grid\nresolution = 25000  # cell size in meters, smaller cell size = smaller pixel = higher resolution\ngridx = np.arange(gpm25.bounds.minx.min(), gpm25.bounds.maxx.max(), resolution)\ngridy = np.arange(gpm25.bounds.miny.min(), gpm25.bounds.maxy.max(), resolution)\n\n# Interpolate\ninterpolator = NearestNDInterpolator(gpm25[[\"Easting\", \"Northing\"]], gpm25[\"PM25\"])\nz = model(*np.meshgrid(gridx, gridy))\nplt.imshow(z);\n\nPlot it back to map:\n\n# Helper function:\ndef pixel2poly(x, y, z, resolution):\n    \"\"\"\n    x: x coords of cell\n    y: y coords of cell\n    z: matrix of values for each (x,y)\n    resolution: spatial resolution of each cell\n    \"\"\"\n    polygons = []\n    values = []\n    half_res = resolution / 2\n    for i, j  in itertools.product(range(len(x)), range(len(y))):\n        minx, maxx = x[i] - half_res, x[i] + half_res\n        miny, maxy = y[j] - half_res, y[j] + half_res\n        polygons.append(Polygon([(minx, miny), (minx, maxy), (maxx, maxy), (maxx, miny)]))\n        if isinstance(z, (int, float)):\n            values.append(z)\n        else:\n            values.append(z[j, i])\n    return polygons, values\npolygons, values = pixel2poly(gridx, gridy, z, resolution)\n\npm25_model = (gpd.GeoDataFrame({\"PM_25_modelled\": values}, geometry=polygons, crs=\"EPSG:3347\")\n                 .to_crs(\"EPSG:4326\")\n             )\n\nfig = px.choropleth_mapbox(pm25_model, geojson=pm25_model.geometry, locations=pm25_model.index,\n                           color=\"PM_25_modelled\", color_continuous_scale=\"RdYlGn_r\", opacity=0.5,\n                           center={\"lat\": 52.261, \"lon\": -123.246}, zoom=3.5,\n                           mapbox_style=\"carto-positron\")\nfig.update_layout(margin=dict(l=0, r=0, t=30, b=10))\nfig.update_traces(marker_line_width=0)\n\n\n\n\nKriging differs from deterministic because we interpolate using statistical models that include estimates of spatial autocorrelation\n\n\\[\\hat{Z}(s_0) = \\sum_{i=1}^{n} \\lambda_i Z(s_i)\\]\n\n\\(\\lambda_i\\) are the weights\n\\(Z(s_i)\\) are observations at locations \\(s_i\\)\n\\(N\\) is the size of \\(s\\) (number of observations) \nKriging uses spatial autocorrelation to estimate the weights\n\nLooking at the variance between points to estimate the weights\n\n\n\n\n\nDefines the spatial variance/ autocorrelation between points (as a function of distance)\n\nSimilar to ACF but for spatial data\n\nUsed to estimate the weights in kriging\ne.g. of a function: \\(\\gamma(s_i, s_j) = \\frac{1}{2}(Z(s_i) - Z(s_j))^2\\)\n\nsemi-variance because of a factor of 1/2\nEach pair is calculated twice\n\nPlot this function to get the variogram (x-axis: distance, y-axis: semivariance)\n\n\n\nNugget: variance at distance 0\n\nIdeally should be 0 (no variance at distance 0), higher nugget = more noise\nCan be thought of as Random error/ measurement error\n\nSill: maximum variance of spatial process\n\nrepresents the amount of spatial autocorrelation that exists at large enough distances to capture the underlying trend of the process\n\nRange: where the semivariance reaches the sill\n\n\n\n\n\nfrom pykrige.ok import OrdinaryKriging\n\nRESOLUTION = 250  # m\n\n# 1. Convert to meter-based\nvan_listings_gdf = van_listings_gdf.to_crs(\"EPSG:3347\")\n\n# 2. Add Easting and Northing columns\nvan_listings_gdf[\"Easting\"] = van_listings_gdf.geometry.x\nvan_listings_gdf[\"Northing\"] = van_listings_gdf.geometry.y\n\n# 3. Create a grid of points\ngridx = np.arange(\n    van_listings_gdf.bounds.minx.min(), van_listings_gdf.bounds.maxx.max(), RESOLUTION\n)\ngridy = np.arange(\n    van_listings_gdf.bounds.miny.min(), van_listings_gdf.bounds.maxy.max(), RESOLUTION\n)\n\n# 4. Kriging\nkrig = OrdinaryKriging(\n    x=van_listings_gdf[\"Easting\"],\n    y=van_listings_gdf[\"Northing\"],\n    z=van_listings_gdf[\"price\"],\n    variogram_model=\"spherical\",\n    verbose=False,\n    enable_plotting=False,\n)\n\n# 5. Execute and plot\nz, ss = krig.execute(\"grid\", gridx, gridy)\nplt.imshow(z)\n\n\n\n\n\nProject data from one set of polygons to another set of polygons\nE.g. Map air pollution data fo FSA (“forward sortation area”, which are groups of postal codes) polygons\n\n# Load the FSA data\nvan_fsa = gpd.read_file(\"data-spatial/van-fsa\")\n\n# Kriging (similar to previous cell)\nresolution = 10_000  # cell size in meters\ngridx = np.arange(gpm25.bounds.minx.min(), gpm25.bounds.maxx.max(), resolution)\ngridy = np.arange(gpm25.bounds.miny.min(), gpm25.bounds.maxy.max(), resolution)\nkrig = OrdinaryKriging(x=gpm25[\"Easting\"], y=gpm25[\"Northing\"], z=gpm25[\"PM_25\"], variogram_model=\"spherical\")\nz, ss = krig.execute(\"grid\", gridx, gridy)\npolygons, values = pixel2poly(gridx, gridy, z, resolution)\npm25_model = (gpd.GeoDataFrame({\"PM_25_modelled\": values}, geometry=polygons, crs=\"EPSG:3347\")\n                 .to_crs(\"EPSG:4326\")\n                 )\nz, ss = krig.execute(\"grid\", gridx, gridy)\n\n# Areal Interpolation\nareal_interp = area_interpolate(pm25_model.to_crs(\"EPSG:3347\"),\n                                van_fsa.to_crs(\"EPSG:3347\"),\n                                intensive_variables=[\"PM_25_modelled\"]).to_crs(\"EPSG:4326\")\nareal_interp.plot(column=\"PM_25_modelled\", figsize=(8, 8),\n                  edgecolor=\"0.2\", cmap=\"RdBu\", legend=True)\nplt.title(\"FSA Air Pollution\");\n\n\n\n\n\nUse Dijkstra’s algorithm to find the shortest path between two points\nCondition:\n\nThe graph must be weighted with non-negative weights\n\nAlgorithm:\n\nLabel start node with 0, all others with infinity\nLabel current node as visited\nGo to all connected nodes (to current) and update label with min(current_label, previous_label + weight)\n\n\nIf updated, then keep track of the previous node (for backtracking later)\n\n\nOnce all nodes around the current node are visited, go to the unvisited node with the smallest label and repeat step 2\nBacktrack from end node to start node using the previous node\n\nTime complexity: \\(O(V^2)\\) but its \\(O(E + V \\log V)\\) with a min-priority queue/ binary heap\nSpace complexity: \\(O(V)\\)\n\nimport osmnx as ox\nimport networkx as nx\n\n# Origin\norig_address = \"UBC bookstore, Vancouver\"\norig_y, orig_x = ox.geocode(orig_address)  # notice the coordinate order (y, x)!\n\n# Destination\ndest_address = \"Orchard Commons Student Residence, Vancouver\"\ndest_y, dest_x = ox.geocode(dest_address)\n\n# Find the nearest nodes\norig_node_id, dist_to_orig = ox.distance.nearest_nodes(G, X=orig_x, Y=orig_y, return_dist=True)\ndest_node_id, dist_to_dest = ox.distance.nearest_nodes(G, X=dest_x, Y=dest_y, return_dist=True)\n\n# Find the shortest path\nroute = nx.shortest_path(G, orig_node_id, dest_node_id, weight=\"length\")\n\n# Plot the shortest path\nox.plot.plot_graph_route(G, route)"
  },
  {
    "objectID": "block_5/574_spat_temp_mod/574_spat_temp.html#time-series",
    "href": "block_5/574_spat_temp_mod/574_spat_temp.html#time-series",
    "title": "Spatial and Temporal Models",
    "section": "",
    "text": "Collection of observations made sequentially in time\nData Types:\n\nUnivariate: Single observation at each time point (e.g. bike sale over time)\nMultivariate: Multiple observations at each time point (e.g. bike sale + profit over time)\nHeirarchical: Multiple time series, each with a hierarchical structure (e.g. bike sale + profit for each store over time)\n\nCommon Tasks:\n\nPrediction/ Forecasting (Supervised Learning)\n\nDifficult since many factors\n\nClustering/ Anomaly Detection (Unsupervised Learning)\n\n\n\n\n\n\n\nTime plot: x-axis = time, y-axis = value\n\n\n\n\n\nObservations close in time are often correlated\n\nCan quanitify using autocorrelation\n\nAutocorrelation: Correlation of a time series with a lagged version of itself\n\nLag: Time difference between two observations\nACF: Autocorrelation function\n\nPlots autocorrelation for different lags\n\nPACF: Partial autocorrelation function\n\nPlots correlation between two observations after removing the effect of other lags\n\ne.g. data[t (lag=1)] = data[t].shift(t)\n\n\n\n\n\nPlot of ACF vs. lag\nHelps identify patterns in time series\nUse statsmodels.graphics.tsaplots.plot_acf()\n\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nfig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(11, 5))\nplot_acf(data, lags=20, title='ACF', ax=ax1)\nax1.set_xlabel(\"Lag (years)\")\nax1.set_ylabel('ACF')\n \n\nShading indicates if correlation is significantly different from 0\n\n\\(CI = \\pm z_{\\alpha/2} SE(r_k)\\), \\(z_{\\alpha/2} \\approx 1.96\\) for 95% CI\n\\(SE(r_k) = \\frac{1}{\\sqrt{T}}\\), where \\(T\\) is the number of observations - Or Bartlett’s formula: \\(SE(r_k) = \\sqrt{\\frac{1 + 2\\sum_{j=1}^{k-1}r_j^2}{T}}\\) \n\nCO2 plot has a trend so ACF for smaller lags tend to be higher\nGeneral Key Observations:\n\nACF almost always decays with lag\nIf a series alternates (oscillates about mean), ACF will alternates too\nIf a series has seasonal or cyclical fluctuations, the ACF will oscillate at the same frequency\nIf there is a trend, ACF will decay slower (due to high correlation of the consecutive observations)\nExperience is required to interpret ACF\n\n\n\n\n\n\n\nTrend: Long-term increase/ decrease\nSeasonality: Regular pattern of up and down fluctuations (fixed interval)\n\ntypically over smaller time frame\n\nCyclic: Fluctuations not of fixed period (unknown and changing interval)\n\ntypically over larger time frame\n\n\n\n\n\n\nTime series with:\n\n0 mean\nConstant variance\nNo autocorrelation\n\nFurther assumed that it is iid and gaussian: \\(N(0, \\sigma^2)\\)\nWhy do we care?\n\nCannot predict white noise\nIf residuals from time series for a forecast should resemble white noise\n\nImplies that the model has captured all the information in the data\n\n\n\n\n\n\n\n\nWhen we decompose, we split the time series into 3 components:\n\nTrend-cycle (T): Long-term increase/ decrease\nSeasonal (S): same as seasonal above\nResidual: Random fluctuations\n\n\n\n\nAdditive Model: \\(Y_t = T_t + S_t + R_t\\)\n\nWhen the magnitude of the seasonal fluctuations does not change with the level of the time series\n\nMultiplicative Model: \\(Y_t = T_t \\times S_t \\times R_t\\)\n\nWhen the magnitude of the seasonal fluctuations does change with the level of the time series\n\n\n\n\n\nCurve Fitting: Fit a polynomial of degree \\(n\\) to the time series\nfrom statsmodels.tsa.tsatools import detrend\n\ndetrended = data - detrend(data, order=2) # order=2 for quadratic\nMoving Average: Smooths out short-term fluctuations and highlights longer-term trends\n# rolling is a pandas function\nrolling_mean = df.rolling(window=5, center=True).mean()\n\n# For even window, common practice to do:\nwindow = 4\ndf.rolling(window).mean().rolling(2).mean().shift(-window//2)\n\nwindow: Number of observations used for calculating the statistic\ncenter: Set the labels at the center of the window\n\nif odd, the label is at the center\nif even, the label is at the right\n\n\n\n\nThe even code does this: \n\n\n\n\n\nSimple steps:\n\nRemove the trend from the data (the detrended data above)\nEstimate the seasonal component by averaging the detrended data over each season\n\n\n\n\n\n\nThe residual is the remainder after removing the trend and seasonal components\nIf additive model: \\(R_t = Y_t - T_t - S_t\\)\nIf multiplicative model: \\(R_t = \\frac{Y_t}{T_t \\times S_t}\\)\n\n\n\n\n\nLuckily, there exists a function to do all of this for us: seasonal_decompose()\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndecomposition = seasonal_decompose(data, model='additive', period=12)\n\ndecomposition.trend # the trend component\ndecomposition.seasonal # the seasonal component\ndecomposition.resid # the residual component\n\n# plot the decomposition\nfig = decomposition.plot()"
  },
  {
    "objectID": "block_5/574_spat_temp_mod/574_spat_temp.html#forecasting",
    "href": "block_5/574_spat_temp_mod/574_spat_temp.html#forecasting",
    "title": "Spatial and Temporal Models",
    "section": "",
    "text": "Forecasting: Predicting future values of a time series\n\n\n\n\nAverage: Use average of all past observations\n\n\n\nNaive: Use the last observation as the forecast\nSeasonally Adjusted Naive: Same as Naive but with seasonally adjusted data (classical decomposition)\n\n\n\nSeasonally Naive: Use the last observation from the same season (only one with seasonality)\n\ndf[\"month\"] = df.index.month\nlast_season = (df.drop_duplicates(\"month\", keep=\"last\")\n                 .sort_values(by=\"month\")\n                 .set_index(\"month\")[\"value\"]\n              )\ndf = df.drop(columns=\"month\")\nlast_season\n\n\nDrift: Linearly extrapolate the trend (only one that is not a straight horizontal line)\n\n\n\n\n\n\n\n\nForecast is a weighted average of all past observations\nRecursively defined: \\(\\hat{y}_{t+1|t} = \\alpha y_t + (1 - \\alpha) \\hat{y}_{t|t-1}\\)\n\\(\\alpha\\): Smoothing parameter\n\nClose to 0: More weight to past observations\nClose to 1: More weight to current observation (closer to Naive forecast)\n\nInitial Forecast:\n\n\\(\\hat{y}_{1|0} = y_1\\)\nHeuristic: linear interpolation of the first few observations\nLearn it by optimizing SSE\n\nForecasts are flat\n\n\n from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n\n SES = SimpleExpSmoothing(data, initialization_method='heuristic')=\n\n # Fit the model\n model = SES.fit(smoothing_level=0.2, optimized=False)\n\n # Forecast\n forecast = model.forecast(steps=5)\n\n\n\n\nExtend SES to include a trend component \\[\\hat{y}_{t+h|t} = \\ell_t + h b_t\\]\n\\[\\ell_t = \\alpha y_t + (1 - \\alpha)(\\ell_{t-1} + b_{t-1})\\]\n\\[b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1 - \\beta)b_{t-1}\\]\n\\(\\ell_t\\): Level\n\\(b_t\\): Smoothness of the trend\n\nClose to 0: Trend is more linear\nClose to 1: Trend changes with each observation\n\n\\(\\alpha\\): Smoothing parameter for level\n\n\n\n\n\nExtend Holt’s method to include a seasonal component \\[\\hat{y}_{t+h|t} = \\ell_t + h b_t + s_{t-m+h_m}\\]\n\\[b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1 - \\beta)b_{t-1}\\]\nFor Additive Seasonal: \\[\\ell_t = \\alpha(y_t - s_{t-m}) + (1 - \\alpha)(\\ell_{t-1} + b_{t-1})\\]\n\\[s_t = \\gamma(y_t - \\ell_{t-1} - b_{t-1}) + (1 - \\gamma)s_{t-m}\\]\nFor Multiplicative Seasonal: \\[\\ell_t = \\alpha\\frac{y_t}{s_{t-m}} + (1 - \\alpha)(\\ell_{t-1} + b_{t-1})\\]\n\\[s_t = \\gamma\\frac{y_t}{\\ell_{t-1} + b_{t-1}} + (1 - \\gamma)s_{t-m}\\]\n\n\n\n\nTrend component\nSeasonal Component\n\n\n\n\nNone (N)\nNone (N)\n\n\nAdditive (A)\nAdditive (A)\n\n\nAdditive Damped (Ad)\nMultiplicative (M)\n\n\n\n\nSimple Exponential Smoothing (N,N)\nHolt’s Method (A,N)\nHolt’s Winter Method (A,A)\n\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\n model = ExponentialSmoothing(data,\n     trend='add',\n     damped_trend=True,\n     seasonal='mul',\n     seasonal_periods=12,\n     initialization_method='estimated'\n ).fit(method=\"least_squares\")\n\n\n\n\nComponents:\n\nError: {A, M}\nTrend: {N, A, Ad}\nSeasonal: {N, A, M}\n\n\nfrom statsmodels.tsa.holtwinters import ETSModel\n\n model = ETSModel(data,\n      error='add',\n      trend='add',\n      damped_trend=True,\n      seasonal='add',\n      seasonal_periods=12\n ).fit()\n\n # Forecast\n model.forecast(steps=5)\n\n # Summary\n  model.summary()\n\nCan generate prediction intervals (confidence intervals):\n\nmodel.get_prediction() (analytical)\nmodel.simulate()\n\n\npred = model.get_prediction(start=df.index[-1] + pd.DateOffset(months=1), end=\"2020\").summary_frame()\n\n# or\nq = 0.975 # 95% CI\nsim = model.simulate(anchor=\"end\", nsimulations=348, repetitions=100, random_errors=\"bootstrap\")\nsimu = pd.DataFrame({\"median\": simulations.median(axis=1),\n                     \"pi_lower\": simulations.quantile((1 - q), axis=1),\n                     \"pi_upper\": simulations.quantile(q, axis=1)},\n                    index=simulations.index)\n\n\n\n\n\nMetrics, Commonly used:\n\nAIC, BIC\nSSE/ MSE/ RMSE\n\n# using ets model from above\nmodel.aic\nmodel.bic\nmodel.mse\nResiduals:\n\nVisual inspection (should be uncorrelated, zero mean, normally distributed)\nRunning diagnostic Portmanteau tests:\n\nLjung-Box Test: \\(H_0\\): Residuals are uncorrelated (white noise)\n\np-value &lt; 0.05: Reject \\(H_0\\) (bad)\n\nJarque-Bera Test: \\(H_0\\): Residuals are normally distributed\n\np-value &lt; 0.05: Reject \\(H_0\\) (bad)\n\n\n\n\n# using ets model from above\nmodel.summary().tables[-1]\n\n# Ljung-Box Test\np = model.test_serial_correlation(method=\"ljungbox\", lags=10)[0,1,-1]\n# Jarque-Bera Test\np = model.test_normality(method=\"jarquebera\")[0,1]\n\nOut-of-sample Forecasting:\n\nSplit data into training and testing\nFit model on training data\nForecast on testing data\nCompare forecast with actuals"
  },
  {
    "objectID": "block_5/574_spat_temp_mod/574_spat_temp.html#arima-models",
    "href": "block_5/574_spat_temp_mod/574_spat_temp.html#arima-models",
    "title": "Spatial and Temporal Models",
    "section": "",
    "text": "ARIMA: AutoRegressive Integrated Moving Average\nCommonly used for time series forecasting (other than exponential smoothing)\nBased on autocorrelation of data\nDo not model trend nor seasonality, so it is typically constrained to stationary data\n\n\n\n\nStatistical properties of a time series do not change over time\n\nMean, variance is constant\nIs roughly horizontal (no strong trend)\nDoes not show predictable patterns (no seasonality)\n\nDOES not mean that the time series is constant, just that the way it changes is constant\nIt is one way of modelling dependence structure\n\nCan only be independent in one way but dependent in many ways\n\n\n\n\n\n\n\n\n\n\n\n\nProperty\nStrong Stationarity\nWeak Stationarity\n\n\n\n\nMean, Variance, Autocovariance\nConstant\nConstant\n\n\nHigher order moments (skewness, kurtosis)\nConstant\nNot necessarily constant\n\n\n\n\nWeak stationarity is often sufficient for time series analysis\n\n\n\n\n\n\nVisual Inspection: Plot the time series\n\nLook for trends, seasonality, and variance (none of these should be present)\nMake a correlogram plot (ACF plot should rapidly decay to 0)\n\nSummary Statistics: Calculate mean, variance, and autocovariance\n\nMean and variance should be roughly constant over time\n\nHypothesis Testing: Use statistical tests\n\nAugmented Dickey-Fuller (ADF) test\n\nNull hypothesis: Time series is non-stationary\nsmall p: it is stationary (reject null)\nUse statsmodels.tsa.stattools.adfuller\n\nKwiatkowski-Phillips-Schmidt-Shin (KPSS) test\n\nNull hypothesis: Time series is stationary\nsmall p: it is non-stationary (reject null)\n\n\n\nfrom statsmodels.tsa.stattools import adfuller\n\n# ADF test\nresult = adfuller(data)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\n\n\n\n\nStabilizing the variance using transformations\n\nLog or box-cox transformation\n\n\\[w_t = \\begin{cases} \\frac{y_t^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0 \\\\ \\ln(y_t) & \\text{if } \\lambda = 0 \\end{cases}\\]\nfrom scipy.stats import boxcox\nimport numpy as np\n\ndata = boxcox(data, lmbda=0)\n\n# log transformation\ndata = np.log(data)\nStabilize the mean using differencing\n\nFirst difference: \\(y' = y_t - y_{t-1}\\)\nSecond difference: \\(y'' = y' - y'_{t-1} = y_t - 2y_{t-1} + y_{t-2}\\)\nSeasonal difference: \\(y' = y_t - y_{t-m}\\), where \\(m\\) is the seasonal period\n\n# First difference\ndata1 = data.diff().dropna()\n# Second difference\ndata2 = data.diff().diff().dropna()\n# Seasonal difference, m is the seasonal period\ndata_m = data.diff(m).dropna()\n\n\n\n\n\n\n\n\n\n\n\nAR (AutoRegressive) Model\nMA (Moving Average) Model\n\n\n\n\nRegression of the time series on its own lagged values\nRegression of the time series on past forecast errors\n\n\n\\(y_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + \\epsilon_t\\)\n\\(y_t = \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\ldots + \\theta_q \\epsilon_{t-q}\\)\n\n\n\\(p\\): order of the AR model\n\\(q\\): order of the MA model\n\n\n\\(\\phi\\): AR coefficients\n\\(\\theta\\): MA coefficients\n\n\n\\(\\epsilon_t\\): white noise\n\\(\\epsilon_t\\): white noise\n\n\nLong memory model: \\(y_1\\) has a direct effect on \\(y_t\\) for all \\(t\\)\nShort memory model: \\(y_t\\) is only affected by recent values of \\(\\epsilon\\)\n\n\nGood for modeling time-series with dependency on past values\nGood for modeling time-series with a lot of volatility and noise\n\n\nLess sensitive to choice of lag or window size\nMore sensitive to choice of lag or window size\n\n\n\n\nBoth values are between -1 and 1\nAR value of 1 means that the time series is a random walk\n\n\n\n\n\nARMA: AutoRegressive Moving Average\nCombines AR and MA models\nKey Idea: Parsimony\n\nfit a simpler, mixed model with fewer parameters, than either a pure AR or a pure MA model\n\n\n\\[y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\ldots + \\theta_q \\epsilon_{t-q}\\]\n\n\\(c\\): constant\n\\(\\phi\\): AR coefficients\n\\(\\theta\\): MA coefficients\nUsually write it as ARMA(p, q)\n\n\n\n\n\nARIMA: AutoRegressive Integrated Moving Average\nCombines ARMA with differencing\nARIMA(p, d, q)\n\np: order of the AR model\nd: degree of differencing\nq: order of the MA model\n\nUse statsmodels.tsa.arima.model.ARIMA\n\nfrom statsmodels.tsa.arima.model import ARIMA\n\n# All with first order differencing\nmodel_ar = ARIMA(data[\"col\"], order=(3, 1, 0)).fit() # AR(3)\nmodel_ma = ARIMA(data[\"col\"], order=(0, 1, 1)).fit() # MA(1)\nmodel_arima = ARIMA(data[\"col\"], order=(3, 1, 3)).fit() # ARIMA(3, 3)\n\nmodel_arma = ARIMA(data[\"col\"], order=(3, 0, 3)).fit() # ARMA(3, 3)\n\n\nimport pmdarima as pm\n\nautoarima = pm.auto_arima(data.col,\n                          start_p=0, star_d=1, start_q=0,\n                          max_p=5, max_d=3, max_q=5,\n                          seasonal=False)\n\nautoarima.summary()\n\n\n\n\nSARIMA: Seasonal ARIMA\nSARIMA(p, d, q)(P, D, Q, m)\n\np, d, q: ARIMA parameters\nP, D, Q: Seasonal ARIMA parameters\nm: seasonal period\n\nE.g. In a dataset with years and 12 months\n\n\\(p=2\\) means Jan is affected by Dec and Nov\n\\(P=2\\) means Jan is affected by Jan of the previous 2 years\n\n\nsarima = ARIMA(data[\"col\"], order=(3, 1, 3), seasonal_order=(1, 1, 1, 12)).fit()\n\nAlso have SARIMAX (with exogenous variables)\n\nadds exogenous variables (other time series) to the model\nNot the most effective model\n\n\n\n\n\n\n\nACF and PACF plots\n\nACF: Autocorrelation Function\nPACF: Partial Autocorrelation Function\nUse these to determine the order of the AR and MA models\n\n\n\n\n\n\n\n\n\nACF Plot\nPACF Plot\n\n\n\n\nMeasures correlation between an observation and its lagged values\nsame but removes intermediate correlations (kinda isolates the direct effect)\n\n\nFor MA(q), cuts off after lag q\nFor AR(p), cuts off after lag p\n\n\nElse, tails off (exp or like damped sin)\nElse, tails off (no clear pattern)\n\n\n\n\nSee the cutoff when the peaks are lower than the shaded region\n\n\n\n\n\\(y_t=-0.9y_{t-1}+\\epsilon_t\\) \n\\(y_t=0.3y_{t-1}+\\epsilon_t\\) \n\\(y_t=0.5y_{t-1}-0.8y_{t-2}+\\epsilon_t\\) \n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_process import ArmaProcess\n\n# Simulate 1\ndf = pd.DataFrame(ArmaProcess(ar=[1, -0.9]).generate_sample())\n# Simulate 2\ndf = pd.DataFrame(ArmaProcess(ar=[1, 0.3]).generate_sample())\n# Simulate 3\ndf = pd.DataFrame(ArmaProcess(ar=[1, -0.5, 0.8]).generate_sample())\n\n# Plot\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\nfig.suptitle(\"y_t = 0.9y_{t-1} + e_t\")\ndf.plot(ax=axes[0])\nplot_acf(df, ax=axes[1])\nplot_pacf(df, ax=axes[2]);\n\n\n\n\n\\(y_t = \\epsilon_t + 0.9\\epsilon_{t-1}\\) \n\ndf = pd.DataFrame(ArmaProcess(ma=[1, -0.9]).generate_sample())\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))\nfig.suptitle(\"$y_t = \\epsilon_t - 0.9 \\epsilon_{t-1} $\")\ndf.plot(ax=axes[0])\nplot_acf(df, ax=axes[1])\nplot_pacf(df, ax=axes[2])\n\n\n\n(Based on lab 2 q4)\n\nLoad Data\nimport pandas as pd\n\n# turns first col into index + parses dates\ndf = pd.read_csv('data.csv', index_col=0, parse_dates=True)\nEDA with plot + ACF + PACF (Stationarity check)\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\ndf.plot(ax=axes[0])\nplot_acf(df, ax=axes[1])\nplot_pacf(df, ax=axes[2]);\n\nCan also check with ADF test (p&gt;0.05 means non-stationary)\n\nfrom statsmodels.tsa.stattools import adfuller\n\n# ADF test\nresult = adfuller(data)\nMake the time series stationary\n# Difference\ndata1 = data.diff().dropna()\n\n # Log transformation\n data = np.log(data)\n\nRepeat step 2 and check, also use ACF and PACF to find the AR and MA orders\n\nARIMA Model\nfrom statsmodels.tsa.arima.model import ARIMA\n\nmodel = ARIMA(train, order=(2, 1, 2), seasonal_order=(0, 1, 0, 12)).fit()\n\nmodel.summary()\nmodel.plot_diagnostics()\n\nCan also use auto_arima from pmdarima for hyperparameter tuning\n\nimport pmdarima as pm\n\nautoarima = pm.auto_arima(data.col,\n                          start_p=0, star_d=1, start_q=0,\n                          max_p=5, max_d=3, max_q=5,\n                          seasonal=False)\n\nautoarima.summary()\nautoarima.plot_diagnostics()\nForecast\nforecast = model.forecast(steps=len(valid))\n\nCan also use predict for in-sample prediction\n\npred = model.predict(start=len(train), end=len(train)+len(valid)-1)\n\nfig, ax = plt.subplots()\nax.plot(valid, label='Valid')\nax.plot(pred, label='Prediction')\nax.legend()"
  },
  {
    "objectID": "block_5/574_spat_temp_mod/574_spat_temp.html#time-series-forecasting-in-ml",
    "href": "block_5/574_spat_temp_mod/574_spat_temp.html#time-series-forecasting-in-ml",
    "title": "Spatial and Temporal Models",
    "section": "",
    "text": "Traditional ML\nTime Series ML\n\n\n\n\nData is IID\nData is ordered\n\n\nCV is random\nUse sliding window CV\n\n\nUse feature engineering\nUse lags, rolling windows, etc.\n\n\nPredict new data\nPredict future (specify horizon)\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n# turns first col into index + parses dates\ndf = pd.read_csv('data.csv', index_col=0, parse_dates=True)\n\n\n\nfrom sktime.transformations.series.lag import Lag\n# Make a new column with lag\ndf['col-1'] = df['col'].shift(1)\n# or use sktime\nt = Lag(lags=[1,2,3], index_out=\"original\")\npd.concat([df, t.fit_transform(df)], axis=1)\n\n\n\n\nNever use random shuffling\nNeed to keep temporal order\n\nfrom sktime.split import temporal_train_test_split\nfrom sklearn.model_selection import train_test_split\n\ny_train, y_test = temporal_train_test_split(y, test_size=0.2)\n\n# or use sklearn\ndf_train, df_test = train_test_split(df.dropna(), test_size=0.2, shuffle=False)\n\n\n\nExpanding Window: start with small training set and increase it \nFixed/sliding Window: use a fixed window size \n\n\n\n\n\nfrom sktime.forecasting.model_evaluation import evaluate\nfrom sktime.forecasting.naive import NaiveForecaster\nfrom sktime.forecasting.arima import AutoARIMA\n\nforecaster = NaiveForecaster(strategy=\"last\", sp=12) # seasonal naive\n# forecaster = AutoARIMA(sp=12)\n\nresults = evaluate(forecaster=forecaster, y=y_train, cv=cv, strategy=\"refit\", return_data=True)\n\n\n\n\n\nOne-step forecasting: one step ahead\nMulti-step forecasting: multiple steps ahead\n\nRecursive strategy: predict t, then it becomes part of the input for t+1\nDirect strategy: have a model for each step (model for t+1, another for t+2, etc)\nHybrid strategy: is dumb and bad\nMulti-output strategy: 2 different series (e.g. temperature and humidity)\n\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sktime.forecasting.compose import make_reduction\n\nregressor = KNeighborsRegressor()\n\n\nforecaster = make_reduction(regressor,\n   window_length=12,\n   strategy=\"recursive\") # or \"direct\" or \"dirrec\" or \"multioutput\"\n\n\n\n\n\n\nCoerce to stationary (via diff or transforms)\nSmoothing (e.g. moving average)\nImpute missing value (e.g. linear interpolation)\nRemoving outliers\n\n\n\n\n\nLagging features/ responses\nAdding time stamps (e.g. day of week, month, etc) 3, Rolling Statistics (e.g. rolling mean, rolling std)\n\n\n\n\n\n\nMeans time series with multiple variables (e.g. temperature and humidity)"
  },
  {
    "objectID": "block_5/574_spat_temp_mod/574_spat_temp.html#probabilistic-forecasting",
    "href": "block_5/574_spat_temp_mod/574_spat_temp.html#probabilistic-forecasting",
    "title": "Spatial and Temporal Models",
    "section": "",
    "text": "We have been dealing with point forecasts (modelling averages)\nWant to estimate the uncertainty of our forecasts\n\nor the extreme (e.g. 90% or 95% quantiles)\n\nexample: find upper quantile of electricity demand so that we can plan for the maximum demand\n\nor predict the variance of the forecast (how volatile a metric will be in the future)\n\n\n\n\n\nAssume distribution of forecasts are normal\n\n\\[\n\\hat{y}_{T+h|T} \\pm c \\hat{\\sigma}_{h}\n\\]\n\n\\(\\hat{\\sigma}_{h}\\) is the standard deviation of the forecast\n\\(c\\): coverage factor (e.g. 1.96 for 95% confidence interval)\n\n\\[\n\\hat{\\sigma}_{h} = \\sqrt{\\frac{1}{T-K}\\sum_{t=1}^{T} e_{t}^{2}}\n\\]\n\nFocus is finding \\(\\hat{\\sigma}_{h}\\)\n\n\\(K\\): number of parameters\n\\(T\\): total length of time series\n\\(e_{t} = y_{t} - \\hat{y}_{t|t-1}\\)\n\nMethods that have been derived mathematically: | Method | Forecast sd | |——–|————–| | Mean | \\(\\hat{\\sigma}_{h} = \\hat{\\sigma_1} \\sqrt{1 + \\frac{h}{T}}\\) | |Naive | \\(\\hat{\\sigma}_{h} = \\hat{\\sigma_1} \\sqrt{h}\\) | | Seasonal Naive | $_{h} = $ | | Drift | \\(\\hat{\\sigma}_{h} = \\hat{\\sigma_1} \\sqrt{h(1+\\frac{h}{T})}\\) |\nRecall: \\(h\\) is the forecast horizon (steps ahead), \\(m\\) is the seasonal period\n\nfrom pandas import pd\n\nc = 1.96 # 95% confidence interval\n\ntrain['pred'] = train['y'].shift(1)\ntrain['residuals'] = train['y'] - train['pred']\nsigma = train['residuals'].std()\n\nh = np.arange(1, len(forecast_index) + 1)\nnaive_forecast = train['y'].iloc[-1]\n\n# create lower and upper bound\nnaive = pd.DataFrame({\"y\": naive_forecast,\n                      \"pi_lower\": naive_forecast - c * sigma * np.sqrt(horizon),\n                      \"pi_upper\": naive_forecast + c * sigma * np.sqrt(horizon),\n                      \"Label\": \"Naive\"},\n                     index=forecast_index)\nplot_prediction_intervals(train[\"y\"], naive, \"y\", valid=valid[\"y\"])\n# ETS\nmodel = ETSModel(train[\"y\"], error=\"add\", trend=\"add\", seasonal=\"add\").fit(disp=0)\n\nets = model.get_prediction(start=forecast_index[0], end=forecast_index[-1]).summary_frame()\nplot_prediction_intervals(train[\"y\"], ets, \"mean\", valid=valid[\"y\"], width=800)\n\n# ARIMA\nmodel = ARIMA(train[\"y\"], order=(3, 1, 0), seasonal_order=(2, 1, 0, 12)).fit()\n\narima = model.get_prediction(start=forecast_index[0], end=forecast_index[-1]).summary_frame()\nplot_prediction_intervals(train[\"y\"], arima, \"mean\", valid=valid[\"y\"], width=800)\n\n\n\n\n\nAssume future errors will be similar to past errors\nDraw from the distribution of past errors to simulate future errors\n\n\\[y_{T+h} = \\hat{y}_{T+h|T} + \\epsilon_{T+h}\\]\n# Fit an ETS model\nmodel = ETSModel(train[\"y\"], error=\"add\", trend=\"add\").fit(disp=0)\n\n# simulate predictions\nets = model.simulate(anchor=\"end\", nsimulations=len(forecast_index),\n                     repetitions=n_simulations,\n                     random_errors=\"bootstrap\")\n# plot\nax = train[\"y\"].plot.line()\nets.plot.line(ax=ax, legend=False, color=\"r\", alpha=0.05,\n              xlabel=\"Time\", ylabel=\"y\", figsize=(8,5));\n\n# get quantiles\nets = pd.DataFrame({\"median\": ets.median(axis=1),\n                    \"pi_lower\": ets.quantile(1-0.975, axis=1),\n                    \"pi_upper\": ets.quantile(0.975, axis=1)},\n                   index=forecast_index)\n\n\n\n\n\nWish to predict particular quantile instead of mean\n\ne.g \\(q=0.9\\) so we expect 90% of the future values to be below the forecast\n\nPinball loss/ Quantile loss: \\[\n\\mathcal{L}=\n     \\left\\{\n\\begin{array}{ll}\n      (1-q)(\\hat{y}_{t,q}-y_t) \\text{,} \\;\\; \\text{ if } y_t &lt; \\hat{y}_{t,q} \\\\\n      q(y_t-\\hat{y}_{t,q}) \\text{,} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\text{ if } y_t \\ge \\hat{y}_{t,q} \\\\\n\\end{array}\n\\right.\n\\]\n\n\n\n\n\n\n\n\nHigh Quantile\nLow Quantile\n\n\n\n\nHigher penalty for predicting OVER\nHigher penalty for predicting UNDER\n\n\n\n\n\nsee here\nQuantile loss is not currently a supported criterion in pytorch but it’s easy to define ourselves. We really have two options:\n\nTrain a network for each quantile we want to predict; or\nTrain a network to output multiple quantiles at once\n\n\n\n\n\n\nThere are 4 main sources of uncertainty:\n\nRandom error term\nUncertainty in model parameter estimates\nUncertainty in model selection\nUncertainty about consistency of data generating process in the future\n\nMost methods only consider the first source of uncertainty\nSimulation tries to consider 2 and 3\n4 is practically impossible to consider"
  },
  {
    "objectID": "block_5/574_spat_temp_mod/574_spat_temp.html#anomaly-detection",
    "href": "block_5/574_spat_temp_mod/574_spat_temp.html#anomaly-detection",
    "title": "Spatial and Temporal Models",
    "section": "",
    "text": "Outliers are observations that are significantly different from the rest of the data\n\nCan be due to measurement error, data entry error, or just unique observations\n\n\n\n\n\nMethodology:\n\nSubtract the rolling median from data (with suitable window size)\nCalculate standard deviation of the residuals (\\(\\hat{\\sigma_r}\\))\nAssume normally distributed residuals then identify outliers as outside the 95% confidence interval (\\(\\pm 1.96 \\hat{\\sigma_r}\\))\n\n\n\n\n\n\nMethodology:\n\nDecompose time series to find residuals:\n\nNon-seasonal data: use LOESS\nSeasonal data: use STL (Seasonal-Trend decomposition using LOESS)\n\nCalculate \\(q_{0.1}\\) and \\(q_{0.9}\\) of the residuals\nIdentify outliers as \\(\\pm2 \\times (q_{0.9} - q_{0.1})\\)\n\n\n\n\n\n\nMethodology:\n\nFit a model to the data\nIdentify outliers as significant deviations from model predictions (e.g. 95% confidence interval)\n\n\n\n\n\n\nTrain an ML model to predict outliers\nA few common packages: pyod, sklearn, luminaire, sklyline, etc.\n\n\n\n\nBuilt on basis of decision trees\nHigh-level idea:\n\nrandomly select a feature\nrandomly splits that feature into 2 values\nrepeat until all data points are isolated\n\nLess splits to isolate a data point = more likely to be an outlier\nScore = [0, 1] where 1 is an outlier, &gt; 0.5 are normal observations.\n\n\n\nExample of sklearn’s IsolationForest:\n\nfrom sklearn.ensemble import IsolationForest\n\noutliers = IsolationForest(contamination=0.05).fit_predict(df) == -1\n\n\n\n\nFor each data point, calculate the distance to its k-th nearest neighbor\n\nLarge distance = outlier\n\nSupports 3 kNN detectors:\n\nLargest: distance to the k-th neighbor\nMean: average distance to k neighbors\nMedian: median distance to k neighbors\n\npyod’s KNN() outlier detection\n\nfrom pyod.models.knn import KNN\n\noutliers = KNN(contamination=0.05).fit_predict(df).labels_ == 1\n\n\n\n\n\nGlobal outliers: A data point with its value is far outside of the entirety of the data set (e.g., billionaires)\nLocal/Contextual outliers: A data point is considered a contextual outlier if its value significantly deviates from the rest the data points in the same context. (e.g., earning 50K income in a developing countries)"
  },
  {
    "objectID": "block_5/574_spat_temp_mod/574_spat_temp.html#imputation",
    "href": "block_5/574_spat_temp_mod/574_spat_temp.html#imputation",
    "title": "Spatial and Temporal Models",
    "section": "",
    "text": "Imputation: Filling in missing values/ outliers in a dataset\nOverarching techniques:\n\nRemove (.dropna())\nFill manually based on some expert-interpreted values (.fillna())\nFill with mean/median/mode (.fillna())\nFill based on rolling statistic, e.g. rolling mean/median\nPolynomial interpolation\nFill based on temporal dependence\n\ni.e. use same value from the same period last season, or average of all periods in the past season\n\nFill with model fitted values\nUse MICE (Multiple Imputation by Chained Equations) from statsmodels or IterativeImputer from sklearn."
  },
  {
    "objectID": "block_5/574_spat_temp_mod/574_spat_temp.html#deep-learning-with-time-series",
    "href": "block_5/574_spat_temp_mod/574_spat_temp.html#deep-learning-with-time-series",
    "title": "Spatial and Temporal Models",
    "section": "",
    "text": "Classical time series (ARIMA, Exponential Smoothing) limitations:\n\nThey are linear models\nInputs must be specified, not learned automatically\nFocus on univariate time series (lack support for multivariate time series)\nFocus on regression (lack support for classification)\nAssumes complete, non-missing data\n\nDeep learning models can address these limitations, as NNs are:\n\nrobust to noise\ncan learn complex, non-linear relationships\nsupport multivariate time series\nfeature learning (learn the inputs)\ntemporal dependencies can be learned\n\n\n\n\n\nNN allows us so that the features do not need to be ordered\n\ne.g. target, lag1. lag2, lag3 is the same as lag2, lag3, target, lag1\n\nNeed to take into account inherent temporal dependencies (approach above does not)\nDo something similar like CNN (retain spatial information) but for time series\n\n\n\n\n\nCan work with 3D data (channels, height, width)\nCan also do 2D data (features, time)\nUse Conv1D from pytorch to work with time series data\n\nLooks at local groups of data (filter of some size)\nMissing memory (does not remember previous values, only looks at local groups of data)\n\ncan be solved with RNN\n\n\n\n\nImage with sequence of 20 values, filtered with 4 kernels of size 3\nimport torch.nn.Conv1d\nimport pandas as pd\nimport plotly.express as px\nfrom sktime.utils.data_processing import lag_df\n\nconv_layer = nn.Conv1d(in_channels=1, out_channels=4, kernel_size=3, padding=1)\n\n# Load data\ntrain = (pd.read_csv(\"data/unemployed_train.csv\", index_col=0, parse_dates=True)\n           .resample(\"1M\").mean()\n           .assign(Label=\"Train\")\n        )\ntest = (pd.read_csv(\"data/unemployed_valid.csv\", index_col=0, parse_dates=True)\n           .resample(\"1M\").mean()\n           .assign(Label=\"Test\")\n        )\n\n# Plot train and test data\npx.line(pd.concat((train, test)), y=\"Unemployed persons\", color=\"Label\", width=600, title=\"Australian Unemployment\")\n\n# Make lag features\nSEQUENCE_LENGTH = 24\nBATCH_SIZE = 16\ncnn_data = lag_df(train, lags=SEQUENCE_LENGTH, dropna=True)\ncnn_data = cnn_data[cnn_data.columns[::-1]]  # Reverse columns to have lag0 at the end\n\nThen make CNN with optimizer Adam and loss function MSE\n\n\n\n\n\nReasonable results with CNN because preserve structure of data\nStructure: - Split up and process one time-step at a time\n \nGreat video on RNNs\nDraw back: Vanishing Gradient Problem\n\nGradient becomes so small that it does not update the weights\nEarly time steps are not updated (long-term dependencies are not learned), they “forget”\nCan be solved with LSTM and GRU\n\n\n\n\n\n\n\nAnother great video on LSTM\nAll the yellow boxes (LSTM cells) are identical (same weights and architecture)\nComponents:\n\nCell State: The horizontal line running through the top of the diagram\n\nIt runs straight down the entire chain, with only some minor linear interactions\n\nForget Gate: Decides what information to throw away from the cell state\nInput Gate: Decides what new information to store in the cell state\nOutput Gate: Decides what to output based on the cell state\n\n\nfrom torch import nn\n\nclass LSTM(nn.Module):\n    def __init__(self, input_size, output_size, hidden_dim):\n        super().__init__()\n\n        self.hidden_dim = hidden_dim\n        self.lstm = nn.LSTM(input_size, hidden_dim, batch_first=True, num_layers=1)\n        self.fc = nn.Linear(hidden_dim, output_size)\n\n    def forward(self, x, hidden):\n        # x :           (batch_size, seq_length, input_size)\n        # hidden :      (short_mem, long_mem), each (n_layers, batch_size, hidden_dim)\n        # output :  (batch_size, seq_length, input_size)\n        # note that the output will have the same shape as the input because\n        # it makes a forecast for each time step in the sequence\n        # but we only care about the last prediction (the forecast after the sequence)\n        # so I'll only take the last value of the output\n\n        prediction, (short_mem, long_mem) = self.lstm(x, hidden)\n        output = self.fc(prediction)\n        return output[:, -1, :], short_mem, long_mem\n\nGRU is similar to LSTM, but has less parameters and is faster to train\n\n\n\n\n\n\n\nFrankenstein of classical time series models (decomposition, regression, exponential smoothing, etc)\n\nimport pandas as pd\nfrom prophet import Prophet\n\n# Sample data\ndata = pd.read_csv('example_data.csv')\ndata['ds'] = pd.to_datetime(data['ds'])\n\n# Initialize Prophet model\nmodel = Prophet(interval_width=0.95, yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True)\n\n# Fit the model\nmodel.fit(data)\n\n# Make future predictions\ntest_dates = model.make_future_dataframe(periods=len(test), freq='M', include_history=False)\nforecast = model.predict(test_dates)\n\n# Visualize the forecast\nfig = model.plot(forecast)\n\n\n\n\nGluonTS: alternative to pytorch.\nPyTorch Forecasting: built on top of pytorch, but with more focus on time series forecasting.\nsktime: scikit-learn for time series data.\nTidyverts: R package for time series forecasting.\n\n\n\n\n\n\nHeirarchical Time Series: Forecasting at different levels of aggregation (e.g. product sales at store level, then at regional level, then at national level)\n\nBottom-up: Forecast at the lowest level and aggregate up\nTop-down: Forecast at the highest level and disaggregate down\n\nMultiple Seasonalities: e.g. daily and weekly seasonality\n\ndecompose independently\ndecompose simultaneously (e.g. propher/ statsmodels)\n\nMultivariate Time Series: e.g. sales and advertising spend\n\nVAR (Vector Auto Regression)\nLSTM with multiple inputs\n\nExplanatory variables:\n\neasy to add features to ML models\nARIMA can but using exog parameter\n\nTime Series Classification:\n\nHidden Markov Models"
  },
  {
    "objectID": "block_5/574_spat_temp_mod/574_spat_temp.html#spatial-data",
    "href": "block_5/574_spat_temp_mod/574_spat_temp.html#spatial-data",
    "title": "Spatial and Temporal Models",
    "section": "",
    "text": "Data with location information\nHas spacial dependence\nMain Tasks:\n\nWrangling\nVisualization\nModelling\n\nRepresentation:\n\nVector\nRaster\n\n\n\n\n\nCollection of discrete locations/ vertices (x, y) to form:\n\nPoints: single location\nLines: series of points\nPolygons: series of lines (closed shape)\n\nStored in .shp (shapefile) format\n\n.shp: geometry\n.shx: index (how geometry relates to one another)\n.dbf: attributes (e.g. population, area)\n\n\n\n\n\nTo read and write vector data\nBuilt off of pandas and shapely\nSimilar to pandas it has:\n\nGeoSeries: series of geometries\nGeoDataFrame: dataframe with geometry column\n\nGeometry column contains vector data\n\n\n\n\nimport geopandas as gpd\n\n# Read data\ngdf = gpd.read_file(“path/to/shp_dir”)\n\n# Plot data\ngdf.plot()\n\n\n\nimport geopandas as gpd\n\nlat = [49.2827, 49.2827, 49.2827, 49.2827]\nlon = [-123.1207, -123.1207, -123.1207, -123.1207]\n\ngdf = gpd.GeoDataFrame(geometry=gpd.points_from_xy(lon, lat))\ngdf.plot()\n\n\n\n\nIt is like the wikipedia of geospacial data\nUse osmnx to get data from OpenStreetMap\n\nimport osmnx as ox\n\nvancouver = ox.geocode_to_gdf(\"Vancouver, Canada\")\nvancouver.plot(edgecolor=\"0.2\")\nplt.title(\"Vancouver\");\n\n# get higher resolution\nvan_bc = gpd.clip(bc, vancouver)\nvan_bc.plot(edgecolor=\"0.2\")\n\n# Plot stanley park in vancouver\nstanley_park = ox.geocode_to_gdf(\"Stanley Park, Vancouver\")\nax = van_bc.plot(edgecolor=\"0.2\")\nstanley_park.plot(ax=ax, edgecolor=\"0.2\", color=\"tomato\")\n\n# Graph bike network in vancouver\nbike_network = ox.graph_from_place(\"Stanley Park, Vancouver\",\n    network_type=\"bike\")\n\nax = stanley_park.plot(edgecolor=\"0.2\")\nbike_network.plot(ax=ax, edgecolor=\"0.2\", color=\"tomato\")\n\n# can be interactive\nbike_network.explore()\n\n\n\n\n\nAdd width to line: gdf.buffer(2) to add a 2m to left and right of the line (4m total)\nGet Length of line: gdf.length.sum() to get the length of the line\n\nNeed to convert to linear meters first\n\nGet Area of polygon: gdf.area.sum() to get the area of the polygon\n\nNeed to convert to linear meters first\n\nJoining: gpd.sjoin(gdf1, gdf2, how=‘left’, predicate=‘intersects’)\n\nhow: left, right, inner, outer\npredicate: intersects, contains, within, touches, crosses, overlaps\n\nGrouping: gdf.groupby(by=‘column’).sum().sort_values(\"length\", ascending=False)\n\n\n\n\n\n\n\nEach pixel has 4 bands: Red, Green, Blue, and Infrared\nResolution: size of each pixel (e.g. 1m x 1m)\n\nsmaller resolution = more detailed\n\nMost common format: GeoTIFF (.tif)\nUse Python library rasterio to read and write raster data\n\nimport rasterio\n\ndataset = rasterio.open(“path/to/raster.tif”)\n\n\n\n\nTypically identified by EPSG (European Petroleum Survey Group) code\nCommon CRS:\n\nAngular units (latitude and longitude): EPSG:4326\nLinear units (meters): Most common is UTM which is divided into zones. For British Columbia, it’s EPSG:32610\nMinimize distortion by choosing the right CRS, for Canada, it’s EPSG:3347 (“Lambert projection”)\n\nChange code in geopandas:gdf.to_crs(“EPSG:3347”)"
  },
  {
    "objectID": "block_5/574_spat_temp_mod/574_spat_temp.html#spatial-visualization",
    "href": "block_5/574_spat_temp_mod/574_spat_temp.html#spatial-visualization",
    "title": "Spatial and Temporal Models",
    "section": "",
    "text": "Easy to use and quick\n\n\nGet data of UBC buildings from osmnx\n\nimport osmnx as ox\nimport geopandas as gpd\n\nubc = (ox.features.features_from_place(\"University of British Columbia, Canada\",\n                                tags={'building':True}) # Just keep building footprints\n         .loc[:, [\"geometry\"]]                 # just keep the geometry column for now\n         .query(\"geometry.type == 'Polygon'\")  # only what polygons (buidling footprints)\n         .assign(Label=\"Building Footprints\")  # assign a label for later use\n         .reset_index(drop=True)               # reset to 0 integer indexing\n        )\n\nGet the building footprint of a specific building from its coordinates\n\npoint_coord = Point(-123.25203756532703,49.26314716306668)\n\nubc[ubc.contains(point_office)] # get the building that contains the point\n\nubc.loc[47, \"Label\"] = \"Earth Science Building\" # change the label\n\nPlot the GeoDataFrame\n\nax = ubc.plot(figsize=(8, 8), column=\"Label\", legend=True,\nedgecolor=\"0.2\", markersize=200, cmap=\"rainbow\")\nplt.title(\"UBC\");\n\nAdd map to the background\n\nimport contextily as cx\n\nax = (ubc.to_crs(\"EPSG:3857\")\n         .plot(figsize=(10, 8), column=\"Label\", legend=True,\n               edgecolor=\"0.2\", markersize=200, cmap=\"rainbow\")\n     )\n\ncx.add_basemap(ax, source=cx.providers.OpenStreetMap.Mapnik)  # I'm using OSM as the source. See all provides with ctx.providers\nplt.axis(\"off\")\nplt.title(\"UBC\");\n\n\n\n\n\nTo add interactivity to the map\nBacked by MapBox (mapping and location data cloud platform)\n\nimport plotly.express as px\n\n# Does the same thing as the previous cell, but with plotly express (interactive)\nfig = px.choropleth_mapbox(ubc,\n                            geojson=ubc.geometry,\n                            locations=ubc.index,\n                            color=\"Label\",\n                            center={\"lat\": 49.261, \"lon\": -123.246},\n                            zoom=12.5,\n                            mapbox_style=\"open-street-map\")\nfig.update_layout(margin=dict(l=0, r=0, t=30, b=10))\n\nCan also plot the buildings with different colours based on building area\n\n# Calculate area\nubc[\"Area\"] = ubc.to_crs(epsg=3347).area  # (https://epsg.io/3347)\n\n# Make plot\nfig = px.choropleth_mapbox(ubc,\n                            geojson=ubc.geometry,\n                            locations=ubc.index,\n                            color=\"Area\",\n                            center={\"lat\": 49.261, \"lon\": -123.246},\n                            zoom=12.5,\n                            mapbox_style=\"carto-positron\")\nfig.update_layout(margin=dict(l=0, r=0, t=30, b=10))\n\n\n\n\n\nWeb-based geospatial analysis tool\nEven more powerful than Plotly Express\nHow it works:\n\nCreate instance of map with keplergl.KeplerGl()\nAdd as much data with .add_data() method\nCustomize and configure the map using GUI\n\n\nfrom keplergl import KeplerGl\n\nubc_map = keplergl.KeplerGl(height=500)\nubc_map.add_data(data=ubc.copy(), name=\"Building heights\")\n\nubc_map\n\nCan also make a 3d map with building heights\n\nLoad the building heights data\nJoin the data with the building footprints\nPlot the 3d map + addjust the GUI settings"
  },
  {
    "objectID": "block_5/574_spat_temp_mod/574_spat_temp.html#spatial-modeling",
    "href": "block_5/574_spat_temp_mod/574_spat_temp.html#spatial-modeling",
    "title": "Spatial and Temporal Models",
    "section": "",
    "text": "Source: https://www.neonscience.org/resources/learning-hub/tutorials/spatial-interpolation-basics\n\nTwo common ways to model spatial data:\n\nSpatial interpolation: use a set of observations in space to estimate the value of a spatial field\nAreal interpolation: project data from one set of polygons to another set of polygons\n\n“everything is related to everything else, but near things are more related than distant things” (Tobler, 1970)\n\n\n\n\nUse scipy module interpolate to do deterministic interpolation\nCommon Techniques:\n\nInverse Distance Weighting (IDW): estimate the value of a point based on the values of its neighbours (farther neighbours have less weight)\n\n\\[\\hat{z}(x) = \\frac{\\sum_{i=1}^{n} w_i(x)z_i}{\\sum_{i=1}^{n} w_i(x)}\\]\nwhere \\(w_i(x) = \\frac{1}{d_i(x)^p}\\), \\(d_i(x)\\) is the distance between \\(x\\) and \\(i\\), and \\(p\\) is the power parameter\n\nNearest Neighbour: estimate the value of a point based on the value of the nearest point (does not consider weights)\n\n\nLess smooth than IDW (more jagged)\n\n\nPolynomial Interpolation: estimate the value of a point based on the values of its neighbours using a polynomial function\nRadial Basis Function: estimate the value of a point based on the values of its neighbours using a radial basis function\n\n\nfrom scipy.interpolate import NearestNDInterpolator\n\n# Get the geodataframe\ngpm25 = (gpd.GeoDataFrame(\n            pm25,\n            crs=\"EPSG:4326\", # angular CRS\n            geometry=gpd.points_from_xy(pm25[\"Lon\"], pm25[\"Lat\"])) # create geometry from coordinates\n        .to_crs(\"EPSG:3347\") # convert to one for Canada\n        )\n\n# Creates points to interpolate\ngpm25[\"Easting\"], gpm25[\"Northing\"] = gpm25.geometry.x, gpm25.geometry.y\n\n# Create a grid\nresolution = 25000  # cell size in meters, smaller cell size = smaller pixel = higher resolution\ngridx = np.arange(gpm25.bounds.minx.min(), gpm25.bounds.maxx.max(), resolution)\ngridy = np.arange(gpm25.bounds.miny.min(), gpm25.bounds.maxy.max(), resolution)\n\n# Interpolate\ninterpolator = NearestNDInterpolator(gpm25[[\"Easting\", \"Northing\"]], gpm25[\"PM25\"])\nz = model(*np.meshgrid(gridx, gridy))\nplt.imshow(z);\n\nPlot it back to map:\n\n# Helper function:\ndef pixel2poly(x, y, z, resolution):\n    \"\"\"\n    x: x coords of cell\n    y: y coords of cell\n    z: matrix of values for each (x,y)\n    resolution: spatial resolution of each cell\n    \"\"\"\n    polygons = []\n    values = []\n    half_res = resolution / 2\n    for i, j  in itertools.product(range(len(x)), range(len(y))):\n        minx, maxx = x[i] - half_res, x[i] + half_res\n        miny, maxy = y[j] - half_res, y[j] + half_res\n        polygons.append(Polygon([(minx, miny), (minx, maxy), (maxx, maxy), (maxx, miny)]))\n        if isinstance(z, (int, float)):\n            values.append(z)\n        else:\n            values.append(z[j, i])\n    return polygons, values\npolygons, values = pixel2poly(gridx, gridy, z, resolution)\n\npm25_model = (gpd.GeoDataFrame({\"PM_25_modelled\": values}, geometry=polygons, crs=\"EPSG:3347\")\n                 .to_crs(\"EPSG:4326\")\n             )\n\nfig = px.choropleth_mapbox(pm25_model, geojson=pm25_model.geometry, locations=pm25_model.index,\n                           color=\"PM_25_modelled\", color_continuous_scale=\"RdYlGn_r\", opacity=0.5,\n                           center={\"lat\": 52.261, \"lon\": -123.246}, zoom=3.5,\n                           mapbox_style=\"carto-positron\")\nfig.update_layout(margin=dict(l=0, r=0, t=30, b=10))\nfig.update_traces(marker_line_width=0)\n\n\n\n\nKriging differs from deterministic because we interpolate using statistical models that include estimates of spatial autocorrelation\n\n\\[\\hat{Z}(s_0) = \\sum_{i=1}^{n} \\lambda_i Z(s_i)\\]\n\n\\(\\lambda_i\\) are the weights\n\\(Z(s_i)\\) are observations at locations \\(s_i\\)\n\\(N\\) is the size of \\(s\\) (number of observations) \nKriging uses spatial autocorrelation to estimate the weights\n\nLooking at the variance between points to estimate the weights\n\n\n\n\n\nDefines the spatial variance/ autocorrelation between points (as a function of distance)\n\nSimilar to ACF but for spatial data\n\nUsed to estimate the weights in kriging\ne.g. of a function: \\(\\gamma(s_i, s_j) = \\frac{1}{2}(Z(s_i) - Z(s_j))^2\\)\n\nsemi-variance because of a factor of 1/2\nEach pair is calculated twice\n\nPlot this function to get the variogram (x-axis: distance, y-axis: semivariance)\n\n\n\nNugget: variance at distance 0\n\nIdeally should be 0 (no variance at distance 0), higher nugget = more noise\nCan be thought of as Random error/ measurement error\n\nSill: maximum variance of spatial process\n\nrepresents the amount of spatial autocorrelation that exists at large enough distances to capture the underlying trend of the process\n\nRange: where the semivariance reaches the sill\n\n\n\n\n\nfrom pykrige.ok import OrdinaryKriging\n\nRESOLUTION = 250  # m\n\n# 1. Convert to meter-based\nvan_listings_gdf = van_listings_gdf.to_crs(\"EPSG:3347\")\n\n# 2. Add Easting and Northing columns\nvan_listings_gdf[\"Easting\"] = van_listings_gdf.geometry.x\nvan_listings_gdf[\"Northing\"] = van_listings_gdf.geometry.y\n\n# 3. Create a grid of points\ngridx = np.arange(\n    van_listings_gdf.bounds.minx.min(), van_listings_gdf.bounds.maxx.max(), RESOLUTION\n)\ngridy = np.arange(\n    van_listings_gdf.bounds.miny.min(), van_listings_gdf.bounds.maxy.max(), RESOLUTION\n)\n\n# 4. Kriging\nkrig = OrdinaryKriging(\n    x=van_listings_gdf[\"Easting\"],\n    y=van_listings_gdf[\"Northing\"],\n    z=van_listings_gdf[\"price\"],\n    variogram_model=\"spherical\",\n    verbose=False,\n    enable_plotting=False,\n)\n\n# 5. Execute and plot\nz, ss = krig.execute(\"grid\", gridx, gridy)\nplt.imshow(z)\n\n\n\n\n\nProject data from one set of polygons to another set of polygons\nE.g. Map air pollution data fo FSA (“forward sortation area”, which are groups of postal codes) polygons\n\n# Load the FSA data\nvan_fsa = gpd.read_file(\"data-spatial/van-fsa\")\n\n# Kriging (similar to previous cell)\nresolution = 10_000  # cell size in meters\ngridx = np.arange(gpm25.bounds.minx.min(), gpm25.bounds.maxx.max(), resolution)\ngridy = np.arange(gpm25.bounds.miny.min(), gpm25.bounds.maxy.max(), resolution)\nkrig = OrdinaryKriging(x=gpm25[\"Easting\"], y=gpm25[\"Northing\"], z=gpm25[\"PM_25\"], variogram_model=\"spherical\")\nz, ss = krig.execute(\"grid\", gridx, gridy)\npolygons, values = pixel2poly(gridx, gridy, z, resolution)\npm25_model = (gpd.GeoDataFrame({\"PM_25_modelled\": values}, geometry=polygons, crs=\"EPSG:3347\")\n                 .to_crs(\"EPSG:4326\")\n                 )\nz, ss = krig.execute(\"grid\", gridx, gridy)\n\n# Areal Interpolation\nareal_interp = area_interpolate(pm25_model.to_crs(\"EPSG:3347\"),\n                                van_fsa.to_crs(\"EPSG:3347\"),\n                                intensive_variables=[\"PM_25_modelled\"]).to_crs(\"EPSG:4326\")\nareal_interp.plot(column=\"PM_25_modelled\", figsize=(8, 8),\n                  edgecolor=\"0.2\", cmap=\"RdBu\", legend=True)\nplt.title(\"FSA Air Pollution\");\n\n\n\n\n\nUse Dijkstra’s algorithm to find the shortest path between two points\nCondition:\n\nThe graph must be weighted with non-negative weights\n\nAlgorithm:\n\nLabel start node with 0, all others with infinity\nLabel current node as visited\nGo to all connected nodes (to current) and update label with min(current_label, previous_label + weight)\n\n\nIf updated, then keep track of the previous node (for backtracking later)\n\n\nOnce all nodes around the current node are visited, go to the unvisited node with the smallest label and repeat step 2\nBacktrack from end node to start node using the previous node\n\nTime complexity: \\(O(V^2)\\) but its \\(O(E + V \\log V)\\) with a min-priority queue/ binary heap\nSpace complexity: \\(O(V)\\)\n\nimport osmnx as ox\nimport networkx as nx\n\n# Origin\norig_address = \"UBC bookstore, Vancouver\"\norig_y, orig_x = ox.geocode(orig_address)  # notice the coordinate order (y, x)!\n\n# Destination\ndest_address = \"Orchard Commons Student Residence, Vancouver\"\ndest_y, dest_x = ox.geocode(dest_address)\n\n# Find the nearest nodes\norig_node_id, dist_to_orig = ox.distance.nearest_nodes(G, X=orig_x, Y=orig_y, return_dist=True)\ndest_node_id, dist_to_dest = ox.distance.nearest_nodes(G, X=dest_x, Y=dest_y, return_dist=True)\n\n# Find the shortest path\nroute = nx.shortest_path(G, orig_node_id, dest_node_id, weight=\"length\")\n\n# Plot the shortest path\nox.plot.plot_graph_route(G, route)"
  },
  {
    "objectID": "list.html",
    "href": "list.html",
    "title": "List of Notes",
    "section": "",
    "text": "This is the list of MDS Courses.\nBelow are my list of notes for each course.\n\n\n\nCourse Code\nBlock #\nTitle\n\n\n\n\n511\n1\nIntro to Python\n\n\n521\n1\nPlatforms\n\n\n523\n1\nR Programming\n\n\n551\n1\nStatistics and Probability\n\n\n512\n2\nAlgorithms and Data Structures\n\n\n531\n2\nData Visualization\n\n\n552\n2\nStatistical Interference\n\n\n571\n2\nSupervised Learning\n\n\n513\n3\nDatabases\n\n\n522\n3\nWorkflows\n\n\n561\n3\nRegression I\n\n\n573\n3\nModel Selection\n\n\n562\n4\nRegression II\n\n\n572\n4\nSupervised Learning II\n\n\n553\n5\nStatistical Inference II\n\n\n563\n5\nUnsupervised Learning\n\n\n574\n5\nSpatial and Temporal Models\n\n\n575\n6\nAdvanced Machine Learning\n\n\n554\n6\nExperimentation and Causal Inference"
  },
  {
    "objectID": "block_6/554_exp_caus_inf/554_exp_caus_inf.html",
    "href": "block_6/554_exp_caus_inf/554_exp_caus_inf.html",
    "title": "Experimentation and Causal Inference",
    "section": "",
    "text": "Want to address causality from:\n\nRandomized studies\n\nCan randomly allocate control (placebo) and treatment groups\n\nNon-randomized studies\n\nTreatment randomization is impossible\nNeed to apply statistical methods to address it\n\n\n\n\n\nExperimental: The researcher manipulates the independent variable and observes the effect on the dependent variable.\n\nE.g See if new gym class reduces body fat percentage, so randomly assign people to new and old class\n\nObservational: The researcher observes the effect of the independent variable on the dependent variable without manipulating the independent variable.\n\nE.g. See if new gym class reduces body fat percentage, so select people who signed up for new and old class\n\n\n\n\n\n\nRecall 552 Hypothesis Testing\nTolerance to Type I Error is \\(\\alpha\\)\n\n\\(\\alpha = P(\\text{reject null} | \\text{null is true})\\)\nReject \\(H_0\\) when \\(p &lt; \\alpha\\)\n\n\n\n\n\nEasier to publish paper if findings are significant at \\(\\alpha = 0.05\\)\nBy Construction:\n\n\\(H_0\\) is “conservative”/ “boring”/ “status quo”\n\\(H_a\\) is “exciting”/ “new”/ “interesting”\n\nWhen \\(p &gt; \\alpha\\), we fail to reject \\(H_0\\)\n\nDoes not mean \\(H_0\\) is true\nJust means we do not have enough evidence to reject it\nMakes bad headlines: (e.g. “Null Hypothesis Not Rejected”)\n\nP-Hacking: Repeatedly testing until \\(p &lt; \\alpha\\)\n\nInflate Type I Error Rate\nDoes not always mean if \\(p &lt; \\alpha\\), the result is significant\n\nBE SCEPTICAL OF PAPERS, DO NOT BELIEVE EVERYTHING\n\nNeed Bonferroni Correction to adjust for multiple comparisons\n\n\n\n\n\n\nHave 20 foods to test and 20 body parts to test\nIn total get 400 tests (20 foods * 20 body parts)\nSimulate that the food is NOT related to any body part\nHowever, when we randomly test, we will get around 40% of the tests significant at \\(\\alpha = 0.05\\)\n\n162 tests is significant of the 400\n\nThis is because of Multiple Comparisons\n\nWe inflate the Type I Error Rate (\\(\\alpha\\) is too high)\nNeed to adjust (decrease) \\(\\alpha\\) to account for multiple comparisons\n\n\n\n\n\\[\nE_ i = \\text{Committing Type I error in the } i \\text{th test} \\\\\nP(E_ i) = \\alpha.\n\\]\nThe probability of NOT committing Type I error in the \\(i\\)th test is the following: \\[P\\left(E_ i^c\\right) = 1 - \\alpha\\]\nProbability of NOT committing Type I error in any of the tests is: \\[P\\left(\\bigcap_{i=1}^n E_ i^c\\right) = \\left(1 - \\alpha\\right)^n\\]\nProbability of committing at least one Type I error in the tests is: \\[P\\left(\\bigcup_{i=1}^n E_ i\\right) = 1 - P\\left(\\bigcap_{i=1}^n E_ i^c\\right) = 1 - \\left(1 - \\alpha\\right)^n\\]\nThe inflated probability corresponds to committing AT LEAST one Type I error in the \\(m\\) tests.\n\n\n\n\n\n\n\n\nConservatively guard against p-hacking\nIdea: If we do \\(m\\) tests, then \\(\\alpha\\) should be \\(\\frac{\\alpha}{m}\\)\n\npval # Matrix of p-values\n\n# Bonferroni Correction\npval_corrected = p.adjust(pval, method = \"bonferroni\")\n\n\n\nLet \\(R_i\\) be the event of rejecting \\(H_0\\) when \\(H_0\\) is true for the \\(i\\)th test. Then:\n\\[\nP\\left(E_ i^c\\right) \\leq \\sum_{i=1}^m P\\left(R_i\\right)\n\\]\nThis leads to the Family-Wise Error Rate (FWER)\n\nThe chance that one or more of the true null hypotheses is rejected\n\n\\[\nFWER \\leq \\sum_{i=1}^m \\frac{\\alpha}{m} = \\alpha\n\\]\n\nThe Bonferroni Correction guarantees that we wrongly reject the null hypothesis with probability less than \\(\\alpha\\)\nDrawback: Very Conservative\n\n\\[P(R_1 \\; \\cup \\; R_2) \\leq P(R_1) + P(R_2)\\]\nWorks if \\(R_1\\) and \\(R_2\\) are mutually exclusive.\n\nIf not, then \\(P(R_1 \\; \\cup \\; R_2) = P(R_1) + P(R_2) - P(R_1 \\; \\cap \\; R_2)\\)\n\nSo then the there will be too much correction -&gt; TOO CONSERVATIVE / More Type II Errors\n\n\n\n\nWe cannot just look at p-value and see if study is significant\nNeed to find out how much “hunting” they did\nThere is a trade-off between false discoveries (FP) and missing real effects (FN)\n\n\\[FDR = \\frac{FP}{FP + TP}\\]\n\nLESS STRICT than Bonferroni Correction\n\nIt is a method to control the Expected Proportion of False Positives instead of the probability of one or more false positives\n\n\n\n\n\nOne method to control FDR\nMethod:\n\nSpecify a maximum acceptable FDR \\(\\alpha\\)\nOrder the p-values from smallest to largest\nFind the largest \\(k\\) such that \\(p_{(k)} \\leq \\frac{k}{m} \\alpha\\)\n\n\\(p_{(k)}\\) is the \\(k\\)th smallest p-value\n\nTake the \\(k\\) smallest p-values as significant\n\nGet BH adjusted p-values as: \\(\\min \\Big\\{ \\frac{p\\text{-value}_i \\times m}{i}, \\text{BH adjusted } p\\text{-value}_{i + 1} \\Big\\}\\)\n\n# Benjamini-Hochberg Procedure\npval_corrected = p.adjust(pval, method = \"fdr\")\n\n\n\n\n\n\n\n\n\n\n\nFWER\nFDR\n\n\n\n\nWhen there is high confidence to TP\nWhen there is certain proportion of FP\n\n\nWant to be conservative\nWant to be less conservative\n\n\nPrefer False Negatives\nPrefer False Positives\n\n\nTukeyHSD\npairwise.prop.test(method=\"BH\")\n\n\n\n\n\n\n\n\nArises when the association/trend between two variables is reversed when a third variable (confounder) is taken into account\n\nLeads to deceptive statistical conclusions\n\n\n\n\n\n\nCriteria to be a confounder:\n\nRelated to the outcome by prognosis/ susceptibility\nthe distribution of the confounding factor is different in the groups being compared\n\nConfounder is a variable related to both the explanatory and response variables\nImportant to consider because without considering them, we may unkowingly observe a false demonstration of an association or the masking of an association between the explanatory and response variables.\n\n\n\n\n\n\nStratification: good for observational data\n\nImplies checking the association between \\(X\\) and \\(Y\\) within each stratum of the confounder\nDrawback: need to know all possible confounders\n\nAnalysis of Variance (ANOVA): good for experimental data\n\nRandomize each subject to a level of \\(X\\)\n\nRandomization breaks the dependency between exploratory/ response variables and confounders\n\nThen compare \\(Y\\) across levels of \\(X\\) \n\n\n\nThe second strategy is the golden standard in causal inference and design and analysis of experiments\n\nThe regressor \\(X\\) is guaranteed to be independent of all possible confounders (known and unknown)\nNo longer need to be concerned with confounding\nCan interpret the association between \\(X\\) and \\(Y\\) in a causal manner\n\n\n\n\n\n\n\nA/B Testing: randomized experiment between control and treatment groups\nA/B/n Testing: randomized experiment between control for more than 2 groups (n is not the same as n samples)\nRecall: randomization makes variables independent of any confounding variables\nHence we can infer causality instead of just association\n\n\n\n\n\n\nExample: Have two categorical variables: color: blue/red and font_size: small/ medium/ large to predict duration of stay on a website\nVariables: color, font_size\n\n6 possible treatment groups: 3 font sizes * 2 colors\nIf all 6 are used -&gt; Full Factorial Design, specifically 2x3 Factorial Design\n\nEach user is experimental unit\nEach treatment group involves replicates (multiple experimental units)\n\nThe experimental units are randomly assigned to treatment groups\n\n\n\n\n\n\n\n\n\nTerms\nDefinitions\n\n\n\n\nStratification\nProcess of splitting the population into homogenous subgroups and sampling observational units from the population independently in each subgroup.\n\n\nFactorial Design\nTechnique to investigate effects of several variables in one study; experimental units are assigned to all possible combinations of factors.\n\n\nCofounder\nA variable that is associated with both the explanatory and response variable, which can result in either the false demonstration of an association, or the masking of an actual association between the explanatory and response variable.\n\n\nBlocking\nGrouping experimental units (i.e., in a sample) together based on similarity.\n\n\nFactor\nExplanatory variable manipulated by the experimenter.\n\n\nExperimental Unit\nThe entity/object in the sample that is assigned to a treatment and for which information is collected.\n\n\nReplicate\nRepetition of an experimental treatment.\n\n\nBalanced Design\nEqual number of experimental units for each treatment group.\n\n\nRandomization\nProcess of randomly assigning explanatory variable(s) of interest to experimental units (e.g., patients, mice, etc.).\n\n\nTreatment\nA combination of factor levels.\n\n\nA/B Testing\nStatistically comparing a key performance indicator (conversion rate, dwell time, etc.) between two (or more) versions of a webpage/app/add to assess which one performs better.\n\n\nObservational Study\nA study where the researcher does not randomize the primary explanatory variables of interest.\n\n\n\n\n\n\n\nTypically initial analysis in A/B testing\nmain effects: standalone regressors (e.g. color, font_size)\n\n\\[\nY_{i,j,k} = \\mu_{i} + \\tau_{j} + \\varepsilon_{i, j, k}\n\\]\n\nFor \\(i\\)th font and \\(j\\)th color, for the \\(k\\)th experimental unit\n\nOLS_ABs_main_effects &lt;- lm(formula = Duration ~ Font + Color, data = data)\nanova(OLS_ABs_main_effects)\n\nANOVA tables go hand-in-hand with randomized experiments\n\n\n\np-values tests the hypothesis that:\n\n\\(H_0\\): \\(\\mu_{1} = \\mu_{2} = \\mu_{3}\\) (no effect of font)\n\\(H_a\\): at least one \\(\\mu_{i}\\) is different\n\nRecall: If p-value &lt; 0.05, we reject the null hypothesis\nDf represents the degrees of freedom\n\nIt is of the formula: # of levels - 1\n\n\n\n\n\n\\[\nY_{i,j,k} = \\mu_{i} + \\tau_{j} + (\\mu\\tau)_{i,j} + \\varepsilon_{i, j, k}\n\\]\n\nAdds the interaction term \\((\\mu\\tau)_{i,j}\\) to the model\n\nDoes not indicate they are multiplied\n\n\nOLS_ABs_interaction &lt;- lm(formula = Duration ~ Font * Color, data = data)\nanova(OLS_ABs_interaction)\n\nDf for interaction term is the product of Df1 and Df2\n\n\n\n\n\nDone if any factor is significant\nCompares all possible pairs among the levels\n\nInvolves multiple testing corrections\n\nWill use Tukey’s HSD test\n\nKeeps the Family-Wise Error Rate (FWER) at \\(\\alpha\\) or less\nOnly for ANOVA models\n\n\n# Need aov: basically lm but for ANOVA\nOLS_aov_ABn &lt;- aov(formula = Duration ~ Font * Color, data = data)\nTukeyHSD(OLS_aov_ABn, conf.level = 0.95) |&gt; tidy()\n\n\n\n\n\n\nBlocking Factor: a variable that is not of primary interest (and cannot be controlled) but is known to affect the response variable\n\nConsider as grouping factor\nExamples: time period when A/B testing is conducted, user demographics, etc.\n\n“Block what you can, randomize what you cannot.”\n\nBlocking: removing the effect of secondary measurable variables from the response\n\nThis is so our model could get statistical association/ causation between the secondary variable and the response variable\n\nRandomization: removing the effect of unmeasurable variables\n\n\n\nStrategy A: Just Randomize, Strategy B: Randomize and Block\n\n\n\nStratify experimental units into homogenous blocks. Each stratum is a block\nRandomize the experimental units into the treatment groups within each block\n\n\n\n\nSorted Best to Worst:\n\nModel with blocking for blocking factors + Post-Hoc adjustments\nNormal data with Post-Hoc adjustments for blocking factors (Treat blocking as a covariate)\n\ncovariate: a variable that is possibly predictive of the response variable\n\nRaw model with no blocking nor Post-Hoc adjustments\n\n\n\n\n\n\nIncreasing sample size would:\n\nIncrease the accuracy of the estimates\nIncrease the power of the test\n\nPower: probability of rejecting the null hypothesis when it is false (predict correctly)\n\nHigh power means our A/B test is robust enough to detect and estimate experimental treatment effects\n\nIn class had example: Need 10x the sample size for raw to perform equal to covariate/ blocking\n\n\nBut this model would not capture the right systematic component of the population, including the stratum effect (demographic categories)\nUsing covariate and blocking would still be more precise\nNeed to consider that increasing sample size is also expensive\n\n\n\n\nRecall that sample size computations involve playing around with three concepts:\n\neffect size: how much we want the experimental treatment to differ from the control treatment in terms of the mean response\nsignificance level \\(\\alpha\\)\n\nLower = more strict (less prone to Type I error/ false positive)\n\\(P(\\text{reject } H_0 | H_0 \\text{ is true}) = \\alpha\\)\n\npower of the test \\(1 - \\beta = 1 - P(\\text{Type II error})\\),\n\nTypically 0.8\nlarger = less prone to Type II error\n\\(P(\\text{reject } H_0 | H_a \\text{ is true}) = 1 - \\beta = 1 - P(\\text{accept } H_0 | H_a \\text{ is true})\\)\n\n\n\n\n\n\n\n\n\n\n\n\nDecision\nTrue Condition Positive (\\(H_a\\) True)\nTrue Condition Negative (\\(H_0\\) True)\n\n\n\n\nTest Positive (Reject \\(H_0\\))\nCorrect (True Positive)\nType I Error (False Positive)\n\n\nTest Negative (Accept \\(H_0\\))\nType II Error (False Negative)\nCorrect (True Negative)\n\n\n\nsee more in my statistical inference notes\n\n\n\nLets say we want to see if changeing website design from “X” to “Y” will increase the average time spent on the website.\nWe know:\n\nCurrent average is between 30s to 2 min\nWe want to see a 3% increase in time spent\n\nCalculation:\n\n\\(\\mu_x = 0.5 + \\frac{2-0.5}{2} = 1.25 \\text{ min}\\)\nUsing 95% CI, \\(Z_i = \\frac{Y_i - \\mu_A}{\\sigma} \\sim \\mathcal{N}(0, 1)\\)\n\n\\(1.96 = \\frac{2 - \\mu_A}{\\sigma}\\)\n\\(\\sigma = \\frac{2 - 1.25}{1.96} = 0.383\\)\n\n\\(\\delta = 0.03 \\times 1.25 = 0.0375\\) (desired difference) \n\nUse pwr.t.test function in R to calculate sample size needed\n\nwill give n (sample size) needed for each group\n\n\npwr.t.test(\n  d = 0.0375 / 0.383, # delta / sigma\n  sig.level = 0.05,\n  power = 0.8,\n  type = \"two.sample\",\n  alternative = \"greater\" # we want to see an increase\n)\n\nLower power means need less sample size\n\nHigher response increase means more power for a given sample size\n\n\n\n\n\nWe want to ensure that the blocks are homogeneous:\n\nAs much variation in \\(\\mu\\) as possible is across the blocks\nAs little variation in \\(\\mu\\) as possible is within the blocks\n\nIf not homogenous: blocking can be worse than raw\nThe blocking setup needs to be carefully planned before running the experiment.\n\n\n\n\n\nEarlier was test with mean duration (continouous response) -&gt; use t-test\nWe use z-test for proportion tests\n\nproportion means value between 0 and 1\n\nE.g. see the click through rate (CTR) of a website between control “X” and treatment “Y”\n\nCTR \\(\\in [0, 1]\\)\n\n\n\n\n\nNo need \\(\\sigma\\) nor \\(\\mu\\) for proportion test\nNeed to know the effect size \\(h\\) (difference in proportions)\n\nUse ES.h(data_control, data_treatment) to calculate\n\nEffect size has an arcsine transformation:\n\n\\[h = 2 \\arcsin(\\sqrt{p_Y}) - 2 \\arcsin(\\sqrt{p_X})\\]\n\nThis means:\n\nSmaller transformed effect (y-axis) in the middle\nLarger transformed effect (y-axis) at the ends\n\n\n\n\nThis behaviour is related to the statistic’s standard error of the two-proportion z-test\n\n\\(\\delta = p_Y - p_X\\)\n\\(H_0: \\delta &lt; some\\_value\\), \\(H_a: \\delta \\geq some\\_value\\)\n\n\n\\[\nZ = \\frac{\\hat{\\delta}_{\\text{CTR}} - some\\_value}{\\sqrt{\\frac{\\hat{\\text{CTR}}_A (1 - \\hat{\\text{CTR}}_A)}{n/2} + \\frac{\\hat{\\text{CTR}}_B (1 - \\hat{\\text{CTR}}_B)}{n/2}}} = \\frac{\\hat{\\delta}_{\\text{CTR}} - some\\_value}{\\text{SE}\\left(\\hat{\\delta}_{\\text{CTR}}\n\\right)}\n\\]\n\nMore error in the middle, less error at the ends. Smaller sample size = more error\n\n\n\nCTR_effect_size = ES.h(data_control, data_treatment)\n\npwr.2p.test(\n  h = CTR_effect_size,\n  sig.level = alpha,\n  power = pow,\n  alternative = \"greater\"\n)\n\nWe can see that we need more sample size when:\n\n\\(\\delta\\) is smaller\n\\(CTR_X\\) is closer to 0.5, more error in the middle\n\n\n\n\n\n\nPeeking: looking at the data before the experiment is over/ through the experiment\nStill compute overall sample size to get \\(n_{max}\\)\nIf at some peek, updated test statistic is significant, we can stop the experiment\n\n\n\nAggresive Peeking: look at the data after every new experimental unit\n\nIf test \\(P_Y &gt; P_X\\) (and in fact it is true), aggressive peeking will improve power of the test if:\n\n\\(P_Y &gt; 1.5 * P_X\\)\nOtherwise, it gives a concerning power decrease\n\nIf test \\(P_Y = P_X\\),\n\nthe proportion of replicates where \\(z_{test} &gt; z_{1- \\alpha}\\) correspond to type I error rate\nwill lead to inflating the type I error rate\n\n\n\n\n\n\n\n\nWe do not have control of the variable of interest.\nWithout randomization, life is harder. Strategy includes:\n\nRecording potential confounders\nUse confounders as part of the analysis\nTempering the causal strength in light of inherent challenges in (i) and (ii)\n\n\n\n\n\nResponse \\(Y\\): binary indicator of disease state. (1: disease, 0: no disease)\n\\(X\\): binary indicator of behavior (1: type A, 0: type B)\n\nType A: more agressive personality, competitive, etc\nType B: more laid back personality, relaxed, etc\n\nConfounders \\(C_j\\) (e.g. age, sex, BMI, cholesterol level, etc.)\n\n\n\nSince it is binary =&gt; binary logistic regression\n\nUse log-odds \\(logit(p) = \\log(\\frac{p}{1-p})\\)\nodds-ratio: \\(\\text{OR} = \\frac{\\frac{n_{X = 1, Y = 1}/ n}{n_{X = 1,Y = 0} / n}}{\\frac{n_{X = 0, Y = 1}/ n}{n_{X = 0,Y = 0} / n}} = \\frac{\\frac{n_{X = 1, Y = 1}}{n_{X = 1,Y = 0}}}{\\frac{n_{X = 0, Y = 1}}{n_{X = 0,Y = 0}}} = \\frac{n_{X = 1, Y = 1} \\times n_{X = 0,Y = 0}}{n_{X = 1,Y = 0} \\times n_{X = 0, Y = 1}}\\)\n\nOR = 1: X does not effect Y\nOR &gt; 1: X increases the odds of Y\nOR &lt; 1: X decreases the odds of Y\n\n\\(\\text{SE} = \\sqrt{\\frac{1}{n_{X = 1, Y = 1}} + \\frac{1}{n_{X = 1,Y = 0}} + \\frac{1}{n_{X = 0, Y = 1}} + \\frac{1}{n_{X = 0, Y =0}}}\\)\n\nCan also just use binary logistic regression in R\n\nglm(Y ~ X, data = data, family = binomial) |&gt;\n  tidy(conf.int = 0.95)\n\n\n\nTurn a continous confounder (e.g. age) into discrete categories and add to the model\n\ndata |&gt; mutate(age_bins = cut(age, breaks = c(min(age), quantile(age, (1:3) / 4), max(age)), include.lowest = TRUE)\n\nBy making stratum-specific inference with multiple confounders, we aim to infer causality between X and Y\n\nHowever, there will be few observations in each strata (not enough data)\nSolution: use binomial logistic regression (Overall Model-Based Inference) with interaction terms\n\n\nglm(Y ~ X * C1 * C2, data = data, family = binomial) |&gt;\n  tidy(conf.int = 0.95)\nrecall: odds ratio is \\(exp(\\beta)\\) where \\(\\beta\\) is the coefficient/ estimate\n\n\n\n\nSimple/ smooth structure in how the Y-specific log-OR varies across the strata\n\nCheck using ANOVA comparing the simple model (all additive terms) and the complex model (with interaction terms of all confounders with each other)\ncomplex_model &lt;- glm(Y ~ X + C1 * C2, data = data, family = binomial)\nanova(simple_model, complex_model, test = \"LRT\")\n\nStrength of \\((X, Y)\\) association is constant across the strata (i.e. no interaction between X and C)\n\ncomplex model: all simple terms + double interactions of X and confounders\ncomplex_model_c1 &lt;- glm(Y ~ X + C1 + C2 + X:C1, data = data, family = binomial)\ncomplex_model_c2 &lt;- glm(Y ~ X + C1 + C2 + X:C2, data = data, family = binomial)\nCompare all models with simple model using ANOVA\n\nNo unmeasured confounders (All confounders are included in the model)\n\nAdd new model with unmeasured confounder\nnew_model &lt;- glm(Y ~ X + C1 + C2 + CU, data = data, family = binomial) where CU is the unmeasured confounder\n\n\n\n\n\n\n\n\nMust Explore how to select our sample before executing the study\nAnalysis must also account for the sampling scheme\n\n\n\n\nGround Truth: checking the results of the study in terms of its estimation accuracy against the real-world\nGround truth is hard to get/ not available in many cases\n\nFrequentist paradigm: we do not have access to true population parameters\nNeed to reply on sample to estimate the population parameters\n\n\n\n\n\nA better simulation technique than Monte Carlo\nCore Idea:\n\nUse a relevant sample dataset: assumes previous sample data is representative of the population\nFit a model with Y ~ X + C_j (where \\(C_j\\) are a determined set of confounders)\nSimulate a proxy ground truth by generating new data from the model\n\nStill use the 3 assumptions for the causal model-based inference\nSteps:\n\nGet a dataset and generate multiple rows of data (break continuous variables into quartile bins)\nFit a model with the data\nSimulate a proxy ground truth by generating new data from the model\n\n\n\n\n\n\n\nThree Sampling Schemes:\n\nCross-Sectional Sampling\nCase-Control Sampling\nCohort Sampling\n\nThese schemes will imply different temporalities\n\n\n\n\nContemporaneous: all data is collected at the same time\n\nGrab a simple random sample of size n from the population\nSimilar to an instantaneous snapshot of all study variables\n\nIdeal in early research stages to get a sense of the data\n\nFast to run\n\n\nset.seed(123) # for reproducibility\n\nCS_sample_index &lt;- sample(1:n, size = n_sample, replace = FALSE)\nCS_data &lt;- data[CS_sample_index, ]\n\n\n\n\nRetrospective: data is collected after the event of interest has occurred\n\nSample into cases Y=1 and controls Y=0 (Equal sample sizes for both)\nThen ask the subjects “have you been exposed to X in the past?”\n\nIdeal where outcome Y=1 is rare\n\nNot have have a lot of cases of Y=1 so recruit a lot of patients with Y=1 for study\n\nIt will oversample \\(Y=1\\) cases and undersample \\(Y=0\\) controls\n\nLeads to a second statistical inquiry: “Is it a winning strategy to oversample cases?”\n\nDo modified Power Analysis using Case:Control ratio\nIf ratio = 1, then control = case\nIf ratio &gt; 1, then case &gt; control\nIf ratio &lt; 1, then case &lt; control\n\nAccording to SE behaviors, in populations when \\(Y=1\\) is rare, get more precise estimates by oversampling cases and undersampling controls\n\n\nset.seed(123) # for reproducibility\n\nCC_sample_index &lt;- c(sample((1:n)[data$Y == 1], size = n_sample/2, replace = FALSE),\n                      sample((1:n)[data$Y == 0], size = n_sample/2, replace = FALSE))\nCC_data &lt;- data[CC_sample_index, ]\n\n\n\n\nProspective: data is collected over time\n\nSample into exposed X=1 and unexposed X=0 (Equal sample sizes for both)\nThen follow the subjects over time to see if they develop the disease Y=1\n\nY is assumed as the recorded outcome at the end of the study\nIdeal for when exposure X=1 is rare\n\nNot have have a lot of cases of X=1 so recruit a lot of patients with X=1 for study\n\n\nset.seed(123) # for reproducibility\n\nCO_sample_index &lt;- c(sample((1:n)[data$X == 1], size = n_sample/2, replace = FALSE),\n                         sample((1:n)[data$X == 0], size = n_sample/2, replace = FALSE))\nCO_data &lt;- data[C0_sample_index, ]\n\n\n\n\n\nNeed to run multiple simulations to get a sense of the variability of the estimates\nUse function sim_study to run the simulation\n\nsim_study &lt;- function(pop_data, n, alpha, log_OR, num_replicates) {\n  res &lt;- list(NULL) # Setting up matrix with metrics\n  res[[1]] &lt;- res[[2]] &lt;- res[[3]] &lt;- matrix(NA, num_replicates, 3)\n\n  suppressMessages(for (lp in 1:num_replicates) { # Otherwise, we get \"Waiting for profiling to be done...\"\n    # Obtaining samples by scheme\n    # CS\n    CS_sampled_subjects &lt;- sample(1:nrow(pop_data), size = n, replace = F)\n    CS_sample &lt;- pop_data[CS_sampled_subjects, ]\n    # CC\n    CC_sampled_subjects &lt;- c(\n      sample((1:nrow(pop_data))[pop_data$chd69 == \"0\"],\n        size = n / 2, replace = F\n      ),\n      sample((1:nrow(pop_data))[pop_data$chd69 == \"1\"],\n        size = n / 2, replace = F\n      )\n    )\n    CC_sample &lt;- pop_data[CC_sampled_subjects, ]\n    # CO\n    CO_sampled_subjects &lt;- c(\n      sample((1:nrow(pop_data))[pop_data$dibpat == \"Type B\"],\n        size = n / 2, replace = F\n      ),\n      sample((1:nrow(pop_data))[pop_data$dibpat == \"Type A\"],\n        size = n / 2, replace = F\n      )\n    )\n    CO_sample &lt;- pop_data[CO_sampled_subjects, ]\n\n    # Do the three analyses\n    # CS\n    CS_bin_log_model &lt;- glm(chd69 ~ dibpat + age_bins + smoke + bmi_bins + chol_bins,\n      family = \"binomial\", data = CS_sample\n    )\n    # CC\n    CC_bin_log_model &lt;- glm(chd69 ~ dibpat + age_bins + smoke + bmi_bins + chol_bins,\n      family = \"binomial\", data = CC_sample\n    )\n    # CO\n    CO_bin_log_model &lt;- glm(chd69 ~ dibpat + age_bins + smoke + bmi_bins + chol_bins,\n      family = \"binomial\", data = CO_sample\n    )\n\n    # and the takeaways\n    res[[1]][lp, ] &lt;- c(coef(CS_bin_log_model)[\"dibpatType A\"], confint(CS_bin_log_model)[\"dibpatType A\", ])\n    res[[2]][lp, ] &lt;- c(coef(CC_bin_log_model)[\"dibpatType A\"], confint(CC_bin_log_model)[\"dibpatType A\", ])\n    res[[3]][lp, ] &lt;- c(coef(CO_bin_log_model)[\"dibpatType A\"], confint(CO_bin_log_model)[\"dibpatType A\", ])\n  })\n\n  # Summaries\n  BIAS &lt;- sapply(\n    res,\n    function(mat) {\n      mean(mat[, 1]) - log_OR\n    }\n  )\n  vrnc &lt;- sapply(res, function(mat) {\n    var(mat[, 1])\n  })\n  CVRG &lt;- sapply(res,\n    function(mat, trg) {\n      mean((mat[, 2] &lt; trg) & (trg &lt; mat[, 3]))\n    },\n    trg = log_OR\n  )\n  PWR &lt;- sapply(res, function(mat) {\n    mean(mat[, 2] &gt; 0)\n  })\n  RMSE &lt;- sqrt(BIAS^2 + vrnc)\n\n  opt &lt;- cbind(BIAS, RMSE, CVRG, PWR)\n  rownames(opt) &lt;- c(\"Cross-Sectional (CS)\", \"Case-Control (CC)\", \"Cohort (CO)\")\n\n  return(opt)\n}\n\nFunction inputs:\n\npop_data: the dataset where we will sample from\nn: sample size\nalpha: significance level \\(\\alpha\\)\nlog_OR: true log odds ratio\nnum_replicates: number of simulations to run\n\nThe function sim_study will return a matrix with the following metrics:\n\nBIAS: difference between the estimated and true log odds ratio\nRMSE: root mean squared error\nCVRG: coverage of the 95% confidence interval (as a proportion of num_replicates)\nPWR: power of the test\n\n\n\n\n\n\n\nAll our previous sampling schemes used a binary logistic regression model\nRecall we checked that CC is the best compared to the other 2 designs (CS and CO) in terms of power when \\(Y=1\\) is rare\n\n\n\n\nUsing CC is acceptable because:\n\nWe assumed \\(Y|X, C_1, ..., C_p\\)\nCreate artificial population by “cloning subjects”\n\nThis is done using the estimated model from (previous reperesentative sample) + induced random noise\nAKA Proxy Ground Truth\n\n\nCC is useful when \\(Y=1\\) is rare and sampling is costly\n\n\n\n\n\nNeed to have artificial population to be representative of the true population\nThen will sample to get a sample size of \\(n\\)\nRecord the confounders of interest as strata\nSample \\(n/2\\) cases then \\(n/2\\) controls\n\nKeep Case:Control ratio = 1\nMatch exactly on the confounder to the case counterpart\n\ne.g. case has confounder \\(C_1=1\\), then control must have \\(C_1=1\\)\n\n\nImportant: Cannot fit Binary Logistic Regression model since we have matched pairs\n\nCan get a sparse data problem\nUse McNemar’s Test instead\n\nCC-matched will show a smaller average bias compared to CC-unmatched\n\nPower is the same once \\(n\\) increases\n\n\n\n\n\n\nControl \\(X=0\\)\nControl \\(X=1\\)\n\n\n\n\nCase \\(X=0\\)\n\\(n_{00}\\)\n\\(n_{01}\\)\n\n\nCase \\(X=1\\)\n\\(n_{10}\\)\n\\(n_{11}\\)\n\n\n\n\n\\(n_{00}\\) and \\(n_{11}\\) are the concordant pairs\n\\(n_{01}\\) and \\(n_{10}\\) are the discordant pairs\nEstimator of the odds ratio is based on discordant pairs: \\(OR = \\frac{n_{10}}{n_{01}}\\)\n\n\\(OR = 1\\) implies no association\n\\(OR &gt; 1\\) implies positive association\n\\(OR &lt; 1\\) implies negative association\n\n\n\n\n\n\\(H_0: log(OR) = 0\\), \\(H_a: log(OR) \\neq 0\\)\n\\(log(OR) = log{n_{10}} - log{n_{01}}\\)\nIt is approximately normally distributed with an SE of:\n\n\\(SE = \\sqrt{\\frac{1}{n_{10}} + \\frac{1}{n_{01}}}\\)\n\nTest statistic: \\(Z = \\frac{log(OR)}{SE}\\)\n\n\n\n\n\n\n\nThe numerical confounding strata we have been using are ordinal\n\n\n\n\n\\(Y\\) is continuous, \\(X\\) is ordinal\n\n\nNeed to convert the variable \\(X\\) to ordinal\n\ndata$X_ord &lt;- ordered(data$X, levels=c(\"low\", \"medium\", \"high\"))\n\nFit a one-way analysis of variance (ANOVA) model\n\n\nR uses polynomial contrasts in ordered type factors\n\nElements of vectors sum to 0\nRoughly, if have k levels, then k-1 polynomials\nE.g. l=4, we will have linear, quadratic, cubic contrasts\nUse contr.poly(l) to get the contrasts when l levels\n\nGives design matrix for the contrasts\nCols sum to 0\nRows are the contrasts and are orthogonal\n\n\n\nOLS &lt;- lm(Y ~ X_ord, data=data)\n\n# Get contrasts\nOLS |&gt; model.matrix()\n\nHypothesis test:\n\n\\(H_0\\): there is no GIVEN trend in the ordered data\n\\(H_1\\): there is a GIVEN trend in the ordered data\nGIVEN will be replaced with linear, quadratic, cubic, etc\n\n\nplot &lt;- eda_plot +\n    geom_smooth(aes(x=unclass(X_ord), color=1),\n        formula=y~x, method=\"lm\", se=FALSE) +\n    geom_smooth(aes(x=unclass(X_ord), color=2),\n        formula=y~poly(x, 2), method=\"lm\", se=FALSE) +\n    geom_smooth(aes(x=unclass(X_ord), color=3),\n        formula=y~poly(x, 3), method=\"lm\", se=FALSE)\n\n\n\nAlternative to make inferential interpretations more straightforward\nWant to answer whether differences exist between ordered levels\n\noptions(contrasts = c(\"contr.treatment\", \"contr.sdif\"))\n\nOLS_sdif &lt;- lm(Y ~ X_ord, data=data) |&gt; tidy()\n\nInterpretation is very straightforward"
  },
  {
    "objectID": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#observational-vs-experimental-studies",
    "href": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#observational-vs-experimental-studies",
    "title": "Experimentation and Causal Inference",
    "section": "",
    "text": "Experimental: The researcher manipulates the independent variable and observes the effect on the dependent variable.\n\nE.g See if new gym class reduces body fat percentage, so randomly assign people to new and old class\n\nObservational: The researcher observes the effect of the independent variable on the dependent variable without manipulating the independent variable.\n\nE.g. See if new gym class reduces body fat percentage, so select people who signed up for new and old class"
  },
  {
    "objectID": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#hypothesis-testing-review",
    "href": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#hypothesis-testing-review",
    "title": "Experimentation and Causal Inference",
    "section": "",
    "text": "Recall 552 Hypothesis Testing\nTolerance to Type I Error is \\(\\alpha\\)\n\n\\(\\alpha = P(\\text{reject null} | \\text{null is true})\\)\nReject \\(H_0\\) when \\(p &lt; \\alpha\\)\n\n\n\n\n\nEasier to publish paper if findings are significant at \\(\\alpha = 0.05\\)\nBy Construction:\n\n\\(H_0\\) is “conservative”/ “boring”/ “status quo”\n\\(H_a\\) is “exciting”/ “new”/ “interesting”\n\nWhen \\(p &gt; \\alpha\\), we fail to reject \\(H_0\\)\n\nDoes not mean \\(H_0\\) is true\nJust means we do not have enough evidence to reject it\nMakes bad headlines: (e.g. “Null Hypothesis Not Rejected”)\n\nP-Hacking: Repeatedly testing until \\(p &lt; \\alpha\\)\n\nInflate Type I Error Rate\nDoes not always mean if \\(p &lt; \\alpha\\), the result is significant\n\nBE SCEPTICAL OF PAPERS, DO NOT BELIEVE EVERYTHING\n\nNeed Bonferroni Correction to adjust for multiple comparisons\n\n\n\n\n\n\nHave 20 foods to test and 20 body parts to test\nIn total get 400 tests (20 foods * 20 body parts)\nSimulate that the food is NOT related to any body part\nHowever, when we randomly test, we will get around 40% of the tests significant at \\(\\alpha = 0.05\\)\n\n162 tests is significant of the 400\n\nThis is because of Multiple Comparisons\n\nWe inflate the Type I Error Rate (\\(\\alpha\\) is too high)\nNeed to adjust (decrease) \\(\\alpha\\) to account for multiple comparisons\n\n\n\n\n\\[\nE_ i = \\text{Committing Type I error in the } i \\text{th test} \\\\\nP(E_ i) = \\alpha.\n\\]\nThe probability of NOT committing Type I error in the \\(i\\)th test is the following: \\[P\\left(E_ i^c\\right) = 1 - \\alpha\\]\nProbability of NOT committing Type I error in any of the tests is: \\[P\\left(\\bigcap_{i=1}^n E_ i^c\\right) = \\left(1 - \\alpha\\right)^n\\]\nProbability of committing at least one Type I error in the tests is: \\[P\\left(\\bigcup_{i=1}^n E_ i\\right) = 1 - P\\left(\\bigcap_{i=1}^n E_ i^c\\right) = 1 - \\left(1 - \\alpha\\right)^n\\]\nThe inflated probability corresponds to committing AT LEAST one Type I error in the \\(m\\) tests."
  },
  {
    "objectID": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#multiple-comparisons",
    "href": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#multiple-comparisons",
    "title": "Experimentation and Causal Inference",
    "section": "",
    "text": "Conservatively guard against p-hacking\nIdea: If we do \\(m\\) tests, then \\(\\alpha\\) should be \\(\\frac{\\alpha}{m}\\)\n\npval # Matrix of p-values\n\n# Bonferroni Correction\npval_corrected = p.adjust(pval, method = \"bonferroni\")\n\n\n\nLet \\(R_i\\) be the event of rejecting \\(H_0\\) when \\(H_0\\) is true for the \\(i\\)th test. Then:\n\\[\nP\\left(E_ i^c\\right) \\leq \\sum_{i=1}^m P\\left(R_i\\right)\n\\]\nThis leads to the Family-Wise Error Rate (FWER)\n\nThe chance that one or more of the true null hypotheses is rejected\n\n\\[\nFWER \\leq \\sum_{i=1}^m \\frac{\\alpha}{m} = \\alpha\n\\]\n\nThe Bonferroni Correction guarantees that we wrongly reject the null hypothesis with probability less than \\(\\alpha\\)\nDrawback: Very Conservative\n\n\\[P(R_1 \\; \\cup \\; R_2) \\leq P(R_1) + P(R_2)\\]\nWorks if \\(R_1\\) and \\(R_2\\) are mutually exclusive.\n\nIf not, then \\(P(R_1 \\; \\cup \\; R_2) = P(R_1) + P(R_2) - P(R_1 \\; \\cap \\; R_2)\\)\n\nSo then the there will be too much correction -&gt; TOO CONSERVATIVE / More Type II Errors\n\n\n\n\nWe cannot just look at p-value and see if study is significant\nNeed to find out how much “hunting” they did\nThere is a trade-off between false discoveries (FP) and missing real effects (FN)\n\n\\[FDR = \\frac{FP}{FP + TP}\\]\n\nLESS STRICT than Bonferroni Correction\n\nIt is a method to control the Expected Proportion of False Positives instead of the probability of one or more false positives\n\n\n\n\n\nOne method to control FDR\nMethod:\n\nSpecify a maximum acceptable FDR \\(\\alpha\\)\nOrder the p-values from smallest to largest\nFind the largest \\(k\\) such that \\(p_{(k)} \\leq \\frac{k}{m} \\alpha\\)\n\n\\(p_{(k)}\\) is the \\(k\\)th smallest p-value\n\nTake the \\(k\\) smallest p-values as significant\n\nGet BH adjusted p-values as: \\(\\min \\Big\\{ \\frac{p\\text{-value}_i \\times m}{i}, \\text{BH adjusted } p\\text{-value}_{i + 1} \\Big\\}\\)\n\n# Benjamini-Hochberg Procedure\npval_corrected = p.adjust(pval, method = \"fdr\")\n\n\n\n\n\n\n\n\n\n\n\nFWER\nFDR\n\n\n\n\nWhen there is high confidence to TP\nWhen there is certain proportion of FP\n\n\nWant to be conservative\nWant to be less conservative\n\n\nPrefer False Negatives\nPrefer False Positives\n\n\nTukeyHSD\npairwise.prop.test(method=\"BH\")"
  },
  {
    "objectID": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#simpsons-paradox",
    "href": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#simpsons-paradox",
    "title": "Experimentation and Causal Inference",
    "section": "",
    "text": "Arises when the association/trend between two variables is reversed when a third variable (confounder) is taken into account\n\nLeads to deceptive statistical conclusions\n\n\n\n\n\n\nCriteria to be a confounder:\n\nRelated to the outcome by prognosis/ susceptibility\nthe distribution of the confounding factor is different in the groups being compared\n\nConfounder is a variable related to both the explanatory and response variables\nImportant to consider because without considering them, we may unkowingly observe a false demonstration of an association or the masking of an association between the explanatory and response variables.\n\n\n\n\n\n\nStratification: good for observational data\n\nImplies checking the association between \\(X\\) and \\(Y\\) within each stratum of the confounder\nDrawback: need to know all possible confounders\n\nAnalysis of Variance (ANOVA): good for experimental data\n\nRandomize each subject to a level of \\(X\\)\n\nRandomization breaks the dependency between exploratory/ response variables and confounders\n\nThen compare \\(Y\\) across levels of \\(X\\) \n\n\n\nThe second strategy is the golden standard in causal inference and design and analysis of experiments\n\nThe regressor \\(X\\) is guaranteed to be independent of all possible confounders (known and unknown)\nNo longer need to be concerned with confounding\nCan interpret the association between \\(X\\) and \\(Y\\) in a causal manner"
  },
  {
    "objectID": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#foundations-of-ab-and-abn-testing",
    "href": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#foundations-of-ab-and-abn-testing",
    "title": "Experimentation and Causal Inference",
    "section": "",
    "text": "A/B Testing: randomized experiment between control and treatment groups\nA/B/n Testing: randomized experiment between control for more than 2 groups (n is not the same as n samples)\nRecall: randomization makes variables independent of any confounding variables\nHence we can infer causality instead of just association\n\n\n\n\n\n\nExample: Have two categorical variables: color: blue/red and font_size: small/ medium/ large to predict duration of stay on a website\nVariables: color, font_size\n\n6 possible treatment groups: 3 font sizes * 2 colors\nIf all 6 are used -&gt; Full Factorial Design, specifically 2x3 Factorial Design\n\nEach user is experimental unit\nEach treatment group involves replicates (multiple experimental units)\n\nThe experimental units are randomly assigned to treatment groups\n\n\n\n\n\n\n\n\n\nTerms\nDefinitions\n\n\n\n\nStratification\nProcess of splitting the population into homogenous subgroups and sampling observational units from the population independently in each subgroup.\n\n\nFactorial Design\nTechnique to investigate effects of several variables in one study; experimental units are assigned to all possible combinations of factors.\n\n\nCofounder\nA variable that is associated with both the explanatory and response variable, which can result in either the false demonstration of an association, or the masking of an actual association between the explanatory and response variable.\n\n\nBlocking\nGrouping experimental units (i.e., in a sample) together based on similarity.\n\n\nFactor\nExplanatory variable manipulated by the experimenter.\n\n\nExperimental Unit\nThe entity/object in the sample that is assigned to a treatment and for which information is collected.\n\n\nReplicate\nRepetition of an experimental treatment.\n\n\nBalanced Design\nEqual number of experimental units for each treatment group.\n\n\nRandomization\nProcess of randomly assigning explanatory variable(s) of interest to experimental units (e.g., patients, mice, etc.).\n\n\nTreatment\nA combination of factor levels.\n\n\nA/B Testing\nStatistically comparing a key performance indicator (conversion rate, dwell time, etc.) between two (or more) versions of a webpage/app/add to assess which one performs better.\n\n\nObservational Study\nA study where the researcher does not randomize the primary explanatory variables of interest.\n\n\n\n\n\n\n\nTypically initial analysis in A/B testing\nmain effects: standalone regressors (e.g. color, font_size)\n\n\\[\nY_{i,j,k} = \\mu_{i} + \\tau_{j} + \\varepsilon_{i, j, k}\n\\]\n\nFor \\(i\\)th font and \\(j\\)th color, for the \\(k\\)th experimental unit\n\nOLS_ABs_main_effects &lt;- lm(formula = Duration ~ Font + Color, data = data)\nanova(OLS_ABs_main_effects)\n\nANOVA tables go hand-in-hand with randomized experiments\n\n\n\np-values tests the hypothesis that:\n\n\\(H_0\\): \\(\\mu_{1} = \\mu_{2} = \\mu_{3}\\) (no effect of font)\n\\(H_a\\): at least one \\(\\mu_{i}\\) is different\n\nRecall: If p-value &lt; 0.05, we reject the null hypothesis\nDf represents the degrees of freedom\n\nIt is of the formula: # of levels - 1\n\n\n\n\n\n\\[\nY_{i,j,k} = \\mu_{i} + \\tau_{j} + (\\mu\\tau)_{i,j} + \\varepsilon_{i, j, k}\n\\]\n\nAdds the interaction term \\((\\mu\\tau)_{i,j}\\) to the model\n\nDoes not indicate they are multiplied\n\n\nOLS_ABs_interaction &lt;- lm(formula = Duration ~ Font * Color, data = data)\nanova(OLS_ABs_interaction)\n\nDf for interaction term is the product of Df1 and Df2\n\n\n\n\n\nDone if any factor is significant\nCompares all possible pairs among the levels\n\nInvolves multiple testing corrections\n\nWill use Tukey’s HSD test\n\nKeeps the Family-Wise Error Rate (FWER) at \\(\\alpha\\) or less\nOnly for ANOVA models\n\n\n# Need aov: basically lm but for ANOVA\nOLS_aov_ABn &lt;- aov(formula = Duration ~ Font * Color, data = data)\nTukeyHSD(OLS_aov_ABn, conf.level = 0.95) |&gt; tidy()"
  },
  {
    "objectID": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#blocking",
    "href": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#blocking",
    "title": "Experimentation and Causal Inference",
    "section": "",
    "text": "Blocking Factor: a variable that is not of primary interest (and cannot be controlled) but is known to affect the response variable\n\nConsider as grouping factor\nExamples: time period when A/B testing is conducted, user demographics, etc.\n\n“Block what you can, randomize what you cannot.”\n\nBlocking: removing the effect of secondary measurable variables from the response\n\nThis is so our model could get statistical association/ causation between the secondary variable and the response variable\n\nRandomization: removing the effect of unmeasurable variables\n\n\n\nStrategy A: Just Randomize, Strategy B: Randomize and Block\n\n\n\nStratify experimental units into homogenous blocks. Each stratum is a block\nRandomize the experimental units into the treatment groups within each block\n\n\n\n\nSorted Best to Worst:\n\nModel with blocking for blocking factors + Post-Hoc adjustments\nNormal data with Post-Hoc adjustments for blocking factors (Treat blocking as a covariate)\n\ncovariate: a variable that is possibly predictive of the response variable\n\nRaw model with no blocking nor Post-Hoc adjustments"
  },
  {
    "objectID": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#increasing-sample-size",
    "href": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#increasing-sample-size",
    "title": "Experimentation and Causal Inference",
    "section": "",
    "text": "Increasing sample size would:\n\nIncrease the accuracy of the estimates\nIncrease the power of the test\n\nPower: probability of rejecting the null hypothesis when it is false (predict correctly)\n\nHigh power means our A/B test is robust enough to detect and estimate experimental treatment effects\n\nIn class had example: Need 10x the sample size for raw to perform equal to covariate/ blocking\n\n\nBut this model would not capture the right systematic component of the population, including the stratum effect (demographic categories)\nUsing covariate and blocking would still be more precise\nNeed to consider that increasing sample size is also expensive\n\n\n\n\nRecall that sample size computations involve playing around with three concepts:\n\neffect size: how much we want the experimental treatment to differ from the control treatment in terms of the mean response\nsignificance level \\(\\alpha\\)\n\nLower = more strict (less prone to Type I error/ false positive)\n\\(P(\\text{reject } H_0 | H_0 \\text{ is true}) = \\alpha\\)\n\npower of the test \\(1 - \\beta = 1 - P(\\text{Type II error})\\),\n\nTypically 0.8\nlarger = less prone to Type II error\n\\(P(\\text{reject } H_0 | H_a \\text{ is true}) = 1 - \\beta = 1 - P(\\text{accept } H_0 | H_a \\text{ is true})\\)\n\n\n\n\n\n\n\n\n\n\n\n\nDecision\nTrue Condition Positive (\\(H_a\\) True)\nTrue Condition Negative (\\(H_0\\) True)\n\n\n\n\nTest Positive (Reject \\(H_0\\))\nCorrect (True Positive)\nType I Error (False Positive)\n\n\nTest Negative (Accept \\(H_0\\))\nType II Error (False Negative)\nCorrect (True Negative)\n\n\n\nsee more in my statistical inference notes\n\n\n\nLets say we want to see if changeing website design from “X” to “Y” will increase the average time spent on the website.\nWe know:\n\nCurrent average is between 30s to 2 min\nWe want to see a 3% increase in time spent\n\nCalculation:\n\n\\(\\mu_x = 0.5 + \\frac{2-0.5}{2} = 1.25 \\text{ min}\\)\nUsing 95% CI, \\(Z_i = \\frac{Y_i - \\mu_A}{\\sigma} \\sim \\mathcal{N}(0, 1)\\)\n\n\\(1.96 = \\frac{2 - \\mu_A}{\\sigma}\\)\n\\(\\sigma = \\frac{2 - 1.25}{1.96} = 0.383\\)\n\n\\(\\delta = 0.03 \\times 1.25 = 0.0375\\) (desired difference) \n\nUse pwr.t.test function in R to calculate sample size needed\n\nwill give n (sample size) needed for each group\n\n\npwr.t.test(\n  d = 0.0375 / 0.383, # delta / sigma\n  sig.level = 0.05,\n  power = 0.8,\n  type = \"two.sample\",\n  alternative = \"greater\" # we want to see an increase\n)\n\nLower power means need less sample size\n\nHigher response increase means more power for a given sample size\n\n\n\n\n\nWe want to ensure that the blocks are homogeneous:\n\nAs much variation in \\(\\mu\\) as possible is across the blocks\nAs little variation in \\(\\mu\\) as possible is within the blocks\n\nIf not homogenous: blocking can be worse than raw\nThe blocking setup needs to be carefully planned before running the experiment.\n\n\n\n\n\nEarlier was test with mean duration (continouous response) -&gt; use t-test\nWe use z-test for proportion tests\n\nproportion means value between 0 and 1\n\nE.g. see the click through rate (CTR) of a website between control “X” and treatment “Y”\n\nCTR \\(\\in [0, 1]\\)\n\n\n\n\n\nNo need \\(\\sigma\\) nor \\(\\mu\\) for proportion test\nNeed to know the effect size \\(h\\) (difference in proportions)\n\nUse ES.h(data_control, data_treatment) to calculate\n\nEffect size has an arcsine transformation:\n\n\\[h = 2 \\arcsin(\\sqrt{p_Y}) - 2 \\arcsin(\\sqrt{p_X})\\]\n\nThis means:\n\nSmaller transformed effect (y-axis) in the middle\nLarger transformed effect (y-axis) at the ends\n\n\n\n\nThis behaviour is related to the statistic’s standard error of the two-proportion z-test\n\n\\(\\delta = p_Y - p_X\\)\n\\(H_0: \\delta &lt; some\\_value\\), \\(H_a: \\delta \\geq some\\_value\\)\n\n\n\\[\nZ = \\frac{\\hat{\\delta}_{\\text{CTR}} - some\\_value}{\\sqrt{\\frac{\\hat{\\text{CTR}}_A (1 - \\hat{\\text{CTR}}_A)}{n/2} + \\frac{\\hat{\\text{CTR}}_B (1 - \\hat{\\text{CTR}}_B)}{n/2}}} = \\frac{\\hat{\\delta}_{\\text{CTR}} - some\\_value}{\\text{SE}\\left(\\hat{\\delta}_{\\text{CTR}}\n\\right)}\n\\]\n\nMore error in the middle, less error at the ends. Smaller sample size = more error\n\n\n\nCTR_effect_size = ES.h(data_control, data_treatment)\n\npwr.2p.test(\n  h = CTR_effect_size,\n  sig.level = alpha,\n  power = pow,\n  alternative = \"greater\"\n)\n\nWe can see that we need more sample size when:\n\n\\(\\delta\\) is smaller\n\\(CTR_X\\) is closer to 0.5, more error in the middle\n\n\n\n\n\n\nPeeking: looking at the data before the experiment is over/ through the experiment\nStill compute overall sample size to get \\(n_{max}\\)\nIf at some peek, updated test statistic is significant, we can stop the experiment\n\n\n\nAggresive Peeking: look at the data after every new experimental unit\n\nIf test \\(P_Y &gt; P_X\\) (and in fact it is true), aggressive peeking will improve power of the test if:\n\n\\(P_Y &gt; 1.5 * P_X\\)\nOtherwise, it gives a concerning power decrease\n\nIf test \\(P_Y = P_X\\),\n\nthe proportion of replicates where \\(z_{test} &gt; z_{1- \\alpha}\\) correspond to type I error rate\nwill lead to inflating the type I error rate"
  },
  {
    "objectID": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#observational-studies",
    "href": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#observational-studies",
    "title": "Experimentation and Causal Inference",
    "section": "",
    "text": "We do not have control of the variable of interest.\nWithout randomization, life is harder. Strategy includes:\n\nRecording potential confounders\nUse confounders as part of the analysis\nTempering the causal strength in light of inherent challenges in (i) and (ii)\n\n\n\n\n\nResponse \\(Y\\): binary indicator of disease state. (1: disease, 0: no disease)\n\\(X\\): binary indicator of behavior (1: type A, 0: type B)\n\nType A: more agressive personality, competitive, etc\nType B: more laid back personality, relaxed, etc\n\nConfounders \\(C_j\\) (e.g. age, sex, BMI, cholesterol level, etc.)\n\n\n\nSince it is binary =&gt; binary logistic regression\n\nUse log-odds \\(logit(p) = \\log(\\frac{p}{1-p})\\)\nodds-ratio: \\(\\text{OR} = \\frac{\\frac{n_{X = 1, Y = 1}/ n}{n_{X = 1,Y = 0} / n}}{\\frac{n_{X = 0, Y = 1}/ n}{n_{X = 0,Y = 0} / n}} = \\frac{\\frac{n_{X = 1, Y = 1}}{n_{X = 1,Y = 0}}}{\\frac{n_{X = 0, Y = 1}}{n_{X = 0,Y = 0}}} = \\frac{n_{X = 1, Y = 1} \\times n_{X = 0,Y = 0}}{n_{X = 1,Y = 0} \\times n_{X = 0, Y = 1}}\\)\n\nOR = 1: X does not effect Y\nOR &gt; 1: X increases the odds of Y\nOR &lt; 1: X decreases the odds of Y\n\n\\(\\text{SE} = \\sqrt{\\frac{1}{n_{X = 1, Y = 1}} + \\frac{1}{n_{X = 1,Y = 0}} + \\frac{1}{n_{X = 0, Y = 1}} + \\frac{1}{n_{X = 0, Y =0}}}\\)\n\nCan also just use binary logistic regression in R\n\nglm(Y ~ X, data = data, family = binomial) |&gt;\n  tidy(conf.int = 0.95)\n\n\n\nTurn a continous confounder (e.g. age) into discrete categories and add to the model\n\ndata |&gt; mutate(age_bins = cut(age, breaks = c(min(age), quantile(age, (1:3) / 4), max(age)), include.lowest = TRUE)\n\nBy making stratum-specific inference with multiple confounders, we aim to infer causality between X and Y\n\nHowever, there will be few observations in each strata (not enough data)\nSolution: use binomial logistic regression (Overall Model-Based Inference) with interaction terms\n\n\nglm(Y ~ X * C1 * C2, data = data, family = binomial) |&gt;\n  tidy(conf.int = 0.95)\nrecall: odds ratio is \\(exp(\\beta)\\) where \\(\\beta\\) is the coefficient/ estimate\n\n\n\n\nSimple/ smooth structure in how the Y-specific log-OR varies across the strata\n\nCheck using ANOVA comparing the simple model (all additive terms) and the complex model (with interaction terms of all confounders with each other)\ncomplex_model &lt;- glm(Y ~ X + C1 * C2, data = data, family = binomial)\nanova(simple_model, complex_model, test = \"LRT\")\n\nStrength of \\((X, Y)\\) association is constant across the strata (i.e. no interaction between X and C)\n\ncomplex model: all simple terms + double interactions of X and confounders\ncomplex_model_c1 &lt;- glm(Y ~ X + C1 + C2 + X:C1, data = data, family = binomial)\ncomplex_model_c2 &lt;- glm(Y ~ X + C1 + C2 + X:C2, data = data, family = binomial)\nCompare all models with simple model using ANOVA\n\nNo unmeasured confounders (All confounders are included in the model)\n\nAdd new model with unmeasured confounder\nnew_model &lt;- glm(Y ~ X + C1 + C2 + CU, data = data, family = binomial) where CU is the unmeasured confounder"
  },
  {
    "objectID": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#sampling-schemes-in-observational-data",
    "href": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#sampling-schemes-in-observational-data",
    "title": "Experimentation and Causal Inference",
    "section": "",
    "text": "Must Explore how to select our sample before executing the study\nAnalysis must also account for the sampling scheme\n\n\n\n\nGround Truth: checking the results of the study in terms of its estimation accuracy against the real-world\nGround truth is hard to get/ not available in many cases\n\nFrequentist paradigm: we do not have access to true population parameters\nNeed to reply on sample to estimate the population parameters\n\n\n\n\n\nA better simulation technique than Monte Carlo\nCore Idea:\n\nUse a relevant sample dataset: assumes previous sample data is representative of the population\nFit a model with Y ~ X + C_j (where \\(C_j\\) are a determined set of confounders)\nSimulate a proxy ground truth by generating new data from the model\n\nStill use the 3 assumptions for the causal model-based inference\nSteps:\n\nGet a dataset and generate multiple rows of data (break continuous variables into quartile bins)\nFit a model with the data\nSimulate a proxy ground truth by generating new data from the model\n\n\n\n\n\n\n\nThree Sampling Schemes:\n\nCross-Sectional Sampling\nCase-Control Sampling\nCohort Sampling\n\nThese schemes will imply different temporalities\n\n\n\n\nContemporaneous: all data is collected at the same time\n\nGrab a simple random sample of size n from the population\nSimilar to an instantaneous snapshot of all study variables\n\nIdeal in early research stages to get a sense of the data\n\nFast to run\n\n\nset.seed(123) # for reproducibility\n\nCS_sample_index &lt;- sample(1:n, size = n_sample, replace = FALSE)\nCS_data &lt;- data[CS_sample_index, ]\n\n\n\n\nRetrospective: data is collected after the event of interest has occurred\n\nSample into cases Y=1 and controls Y=0 (Equal sample sizes for both)\nThen ask the subjects “have you been exposed to X in the past?”\n\nIdeal where outcome Y=1 is rare\n\nNot have have a lot of cases of Y=1 so recruit a lot of patients with Y=1 for study\n\nIt will oversample \\(Y=1\\) cases and undersample \\(Y=0\\) controls\n\nLeads to a second statistical inquiry: “Is it a winning strategy to oversample cases?”\n\nDo modified Power Analysis using Case:Control ratio\nIf ratio = 1, then control = case\nIf ratio &gt; 1, then case &gt; control\nIf ratio &lt; 1, then case &lt; control\n\nAccording to SE behaviors, in populations when \\(Y=1\\) is rare, get more precise estimates by oversampling cases and undersampling controls\n\n\nset.seed(123) # for reproducibility\n\nCC_sample_index &lt;- c(sample((1:n)[data$Y == 1], size = n_sample/2, replace = FALSE),\n                      sample((1:n)[data$Y == 0], size = n_sample/2, replace = FALSE))\nCC_data &lt;- data[CC_sample_index, ]\n\n\n\n\nProspective: data is collected over time\n\nSample into exposed X=1 and unexposed X=0 (Equal sample sizes for both)\nThen follow the subjects over time to see if they develop the disease Y=1\n\nY is assumed as the recorded outcome at the end of the study\nIdeal for when exposure X=1 is rare\n\nNot have have a lot of cases of X=1 so recruit a lot of patients with X=1 for study\n\n\nset.seed(123) # for reproducibility\n\nCO_sample_index &lt;- c(sample((1:n)[data$X == 1], size = n_sample/2, replace = FALSE),\n                         sample((1:n)[data$X == 0], size = n_sample/2, replace = FALSE))\nCO_data &lt;- data[C0_sample_index, ]\n\n\n\n\n\nNeed to run multiple simulations to get a sense of the variability of the estimates\nUse function sim_study to run the simulation\n\nsim_study &lt;- function(pop_data, n, alpha, log_OR, num_replicates) {\n  res &lt;- list(NULL) # Setting up matrix with metrics\n  res[[1]] &lt;- res[[2]] &lt;- res[[3]] &lt;- matrix(NA, num_replicates, 3)\n\n  suppressMessages(for (lp in 1:num_replicates) { # Otherwise, we get \"Waiting for profiling to be done...\"\n    # Obtaining samples by scheme\n    # CS\n    CS_sampled_subjects &lt;- sample(1:nrow(pop_data), size = n, replace = F)\n    CS_sample &lt;- pop_data[CS_sampled_subjects, ]\n    # CC\n    CC_sampled_subjects &lt;- c(\n      sample((1:nrow(pop_data))[pop_data$chd69 == \"0\"],\n        size = n / 2, replace = F\n      ),\n      sample((1:nrow(pop_data))[pop_data$chd69 == \"1\"],\n        size = n / 2, replace = F\n      )\n    )\n    CC_sample &lt;- pop_data[CC_sampled_subjects, ]\n    # CO\n    CO_sampled_subjects &lt;- c(\n      sample((1:nrow(pop_data))[pop_data$dibpat == \"Type B\"],\n        size = n / 2, replace = F\n      ),\n      sample((1:nrow(pop_data))[pop_data$dibpat == \"Type A\"],\n        size = n / 2, replace = F\n      )\n    )\n    CO_sample &lt;- pop_data[CO_sampled_subjects, ]\n\n    # Do the three analyses\n    # CS\n    CS_bin_log_model &lt;- glm(chd69 ~ dibpat + age_bins + smoke + bmi_bins + chol_bins,\n      family = \"binomial\", data = CS_sample\n    )\n    # CC\n    CC_bin_log_model &lt;- glm(chd69 ~ dibpat + age_bins + smoke + bmi_bins + chol_bins,\n      family = \"binomial\", data = CC_sample\n    )\n    # CO\n    CO_bin_log_model &lt;- glm(chd69 ~ dibpat + age_bins + smoke + bmi_bins + chol_bins,\n      family = \"binomial\", data = CO_sample\n    )\n\n    # and the takeaways\n    res[[1]][lp, ] &lt;- c(coef(CS_bin_log_model)[\"dibpatType A\"], confint(CS_bin_log_model)[\"dibpatType A\", ])\n    res[[2]][lp, ] &lt;- c(coef(CC_bin_log_model)[\"dibpatType A\"], confint(CC_bin_log_model)[\"dibpatType A\", ])\n    res[[3]][lp, ] &lt;- c(coef(CO_bin_log_model)[\"dibpatType A\"], confint(CO_bin_log_model)[\"dibpatType A\", ])\n  })\n\n  # Summaries\n  BIAS &lt;- sapply(\n    res,\n    function(mat) {\n      mean(mat[, 1]) - log_OR\n    }\n  )\n  vrnc &lt;- sapply(res, function(mat) {\n    var(mat[, 1])\n  })\n  CVRG &lt;- sapply(res,\n    function(mat, trg) {\n      mean((mat[, 2] &lt; trg) & (trg &lt; mat[, 3]))\n    },\n    trg = log_OR\n  )\n  PWR &lt;- sapply(res, function(mat) {\n    mean(mat[, 2] &gt; 0)\n  })\n  RMSE &lt;- sqrt(BIAS^2 + vrnc)\n\n  opt &lt;- cbind(BIAS, RMSE, CVRG, PWR)\n  rownames(opt) &lt;- c(\"Cross-Sectional (CS)\", \"Case-Control (CC)\", \"Cohort (CO)\")\n\n  return(opt)\n}\n\nFunction inputs:\n\npop_data: the dataset where we will sample from\nn: sample size\nalpha: significance level \\(\\alpha\\)\nlog_OR: true log odds ratio\nnum_replicates: number of simulations to run\n\nThe function sim_study will return a matrix with the following metrics:\n\nBIAS: difference between the estimated and true log odds ratio\nRMSE: root mean squared error\nCVRG: coverage of the 95% confidence interval (as a proportion of num_replicates)\nPWR: power of the test"
  },
  {
    "objectID": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#matched-case-control-studies",
    "href": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#matched-case-control-studies",
    "title": "Experimentation and Causal Inference",
    "section": "",
    "text": "All our previous sampling schemes used a binary logistic regression model\nRecall we checked that CC is the best compared to the other 2 designs (CS and CO) in terms of power when \\(Y=1\\) is rare\n\n\n\n\nUsing CC is acceptable because:\n\nWe assumed \\(Y|X, C_1, ..., C_p\\)\nCreate artificial population by “cloning subjects”\n\nThis is done using the estimated model from (previous reperesentative sample) + induced random noise\nAKA Proxy Ground Truth\n\n\nCC is useful when \\(Y=1\\) is rare and sampling is costly\n\n\n\n\n\nNeed to have artificial population to be representative of the true population\nThen will sample to get a sample size of \\(n\\)\nRecord the confounders of interest as strata\nSample \\(n/2\\) cases then \\(n/2\\) controls\n\nKeep Case:Control ratio = 1\nMatch exactly on the confounder to the case counterpart\n\ne.g. case has confounder \\(C_1=1\\), then control must have \\(C_1=1\\)\n\n\nImportant: Cannot fit Binary Logistic Regression model since we have matched pairs\n\nCan get a sparse data problem\nUse McNemar’s Test instead\n\nCC-matched will show a smaller average bias compared to CC-unmatched\n\nPower is the same once \\(n\\) increases\n\n\n\n\n\n\nControl \\(X=0\\)\nControl \\(X=1\\)\n\n\n\n\nCase \\(X=0\\)\n\\(n_{00}\\)\n\\(n_{01}\\)\n\n\nCase \\(X=1\\)\n\\(n_{10}\\)\n\\(n_{11}\\)\n\n\n\n\n\\(n_{00}\\) and \\(n_{11}\\) are the concordant pairs\n\\(n_{01}\\) and \\(n_{10}\\) are the discordant pairs\nEstimator of the odds ratio is based on discordant pairs: \\(OR = \\frac{n_{10}}{n_{01}}\\)\n\n\\(OR = 1\\) implies no association\n\\(OR &gt; 1\\) implies positive association\n\\(OR &lt; 1\\) implies negative association\n\n\n\n\n\n\\(H_0: log(OR) = 0\\), \\(H_a: log(OR) \\neq 0\\)\n\\(log(OR) = log{n_{10}} - log{n_{01}}\\)\nIt is approximately normally distributed with an SE of:\n\n\\(SE = \\sqrt{\\frac{1}{n_{10}} + \\frac{1}{n_{01}}}\\)\n\nTest statistic: \\(Z = \\frac{log(OR)}{SE}\\)"
  },
  {
    "objectID": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#ordinal-regressors",
    "href": "block_6/554_exp_caus_inf/554_exp_caus_inf.html#ordinal-regressors",
    "title": "Experimentation and Causal Inference",
    "section": "",
    "text": "The numerical confounding strata we have been using are ordinal\n\n\n\n\n\\(Y\\) is continuous, \\(X\\) is ordinal\n\n\nNeed to convert the variable \\(X\\) to ordinal\n\ndata$X_ord &lt;- ordered(data$X, levels=c(\"low\", \"medium\", \"high\"))\n\nFit a one-way analysis of variance (ANOVA) model\n\n\nR uses polynomial contrasts in ordered type factors\n\nElements of vectors sum to 0\nRoughly, if have k levels, then k-1 polynomials\nE.g. l=4, we will have linear, quadratic, cubic contrasts\nUse contr.poly(l) to get the contrasts when l levels\n\nGives design matrix for the contrasts\nCols sum to 0\nRows are the contrasts and are orthogonal\n\n\n\nOLS &lt;- lm(Y ~ X_ord, data=data)\n\n# Get contrasts\nOLS |&gt; model.matrix()\n\nHypothesis test:\n\n\\(H_0\\): there is no GIVEN trend in the ordered data\n\\(H_1\\): there is a GIVEN trend in the ordered data\nGIVEN will be replaced with linear, quadratic, cubic, etc\n\n\nplot &lt;- eda_plot +\n    geom_smooth(aes(x=unclass(X_ord), color=1),\n        formula=y~x, method=\"lm\", se=FALSE) +\n    geom_smooth(aes(x=unclass(X_ord), color=2),\n        formula=y~poly(x, 2), method=\"lm\", se=FALSE) +\n    geom_smooth(aes(x=unclass(X_ord), color=3),\n        formula=y~poly(x, 3), method=\"lm\", se=FALSE)\n\n\n\nAlternative to make inferential interpretations more straightforward\nWant to answer whether differences exist between ordered levels\n\noptions(contrasts = c(\"contr.treatment\", \"contr.sdif\"))\n\nOLS_sdif &lt;- lm(Y ~ X_ord, data=data) |&gt; tidy()\n\nInterpretation is very straightforward"
  },
  {
    "objectID": "block_1/523_R/523_R.html",
    "href": "block_1/523_R/523_R.html",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "read_csv(url): reads a csv file\n\nread_csv(url, col_types = cols()): reads a csv file with no column types\nread_csv(url, col_types = cols(col_name = col_type)): reads a csv file with column types\nread_csv(url, skip = n, n_max = m): reads a csv file skipping n rows and reading m rows\n\nread_csv2(url): reads a csv file with a comma as decimal separator\nread_tsv(url): reads a tsv file\nread_delim(url, delim = \"\"): reads a file with a delimiter\n\n\n\n\n\nread_excel(file_path, sheet=\"name\"): reads an excel file\n\nto read url need to do download.file(url, destfile = \"file.xlsx\", mode = \"wb\")\n\n\n\n\n\n\nclean_names(df): cleans column names to match them with R conventions (e.g., col_name1)\n\n\n\n\n\nselect(df, col_name1, col_name2): selects cols\nfilter(df, col_name1 == \"value\", col_name2 &gt; 5): filters rows\n\nfilter(df, col_name1 %in% c(\"value1\", \"value2\")): filters if col_name1 is in a vector of values\n\narrange(df, col_name1): sorts rows, default is ascending\n\narrange(df, desc(col_name1)): sorts rows descending\n\nmutate(df, new_col_name = col_name1 + col_name2): creates new cols\nslice(df, 1:10): selects rows\n\nslice(df, 1): selects first row\n\npull(df, col_name1): extracts a column as a vector\n\n\n\n\n\nstr_detect(df$col_name, \"value\"): detects if a string contains a value\nstr_subset(df$col_name, \"value\"): subsets a string if it contains a value\nstr_split(df$col_name, \"value\"): splits a string by a value\n\nstr_split_fixed(df$col_name, \"value\", n): splits a string by a value and returns n columns (gets character matrix)\n\nseparate(df, col_name, into = c(\"new_col_name1\", \"new_col_name2\"), sep = \"value\"): separates a column into two columns\nstr_length(df$col_name): gets length of string\nstr_sub(df$col_name, start = n, end = m): gets substring from n to m\nstr_c(df$col_name1, df$col_name2, sep = \"value\"): concatenates two strings\n\nstr_c(df$col_name1, sep = \"value\", collapse = \"value\"): concatenates vector of string and collapses them into one string\n\nstr_replace(df$col_name, \"value\", \"new_value\"): replaces a value in a string\n\n\n\n\n\nfct_drop(df$col_name): drops unused levels\nfct_infreq(df$col_name): orders levels by frequency\nfct_reorder(df$col_name, df$col_name2): orders levels by another column\nfct_relevel(df$col_name, \"value\"): moves a level to the front\nfct_rev(df$col_name): reverses order of levels\n\n\n\n\n\npivot_longer(df, cols = c(col_name1, col_name2), names_to = \"new_col_name\", values_to = \"new_col_name\"): pivots cols to rows\npivot_wider(df, names_from = \"col_name1\", values_from = \"col_name2\"): pivots rows to cols\n\n\n\n\nCriteria:\n\nEach row is a single observation\nEach variable is a single column\nEach value is a single cell\n\n\n\n\n\nwe use &lt;- to assign values to variables.\nThis is because when we do median(x &lt;- c(1, 2, 3)) x is assigned to c(1, 2, 3) globally. \n\n\n\nobjects that contain 1 or more elements of the same type\nelements are ordered\nHeirarchy for coercion: character &gt; double &gt; integer &gt; logical\nto change type of vector use as.character(), as.double(), as.integer(), as.logical()\nto check if vector is of a certain type use is.character(), is.double(), is.integer(), is.logical()\nto check length of vector use length()\nto check type of vector use typeof()\n\nCan get vector from df using: df$col_name\n\n\n\nname &lt;- c(\"a\", \"b\", \"c\")\nname[1] # \"a\"\nname[2:3] # \"b\" \"c\"\nname[-1] # \"b\" \"c\"\nname[length(name)] # \"c\"]\n\n# Also...\nx &lt;- c(1, 2, 3)\ny &lt;- x\n\ny[3] &lt;- 4\ny\n#&gt; [1] 1 2 4\n\n\n\n\n\nTibles inherit from data frames but are more strict. They are more consistent and have better printing.\nImportant properties:\n\nTibbles only output first 10 rows and all columns that fit on screen\nwhen you subset a tibble you always get a tibble, in data frames you get a vector\n\n\n\n\nuses lubridate package\n\ntoday(): gets today’s date, class is Date\nnow(): gets today’s date and time, class is POSIXct\nymd(), ydm(), mdy(), myd(), dmy(), dym(): converts character to date\nymd_hms(): converts character to date and time\nCan mutate date: dates |&gt; mutate = make_date(year, month, day)\nwdays(): gets day of week\n\nwdays(date, label = TRUE): gets abbreviated day of week (e.g., Mon)\nwday(date, label = TRUE, abbr = FALSE): gets day of week as full name (e.g., Monday)\n\nmdays(): gets day of month\nydays(): gets day of year\n\n\n\n\n\nbind_rows(df1, df2): binds rows of two dfs\nbind_cols(df1, df2): binds cols of two dfs\ninner_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, only keeps rows that match\nleft_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps all rows from df1\nsemi_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps only column names from df1, only keeps rows that match\nanti_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps only column names from df1, keeps only rows that don’t match\nfull_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps all rows from both dfs\n\n\n\n\n\n\n\ncase_when(): selectively modify column values based on conditions\n\ngapminder |&gt;\n    mutate(country = case_when(country == \"Cambodia\" ~ \"Kingdom of Cambodia\",\n    # only work if country is character (not factor)\n                            TRUE ~ country))\n\n# For multiple values\ngapminder |&gt;\n    mutate(continent = case_when(continent == \"Asia\" ~ \"Asie\",\n                                 continent == \"Europe\" ~ \"L'Europe\",\n                                 continent == \"Africa\" ~ \"Afrique\",\n                                 TRUE ~ continent)) #This is to keep the original value (not NA)\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ndrop_na()\nRemove rows based on NA in cols x to y\ndf %&gt;% drop_na(x:y)\n\n\n\nRemove rows if any column has NA\ndf %&gt;% drop_na()\n\n\n\n\n\n\n\n\nsummarise() or summarize(): returns a single value for each group\ngroup_by(): groups rows by a column\n\n# calculate the average life expectancy for the entire dataset\ngapminder |&gt;\n    summarise(mean_life_exp = mean(lifeExp))\n\n# calculate the average life expectancy for each continent and year\ngapminder |&gt;\n    group_by(continent, year) |&gt;\n    summarise(mean_life_exp = mean(lifeExp, na.rm = TRUE))\n    # na.rm = TRUE removes NAs from calculation\n\n# does not collapse the data frame, compute with group\ngapminder %&gt;%\n    group_by(country) %&gt;%\n    mutate(life_exp_gain = lifeExp - first(lifeExp)) %&gt;%\n    # first() returns the first value of a vector\n    head()\n\n\n\n\n\nmap(df, mean, na.rm = TRUE): retuirns a list\n\nna.rm = TRUE removes NAs from calculation\n\nmap_dfc(df, median): returns a tibble\nmap_dbl(df, max): returns a double vector\n\nCan use anonymous functions with map():\n# Long form\nmap_*(data, function(arg) function_being_called(arg, other_arg))\n# e.g.\nmap_df(data_entry, function(vect) str_replace(vect, pattern = \"Cdn\", replacement = \"Canadian\"))\n\n# short form\nmap_*(data, ~ function_being_called(., other_arg))\n# e.g.\nmap_df(data_entry, ~str_replace(., pattern = \"Cdn\", replacement = \"Canadian\"))\n\n\n\n\nHas roxygen comments, same as python docstrings\n\n#' Calculates the variance of a vector of numbers.\n#'\n#' Calculates the sample variance of data generated from a normal/Gaussian distribution,\n#' omitting NA's in the data.\n#'\n#' @param data numeric vector of numbers whose length is &gt; 1.\n#'\n#' @return numeric vector of length one, the variance.\n#'\n#' @examples\n#' variance(c(1, 2, 3))\nvariance &lt;- function(observations) {\n  if (!is.numeric(observations)) {\n    # Throws an error\n    stop(\"All inputs must be numeric.\")\n  }\n  sum((observations - mean(observations)) ^ 2) / (length(observations) - 1)\n}\n\nName Masking: if a variable is defined in the function, it will be used instead of the global variable\n\nif not in function, looks one level up, until it reaches the global environment\n\nR looks for values when the function is run, not when it is defined\nEach run is independent of the other\nLazy Evaluation: R only evaluates the arguments that are needed\n\nforce() forces R to evaluate an argument\n\n\n\n\n\ntest_that(\"Message to print if test fails\", expect_*(...))\n\ntest_that('variance expects a numeric vector', {\n    expect_error(variance(list(1, 2, 3)))\n    expect_error(variance(data.frame(1, 2, 3)))\n    expect_error(variance(c(\"one\", \"two\", \"three\")))\n})\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nexpect_identical\nTest two objects for being exactly equal\n\n\nexpect_equal\nCompare R objects x and y testing ‘near equality’ (can set a tolerance)\n\n\n\n- expect_equal(x, y, tolerance = 0.00001)\n\n\nexpect_equivalent\nCompare R objects x and y testing ‘near equality’ (can set a tolerance) and does not assess attributes\n\n\nexpect_error\nTests if an expression throws an error\n\n\nexpect_warning\nTests whether an expression outputs a warning\n\n\nexpect_output\nTests that print output matches a specified value\n\n\nexpect_true\nTests if the object returns TRUE\n\n\nexpect_false\nTests if the object returns FALSE\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nsource(\"path/to/script.R\")\nTake functions from another script\n\n\nlibrary(package_name)\nImport a package\n\n\n\n\n\n(function(x) x + 1)(1) (see purrr package for examples)\n\n\n\n\n# create a nested data frame\nby_country &lt;- gapminder %&gt;%\n    group_by(continent, country) %&gt;%\n    nest() # turns all other columns into a column called data (list of data frames)\nCommon workflow:\n\ngroup_by() + nest() to create a nested data frame\nmutate() + map() to add new columns\nunnest() to return to a regular data frame\n\nweather |&gt;\n# step 1\n  group_by(origin, month) |&gt;\n  nest() |&gt;\n# step 2\n  mutate(min_temp = map_dbl(data, ~min(.$temp, na.rm = T)),\n         max_temp = map_dbl(data, ~max(.$temp, na.rm = T)),\n         avg_temp = map_dbl(data, ~mean(.$temp, na.rm = T)))\n# step 3\nunnest(avg_temp) # only unnest if we get some intermediate list-columns from map\nAlternative to above code:\nweather_nested_2 &lt;- weather |&gt;\n  group_by(origin, month) |&gt;\n  summarise(min_temp = min(temp, na.rm = T),\n            max_temp = max(temp, na.rm = T),\n            avg_temp = mean(temp, na.rm = T))\n\n\n\nmetaprogramming: writing code that writes code\nWith tidyverse, they have a feautre called “non-standard evaluation” (NSE). Part of this is data masking.\n\nData Masking: data frame is promised to be first argument (data mask)\n\ncolumns act as if they are variables, filter(gapminder, country == \"Canada\", year == 1952)\nchecks dataframe first before global environment\n\nDelay in Evaluation: expressions are captured and evaluated later\nenquo(): quotes the argument\nsym(): turns column name into a function as a string\n!!: unquotes the argument\n{{ arg_name }}: unquotes and quotes the argument\n:=: Walrus operator - needed when assigning values\n\n# e.g.\nfilter_gap &lt;- function(col, val) {\n    col &lt;- enquo(col)\n    filter(gapminder, !!col == val)\n}\n\n# better way\nfilter_gap &lt;- function(col, val) {\n    filter(gapminder, {{col}} == val)\n}\n\nfilter_gap(country, \"Canada\")\n# e.g. of walrus operator\nfunction(data, group, col, fun) {\n    data %&gt;%\n        group_by({{ group }}) %&gt;%\n        summarise( {{ col }} := fun({{ col }}))\n}\n\n\n\nif passing varibales to tidyverse functions, use ...\n\nwhen variable not used in logical comparisons or variable assignment\n\nshould be last argument in function\ncan add multiple arguments\n\n\n\nsort_gap &lt;- function(x, ...) {\n    print(x + 1)\n    arrange(gapminder, ...)\n}\n\nsort_gap(1, year, continent, country)"
  },
  {
    "objectID": "block_1/523_R/523_R.html#r-packages",
    "href": "block_1/523_R/523_R.html#r-packages",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "read_csv(url): reads a csv file\n\nread_csv(url, col_types = cols()): reads a csv file with no column types\nread_csv(url, col_types = cols(col_name = col_type)): reads a csv file with column types\nread_csv(url, skip = n, n_max = m): reads a csv file skipping n rows and reading m rows\n\nread_csv2(url): reads a csv file with a comma as decimal separator\nread_tsv(url): reads a tsv file\nread_delim(url, delim = \"\"): reads a file with a delimiter\n\n\n\n\n\nread_excel(file_path, sheet=\"name\"): reads an excel file\n\nto read url need to do download.file(url, destfile = \"file.xlsx\", mode = \"wb\")\n\n\n\n\n\n\nclean_names(df): cleans column names to match them with R conventions (e.g., col_name1)\n\n\n\n\n\nselect(df, col_name1, col_name2): selects cols\nfilter(df, col_name1 == \"value\", col_name2 &gt; 5): filters rows\n\nfilter(df, col_name1 %in% c(\"value1\", \"value2\")): filters if col_name1 is in a vector of values\n\narrange(df, col_name1): sorts rows, default is ascending\n\narrange(df, desc(col_name1)): sorts rows descending\n\nmutate(df, new_col_name = col_name1 + col_name2): creates new cols\nslice(df, 1:10): selects rows\n\nslice(df, 1): selects first row\n\npull(df, col_name1): extracts a column as a vector\n\n\n\n\n\nstr_detect(df$col_name, \"value\"): detects if a string contains a value\nstr_subset(df$col_name, \"value\"): subsets a string if it contains a value\nstr_split(df$col_name, \"value\"): splits a string by a value\n\nstr_split_fixed(df$col_name, \"value\", n): splits a string by a value and returns n columns (gets character matrix)\n\nseparate(df, col_name, into = c(\"new_col_name1\", \"new_col_name2\"), sep = \"value\"): separates a column into two columns\nstr_length(df$col_name): gets length of string\nstr_sub(df$col_name, start = n, end = m): gets substring from n to m\nstr_c(df$col_name1, df$col_name2, sep = \"value\"): concatenates two strings\n\nstr_c(df$col_name1, sep = \"value\", collapse = \"value\"): concatenates vector of string and collapses them into one string\n\nstr_replace(df$col_name, \"value\", \"new_value\"): replaces a value in a string\n\n\n\n\n\nfct_drop(df$col_name): drops unused levels\nfct_infreq(df$col_name): orders levels by frequency\nfct_reorder(df$col_name, df$col_name2): orders levels by another column\nfct_relevel(df$col_name, \"value\"): moves a level to the front\nfct_rev(df$col_name): reverses order of levels\n\n\n\n\n\npivot_longer(df, cols = c(col_name1, col_name2), names_to = \"new_col_name\", values_to = \"new_col_name\"): pivots cols to rows\npivot_wider(df, names_from = \"col_name1\", values_from = \"col_name2\"): pivots rows to cols\n\n\n\n\nCriteria:\n\nEach row is a single observation\nEach variable is a single column\nEach value is a single cell"
  },
  {
    "objectID": "block_1/523_R/523_R.html#assignment-environment",
    "href": "block_1/523_R/523_R.html#assignment-environment",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "we use &lt;- to assign values to variables.\nThis is because when we do median(x &lt;- c(1, 2, 3)) x is assigned to c(1, 2, 3) globally. \n\n\n\nobjects that contain 1 or more elements of the same type\nelements are ordered\nHeirarchy for coercion: character &gt; double &gt; integer &gt; logical\nto change type of vector use as.character(), as.double(), as.integer(), as.logical()\nto check if vector is of a certain type use is.character(), is.double(), is.integer(), is.logical()\nto check length of vector use length()\nto check type of vector use typeof()\n\nCan get vector from df using: df$col_name\n\n\n\nname &lt;- c(\"a\", \"b\", \"c\")\nname[1] # \"a\"\nname[2:3] # \"b\" \"c\"\nname[-1] # \"b\" \"c\"\nname[length(name)] # \"c\"]\n\n# Also...\nx &lt;- c(1, 2, 3)\ny &lt;- x\n\ny[3] &lt;- 4\ny\n#&gt; [1] 1 2 4"
  },
  {
    "objectID": "block_1/523_R/523_R.html#tibbles-vs-data-frames",
    "href": "block_1/523_R/523_R.html#tibbles-vs-data-frames",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "Tibles inherit from data frames but are more strict. They are more consistent and have better printing.\nImportant properties:\n\nTibbles only output first 10 rows and all columns that fit on screen\nwhen you subset a tibble you always get a tibble, in data frames you get a vector"
  },
  {
    "objectID": "block_1/523_R/523_R.html#dates-and-times",
    "href": "block_1/523_R/523_R.html#dates-and-times",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "uses lubridate package\n\ntoday(): gets today’s date, class is Date\nnow(): gets today’s date and time, class is POSIXct\nymd(), ydm(), mdy(), myd(), dmy(), dym(): converts character to date\nymd_hms(): converts character to date and time\nCan mutate date: dates |&gt; mutate = make_date(year, month, day)\nwdays(): gets day of week\n\nwdays(date, label = TRUE): gets abbreviated day of week (e.g., Mon)\nwday(date, label = TRUE, abbr = FALSE): gets day of week as full name (e.g., Monday)\n\nmdays(): gets day of month\nydays(): gets day of year"
  },
  {
    "objectID": "block_1/523_R/523_R.html#joining-data",
    "href": "block_1/523_R/523_R.html#joining-data",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "bind_rows(df1, df2): binds rows of two dfs\nbind_cols(df1, df2): binds cols of two dfs\ninner_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, only keeps rows that match\nleft_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps all rows from df1\nsemi_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps only column names from df1, only keeps rows that match\nanti_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps only column names from df1, keeps only rows that don’t match\nfull_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps all rows from both dfs"
  },
  {
    "objectID": "block_1/523_R/523_R.html#change-or-remove-specific-values",
    "href": "block_1/523_R/523_R.html#change-or-remove-specific-values",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "case_when(): selectively modify column values based on conditions\n\ngapminder |&gt;\n    mutate(country = case_when(country == \"Cambodia\" ~ \"Kingdom of Cambodia\",\n    # only work if country is character (not factor)\n                            TRUE ~ country))\n\n# For multiple values\ngapminder |&gt;\n    mutate(continent = case_when(continent == \"Asia\" ~ \"Asie\",\n                                 continent == \"Europe\" ~ \"L'Europe\",\n                                 continent == \"Africa\" ~ \"Afrique\",\n                                 TRUE ~ continent)) #This is to keep the original value (not NA)\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ndrop_na()\nRemove rows based on NA in cols x to y\ndf %&gt;% drop_na(x:y)\n\n\n\nRemove rows if any column has NA\ndf %&gt;% drop_na()"
  },
  {
    "objectID": "block_1/523_R/523_R.html#iterate-over-groups-of-rows",
    "href": "block_1/523_R/523_R.html#iterate-over-groups-of-rows",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "summarise() or summarize(): returns a single value for each group\ngroup_by(): groups rows by a column\n\n# calculate the average life expectancy for the entire dataset\ngapminder |&gt;\n    summarise(mean_life_exp = mean(lifeExp))\n\n# calculate the average life expectancy for each continent and year\ngapminder |&gt;\n    group_by(continent, year) |&gt;\n    summarise(mean_life_exp = mean(lifeExp, na.rm = TRUE))\n    # na.rm = TRUE removes NAs from calculation\n\n# does not collapse the data frame, compute with group\ngapminder %&gt;%\n    group_by(country) %&gt;%\n    mutate(life_exp_gain = lifeExp - first(lifeExp)) %&gt;%\n    # first() returns the first value of a vector\n    head()"
  },
  {
    "objectID": "block_1/523_R/523_R.html#purrr-package",
    "href": "block_1/523_R/523_R.html#purrr-package",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "map(df, mean, na.rm = TRUE): retuirns a list\n\nna.rm = TRUE removes NAs from calculation\n\nmap_dfc(df, median): returns a tibble\nmap_dbl(df, max): returns a double vector\n\nCan use anonymous functions with map():\n# Long form\nmap_*(data, function(arg) function_being_called(arg, other_arg))\n# e.g.\nmap_df(data_entry, function(vect) str_replace(vect, pattern = \"Cdn\", replacement = \"Canadian\"))\n\n# short form\nmap_*(data, ~ function_being_called(., other_arg))\n# e.g.\nmap_df(data_entry, ~str_replace(., pattern = \"Cdn\", replacement = \"Canadian\"))"
  },
  {
    "objectID": "block_1/523_R/523_R.html#functions",
    "href": "block_1/523_R/523_R.html#functions",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "Has roxygen comments, same as python docstrings\n\n#' Calculates the variance of a vector of numbers.\n#'\n#' Calculates the sample variance of data generated from a normal/Gaussian distribution,\n#' omitting NA's in the data.\n#'\n#' @param data numeric vector of numbers whose length is &gt; 1.\n#'\n#' @return numeric vector of length one, the variance.\n#'\n#' @examples\n#' variance(c(1, 2, 3))\nvariance &lt;- function(observations) {\n  if (!is.numeric(observations)) {\n    # Throws an error\n    stop(\"All inputs must be numeric.\")\n  }\n  sum((observations - mean(observations)) ^ 2) / (length(observations) - 1)\n}\n\nName Masking: if a variable is defined in the function, it will be used instead of the global variable\n\nif not in function, looks one level up, until it reaches the global environment\n\nR looks for values when the function is run, not when it is defined\nEach run is independent of the other\nLazy Evaluation: R only evaluates the arguments that are needed\n\nforce() forces R to evaluate an argument\n\n\n\n\n\ntest_that(\"Message to print if test fails\", expect_*(...))\n\ntest_that('variance expects a numeric vector', {\n    expect_error(variance(list(1, 2, 3)))\n    expect_error(variance(data.frame(1, 2, 3)))\n    expect_error(variance(c(\"one\", \"two\", \"three\")))\n})\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nexpect_identical\nTest two objects for being exactly equal\n\n\nexpect_equal\nCompare R objects x and y testing ‘near equality’ (can set a tolerance)\n\n\n\n- expect_equal(x, y, tolerance = 0.00001)\n\n\nexpect_equivalent\nCompare R objects x and y testing ‘near equality’ (can set a tolerance) and does not assess attributes\n\n\nexpect_error\nTests if an expression throws an error\n\n\nexpect_warning\nTests whether an expression outputs a warning\n\n\nexpect_output\nTests that print output matches a specified value\n\n\nexpect_true\nTests if the object returns TRUE\n\n\nexpect_false\nTests if the object returns FALSE"
  },
  {
    "objectID": "block_1/523_R/523_R.html#importing-packages-or-scripts",
    "href": "block_1/523_R/523_R.html#importing-packages-or-scripts",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "Command\nDescription\n\n\n\n\nsource(\"path/to/script.R\")\nTake functions from another script\n\n\nlibrary(package_name)\nImport a package\n\n\n\n\n\n(function(x) x + 1)(1) (see purrr package for examples)"
  },
  {
    "objectID": "block_1/523_R/523_R.html#nested-data-frames",
    "href": "block_1/523_R/523_R.html#nested-data-frames",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "# create a nested data frame\nby_country &lt;- gapminder %&gt;%\n    group_by(continent, country) %&gt;%\n    nest() # turns all other columns into a column called data (list of data frames)\nCommon workflow:\n\ngroup_by() + nest() to create a nested data frame\nmutate() + map() to add new columns\nunnest() to return to a regular data frame\n\nweather |&gt;\n# step 1\n  group_by(origin, month) |&gt;\n  nest() |&gt;\n# step 2\n  mutate(min_temp = map_dbl(data, ~min(.$temp, na.rm = T)),\n         max_temp = map_dbl(data, ~max(.$temp, na.rm = T)),\n         avg_temp = map_dbl(data, ~mean(.$temp, na.rm = T)))\n# step 3\nunnest(avg_temp) # only unnest if we get some intermediate list-columns from map\nAlternative to above code:\nweather_nested_2 &lt;- weather |&gt;\n  group_by(origin, month) |&gt;\n  summarise(min_temp = min(temp, na.rm = T),\n            max_temp = max(temp, na.rm = T),\n            avg_temp = mean(temp, na.rm = T))"
  },
  {
    "objectID": "block_1/523_R/523_R.html#tidy-evaluation",
    "href": "block_1/523_R/523_R.html#tidy-evaluation",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "metaprogramming: writing code that writes code\nWith tidyverse, they have a feautre called “non-standard evaluation” (NSE). Part of this is data masking.\n\nData Masking: data frame is promised to be first argument (data mask)\n\ncolumns act as if they are variables, filter(gapminder, country == \"Canada\", year == 1952)\nchecks dataframe first before global environment\n\nDelay in Evaluation: expressions are captured and evaluated later\nenquo(): quotes the argument\nsym(): turns column name into a function as a string\n!!: unquotes the argument\n{{ arg_name }}: unquotes and quotes the argument\n:=: Walrus operator - needed when assigning values\n\n# e.g.\nfilter_gap &lt;- function(col, val) {\n    col &lt;- enquo(col)\n    filter(gapminder, !!col == val)\n}\n\n# better way\nfilter_gap &lt;- function(col, val) {\n    filter(gapminder, {{col}} == val)\n}\n\nfilter_gap(country, \"Canada\")\n# e.g. of walrus operator\nfunction(data, group, col, fun) {\n    data %&gt;%\n        group_by({{ group }}) %&gt;%\n        summarise( {{ col }} := fun({{ col }}))\n}\n\n\n\nif passing varibales to tidyverse functions, use ...\n\nwhen variable not used in logical comparisons or variable assignment\n\nshould be last argument in function\ncan add multiple arguments\n\n\n\nsort_gap &lt;- function(x, ...) {\n    print(x + 1)\n    arrange(gapminder, ...)\n}\n\nsort_gap(1, year, continent, country)"
  },
  {
    "objectID": "block_1/551_stats-and-prob/551_stats.html",
    "href": "block_1/551_stats-and-prob/551_stats.html",
    "title": "Statistics and Probability Cheat Sheet",
    "section": "",
    "text": "Statistics and Probability Cheat Sheet\n\n&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;Nando’s MDS Notes&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;Nando’s MDS Notes&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Home\"&gt;Home&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/index.html\"&gt;/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:List of Notes\"&gt;List of Notes&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/list.html\"&gt;/list.html&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;Nando’s MDS Notes&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/section&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  }
]