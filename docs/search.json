[
  {
    "objectID": "block_1/511_python/511_python.html",
    "href": "block_1/511_python/511_python.html",
    "title": "Python Basics",
    "section": "",
    "text": "int, float, str, bool, list, tuple, dict, set, None\n\n\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nlen(text)\nLength of text\n\n\ntext.upper()\nUppercase\n\n\ntext.lower()\nLowercase\n\n\ntext.capitalize()\nCapitalize first letter\n\n\ntext.title()\nCapitalize first letter of each word\n\n\ntext.strip()\nRemove leading and trailing whitespace\n\n\ntext.split(' ')\nSplit string into list of words, using ’ ’ as delimiter. Default delimiter is ’ ’.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nlen(list)\nLength of list\n\n\nlist.append(item)\nAppend item to end of list\n\n\nlist.insert(index, item)\nInsert item at index\n\n\nlist.pop(n)\nRemove and return item at index n. Default n is -1\n\n\nsorted(list)\nReturns a new sorted list without modifying original list\n\n\nlist.sort()\nSort list in ascending order. To sort in descending order, use list.sort(reverse=True). (edit original list)\n\n\nlist.reverse()\nReverse list in place (edit original list)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nCreate dictionary\ndict = {'key1': 'value1', 'key2': 'value2'} or dict = dict(key1='value1', key2='value2')\n\n\nCreate empty dictionary\ndict = {} or dict = dict()\n\n\ndict['key']\nGet value of key\n\n\ndict.get('key', default)\nGet value of key, if key does not exist, return default value\n\n\ndict.pop('key')\nRemove and return value of key\n\n\ndict.keys()\nReturn list of keys\n\n\ndict.values()\nReturn list of values\n\n\ndict.items()\nReturn list of tuples (key, value)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nList\nTuple\nDictionary (dict)\nSet\n\n\n\n\nMutable\nYes\nNo\nYes\nYes\n\n\nOrdered\nYes\nYes\nNo\nNo\n\n\nIndexing\nBy index (0-based)\nBy index (0-based)\nBy key\nNo\n\n\nDuplicates\nAllowed\nAllowed\nKeys must be unique\nDuplicates not allowed\n\n\nModification\nCan change elements\nCannot change elements\nValues can be updated\nElements can be added/removed\n\n\nSyntax\nSquare brackets []\nParentheses ()\nCurly braces {}\nCurly braces {}\n\n\nUse Case\nWhen order matters\nWhen data should not change\nMapping keys to values\nFor unique values and set operations\n\n\nExample\n[1, 2, 'three']\n(1, 2, 'three')\n{'name': 'Alice', 'age': 30}\n{'apple', 'banana', 'cherry'}\n\n\n\n\n\n\n\n    # syntax: [expression for item in list]\n    # syntax: [expression for item in list if condition]\n    # syntax: [expression if condition else other_expression for item in list]\n    words = [\"the\", \"list\", \"of\", \"words\"]\n\n    x = [len(word) for word in words] # [3, 4, 2, 5]\n    x = [word for word in words if len(word) &gt; 2] # [\"the\", \"list\", \"words\"]\n    x = [word if len(word) &gt; 2 else \"short\" for word in words] # [\"the\", \"short\", \"short\", \"words\"]\n\n\n\n    # syntax: value_if_true if condition else value_if_false\n    words = [\"the\", \"list\", \"of\", \"words\"]\n\n    x = \"long list\" if len(words) &gt; 10 else \"short list\"\n\n\n\n    try:\n        # code that might raise an exception\n    except:\n        # code to handle exception\n        raise TypeError(\"Error message\") # raise exception\n\n\n\n    def function_name(arg1, arg2, arg3=default_value): # default values are optional\n        # code\n        return value\n\n    def function_name(*args): # takes in multiple arguments\n        for arg in args:\n            # code\n        return value\n\nside effects: If a function does anything other than returning a value, it is said to have side effects. An example of this is when a function changes the variables passed into it, or when a function prints something to the screen.\n\n\n\n    # Syntax: lambda arg1, arg2: expression\n    lambda x: x+1\n\n\n\n    # NumPy/SciPy style\n    def repeat_string(s, n=2):\n        \"\"\"\n        Repeat the string s, n times.\n\n        Parameters\n        ----------\n        s : str\n            the string\n        n : int, optional\n            the number of times, by default = 2\n\n        Returns\n        -------\n        str\n            the repeated string\n\n        Examples\n        --------\n        &gt;&gt;&gt; repeat_string(\"Blah\", 3)\n        \"BlahBlahBlah\"\n        \"\"\"\n        return s * n\n\n\n\n    def repeat_string(s: str, n: int = 2) -&gt; str:\n        return s * n\n\n\n\n\n    class ClassName:\n        def __init__(self, arg1, arg2):\n            # code\n        def method_name(self, arg1, arg2):\n            # code\n        @classmethod\n        def other_method_name(cls, arg1, arg2):\n            # classmethod is used to create factory methods, aka other ways to create objects\n            # code\n\n    # Inheritance\n    class SubClassName(ClassName):\n        def __init__(self, arg1, arg2):\n            super().__init__(arg1, arg2)\n            # code\n        def method2_name(self, arg1, arg2):\n            # code\n\n\n    class ClassName:\n        static_var = 0\n\n        @staticmethod\n        def method_name(arg1, arg2):\n            # code\n            ClassName.static_var += 1\n            return ClassName.static_var\n        @staticmethod\n        def reset_static_var():\n            ClassName.static_var = 0\n\n\n\n    class TestVector(unittest.TestCase):\n        def test_str1(self):\n            v1 = Vector(1, 2, 3)\n            self.assertIn(\"Vector = [1, 2, 3]\", v1.__str__())\n            self.assertEqual(len(v1.elements), 3)\n\n        def test_str2(self):\n            v1 = Vector(500)\n            self.assertIn(\"Vector = [500]\", v1.__str__())\n\n\n    TestVector = unittest.main(argv=[\"\"], verbosity=0, exit=False)\n    assert TestVector.result.wasSuccessful()\n\n\n\n\n\n\n\n\n\n\n\n\nCategory\nRule/Convention\nExample\n\n\n\n\nIndentation\nUse 4 spaces per indentation level\nif x: # four spaces here\n\n\n\nUse spaces around operators and after commas\na = b + c, d = e + f\n\n\nMaximum Line Length\nLimit all lines to a maximum of 79 characters for code, 72 for comments and docstrings\n\n\n\nImports\nAlways put imports at the top of the file\nimport os\n\n\n\nGroup imports: standard, third-party, local\nimport os import numpy as np from . import my_module\n\n\n\nUse absolute imports\nfrom my_pkg import module\n\n\nWhitespace\nAvoid extraneous whitespace\nspam(ham[1], {eggs: 2})\n\n\n\nUse blank lines to separate functions, classes, blocks of code inside functions\n\n\n\nComments\nComments should be complete sentences\n# This is a complete sentence.\n\n\n\nUse inline comments sparingly\nx = x + 1 # Increment x\n\n\nNaming Conventions\nFunction names should be lowercase with underscores\nmy_function()\n\n\n\nClass names should use CapWords convention\nMyClass\n\n\n\nConstants should be in all capital letters\nCONSTANT_NAME\n\n\nString Quotes\nUse double quotes for docstrings and single quotes for everything else when you start a project (or adhere to the project’s conventions)\n‘string’, “““docstring”“”\n\n\nExpressions and Statements\nDon’t use semicolons to terminate statements\nx = 1 (not x = 1;)\n\n\n\nUse is for identity comparisons and == for value comparisons\nif x is None\n\n\nOther Recommendations\nUse built-in types like list, dict instead of List, Dict from the typing module for simple use-cases\ndef func(a: list) -&gt; None:"
  },
  {
    "objectID": "block_1/511_python/511_python.html#data-types",
    "href": "block_1/511_python/511_python.html#data-types",
    "title": "Python Basics",
    "section": "",
    "text": "int, float, str, bool, list, tuple, dict, set, None\n\n\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nlen(text)\nLength of text\n\n\ntext.upper()\nUppercase\n\n\ntext.lower()\nLowercase\n\n\ntext.capitalize()\nCapitalize first letter\n\n\ntext.title()\nCapitalize first letter of each word\n\n\ntext.strip()\nRemove leading and trailing whitespace\n\n\ntext.split(' ')\nSplit string into list of words, using ’ ’ as delimiter. Default delimiter is ’ ’.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nlen(list)\nLength of list\n\n\nlist.append(item)\nAppend item to end of list\n\n\nlist.insert(index, item)\nInsert item at index\n\n\nlist.pop(n)\nRemove and return item at index n. Default n is -1\n\n\nsorted(list)\nReturns a new sorted list without modifying original list\n\n\nlist.sort()\nSort list in ascending order. To sort in descending order, use list.sort(reverse=True). (edit original list)\n\n\nlist.reverse()\nReverse list in place (edit original list)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nCreate dictionary\ndict = {'key1': 'value1', 'key2': 'value2'} or dict = dict(key1='value1', key2='value2')\n\n\nCreate empty dictionary\ndict = {} or dict = dict()\n\n\ndict['key']\nGet value of key\n\n\ndict.get('key', default)\nGet value of key, if key does not exist, return default value\n\n\ndict.pop('key')\nRemove and return value of key\n\n\ndict.keys()\nReturn list of keys\n\n\ndict.values()\nReturn list of values\n\n\ndict.items()\nReturn list of tuples (key, value)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nList\nTuple\nDictionary (dict)\nSet\n\n\n\n\nMutable\nYes\nNo\nYes\nYes\n\n\nOrdered\nYes\nYes\nNo\nNo\n\n\nIndexing\nBy index (0-based)\nBy index (0-based)\nBy key\nNo\n\n\nDuplicates\nAllowed\nAllowed\nKeys must be unique\nDuplicates not allowed\n\n\nModification\nCan change elements\nCannot change elements\nValues can be updated\nElements can be added/removed\n\n\nSyntax\nSquare brackets []\nParentheses ()\nCurly braces {}\nCurly braces {}\n\n\nUse Case\nWhen order matters\nWhen data should not change\nMapping keys to values\nFor unique values and set operations\n\n\nExample\n[1, 2, 'three']\n(1, 2, 'three')\n{'name': 'Alice', 'age': 30}\n{'apple', 'banana', 'cherry'}"
  },
  {
    "objectID": "block_1/511_python/511_python.html#inline-for-loop",
    "href": "block_1/511_python/511_python.html#inline-for-loop",
    "title": "Python Basics",
    "section": "",
    "text": "# syntax: [expression for item in list]\n    # syntax: [expression for item in list if condition]\n    # syntax: [expression if condition else other_expression for item in list]\n    words = [\"the\", \"list\", \"of\", \"words\"]\n\n    x = [len(word) for word in words] # [3, 4, 2, 5]\n    x = [word for word in words if len(word) &gt; 2] # [\"the\", \"list\", \"words\"]\n    x = [word if len(word) &gt; 2 else \"short\" for word in words] # [\"the\", \"short\", \"short\", \"words\"]"
  },
  {
    "objectID": "block_1/511_python/511_python.html#inline-if-else",
    "href": "block_1/511_python/511_python.html#inline-if-else",
    "title": "Python Basics",
    "section": "",
    "text": "# syntax: value_if_true if condition else value_if_false\n    words = [\"the\", \"list\", \"of\", \"words\"]\n\n    x = \"long list\" if len(words) &gt; 10 else \"short list\""
  },
  {
    "objectID": "block_1/511_python/511_python.html#try-except",
    "href": "block_1/511_python/511_python.html#try-except",
    "title": "Python Basics",
    "section": "",
    "text": "try:\n        # code that might raise an exception\n    except:\n        # code to handle exception\n        raise TypeError(\"Error message\") # raise exception"
  },
  {
    "objectID": "block_1/511_python/511_python.html#functions",
    "href": "block_1/511_python/511_python.html#functions",
    "title": "Python Basics",
    "section": "",
    "text": "def function_name(arg1, arg2, arg3=default_value): # default values are optional\n        # code\n        return value\n\n    def function_name(*args): # takes in multiple arguments\n        for arg in args:\n            # code\n        return value\n\nside effects: If a function does anything other than returning a value, it is said to have side effects. An example of this is when a function changes the variables passed into it, or when a function prints something to the screen.\n\n\n\n    # Syntax: lambda arg1, arg2: expression\n    lambda x: x+1\n\n\n\n    # NumPy/SciPy style\n    def repeat_string(s, n=2):\n        \"\"\"\n        Repeat the string s, n times.\n\n        Parameters\n        ----------\n        s : str\n            the string\n        n : int, optional\n            the number of times, by default = 2\n\n        Returns\n        -------\n        str\n            the repeated string\n\n        Examples\n        --------\n        &gt;&gt;&gt; repeat_string(\"Blah\", 3)\n        \"BlahBlahBlah\"\n        \"\"\"\n        return s * n\n\n\n\n    def repeat_string(s: str, n: int = 2) -&gt; str:\n        return s * n"
  },
  {
    "objectID": "block_1/511_python/511_python.html#classes",
    "href": "block_1/511_python/511_python.html#classes",
    "title": "Python Basics",
    "section": "",
    "text": "class ClassName:\n        def __init__(self, arg1, arg2):\n            # code\n        def method_name(self, arg1, arg2):\n            # code\n        @classmethod\n        def other_method_name(cls, arg1, arg2):\n            # classmethod is used to create factory methods, aka other ways to create objects\n            # code\n\n    # Inheritance\n    class SubClassName(ClassName):\n        def __init__(self, arg1, arg2):\n            super().__init__(arg1, arg2)\n            # code\n        def method2_name(self, arg1, arg2):\n            # code\n\n\n    class ClassName:\n        static_var = 0\n\n        @staticmethod\n        def method_name(arg1, arg2):\n            # code\n            ClassName.static_var += 1\n            return ClassName.static_var\n        @staticmethod\n        def reset_static_var():\n            ClassName.static_var = 0\n\n\n\n    class TestVector(unittest.TestCase):\n        def test_str1(self):\n            v1 = Vector(1, 2, 3)\n            self.assertIn(\"Vector = [1, 2, 3]\", v1.__str__())\n            self.assertEqual(len(v1.elements), 3)\n\n        def test_str2(self):\n            v1 = Vector(500)\n            self.assertIn(\"Vector = [500]\", v1.__str__())\n\n\n    TestVector = unittest.main(argv=[\"\"], verbosity=0, exit=False)\n    assert TestVector.result.wasSuccessful()"
  },
  {
    "objectID": "block_1/511_python/511_python.html#pep8-guidelines",
    "href": "block_1/511_python/511_python.html#pep8-guidelines",
    "title": "Python Basics",
    "section": "",
    "text": "Category\nRule/Convention\nExample\n\n\n\n\nIndentation\nUse 4 spaces per indentation level\nif x: # four spaces here\n\n\n\nUse spaces around operators and after commas\na = b + c, d = e + f\n\n\nMaximum Line Length\nLimit all lines to a maximum of 79 characters for code, 72 for comments and docstrings\n\n\n\nImports\nAlways put imports at the top of the file\nimport os\n\n\n\nGroup imports: standard, third-party, local\nimport os import numpy as np from . import my_module\n\n\n\nUse absolute imports\nfrom my_pkg import module\n\n\nWhitespace\nAvoid extraneous whitespace\nspam(ham[1], {eggs: 2})\n\n\n\nUse blank lines to separate functions, classes, blocks of code inside functions\n\n\n\nComments\nComments should be complete sentences\n# This is a complete sentence.\n\n\n\nUse inline comments sparingly\nx = x + 1 # Increment x\n\n\nNaming Conventions\nFunction names should be lowercase with underscores\nmy_function()\n\n\n\nClass names should use CapWords convention\nMyClass\n\n\n\nConstants should be in all capital letters\nCONSTANT_NAME\n\n\nString Quotes\nUse double quotes for docstrings and single quotes for everything else when you start a project (or adhere to the project’s conventions)\n‘string’, “““docstring”“”\n\n\nExpressions and Statements\nDon’t use semicolons to terminate statements\nx = 1 (not x = 1;)\n\n\n\nUse is for identity comparisons and == for value comparisons\nif x is None\n\n\nOther Recommendations\nUse built-in types like list, dict instead of List, Dict from the typing module for simple use-cases\ndef func(a: list) -&gt; None:"
  },
  {
    "objectID": "block_1/511_python/511_python.html#numpy-arrays",
    "href": "block_1/511_python/511_python.html#numpy-arrays",
    "title": "Python Basics",
    "section": "Numpy Arrays",
    "text": "Numpy Arrays\n\nDifference with lists:\n\n\n\n\n\n\n\n\nFeature\nNumpy arrays\nLists\n\n\n\n\nData type uniformity\nAll elements must be of the same data type\nNo restriction\n\n\nStorage efficiency\nStored more efficiently\nLess efficient\n\n\nVectorized operations\nSupports\nDoes not support\n\n\n\n\n\nCreating arrays:\n\n\n\n\n\n\n\n\nFunction/Method\nDescription\nExample\n\n\n\n\nnp.array([list], dtype)\nCreate an array from a list\nnp.array([1, 2, 3], dtype='int8')\n\n\nnp.arange(start, stop, step)\nCreate an array of evenly spaced values\nnp.arange(0, 10, 2) returns [0, 2, 4, 6, 8]\n\n\nnp.ones(shape, dtype)\nCreate an array of ones\nnp.ones((3, 2), dtype='int8') returns [[1, 1], [1, 1], [1, 1]]\n\n\nnp.zeros(shape, dtype)\nCreate an array of zeros\n\n\n\nnp.full(shape, fill_value, dtype)\nCreate an array of a specific value\n\n\n\nnp.random.rand(shape)\nCreate an array of random values between 0 and 1\nnp.random.randn(3, 3, 3)\n\n\n\n\n\nOther useful functions:\n\n\n\n\n\n\n\n\nFunction/Method\nDescription\nExample\n\n\n\n\narray.transpose() or array.T\nTranspose an array\n\n\n\narray.ndim\nReturns the number of dimensions\nx = np.ones((3,2)) and print(x.ndim) returns 2\n\n\narray.size\nReturns the number of elements\nprint(x.size) returns 6\n\n\narray.shape\nReturns the shape of the array\nprint(x.shape) returns (3, 2)\n\n\nnp.flip(array)\nReverse an array , default: flips row and cols\nnp.flip(array)\n\n\narray.reshape(shape)\nReshape an array\narray.reshape(2, 3) for 2 rows, 3 columns\n\n\narray.sort()\nSort an array in ascending order\nnew_arr = array.sort()\n\n\narray.flatten()\nFlatten an array, same as reshape(-1)\nnew_arr = array.flatten()\n\n\narray.concatenate()\nConcatenate arrays, default: axis=0 (adds to the bottom)\nnp.concatenate([array1, array2], axis=0)\n\n\n\nNote: For the reshape function, you can use -1 to infer dimension (e.g. array.reshape(-1, 3)). Also, the default is row-major order, but you can specify order='F' for column-major order.\n\n\narray.dtype: returns the data type\narray.astype(dtype): convert an array to a different data type\nnp.array_equal(array1, array2): check if two arrays are equal\n\n\n\nArray operations and broadcasting\n\nsmaller arrays are broadcasted to match the shape of larger arrays \nCan only broadcast if compatible in all dimensions. They are compatible if:\n\nthey are equal in size\none of them is 1\n\nChecks starting from the right-most dimension"
  },
  {
    "objectID": "block_1/511_python/511_python.html#series",
    "href": "block_1/511_python/511_python.html#series",
    "title": "Python Basics",
    "section": "Series",
    "text": "Series\n\nnumpy array with labels\ncan store any data type, string takes the most space\n\nIf any NaN, dtype of Series becomes float64\nif any mixed data types, dtype of Series becomes object\n\npd.Series(): create a Series\n\npd.Series([1, 2, 3], index=['a', 'b', 'c'], name='col1'): create a Series from a list with labels and a name\npd.Series({'a': 1, 'b': 2, 'c': 3}): create a Series from a dictionary\n\ns.index: returns the index\ns.to_numpy(): convert a Series to a numpy array\n\n\nIndexing:\n\ns['a']: returns the value at index ‘a’\ns[['a', 'b']]: returns a Series with values at indices ‘a’ and ‘b’\ns[0:3]: returns a Series with values at indices 0, 1, and 2\n\n\n\nOperations:\n\naligns values based on index\ns1 + s2: returns a Series with values at indices in both s1 and s2\n\nwill return NaN (Not a Number) if index is not in both s1 and s2\nkind of like a left join in SQL"
  },
  {
    "objectID": "block_1/511_python/511_python.html#dataframes",
    "href": "block_1/511_python/511_python.html#dataframes",
    "title": "Python Basics",
    "section": "DataFrames",
    "text": "DataFrames\n\nCreating DataFrames\n\npd.DataFrame([2d list], columns=['col1', 'col2'], index=['row1', 'row2']): creates a DataFrame from a list with column names and row names\n\n\n\n\n\n\n\n\nSource\nCode\n\n\n\n\nLists of lists\npd.DataFrame([['Quan', 7], ['Mike', 15], ['Tiffany', 3]])\n\n\nndarray\npd.DataFrame(np.array([['Quan', 7], ['Mike', 15], ['Tiffany', 3]]))\n\n\nDictionary\npd.DataFrame({\"Name\": ['Quan', 'Mike', 'Tiffany'], \"Number\": [7, 15, 3]})\n\n\nList of tuples\npd.DataFrame(zip(['Quan', 'Mike', 'Tiffany'], [7, 15, 3]))\n\n\nSeries\npd.DataFrame({\"Name\": pd.Series(['Quan', 'Mike', 'Tiffany']), \"Number\": pd.Series([7, 15, 3])})\n\n\nCsv\npd.read_csv('file.csv', sep='\\t')\n\n\n\n\n\nIndexing\n\n\n\n\n\n\n\n\nMethod\nDescription\nExample\n\n\n\n\nSingle Column\nReturns single column as a Series\ndf['col1'] or df.col1\n\n\n\nReturns single column as a DataFrame\ndf[['col1']]\n\n\nMultiple Columns\nReturns multiple columns as a DataFrame\ndf[['col1', 'col2']]\n\n\niloc (integer)\nReturns first row as a Series\ndf.iloc[0]\n\n\n\nReturns first row as a DataFrame\ndf.iloc[0:1] OR df.iloc[[0]]\n\n\n\nReturns specific rows and columns\ndf.iloc[2:5, 1:4] (rows 2-4 and columns 1-3)\n\n\n\nReturns specific rows and columns\ndf.iloc[[0, 2], [0, 2]] (rows 0 and 2 and columns 0 and 2)\n\n\nloc (label)\nReturns all rows and a specific column as a Series\ndf.loc[:, 'col1']\n\n\n\nReturns all rows and a specific column as a DataFrame\ndf.loc[:, ['col1']]\n\n\n\nReturns all rows and specific range of columns as a DataFrame\ndf.loc[:, 'col1':'col3']\n\n\n\nReturns item at row1, col1\ndf.loc['row1', 'col1']\n\n\nBoolean indexing\nReturns rows based on a boolean condition\ndf[df['col1'] &gt; 5]\n\n\nquery\nReturns rows based on a query (same as above)\ndf.query('col1 &gt; 5')\n\n\n\nNote: Indexing with just a single number like df[0] or a slice like df[0:1] without iloc or loc doesn’t work for DataFrame columns.\n\nCan do indexing with boolean with loc: df.loc[:, df['col1'] &gt; 5] Gets all rows and columns where col1 &gt; 5\nWant to *2 for rows in col2 when col1&gt;5: df.loc[df['col1'] &gt; 5, 'col2'] = 0\n\n\n\nOther useful functions\n\n\n\n\n\n\n\n\nFunction/Method\nDescription\nExample\n\n\n\n\nmax()\nReturns the maximum value\ndf['col1'].max()\n\n\nidxmax()\nReturns the index of the maximum value\ndf['col1'].idxmax()\n\n\nmin()\nReturns the minimum value\ndf['col1'].min()\n\n\nidxmin()\nReturns the index of the minimum value\ndf['col1'].idxmin()\n\n\n\nCan return row of max value of col1 by: df.iloc[[df['col1'].idxmax()]] or df.iloc[df.loc[:, 'col1'].idxmax()]"
  },
  {
    "objectID": "block_1/511_python/511_python.html#data-wrangling",
    "href": "block_1/511_python/511_python.html#data-wrangling",
    "title": "Python Basics",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nData Summary\n\nshape: returns (rows, columns)\ninfo(): returns column names, data types, and number of non-null values\ndescribe(): returns summary statistics for each column\n\n\n\nViews vs Copies\n\nCorrect way to replace: df.loc[df['Released_Year'] &gt; 2021, 'Released_Year'] = 2010"
  },
  {
    "objectID": "block_1/511_python/511_python.html#data-manipulation",
    "href": "block_1/511_python/511_python.html#data-manipulation",
    "title": "Python Basics",
    "section": "Data Manipulation",
    "text": "Data Manipulation\n\nrename(): rename columns\n\ndf.rename(columns={'old_name': 'new_name', 'old_name_2': 'new_name_2'}, inplace=True)\ninplace=True to modify DataFrame, instead of returning a new DataFrame, default is False\nrecommended to just assign to a new variable\n\ncolumns: returns column names, can change column names by assigning a list of new names\n\ndf.columns.to_list(): returns column names as list\n\n\n\nChanging Index\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\ndf.set_index('col1', inplace=True)\nSet a column as the index\n\n\ndf = pd.read_csv('file.csv', index_col='col1')\nSet index when reading in a file\n\n\ndf.reset_index()\nReset index to default, starting at 0\n\n\ndf.index = df['col1']\nDirectly modify index\n\n\ndf.index.name = \"a\"\nRename index to ‘a’\n\n\n\n\n\nAdding/ Removing Columns\n\ndrop(): remove columns\n\ndf.drop(['col1', 'col2'], axis=1, inplace=True)\ndf.drop(df.columns[5:], axis=1)\n\ninsert(): insert a column at a specific location\n\ndf.insert(0, 'col1', df['col2'])\n\ndf['new_col'] = df['col1'] + df['col2']: add a new column\n\n\n\nAdding/ Removing Rows\n\ndf.drop([5:], axis=0): remove rows 5 and after\ndf = df.iloc[5:]: returns rows 5 and after\n\n\n\nReshaping Data\n\nmelt(): unpivot a DataFrame from wide to long format\n\ne.g: df_melt = df.melt(id_vars=\"Name\", value_vars=[\"2020\", \"2019\"], var_name=\"Year\", value_name=\"Num_Courses\")\nid_vars: specifies which column should be used as identifier variables (the key)\nvalue_vars: Column(s) to unpivot. If not specified, uses all columns that are not set as id_vars.\nvar_name: select which variables to melt\nvalue_name: select which variables to keep\nignore_index=False: keeps the index, default is True (resets index)\n\npivot(): pivot a DataFrame from long to wide format\n\ne.g: df_pivot = df_melt.pivot(index=\"Name\", columns=\"Year\", values=\"Num_Courses\")\nindex: specifies which column should be used as index\ncolumns: specifies which column should be used as columns\nvalues: specifies which column should be used as values\n\n\nAfter pivot, if want to remove column names, use df.columns.name = None\n\npivot_table(): pivot a DataFrame from long to wide format, but can handle duplicate values\n\ne.g: df_pivot = df.pivot_table(index=\"Name\", columns=\"Year\", values=\"Num_Courses\", aggfunc='sum')\naggfunc: specifies how to aggregate duplicate values\n\nconcat(): concatenate DataFrames\n\ndf = pd.concat([df1, df2], axis=0, ignore_index=True)\naxis=0: concatenate along rows\naxis=1: concatenate along columns\nignore_index=True: ignore index (resets the index), default is False (leaves index as is)\n\nmerge(df1, df2, on='col1', how='inner'): merge DataFrames, analogous to join in R\n\non: specifies which column to merge on\nhow: specifies how to merge\n\ninner: only keep rows that are in both DataFrames\nouter: keep all rows from both DataFrames\nleft: keep all rows from the left DataFrame\nright: keep all rows from the right DataFrame\n\n\n\n\n\nApply Functions\n\ndf.apply(): apply a function to a DataFrame column or row-wise, takes/returns series or df\n\ndf.apply(lambda x: x['col1'] + x['col2'], axis=1): apply a function to each row\ndf.apply(lambda x: x['col1'] + x['col2'], axis=0): apply a function to each column\ndf['col1'].apply(lambda x: x + 1): apply a function to column ‘col1’\n\ndf.applymap(): apply a function to each element in a DataFrame\n\ndf.loc[:, [\"Mean Max Temp (°C)\"]].applymap(int): apply int to each element in column ’Mean Max Temp (°C)\ndf.loc[[\"Mean Max Temp (°C)\"]].astype(int): Faster…\nOnly works for DataFrames, not Series\n\n\n\n\nGrouping Data\n\ngroupby(): group data by a column\n\ndf.groupby('col1'): returns a DataFrameGroupBy object\ndf.groupby('col1').groups: returns a dictionary of groups\ndf.groupby('col1').get_group('group1'): returns a DataFrame of group1\n\ndf.agg(): aggregate data\n\ndf.groupby('col1').agg({'col2': 'mean', 'col3': 'sum'}): returns a DataFrame with mean of col2 and sum of col3 for each group (needs to be numeric, or else error)"
  },
  {
    "objectID": "block_1/511_python/511_python.html#string-dtype",
    "href": "block_1/511_python/511_python.html#string-dtype",
    "title": "Python Basics",
    "section": "String dtype",
    "text": "String dtype\n\ndf.str.func(): can apply any string function to each string in df\n\ne.g. df['col1'].str.lower(): convert all strings in col1 to lowercase\ndf['col1'].str.cat(sep=' '): concatenate all strings in col1 with a space in between, default is no space\n\nCan do regex as well:\n\ndf['col1'].str.contains(r'regex'): returns a boolean Series\ndf['col1'].str.replace(r'regex', 'new_string'): replace all strings in col1 that match regex with ‘new_string’"
  },
  {
    "objectID": "block_1/511_python/511_python.html#datetime-dtype",
    "href": "block_1/511_python/511_python.html#datetime-dtype",
    "title": "Python Basics",
    "section": "Datetime dtype",
    "text": "Datetime dtype\nfrom datetime import datetime, timedelta\n\nConstruct a datetime object: datetime(year, month, day, hour, minute, second)\ndatetime.now(): returns current datetime\ndatetime.strptime('July 9 2005, 13:54', '%B %d %Y, %H:%M'): convert a string to a datetime object\nAdd time to a datetime object: datetime + timedelta(days=1, hours=2, minutes=3, seconds=4)\n\n\nDatetime dtype in Pandas\n\npd.Timestamp('2021-07-09 13:54'): convert a string to a datetime object\n\npd.Timestamp(year, month, day, hour, minute, second): construct a datetime object\npd.Timestamp(datetime(year, month, day, hour, minute, second)): convert a datetime object to a Timestamp object\n\npd.Period('2021-07'): convert a string to a Period object\n\npd.Period(year, month): construct a Period object\npd.Period(datetime(year, month, day, hour, minute, second)): convert a datetime object to a Period objects\n\n\nConverting existing columns to datetime dtype:\n\npd.to_datetime(df['col1']): convert col1 to datetime dtype\npd.read_csv('file.csv', parse_dates=True): convert all columns with datetime dtype to datetime dtype\n\n\n\nOther datetime functions\nThe index of a DataFrame can be a datetime dtype. These functions can be applied to the index.\n\ndf.index.year: returns a Series of years\ndf.index.month: returns a Series of months\n\ndf.index.month_name(): returns a Series of month names\n\ndf.between_time('9:00', '12:00'): returns rows between 9am and 12pm\ndf.resample('D').mean(): resample data to daily and take the mean\n\nD: daily\nW: weekly\nM: monthly\nQ: quarterly\nY: yearly\n\npd.date_range(start, end, freq): returns a DatetimeIndex\n\nfreq same as above (e.g. D, W, M, Q, Y)"
  },
  {
    "objectID": "block_1/511_python/511_python.html#visualization",
    "href": "block_1/511_python/511_python.html#visualization",
    "title": "Python Basics",
    "section": "Visualization",
    "text": "Visualization\n\ndf['Distance'].plot.line(): plot a line chart\ndf['Distance'].plot.bar(): plot a bar chart\ndf['Distance'].cumsum().plot.line(): plot a line chart of cumulative sum"
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html",
    "href": "block_1/521_platforms/521_platforms.html",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "ls - list files and directories. Default behavior is to list the contents of the current working directory.\ncd - change directory. Used to navigate the filesystem. Default behavior is to change to the home directory.\npwd - print working directory. It will return the absolute path of the current working directory.\nmkdir - make directory\ntouch - create file\nrm - remove file\nrmdir - remove directory\nmv - move file. also used to rename file.\ncp - copy file\nwhich - locate a program. It will return the path of the program.\n\n\n\nThese flags allow us to modify the default behaviour of a program.\n\n-a - all\n-l - long\n-h - human readable\n-r - recursive\n-f - force\n\n\n\n\n\n. - current directory\n.. - parent directory\n~ - home directory\n/ - root directory\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nGraphical user interface (GUI)\nA user interface that relies on windows, menus, pointers, and other graphical elements\n\n\nCommand-line interface (CLI)\nA user interface that relies solely on text for commands and output, typically running in a shell.\n\n\nOperating system\nA program that provides a standard interface to whatever hardware it is running on.\n\n\nFilesystem\nThe part of the operating system that manages how files are stored and retrieved. Also used to refer to all of those files and directories or the specific way they are stored.\n\n\nSubdirectory\nA directory that is below another directory\n\n\nParent directory\nThe directory that contains another directory of interest. Going from a directory to its parent, then its parent, and so on eventually leads to the root directory of the filesystem.\n\n\nHome directory\nA directory that contains a user’s files.\n\n\nCurrent working directory\nThe folder or directory location in which the program operates. Any action taken by the program occurs relative to this directory.\n\n\nPath (in filesystem)\nA string that specifies a location in a filesystem.\n\n\nAbsolute path\nA path that points to the same location in the filesystem regardless of where it is evaluated. It is the equivalent of latitude and longitude in geography.\n\n\nRelative path\nA path whose destination is interpreted relative to some other location, such as the current working directory.\n\n\nDirectory\nAn item within a filesystem that can contain files and other directories. Also known as “folder”.\n\n\nRoot directory\nThe directory that contains everything else, either directly or indirectly.\n\n\nPrompt\nThe text printed by the shell that indicates it is ready to accept another command.\n\n\n\n\n\n\n\nSymbol\n\n\n\n\nA. Root directory (see note)\n/\n\n\nB. Parent directory\n..\n\n\nC. Current working directory\n.\n\n\nD. Home directory\n~ \n\n\nE. Command line argument\n-y or --yes\n\n\nF. Prompt (R)\n&gt;\n\n\nG. Prompt (Python)\n&gt;&gt;&gt;\n\n\nH. Prompt (Bash)\n$\n\n\n\n\n\n\n\n\ngit init - initialize a git repository\ngit add - add files to staging area.\n\nStaging area is a place where we can group files together before we “commit” them to git.\n\ngit commit - commit changes to git. Records a new version of the files in the repository.\ngit push - push changes to remote repository from local repository\ngit pull - pull changes from remote repository to local repository\ngit status - check status of git repository\ngit log - check commit history\n\ngit log --oneline - check commit history in one line\ngit log -p - check commit history with changes\n\ngit diff - check difference between two commits\ngit reset - reset git repository\n\ngit reset --hard - reset git repository to last commit\ngit reset --soft - reset git repository to last commit but keep changes\n\ngit revert - revert git repository. The difference between revert and reset is that revert creates a new commit with the changes from the commit we want to revert.\ngit stash - saves changes that you don’t want to commit immediately. It takes the dirty state of your working directory — that is, your modified tracked files and staged changes — and saves it on a stack of unfinished changes that you can reapply at any time.\n\n\n\n\nRepository - a collection of files and folders that are tracked by git.\nCommit - a snapshot of the repository at a specific point in time.\nCommit Hash - These are the commits’ hashes (SHA-1), which are used to uniquely identify the commits within a project.\nBranch - a parallel version of a repository. It is contained within the repository, but does not affect the primary or master branch allowing you to work freely without disrupting the “live” version.\n\n\n\n\n\nPublic key is used to encrypt data and private key is used to decrypt data.\nConfidentiality - only the intended recipient can decrypt the data.\nAuthentication - only the intended recipient can encrypt the data.\n\n\n\n\nSSH is more secure than HTTPS because it uses public and private keys to encrypt and decrypt data. HTTPS uses username and password to authenticate users.\n\n\n\n\nGithub pages will look in either the repository root / directory or in the repository docs/ directory for website content to render.\n\n\n\n\nDynamic Documents: Rooted in Knuth’s “literate programming” concept from 1984.\nMain Goals:\n\nWrite program code.\nCreate narratives to elucidate code function.\n\nBenefits:\n\nEnhances understanding and provides comprehensive documentation.\nGives a way to run code and view results.\nAllows text and code to be combined in a single document.\nFacilitates reproducibility of results and diagrams.\n\nPopular Formats:\n\nJupyter Notebooks (.ipynb)\nRMarkdown documents (.Rmd)\n\nKey Features:\n\nNarratives formatted with markdown.\nExecutable code:\n\nInterwoven in text (RMarkdown’s inline code).\nSeparate sections: code cells (Jupyter) or code chunks (RMarkdown).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\ngetwd()\nReturns the current working directory of the R session.\n\n\nsetwd(path)\nChanges the working directory to the specified path.\n\n\nhere::here()\nCreates file paths relative to the project’s root (where the .Rproj file is) to ensure consistent and portable references.\n\n\n\nNote: For portability, prefer here::here() over setwd() to avoid path inconsistencies across different systems.\n\n\n\n\nGlobal looks like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature/Aspect\nJupyter Notebooks\nRMarkdown\nQuarto (.qmd)\n\n\n\n\nPrimary Language\nPython\nR\nPython, R, Julia\n\n\nSecondary Language\nR (via R kernel)\nPython (via reticulate)\nR with Python (via reticulate), Python with R (via rpy2)\n\n\nCompatible Editors\nJupyterLab, VS Code\nRStudio\nRStudio, VS Code, JupyterLab\n\n\nSpecial Features\n-\n-\nSingle engine processing, cross-language integration\n\n\nRecommended Environments\nJupyterLab or VS Code\nRStudio\nRStudio (offers code-completion, incremental cell execution, and other tools for working with executable code)\n\n\n\n\n\n\n\nOn top of the RMarkdown document, you can specify a template.\nTemplates are .Rmd files that contain the YAML header and some default text.\n\n---\ntitle: \"Untitled\"\noutput: github_document / html_document / pdf_document / word_document\nauthor: \"farrandi\"\ndate: \"10/2/2023\"\n---\n\n\n\n# Same YAML header as above\n---\ntitle: \"521 Quiz 2\"\nauthor: \"Daniel Chen\"\nformat: revealjs\n---\n\n# In the morning\n\n## Getting up\n\n- Turn off alarm\n- Get out of bed\n\n## Breakfast\n\n- Eat eggs\n- Drink coffee\n\n# In the evening\n\n## Dinner\n\n- Eat spaghetti\n- Drink wine\n\n## Going to sleep\n\n- Get in bed\n- Count sheep\n\n\n\n\n\nJupyterLab supports presentations using the JavaScript framework: reveal.js.\nreveal.js is the same framework used by Quarto.\nCells for presentations are marked via the property inspector:\n\nFound at the settings wheel in the left side panel.\nSelect slide type from the dropdown menu.\n\nreveal.js presentations are two dimensional:\n\nHorizontal slides\nVertical sub-slides\n\n\n\n\n\n\n\n\n\n\n\n\nSlide Type\nDescription\nNavigation\n\n\n\n\nSlide\nStandard slide\nLeft and right arrows\n\n\nSub-slide\nSub-topic of a slide\nUp and down arrows\n\n\nFragment\nAnimated part of the previous slide, e.g. a bullet\nPart of slide animation\n\n\n- (or empty)\nAppears in same cell as previous slide\nPart of slide\n\n\nNotes\nSpeaker notes\nVisible when pressing ‘t’\n\n\nSkip\nNot included in the presentation\n-\n\n\n\n\n\n\n\nImages using ![]() or &lt;img&gt; tags don’t show up in exports.\nWorkaround: Paste images into a Markdown cell to include them as attachments. This ensures visibility in HTML and slide exports.\n\n\n\n\n\n\nVirtual Environments: Isolated Python environments that allow for the installation of packages without affecting the system’s Python installation.\nBenefits:\n\nIsolation: Packages installed in a virtual environment are isolated from the system’s Python installation.\nReproducibility: Virtual environments can be shared with others to ensure reproducibility.\nVersion control: Allow managing the versions of libraries and tools used in a project.\nCross-platform: Ensures that the project’s dependencies are consistent across different systems.\nExperimentation: Allows for experimentation with different versions of packages.\nClean environment: When starting a new project, starts with a clean slate.\nConsistency: Ensures that the project’s dependencies are consistent across different systems.\nSecurity: Isolates the project’s dependencies from the system’s Python installation.\n\n\n\n\n\nconda create -n &lt;env_name&gt; python=&lt;version&gt;: create a new environment\n\nconda env create -f path/to/environment.yml: create an environment from a file\nconda create --name live_env --clone test_env: create an environment from an existing environment\n\nconda env list: list all environments\nconda activate &lt;env_name&gt;: activate the environment\nconda deactivate: deactivate the environment\nconda env remove -n &lt;env_name&gt; --all: remove the environment\nconda env export -f environment.yml --from-history: export the environment to a file\n\n--from-history: only include packages that were explicitly installed\n\n\nManaging packages:\n\nconda config --add channels conda-forge: add a channel\nconda list: list all packages in the environment\nconda search &lt;package&gt;: search for a package\nconda install &lt;package&gt;: install a package\n\nconda install &lt;package&gt;=&lt;version&gt;: install a specific version of a package\n\nconda remove &lt;package&gt;: remove a package\n\nExample environment.yml file:\nname: test_env\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - conda\n  - python=3.7\n  - pandas==1.0.2\n  - jupyterlab\n\n\n\n\n\n\n\nComponent\nDescription\n\n\n\n\nname\n- Identifies the environment’s name. - Useful for distinguishing multiple environments.\n\n\nchannels\n- Locations where Conda searches for packages. - Default: defaults channel. - Popular options: conda-forge, bioconda.\n\n\ndependencies\n- Lists required packages for the environment. - Can specify versions or ranges. - Can include Conda or pip packages.\n\n\nprefix\n- (Optional) Directory where Conda installs the environment. - Defaults to Conda’s main directory if not provided.\n\n\n\n\n\n\n\nMake a new project in RStudio 1.1. Select setting to use renv 1.2. library(renv): to use renv\nrenv::init(): initialize the project\n\n\n\n\n\n3 Principles:\n\nMachine readable:\n\nregex and globbing friendly (avoid spaces, special characters, case sensitivity, etc.)\ndeliberate use of delimiters (e.g. _, -, .)\n\nHuman readable: Helps other people and ourselves in the future quickly understand the file structure and contents of a project/ file.\nPlays well with default ordering: Makes files more organized and easily searchable. Easy for us humans to find the files we are looking for.\n\nDates go: YYYY-MM-DD\n\n\n\n\n\n\nReproducible example\nCode formatting\nMinimal, complete, verifiable example\n\nEffective Questioning & Creating an MRE:\n\nSearch for similar questions before asking.\nClearly state the problem in the title and provide brief details in the body.\nProvide the shortest version of your code that replicates the error.\nInclude definitions if you’ve used functions or classes.\nUse toy datasets rather than real data.\nUse markdown for code to ensure readability and syntax highlighting.\nShare attempts, points of confusion, and full error tracebacks."
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#shell",
    "href": "block_1/521_platforms/521_platforms.html#shell",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "ls - list files and directories. Default behavior is to list the contents of the current working directory.\ncd - change directory. Used to navigate the filesystem. Default behavior is to change to the home directory.\npwd - print working directory. It will return the absolute path of the current working directory.\nmkdir - make directory\ntouch - create file\nrm - remove file\nrmdir - remove directory\nmv - move file. also used to rename file.\ncp - copy file\nwhich - locate a program. It will return the path of the program.\n\n\n\nThese flags allow us to modify the default behaviour of a program.\n\n-a - all\n-l - long\n-h - human readable\n-r - recursive\n-f - force\n\n\n\n\n\n. - current directory\n.. - parent directory\n~ - home directory\n/ - root directory\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nGraphical user interface (GUI)\nA user interface that relies on windows, menus, pointers, and other graphical elements\n\n\nCommand-line interface (CLI)\nA user interface that relies solely on text for commands and output, typically running in a shell.\n\n\nOperating system\nA program that provides a standard interface to whatever hardware it is running on.\n\n\nFilesystem\nThe part of the operating system that manages how files are stored and retrieved. Also used to refer to all of those files and directories or the specific way they are stored.\n\n\nSubdirectory\nA directory that is below another directory\n\n\nParent directory\nThe directory that contains another directory of interest. Going from a directory to its parent, then its parent, and so on eventually leads to the root directory of the filesystem.\n\n\nHome directory\nA directory that contains a user’s files.\n\n\nCurrent working directory\nThe folder or directory location in which the program operates. Any action taken by the program occurs relative to this directory.\n\n\nPath (in filesystem)\nA string that specifies a location in a filesystem.\n\n\nAbsolute path\nA path that points to the same location in the filesystem regardless of where it is evaluated. It is the equivalent of latitude and longitude in geography.\n\n\nRelative path\nA path whose destination is interpreted relative to some other location, such as the current working directory.\n\n\nDirectory\nAn item within a filesystem that can contain files and other directories. Also known as “folder”.\n\n\nRoot directory\nThe directory that contains everything else, either directly or indirectly.\n\n\nPrompt\nThe text printed by the shell that indicates it is ready to accept another command.\n\n\n\n\n\n\n\nSymbol\n\n\n\n\nA. Root directory (see note)\n/\n\n\nB. Parent directory\n..\n\n\nC. Current working directory\n.\n\n\nD. Home directory\n~ \n\n\nE. Command line argument\n-y or --yes\n\n\nF. Prompt (R)\n&gt;\n\n\nG. Prompt (Python)\n&gt;&gt;&gt;\n\n\nH. Prompt (Bash)\n$"
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#git-and-github",
    "href": "block_1/521_platforms/521_platforms.html#git-and-github",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "git init - initialize a git repository\ngit add - add files to staging area.\n\nStaging area is a place where we can group files together before we “commit” them to git.\n\ngit commit - commit changes to git. Records a new version of the files in the repository.\ngit push - push changes to remote repository from local repository\ngit pull - pull changes from remote repository to local repository\ngit status - check status of git repository\ngit log - check commit history\n\ngit log --oneline - check commit history in one line\ngit log -p - check commit history with changes\n\ngit diff - check difference between two commits\ngit reset - reset git repository\n\ngit reset --hard - reset git repository to last commit\ngit reset --soft - reset git repository to last commit but keep changes\n\ngit revert - revert git repository. The difference between revert and reset is that revert creates a new commit with the changes from the commit we want to revert.\ngit stash - saves changes that you don’t want to commit immediately. It takes the dirty state of your working directory — that is, your modified tracked files and staged changes — and saves it on a stack of unfinished changes that you can reapply at any time.\n\n\n\n\nRepository - a collection of files and folders that are tracked by git.\nCommit - a snapshot of the repository at a specific point in time.\nCommit Hash - These are the commits’ hashes (SHA-1), which are used to uniquely identify the commits within a project.\nBranch - a parallel version of a repository. It is contained within the repository, but does not affect the primary or master branch allowing you to work freely without disrupting the “live” version.\n\n\n\n\n\nPublic key is used to encrypt data and private key is used to decrypt data.\nConfidentiality - only the intended recipient can decrypt the data.\nAuthentication - only the intended recipient can encrypt the data.\n\n\n\n\nSSH is more secure than HTTPS because it uses public and private keys to encrypt and decrypt data. HTTPS uses username and password to authenticate users."
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#quarto-and-github-pages",
    "href": "block_1/521_platforms/521_platforms.html#quarto-and-github-pages",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "Github pages will look in either the repository root / directory or in the repository docs/ directory for website content to render."
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#dynamic-documents",
    "href": "block_1/521_platforms/521_platforms.html#dynamic-documents",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "Dynamic Documents: Rooted in Knuth’s “literate programming” concept from 1984.\nMain Goals:\n\nWrite program code.\nCreate narratives to elucidate code function.\n\nBenefits:\n\nEnhances understanding and provides comprehensive documentation.\nGives a way to run code and view results.\nAllows text and code to be combined in a single document.\nFacilitates reproducibility of results and diagrams.\n\nPopular Formats:\n\nJupyter Notebooks (.ipynb)\nRMarkdown documents (.Rmd)\n\nKey Features:\n\nNarratives formatted with markdown.\nExecutable code:\n\nInterwoven in text (RMarkdown’s inline code).\nSeparate sections: code cells (Jupyter) or code chunks (RMarkdown)."
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#rsudio-and-quarto",
    "href": "block_1/521_platforms/521_platforms.html#rsudio-and-quarto",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "Function\nDescription\n\n\n\n\ngetwd()\nReturns the current working directory of the R session.\n\n\nsetwd(path)\nChanges the working directory to the specified path.\n\n\nhere::here()\nCreates file paths relative to the project’s root (where the .Rproj file is) to ensure consistent and portable references.\n\n\n\nNote: For portability, prefer here::here() over setwd() to avoid path inconsistencies across different systems.\n\n\n\n\nGlobal looks like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature/Aspect\nJupyter Notebooks\nRMarkdown\nQuarto (.qmd)\n\n\n\n\nPrimary Language\nPython\nR\nPython, R, Julia\n\n\nSecondary Language\nR (via R kernel)\nPython (via reticulate)\nR with Python (via reticulate), Python with R (via rpy2)\n\n\nCompatible Editors\nJupyterLab, VS Code\nRStudio\nRStudio, VS Code, JupyterLab\n\n\nSpecial Features\n-\n-\nSingle engine processing, cross-language integration\n\n\nRecommended Environments\nJupyterLab or VS Code\nRStudio\nRStudio (offers code-completion, incremental cell execution, and other tools for working with executable code)\n\n\n\n\n\n\n\nOn top of the RMarkdown document, you can specify a template.\nTemplates are .Rmd files that contain the YAML header and some default text.\n\n---\ntitle: \"Untitled\"\noutput: github_document / html_document / pdf_document / word_document\nauthor: \"farrandi\"\ndate: \"10/2/2023\"\n---\n\n\n\n# Same YAML header as above\n---\ntitle: \"521 Quiz 2\"\nauthor: \"Daniel Chen\"\nformat: revealjs\n---\n\n# In the morning\n\n## Getting up\n\n- Turn off alarm\n- Get out of bed\n\n## Breakfast\n\n- Eat eggs\n- Drink coffee\n\n# In the evening\n\n## Dinner\n\n- Eat spaghetti\n- Drink wine\n\n## Going to sleep\n\n- Get in bed\n- Count sheep"
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#presentations-in-jupyterlab",
    "href": "block_1/521_platforms/521_platforms.html#presentations-in-jupyterlab",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "JupyterLab supports presentations using the JavaScript framework: reveal.js.\nreveal.js is the same framework used by Quarto.\nCells for presentations are marked via the property inspector:\n\nFound at the settings wheel in the left side panel.\nSelect slide type from the dropdown menu.\n\nreveal.js presentations are two dimensional:\n\nHorizontal slides\nVertical sub-slides\n\n\n\n\n\n\n\n\n\n\n\n\nSlide Type\nDescription\nNavigation\n\n\n\n\nSlide\nStandard slide\nLeft and right arrows\n\n\nSub-slide\nSub-topic of a slide\nUp and down arrows\n\n\nFragment\nAnimated part of the previous slide, e.g. a bullet\nPart of slide animation\n\n\n- (or empty)\nAppears in same cell as previous slide\nPart of slide\n\n\nNotes\nSpeaker notes\nVisible when pressing ‘t’\n\n\nSkip\nNot included in the presentation\n-\n\n\n\n\n\n\n\nImages using ![]() or &lt;img&gt; tags don’t show up in exports.\nWorkaround: Paste images into a Markdown cell to include them as attachments. This ensures visibility in HTML and slide exports."
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#virtual-environments",
    "href": "block_1/521_platforms/521_platforms.html#virtual-environments",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "Virtual Environments: Isolated Python environments that allow for the installation of packages without affecting the system’s Python installation.\nBenefits:\n\nIsolation: Packages installed in a virtual environment are isolated from the system’s Python installation.\nReproducibility: Virtual environments can be shared with others to ensure reproducibility.\nVersion control: Allow managing the versions of libraries and tools used in a project.\nCross-platform: Ensures that the project’s dependencies are consistent across different systems.\nExperimentation: Allows for experimentation with different versions of packages.\nClean environment: When starting a new project, starts with a clean slate.\nConsistency: Ensures that the project’s dependencies are consistent across different systems.\nSecurity: Isolates the project’s dependencies from the system’s Python installation.\n\n\n\n\n\nconda create -n &lt;env_name&gt; python=&lt;version&gt;: create a new environment\n\nconda env create -f path/to/environment.yml: create an environment from a file\nconda create --name live_env --clone test_env: create an environment from an existing environment\n\nconda env list: list all environments\nconda activate &lt;env_name&gt;: activate the environment\nconda deactivate: deactivate the environment\nconda env remove -n &lt;env_name&gt; --all: remove the environment\nconda env export -f environment.yml --from-history: export the environment to a file\n\n--from-history: only include packages that were explicitly installed\n\n\nManaging packages:\n\nconda config --add channels conda-forge: add a channel\nconda list: list all packages in the environment\nconda search &lt;package&gt;: search for a package\nconda install &lt;package&gt;: install a package\n\nconda install &lt;package&gt;=&lt;version&gt;: install a specific version of a package\n\nconda remove &lt;package&gt;: remove a package\n\nExample environment.yml file:\nname: test_env\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - conda\n  - python=3.7\n  - pandas==1.0.2\n  - jupyterlab\n\n\n\n\n\n\n\nComponent\nDescription\n\n\n\n\nname\n- Identifies the environment’s name. - Useful for distinguishing multiple environments.\n\n\nchannels\n- Locations where Conda searches for packages. - Default: defaults channel. - Popular options: conda-forge, bioconda.\n\n\ndependencies\n- Lists required packages for the environment. - Can specify versions or ranges. - Can include Conda or pip packages.\n\n\nprefix\n- (Optional) Directory where Conda installs the environment. - Defaults to Conda’s main directory if not provided.\n\n\n\n\n\n\n\nMake a new project in RStudio 1.1. Select setting to use renv 1.2. library(renv): to use renv\nrenv::init(): initialize the project"
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#file-names",
    "href": "block_1/521_platforms/521_platforms.html#file-names",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "3 Principles:\n\nMachine readable:\n\nregex and globbing friendly (avoid spaces, special characters, case sensitivity, etc.)\ndeliberate use of delimiters (e.g. _, -, .)\n\nHuman readable: Helps other people and ourselves in the future quickly understand the file structure and contents of a project/ file.\nPlays well with default ordering: Makes files more organized and easily searchable. Easy for us humans to find the files we are looking for.\n\nDates go: YYYY-MM-DD"
  },
  {
    "objectID": "block_1/521_platforms/521_platforms.html#asking-questions",
    "href": "block_1/521_platforms/521_platforms.html#asking-questions",
    "title": "Platforms Cheat Sheet",
    "section": "",
    "text": "Reproducible example\nCode formatting\nMinimal, complete, verifiable example\n\nEffective Questioning & Creating an MRE:\n\nSearch for similar questions before asking.\nClearly state the problem in the title and provide brief details in the body.\nProvide the shortest version of your code that replicates the error.\nInclude definitions if you’ve used functions or classes.\nUse toy datasets rather than real data.\nUse markdown for code to ensure readability and syntax highlighting.\nShare attempts, points of confusion, and full error tracebacks."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Made by Nando (@farrandi)\nThis is a list of notes from my classes in the Master of Data Science program (2023/2024) at the University of British Columbia.\nI hope you find them useful!"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html",
    "href": "block_2/571_sup_learn/571_sup_learn.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "Machine Learning: A field of study that gives computers the ability to learn without being explicitly programmed.\n\nauto detect patterns in data and make predictions\nPopular def of supervised learning: input: data + labels, output: model\n\nTraining a model from input data and its corresponding targets to predict targets for new examples.\n\n\nFundamental goal of machine learning: generalize beyond the examples in the training set.\n\n\n\n\n\n\n\n\n\n\n\n\nSupervised Learning\nUnsupervised Learning\n\n\n\n\nDescription\nThe training data includes the desired solutions, called labels.\nThe training data is unlabeled.\n\n\nSubtypes/Goals\n- Predict based on input data and its corresponding targets- Classification: Predict a class label e.g. will someone pass or fail final based on previous scores- Regression: Predict a continuous number e.g. what score will someone get on final based on previous score\n- Clustering: Group similar instances- Anomaly detection: Detect abnormal instances- Visualization and dimensionality reduction: Simplify data- Association rule learning: Discover relations between attributes\n\n\nOther Examples\nDecision tree, naive bayes, kNN, random forest, support vector machine, neural network\nk-means, hierarchical cluster analysis, expectation maximization, t-SNE, apriori, FP-growth\n\n\n\n\n\n\n\nFeatures/ X[n x d]: The input variables used to make predictions. (d = # of features)\nTarget/ y [n x 1]: The output variable we are trying to predict.\nExample: A particular instance of data, usually represented as a vector of features. (n = # of training examples)\nTraining: Fitting the model to examples.\nLabel: The target value for a particular example. (y)\n\n\n\n\n\n\n\n\n\n\n\n\nParameters\nHyperparameters\n\n\n\n\nDescription\nThe coefficients learned by the model during training.\nSettings used to control the training process.\n\n\nLearning/Setting Process\nAre learned automatically during training.\nAre set before training.\n\n\nPurpose or Role\nRule values that are learned from the training data (examples/features).\nControls how complex the model is. Validate using validation score (can overfit if too complex).\n\n\nExamples\ne.g. coefficients in linear regression, weights in neural networks\ne.g. learning rate, number of iterations, number of hidden layers, depth of decision tree, k in kNN and k-means\n\n\n\n\n\n\n\n\nGenerally there are 2 kinds of error:\n\nTraining error (\\(E_{train}\\)): Error on the training data.\nDistribution error/ test error/ generalization error (\\(E_{D}\\)): Error on new data.\n\n\\(E\\_{approx} = E_{D} - E_{train}\\)\n\n\n\n\n\nTraining set: The data used to train the model. Used a lot to set parameters.\nValidation set: The data used to evaluate the model during training. Used a few times to set hyperparameters.\nTest set: The data used to evaluate the model after training. Used once to estimate \\(E_{D}\\).\nDeployment: The model is used in the real world.\n\nfrom sklearn.model_selection import train_test_split\n\n# Used directly on X and y\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=123\n)  # 80%-20% train test split on X and y\n\n# Used on a dataframe\ntrain_df, test_df = train_test_split(\n    df, test_size=0.2, random_state=123\n)  # 80%-20% train test split on df\n\n\n\nA method of estimating \\(E_{D}\\) using the training set.\n\nDivide the training set into \\(k\\) folds.\nFor each fold, train on the other \\(k-1\\) folds and evaluate on the current fold.\n\nBenefits:\n\nMore accurate estimate of \\(E_{D}\\). Sometimes just unlucky with train/test split, this helps.\nMore efficient use of data.\n\n\nfrom sklearn.model_selection import cross_val_score, cross_validate\n\n# cross_val_score not as comprehensive as cross_validate\n# cross_val_score only returns the scores\ncv_scores = cross_val_score(model, X_train, y_train, cv=10)\n\n# using cross_validate\nscores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n# returns a dictionary with keys: ['fit_time', 'score_time', 'test_score', 'train_score']\npd.DataFrame(scores)\ncross-validation does not return a model. It is not a way to build a model that can be applied to new data. The purpose of cross-validation is to evaluate how well the model will generalize to unseen data.\n\n\n\n\n\nAKA the bias/ variance trade-off in supervised learning.\n\nBias: Tendency to consistently learn the same wrong thing (high bias = underfitting).\nVariance: Tenency to learn random things irrespective of the real signal (high variance = overfitting).\n\nAs you increase model complexity, \\(E_{train}\\) goes down but \\(E_{approx} = E_{D} - E_{train}\\) goes up."
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#supervised-vs.-unsupervised-learning",
    "href": "block_2/571_sup_learn/571_sup_learn.html#supervised-vs.-unsupervised-learning",
    "title": "Supervised Learning",
    "section": "",
    "text": "Supervised Learning\nUnsupervised Learning\n\n\n\n\nDescription\nThe training data includes the desired solutions, called labels.\nThe training data is unlabeled.\n\n\nSubtypes/Goals\n- Predict based on input data and its corresponding targets- Classification: Predict a class label e.g. will someone pass or fail final based on previous scores- Regression: Predict a continuous number e.g. what score will someone get on final based on previous score\n- Clustering: Group similar instances- Anomaly detection: Detect abnormal instances- Visualization and dimensionality reduction: Simplify data- Association rule learning: Discover relations between attributes\n\n\nOther Examples\nDecision tree, naive bayes, kNN, random forest, support vector machine, neural network\nk-means, hierarchical cluster analysis, expectation maximization, t-SNE, apriori, FP-growth"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#common-terms",
    "href": "block_2/571_sup_learn/571_sup_learn.html#common-terms",
    "title": "Supervised Learning",
    "section": "",
    "text": "Features/ X[n x d]: The input variables used to make predictions. (d = # of features)\nTarget/ y [n x 1]: The output variable we are trying to predict.\nExample: A particular instance of data, usually represented as a vector of features. (n = # of training examples)\nTraining: Fitting the model to examples.\nLabel: The target value for a particular example. (y)\n\n\n\n\n\n\n\n\n\n\n\n\nParameters\nHyperparameters\n\n\n\n\nDescription\nThe coefficients learned by the model during training.\nSettings used to control the training process.\n\n\nLearning/Setting Process\nAre learned automatically during training.\nAre set before training.\n\n\nPurpose or Role\nRule values that are learned from the training data (examples/features).\nControls how complex the model is. Validate using validation score (can overfit if too complex).\n\n\nExamples\ne.g. coefficients in linear regression, weights in neural networks\ne.g. learning rate, number of iterations, number of hidden layers, depth of decision tree, k in kNN and k-means"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#errors",
    "href": "block_2/571_sup_learn/571_sup_learn.html#errors",
    "title": "Supervised Learning",
    "section": "",
    "text": "Generally there are 2 kinds of error:\n\nTraining error (\\(E_{train}\\)): Error on the training data.\nDistribution error/ test error/ generalization error (\\(E_{D}\\)): Error on new data.\n\n\\(E\\_{approx} = E_{D} - E_{train}\\)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#training-validation-and-test-sets",
    "href": "block_2/571_sup_learn/571_sup_learn.html#training-validation-and-test-sets",
    "title": "Supervised Learning",
    "section": "",
    "text": "Training set: The data used to train the model. Used a lot to set parameters.\nValidation set: The data used to evaluate the model during training. Used a few times to set hyperparameters.\nTest set: The data used to evaluate the model after training. Used once to estimate \\(E_{D}\\).\nDeployment: The model is used in the real world.\n\nfrom sklearn.model_selection import train_test_split\n\n# Used directly on X and y\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=123\n)  # 80%-20% train test split on X and y\n\n# Used on a dataframe\ntrain_df, test_df = train_test_split(\n    df, test_size=0.2, random_state=123\n)  # 80%-20% train test split on df\n\n\n\nA method of estimating \\(E_{D}\\) using the training set.\n\nDivide the training set into \\(k\\) folds.\nFor each fold, train on the other \\(k-1\\) folds and evaluate on the current fold.\n\nBenefits:\n\nMore accurate estimate of \\(E_{D}\\). Sometimes just unlucky with train/test split, this helps.\nMore efficient use of data.\n\n\nfrom sklearn.model_selection import cross_val_score, cross_validate\n\n# cross_val_score not as comprehensive as cross_validate\n# cross_val_score only returns the scores\ncv_scores = cross_val_score(model, X_train, y_train, cv=10)\n\n# using cross_validate\nscores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n# returns a dictionary with keys: ['fit_time', 'score_time', 'test_score', 'train_score']\npd.DataFrame(scores)\ncross-validation does not return a model. It is not a way to build a model that can be applied to new data. The purpose of cross-validation is to evaluate how well the model will generalize to unseen data."
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#fundamental-trade-off",
    "href": "block_2/571_sup_learn/571_sup_learn.html#fundamental-trade-off",
    "title": "Supervised Learning",
    "section": "",
    "text": "AKA the bias/ variance trade-off in supervised learning.\n\nBias: Tendency to consistently learn the same wrong thing (high bias = underfitting).\nVariance: Tenency to learn random things irrespective of the real signal (high variance = overfitting).\n\nAs you increase model complexity, \\(E_{train}\\) goes down but \\(E_{approx} = E_{D} - E_{train}\\) goes up."
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#baseline",
    "href": "block_2/571_sup_learn/571_sup_learn.html#baseline",
    "title": "Supervised Learning",
    "section": "Baseline",
    "text": "Baseline\n\nA simple, fast, and easily explainable model that is used as a starting point for a more sophisticated model.\n\nMost common baseline for classification is majority class classifier.\nMost common baseline for regression is mean predictor.\nin sklearn, DummyClassifier and DummyRegressor are used to create baselines."
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#decision-trees",
    "href": "block_2/571_sup_learn/571_sup_learn.html#decision-trees",
    "title": "Supervised Learning",
    "section": "Decision Trees",
    "text": "Decision Trees\n\nbasic idea: predict using a series of if-then-else questions\ndepth of tree: number of questions asked (hyperparameter)\ndecision boundary: region of feature space where all instances are assigned to the same class\ndecision stump: a decision tree with only one split (depth = 1)\n\n\n\n\n\n\n\n\nAdvantages\nDisadvantages\n\n\n\n\n- Easy to interpret and explain.\n- Biased with imbalanced datasets.\n\n\n- Can handle both numerical and categorical data.\n- Greedy splitting algorithm might not find the globally optimal tree.\n\n\n- Can handle multi-output problems.\n- Hard to learn the true relationship between features and target (can only ask yes/no questions).\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier(max_depth=3)  # create model object\nclf.fit(X_train, y_train)  # fit model on training data\nclf.score(X_test, y_test)  # score model on test data or use clf.predict(X_test)\n\ncan also use DecisionTreeRegressor for regression problems (continuous target)\n\ndifference:\n\nscore: returns R^2 score (1 is best, 0 is worst)\nleaf nodes: returns average of target values in leaf node\nDecisionTreeClassifier uses entropy and DecisionTreeRegressor uses variance"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#knn",
    "href": "block_2/571_sup_learn/571_sup_learn.html#knn",
    "title": "Supervised Learning",
    "section": "kNN",
    "text": "kNN\n\nkNN is a non-parametric model (no parameters to learn and stores all training data)\nkNN is a lazy learner (no training, just prediction)\n\nslow at prediction time\n\nkNN is a supervised model (needs labels)\nhyperparameters:\n\nk: number of neighbors to consider, smaller k means more complex decision boundary\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)\n\n\n\n\n\n\n\nPros of k-NNs for Supervised Learning\nCons of k-NNs for Supervised Learning\n\n\n\n\nEasy to understand, interpret.\nCan be potentially VERY slow during prediction time with a large training set.\n\n\nSimple hyperparameter (n_neighbors) controlling the tradeoff\nOften not great test accuracy compared to modern approaches.\n\n\nCan learn very complex functions given enough data.\nDoesn’t work well on datasets with many features or sparse datasets.\n\n\nLazy learning: Takes no time to fit\nFalls apart when # dimensions increase (curse of dimensionality)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#svm-rbf",
    "href": "block_2/571_sup_learn/571_sup_learn.html#svm-rbf",
    "title": "Supervised Learning",
    "section": "SVM RBF",
    "text": "SVM RBF\n\nSVM is a parametric model (needs to learn parameters)\n\nremembers the support vectors\nuses a kernel function to transform the data (RBF, Radial Basis Func, is the default)\n\nDecision boundary only depends on support vectors (smooth)\nhyperparameters:\n\nC: regularization parameter, larger C means more complex\ngamma: kernel coefficient, larger gamma means more complex\n\n\nfrom sklearn.svm import SVC\n\nsvm = SVC(C=10, gamma=0.1)\nsvm.fit(X_train, y_train)\nsvm.score(X_test, y_test)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#scaling-values-using-standardscaler",
    "href": "block_2/571_sup_learn/571_sup_learn.html#scaling-values-using-standardscaler",
    "title": "Supervised Learning",
    "section": "Scaling values using StandardScaler",
    "text": "Scaling values using StandardScaler\n\nin KNN, we need to scale the data (in classification, we don’t need to scale the data)\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()  # create feature trasformer object\nscaler.fit(X_train)  # fitting the transformer on the train split\nX_train_scaled = scaler.transform(X_train)  # transforming the train split\nX_test_scaled = scaler.transform(X_test)  # transforming the test split"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#address-missing-values-using-simpleimputer",
    "href": "block_2/571_sup_learn/571_sup_learn.html#address-missing-values-using-simpleimputer",
    "title": "Supervised Learning",
    "section": "Address missing values using SimpleImputer",
    "text": "Address missing values using SimpleImputer\n\nreplace all missing values with the mean/ median of the column\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy='median')  # create imputer object\nimputer.fit(X_train_num_only)  # fitting the imputer on the train split\nX_train_num_only_imputed = imputer.transform(X_train_num_only)  # transforming the train split\nX_test_num_only_imputed = imputer.transform(X_test_num_only)  # transforming the test split"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#address-cataegorical-values-using-onehotencoder",
    "href": "block_2/571_sup_learn/571_sup_learn.html#address-cataegorical-values-using-onehotencoder",
    "title": "Supervised Learning",
    "section": "Address cataegorical values using OneHotEncoder",
    "text": "Address cataegorical values using OneHotEncoder\n\nturn categorical values into one-hot encoding\nto get the column names, use get_feature_names(): encoder.get_feature_names().tolist()\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(handle_unknown='ignore')  # create encoder object\nencoder.fit(X_train_cat_only)  # fitting the encoder on the train split\nX_train_cat_only_encoded = encoder.transform(X_train_cat_only)  # transforming the train split\nX_test_cat_only_encoded = encoder.transform(X_test_cat_only)  # transforming the test split\n\nother arguments for OneHotEncoder:\n\nhandle_unknown='ignore' will ignore unknown categories\n\nif you don’t set this, you will get an error if there are unknown categories in the test set\n\nsparse_output=False will return a dense matrix instead of a sparse matrix\n\ndefault is sparse_output=True (returns a sparse matrix - only stores non-zero values)\n\ndrop=\"if_binary\" will drop one of the columns if there are only two categories\n\ndefault is drop=None (no columns are dropped)\ndrop=\"first\" will drop the first column\ndrop=[0, 2] will drop the first and third columns\n\n\n\n\nDiscretizing\n\ne.g turning age into age groups (e.g. child, adult, senior or 0-20, 20-40, 40-60, 60+)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#address-catagorical-values-using-ordinalencoder",
    "href": "block_2/571_sup_learn/571_sup_learn.html#address-catagorical-values-using-ordinalencoder",
    "title": "Supervised Learning",
    "section": "Address catagorical values using OrdinalEncoder",
    "text": "Address catagorical values using OrdinalEncoder\n\nturn categorical values into ordinal encoding (e.g. low, medium, high)\ndtype=int will make the output an integer\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\nordered_categories = ['low', 'medium', 'high']\nencoder = OrdinalEncoder(categories=[ordered_categories], dtype=int)  # create encoder object\nencoder.fit(X_train_cat_only)  # fitting the encoder on the train split\nX_train_cat_only_encoded = encoder.transform(X_train_cat_only)  # transforming the train split\nX_test_cat_only_encoded = encoder.transform(X_test_cat_only)  # transforming the test split"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#address-bag-of-words-using-countvectorizer",
    "href": "block_2/571_sup_learn/571_sup_learn.html#address-bag-of-words-using-countvectorizer",
    "title": "Supervised Learning",
    "section": "Address Bag of Words using CountVectorizer",
    "text": "Address Bag of Words using CountVectorizer\n\nturn a string of words into a vector of word counts (e.g., “white couch” -&gt; [“white”: 1, “couch”: 1])\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(stop_words='english')  # create vectorizer object\nX_train_text_only_vectorized = vectorizer.fit_transform(X_train_text_only)  # fitting and transforming the train split\nX_test_text_only_vectorized = vectorizer.transform(X_test_text_only)  # transforming the test split\n\nParameters:\n\nstop_words='english' will remove common English words (e.g., “the”, “a”, “an”, “and”, “or”, “but”, “not”)\n\ndefault is stop_words=None (no words are removed)\n\nmax_features=100 will only keep the 100 most common words\n\ndefault is max_features=None (all words are kept)\n\n\nhandles unknown words by ignoring them"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#sklearn-summary",
    "href": "block_2/571_sup_learn/571_sup_learn.html#sklearn-summary",
    "title": "Supervised Learning",
    "section": "sklearn summary",
    "text": "sklearn summary\n\n\n\n\n\n\n\n\n\nEstimators\nTransformers\n\n\n\n\nPurpose\nused to fit and predict\nused to change input data\n\n\nUsage\nNeed to fit X_train, y_train\nNeed to fit X_train (no y_train)\n\n\n\nCan score on X_test, y_test\nnothing to score\n\n\nExamples\n- DecisionTreeClassifier\n- StandardScaler\n\n\n\n- KNeighborsClassifier\n- SimpleImputer\n\n\n\n- LogisticRegression\n- OneHotEncoder\n\n\n\n- SVC\n- OrdinalEncoder\n\n\n\n\nDon’t fit with transformer then cross validate with estimator\n\nThis is data leakage (train is influenced by validation)\n\nsolution: Use Sklearn Pipeline!"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#pipeline",
    "href": "block_2/571_sup_learn/571_sup_learn.html#pipeline",
    "title": "Supervised Learning",
    "section": "Pipeline",
    "text": "Pipeline\n\nuse sklearn.pipeline.Pipeline\nmake a pipeline:\n\nfrom sklearn.pipeline import make_pipeline\n\npipe_knn = make_pipeline(\n    SimpleImputer(strategy='median'),\n    StandardScaler(),\n    KNeighborsClassifier(n_neighbors=10)\n)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#column-transformer",
    "href": "block_2/571_sup_learn/571_sup_learn.html#column-transformer",
    "title": "Supervised Learning",
    "section": "Column Transformer",
    "text": "Column Transformer\n\nThis is a transformer that can handle multiple columns\n\nfrom sklearn.compose import make_column_transformer\n\npreprocessor = make_column_transformer(\n    (make_pipeline(SimpleImputer(), StandardScaler()), numeric_feats),  # scaling on numeric features\n    (\"passthrough\", passthrough_feats),  # no transformations on the binary features\n    (OneHotEncoder(), categorical_feats),  # OHE on categorical features\n    # normally OHE is put at the end since it makes new cols\n    (\"drop\", drop_feats),  # drop the drop features\n)\n\nget names of transformers: preprocessor.named_transformers_\nget new column names: preprocessor.named_transformers_[\"onehotencoder\"].get_feature_names()"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#preprocessing-1",
    "href": "block_2/571_sup_learn/571_sup_learn.html#preprocessing-1",
    "title": "Supervised Learning",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nScaling values using StandardScaler\n\nin KNN, we need to scale the data (in classification, we don’t need to scale the data)\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()  # create feature trasformer object\nscaler.fit(X_train)  # fitting the transformer on the train split\nX_train_scaled = scaler.transform(X_train)  # transforming the train split\nX_test_scaled = scaler.transform(X_test)  # transforming the test split\n\n\nAddress missing values using SimpleImputer\n\nreplace all missing values with the mean/ median of the column\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy='median')  # create imputer object\nimputer.fit(X_train_num_only)  # fitting the imputer on the train split\nX_train_num_only_imputed = imputer.transform(X_train_num_only)  # transforming the train split\nX_test_num_only_imputed = imputer.transform(X_test_num_only)  # transforming the test split\n\n\nAddress cataegorical values using OneHotEncoder\n\nturn categorical values into one-hot encoding\nto get the column names, use get_feature_names(): encoder.get_feature_names().tolist()\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(handle_unknown='ignore')  # create encoder object\nencoder.fit(X_train_cat_only)  # fitting the encoder on the train split\nX_train_cat_only_encoded = encoder.transform(X_train_cat_only)  # transforming the train split\nX_test_cat_only_encoded = encoder.transform(X_test_cat_only)  # transforming the test split\n\nother arguments for OneHotEncoder:\n\nhandle_unknown='ignore' will ignore unknown categories\n\nif you don’t set this, you will get an error if there are unknown categories in the test set\n\nsparse_output=False will return a dense matrix instead of a sparse matrix\n\ndefault is sparse_output=True (returns a sparse matrix - only stores non-zero values)\n\ndrop=\"if_binary\" will drop one of the columns if there are only two categories\n\ndefault is drop=None (no columns are dropped)\ndrop=\"first\" will drop the first column\ndrop=[0, 2] will drop the first and third columns\n\n\n\n\nDiscretizing\n\ne.g turning age into age groups (e.g. child, adult, senior or 0-20, 20-40, 40-60, 60+)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#address-catagorical-values-using-ordinalencoder-1",
    "href": "block_2/571_sup_learn/571_sup_learn.html#address-catagorical-values-using-ordinalencoder-1",
    "title": "Supervised Learning",
    "section": "Address catagorical values using OrdinalEncoder",
    "text": "Address catagorical values using OrdinalEncoder\n\nturn categorical values into ordinal encoding (e.g. low, medium, high)\ndtype=int will make the output an integer\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\nordered_categories = ['low', 'medium', 'high']\nencoder = OrdinalEncoder(categories=[ordered_categories], dtype=int)  # create encoder object\nencoder.fit(X_train_cat_only)  # fitting the encoder on the train split\nX_train_cat_only_encoded = encoder.transform(X_train_cat_only)  # transforming the train split\nX_test_cat_only_encoded = encoder.transform(X_test_cat_only)  # transforming the test split\n\nAddress Bag of Words using CountVectorizer\n\nturn a string of words into a vector of word counts (e.g., “white couch” -&gt; [“white”: 1, “couch”: 1])\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(stop_words='english')  # create vectorizer object\nX_train_text_only_vectorized = vectorizer.fit_transform(X_train_text_only)  # fitting and transforming the train split\nX_test_text_only_vectorized = vectorizer.transform(X_test_text_only)  # transforming the test split\n\nParameters:\n\nstop_words='english' will remove common English words (e.g., “the”, “a”, “an”, “and”, “or”, “but”, “not”)\n\ndefault is stop_words=None (no words are removed)\n\nmax_features=100 will only keep the 100 most common words\n\ndefault is max_features=None (all words are kept)\n\nbinary=True will only keep 0 or 1 for each word (instead of the count)\n\ndefault is binary=False (the count is kept)\n\n\nhandles unknown words by ignoring them\n\n\nBreaking Golden Rule\n\nIf we know fixed categories (i.e., provinces in Canada), we can break the golden rule and pass the list of known/possible categories\n\n\n\n\nsklearn summary\n\n\n\n\n\n\n\n\n\nEstimators\nTransformers\n\n\n\n\nPurpose\nused to fit and predict\nused to change input data\n\n\nUsage\nNeed to fit X_train, y_train\nNeed to fit X_train (no y_train)\n\n\n\nCan score on X_test, y_test\nnothing to score\n\n\nExamples\n- DecisionTreeClassifier\n- StandardScaler\n\n\n\n- KNeighborsClassifier\n- SimpleImputer\n\n\n\n- LogisticRegression\n- OneHotEncoder\n\n\n\n- SVC\n- OrdinalEncoder\n\n\n\n\nDon’t fit with transformer then cross validate with estimator\n\nThis is data leakage (train is influenced by validation)\n\nsolution: Use Sklearn Pipeline!\n\n\n\nPipeline\n\nuse sklearn.pipeline.Pipeline\nmake a pipeline:\n\nfrom sklearn.pipeline import make_pipeline\n\npipe_knn = make_pipeline(\n    SimpleImputer(strategy='median'),\n    StandardScaler(),\n    KNeighborsClassifier(n_neighbors=10)\n)\n\nscores = cross_validate(pipe_knn, X_train, y_train, return_train_score=True)\npipe_knn.fit(X_train, y_train)\n\n\nColumn Transformer\n\nThis is a transformer that can handle multiple columns\n\nfrom sklearn.compose import make_column_transformer\n\npreprocessor = make_column_transformer(\n    (make_pipeline(SimpleImputer(), StandardScaler()), numeric_feats),  # scaling on numeric features\n    (\"passthrough\", passthrough_feats),  # no transformations on the binary features\n    (OneHotEncoder(), categorical_feats),  # OHE on categorical features\n    # normally OHE is put at the end since it makes new cols\n    (\"drop\", drop_feats),  # drop the drop features\n)\n\nget names of transformers: preprocessor.named_transformers_\nget new column names: preprocessor.named_transformers_[\"onehotencoder\"].get_feature_names()"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#hyperparamter-optimization",
    "href": "block_2/571_sup_learn/571_sup_learn.html#hyperparamter-optimization",
    "title": "Supervised Learning",
    "section": "Hyperparamter Optimization",
    "text": "Hyperparamter Optimization\n\nMethods\n\nManual\n\nTakes a lot of time\nintuition is not always correct\nsome hyperparameters work together\n\nAutomated\n\nGrid search\n\n\n\n\nGrid Search\n\nExhaustive search over specified parameter values for an estimator\n\nruns \\(n^m\\) CV for m hyperparameters and n values for each parameter\n\nAfter finding best parameter, it trains/fits the model on the whole training set\n\n\nParameters:\n\nGridSearchCV(estimator, param_grid, scoring=None, cv=None, n_jobs=None))\nestimator: estimator object\nparam_grid: dictionary with parameters names as keys and lists of parameter settings to try as values\n\nuses __ syntax to specify parameters of the estimator\ne.g:\n\ncolumntransformer__countvectorizer__max_features: max features of count vectorizer in column transformer\nsvc__gamma: gamma of SVC\n\n\nscoring: scoring method\ncv: cross-validation method\nn_jobs: number of jobs to run in parallel (-1 means use all processors)\n\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\npipe_svm = make_pipeline(preprocessor, SVC())\n\nparam_grid = {\n    \"columntransformer__countvectorizer__max_features\": [100, 200, 400, 800, 1000, 2000],\n    \"svc__gamma\": [0.001, 0.01, 0.1, 1.0, 10, 100],\n    \"svc__C\": [0.001, 0.01, 0.1, 1.0, 10, 100],\n}\n\n# Create a grid search object\ngs = GridSearchCV(pipe_svm, param_grid=param_grid, cv=5, n_jobs=-1)\ngs.fit(X_train, y_train)\n\ngs.best_score_ # returns the best score\ngs.best_params_ # returns the best parameters\n# Returns a dataframe of all the results\npd.DataFrame(random_search.cv_results_)[\n    [\n        \"mean_test_score\",\n        \"param_columntransformer__countvectorizer__max_features\",\n        \"param_svc__gamma\",\n        \"param_svc__C\",\n        \"mean_fit_time\",\n        \"rank_test_score\",\n    ]\n].set_index(\"rank_test_score\").sort_index().T\n\n# Can score on the test set\ngs.score(X_test, y_test)\n\n\n\nRandom Search\n\nPicks random values for the hyperparameters according to a distribution\nonly runs n_iter CV\n\n\nParameters\n\nRandomizedSearchCV(estimator, param_distributions, n_iter=10, scoring=None, cv=None, n_jobs=None))\nestimator: estimator object\nparam_distributions: dictionary with parameters names as keys and distributions or lists of parameters to try\n\ncan also pass param_grid from grid search (but does not exhaustively search)\n\nn_iter: number of parameter settings that are sampled\nscoring: scoring method\ncv: cross-validation method\nn_jobs: number of jobs to run in parallel (-1 means use all processors)\n\n\n\nCode\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {\n    \"columntransformer__countvectorizer__max_features\": randint(100, 2000),\n    \"svc__C\": uniform(0.1, 1e4),  # loguniform(1e-3, 1e3),\n    \"svc__gamma\": loguniform(1e-5, 1e3),\n}\n\nrs = RandomizedSearchCV(\n    pipe_svm,\n    param_distributions=param_dist,\n    n_iter=10,\n    cv=5,\n    n_jobs=-1,\n    random_state=42,\n)\n\n\n\nRandom vs Grid Search\n\nAdvantages of Random Search\n\nFaster\nAdding hyperparameters that do not influence the performance does not decrease the performance\nWorks better when some hyperparameters are more important than others\nrecommended more than grid search"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#naive-bayes",
    "href": "block_2/571_sup_learn/571_sup_learn.html#naive-bayes",
    "title": "Supervised Learning",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nBayes’ Theorem\n\\[ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\]\n\n\nBasic idea\n\nWe have a set of classes (e.g. spam or not spam)\nWe have a set of features (e.g. words in an email)\n\nWe want to find the probability of a class given a set of features.\n\\[ P(C|F_1, F_2, ..., F_n) = \\frac{P(F_1, F_2, ..., F_n|C)P(C)}{P(F_1, F_2, ..., F_n)} \\]\n\n\nNaive Bayes\n\nBag of words model (order of words doesn’t matter)\nAssume that all features are conditionally independent of each other (naive assumption)\nThis allows us to simplify the equation to:\n\n\\[ P(C|F*1, F_2, ..., F_n) \\approx \\frac{P(C)* \\prod P(F_i|C)}{P(F_1, F_2, ..., F_n)} \\]\n\n\nLaplace Smoothing\n\nIf a word is not in the training set, then the probability of that word given a class is 0\nThis will cause the entire probability to be 0\nWe can fix this by adding 1 to the numerator and adding the number of words to the denominator\n\n\\[ P(F_i|C) = \\frac{count(F_i, C) + 1}{count(C) + |V|} \\]\nwhere V is the number of possible word values\n\n\nSklearn Implementation\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\n\npipe_nb = make_pipeline(CountVectorizer(), MultinomialNB())\nresults_dict[\"Naive Bayes\"] = mean_std_cross_val_scores(\n    pipe_nb, X_train, y_train, return_train_score=True\n)\n\nMultinomialNB generally works better than BernoulliNB, especially for large text datasets\n\nBernoulliNB assumes that the features are binary (e.g. 0 or 1)\nMultinomialNB assumes that the features are counts (e.g. 0, 1, 2, 3, …)\n\nParameters:\n\nalpha is the Laplace smoothing parameter (actually hyperparameter), default is alpha=1.0\n\nHigh alpha means more smoothing =&gt; underfitting\nLow alpha means less smoothing =&gt; overfitting\n\n\n\n\n\nContinuous Features\n\nWe can use a Gaussian Naive Bayes model for continuous features\nThis assumes that the features are normally distributed\n\nIf not, can use sklearn.preprocessing.PowerTransformer to transform the data to be more normal\n\n\nfrom sklearn.naive_bayes import GaussianNB\n\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n\n# view\nmodel.theta_  # mean of each feature per class\nmodel.sigma_  # variance of each feature per class\nmodel.var_ # overall variance of each feature\nmodel.class_prior_  # prior probability of each class\n\nmodel.predict_proba(X_test)  # probability of each class"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#linear-models",
    "href": "block_2/571_sup_learn/571_sup_learn.html#linear-models",
    "title": "Supervised Learning",
    "section": "Linear Models",
    "text": "Linear Models\n\nmake predictions using a linear function of the input features\ndecision boundary is a hyperplane\n\nif 2d, decision boundary is a line\nuncertain near the decision boundary\n\nLimitations:\n\ncan only learn linear decision boundaries\ncan only learn linear functions of the input features\n\n\n\nLinear Regression\n\nMain idea: find the line that minimizes the sum of squared errors\nComponents:\n\ninput features (d features)\ncoefficients (d coefficients)\nintercept/ bias (1 intercept)\n\nNormally has d+1 parameters (one for each feature plus the intercept)\nMakes d-1 hyperplanes (separating lines) in d dimensions\nMore complex normally means the coefficients are larger\nRaw output score can be used to calculate probability score for a given prediction\nSCALING IS IMPORTANT\n\nIf features are on different scales, the coefficients will be on different scales\n\n\n\nRidge\n\\[ \\min\\_{w} ||Xw - y||\\_2^2 + \\alpha ||w||\\_2^2 \\]\n\nL2 regularization\nHyperparameters:\n\nalpha: regularization strength\n\nlarger values =&gt; more regularization =&gt; simpler model =&gt; underfitting\nmore regularization =&gt; smaller coefficients =&gt; less sensitive to changes in input features (outliers)\n\n\n\nfrom sklearn.linear_model import Ridge\n\npipe = make_pipeline(StandardScaler(), Ridge())\nscores = cross_validate(pipe, X_train, y_train, return_train_score=True)\n\ncoeffs = pipe_ridge.named_steps[\"ridge\"].coef_ # view coefficients\n# coeffs.shape = (n_features,), one coefficient for each feature\nintercept = pipe_ridge.named_steps[\"ridge\"].intercept_ # view intercept/ bias\n\n\nLaso\n\\[ \\min\\_{w} ||Xw - y||\\_2^2 + \\alpha ||w||\\_1 \\]\n\nL1 regularization\nHyperparameters:\n\nalpha: regularization strength\n\nlarger values =&gt; more regularization =&gt; simpler model =&gt; underfitting\n\n\n\n\n\n\nLogistic Regression\n\nMain idea: use linear regression to predict the probability of an event\nApplies a “threshold” to the raw output score to make a prediction -&gt; decides whether to predict 0/1 or -1/1\nComponents:\n\ninput features (d features)\ncoefficients (d coefficients)\nintercept/ bias (1 intercept)\nthreshold r (1 threshold)\n\nHyperparameters:\n\nC: inverse of regularization strength\n\nlarger values =&gt; less regularization =&gt; more complex model =&gt; overfitting\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nscores = cross_validate(lr, X_train, y_train, return_train_score=True)\n\n# access coefficients and intercept\nlr.coef_ # shape = (n_classes, n_features)\nlr.intercept_ # shape = (n_classes,)\n\nlr.classes_ # array of classes\n\npredict_proba returns the probability of each class\n\nfor binary classification, returns both classes (although one is redundant)\nbased on the order of lr.classes_\nsum of probabilities for each sample is 1\n\npredict returns the class with the highest probability\n\n\nsigmoid function\n\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\]\n\nturns -inf to 0 and inf to 1 \n\n\n\n\nLinear SVM\n\nMain idea: find the line that maximizes the margin between the decision boundary and the closest points\n\nfrom sklearn.svm import SVC\n\nlinear_svc = SVC(kernel=\"linear\")\nscores = cross_validate(linear_svc, X_train, y_train, return_train_score=True)"
  },
  {
    "objectID": "block_2/571_sup_learn/571_sup_learn.html#multi-class-meta-strategies",
    "href": "block_2/571_sup_learn/571_sup_learn.html#multi-class-meta-strategies",
    "title": "Supervised Learning",
    "section": "Multi-class, meta-strategies",
    "text": "Multi-class, meta-strategies\n\nCan do multiclass naturally: KNN, decision trees\n2 hacky ways to use binary classifiers for multi-class classification:\n\n\nOne-vs-Rest (OVR)\n\nTrain a binary classifier for each class\n\ncreates binary linear classifiers separating each class from the rest (i.e blue vs rest, red vs rest, green vs rest)\n\nClassify by choosing the class with the highest probability\n\n\ne.g. a point on (0, -5) would get:\n\nlr.coef_ would give 3x2 array (3 classes, 2 features)\nlr.intercept_ would give 3x1 array (3 classes, 1 intercept)\nGet score with test_points[4]@lr.coef_.T + lr.intercept_ return array size 3, choose the class with the highest score\n\n\n\nOne-vs-One (OVO)\n\nTrain a binary classifier for each pair of classes\n\ncreates binary linear classifiers separating each class from each other class (i.e blue vs red, blue vs green, red vs green)\ntrains \\(\\frac{n(n-1)}{2}\\) classifiers\n\ncount the number of times each class wins\nClassify by choosing the class with the most wins\n\n\n\nUsing this in Python\nfrom sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n\nmodel = OneVsOneClassifier(LogisticRegression())\n%timeit model.fit(X_train_multi, y_train_multi);\n\nmodel = OneVsRestClassifier(LogisticRegression())\n%timeit model.fit(X_train_multi, y_train_multi);"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html",
    "href": "block_2/512_algs_ds/512_algs_ds.html",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Big O\nname\nchange in runtime if I double \\(n\\)?\n\n\n\n\n\\(O(1)\\)\nconstant\nsame\n\n\n\\(O(\\log n)\\)\nlogarithmic\nincreased by a constant\n\n\n\\(O(\\sqrt{n})\\)\nsquare root\nincreased by roughly 1.4x\n\n\n\\(O(n)\\)\nlinear\n2x\n\n\n\\(O(n \\log n)\\)\nlinearithmic\nroughly 2x\n\n\n\\(O(n^2)\\)\nquadratic\n4x\n\n\n\\(O(n^3)\\)\ncubic\n8x\n\n\n\\(O(n^k)\\)\npolynomial\nincrease by a factor of \\(2^k\\)\n\n\n\\(O(2^n)\\)\nexponential\nsquared\n\n\n\nsorted from fastest to slowest\nIf mult/ div by contant c: \\(log_c(n)\\) e.g. for (int i = 0; i &lt; n; i *= 2) If add/ sub by constant c: \\(n/c\\) e.g. for (int i = 0; i &lt; n; i += 2)\n\nWe write \\(O(f(n))\\) for some function \\(f(n)\\).\nYou get the doubling time by taking \\(f(2n)/f(n)\\).\nE.g. if \\(f(n)=n^3\\), then \\(f(2n)/f(n)=(2n)^3/n^3=8\\).\n\nSo if you double \\(n\\), the running time goes up 8x.\n\nFor \\(O(2^n)\\), increasing \\(n\\) by 1 causes the runtime to double!\n\nNote: these are common cases of big O, but this list is not exhaustive.\n\n\n\n\n\n\nSpace complexity is the amount of memory used by an algorithm.\nWe can use big O notation to describe space complexity.\n\n\n\n\nrange() is a generator, so it doesn’t take up memory\nlist(range()) is a list, so it takes up memory\nnp.arange() is an array, so it takes up memory\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nLinear Search\nBinary Search\n\n\n\n\nPrinciple\nSequentially checks each element until a match is found or end is reached.\nRepeatedly divides in half the portion of the list that could contain the item until you’ve narrowed down the possible locations to just one.\n\n\nBest-case Time Complexity\n(O(1))\n(O(1))\n\n\nSpace Complexity\n(O(1))\n(O(1))\n\n\nWorks on\nUnsorted and sorted lists\nSorted lists only\n\n\n\n\n\ndef linear_search(arr, x):\n    for i in range(len(arr)):\n        if arr[i] == x:\n            return i\n    return -1  # not found\n\n# Example usage:\narr = [10, 20, 80, 30, 60, 50, 110, 100, 130, 170]\nx = 110\nresult = linear_search(arr, x)\nprint(\"Element is present at index\" if result != -1 else \"Element is not present in array\", result)\n\n\n\ndef binary_search(arr, l, r, x):\n    while l &lt;= r:\n        mid = l + (r - l) // 2\n        # Check if x is present at mid\n        if arr[mid] == x:\n            return mid\n        # If x is greater, ignore left half\n        elif arr[mid] &lt; x:\n            l = mid + 1\n        # If x is smaller, ignore right half\n        else:\n            r = mid - 1\n    # Element was not present\n    return -1\n\n# Example usage:\narr = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nx = 70\nresult = binary_search(arr, 0, len(arr)-1, x)\nprint(\"Element is present at index\" if result != -1 else \"Element is not present in array\", result)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nWorst-case Time Complexity\nSpace Complexity\nDescription\nViz\n\n\n\n\nInsertion Sort\n(O(n^2))\n(O(1))\nBuilds the final sorted list one item at a time. It takes one input element per iteration and finds its correct position in the sorted list.\n\n\n\nSelection Sort\n(O(n^2))\n(O(1))\nDivides the input list into two parts: a sorted and an unsorted sublist. It repeatedly selects the smallest (or largest) element from the unsorted sublist and moves it to the end of the sorted sublist.\n\n\n\nBubble Sort\n(O(n^2))\n(O(1))\nRepeatedly steps through the list, compares adjacent elements and swaps them if they are in the wrong order. The process is repeated for each item.\n\n\n\nMerge Sort\n(O(n n))\n(O(n))\nDivides the unsorted list into n sublists, each containing one element, then repeatedly merges sublists to produce new sorted sublists until there is only one sublist remaining.\n\n\n\nHeap Sort\n(O(n n))\n(O(1))\nConverts the input data into a heap data structure. It then extracts the topmost element (max or min) and reconstructs the heap, repeating this process until the heap is empty.\n\n\n\n\ngifs from https://emre.me/algorithms/sorting-algorithms/\n\n\n\n\n\n\nHashing is a technique that is used to uniquely identify a specific object from a group of similar objects.\nin python: hash()\nonly immutable objects can be hashed\n\nlists, sets, and dictionaries are mutable and cannot be hashed\ntuples are immutable and can be hashed\n\n\n\n\n\n\nCreating dictionaries:\n\nx = {}\n\nx = {'a': 1, 'b': 2}\n\nx = dict()\n\nx = dict(a=1, b=2)\nx = dict([('a', 1), ('b', 2)])\nx = dict(zip(['a', 'b'], [1, 2]))\nx = dict({'a': 1, 'b': 2})\n\n\nAccessing values:\n\nx['a']: if key is not found, raises KeyError\nx.get('a', 0): returns 0 if key is not found\n\n\n\n\n\n\ndefaultdict is a subclass of dict that returns a default value when a key is not found\n\nfrom collections import defaultdict\nd = defaultdict(int)\n\nd['a'] returns 0\n\nd = defaultdict(list)\n\nd['a'] returns []\nnot list() because list() is a function that returns an empty list, list is a type\n\nd = defaultdict(set)\n\nd['a'] returns set()\n\nd = defaultdict(lambda: \"hello I am your friendly neighbourhood default value\")\n\nd['a'] returns \"hello I am your friendly neighbourhood default value\"\n\n\n\n\n\n\n\nCounter is a subclass of dict that counts the number of occurrences of an element in a list\n\nfrom collections import Counter\nc = Counter(['a', 'b', 'c', 'a', 'b', 'b'])\n\nc['a'] returns 2\nc['b'] returns 3\nc['c'] returns 1\nc['d'] returns 0\n\nc = Counter({'a': 2, 'b': 3, 'c': 1})\nc = Counter(a=2, b=3, c=1)\n\nother functions:\n\nc.most_common(2) returns the 2 most common elements in the list: [('b', 3), ('a', 2)]\n\n\n\n\n\n\n\ncontains vertices (or nodes) and edges\nuse networkx to create graphs in Python\n\nimport networkx as nx\n\nG = nx.Graph()  # create empty graph\nG.add_node(\"YVR\")  # add node 1\nG.add_node(\"YYZ\")  # add node 2\nG.add_node(\"YUL\")  # add node 3\n\nG.add_edge(\"YVR\", \"YYZ\", weight=4)  # add edge between node 1 and node 2\nG.add_edge(\"YVR\", \"YUL\", weight=5)  # add edge between node 1 and node 3\n\nnx.draw(G, with_labels=True) # draw graph but random layout\nnx.draw(G, with_labels=True, pos=nx.spring_layout(G, seed=5)) # not random layout\n\n\n\ndirected graph: edges have direction\n\nnx.DiGraph()\n\n\n\n\n\n\nDegree: number of edges connected to a node\nPath: sequence of nodes connected by edges\nConnected: graph where there is a path between every pair of nodes\nComponent: connected subgraph\n\n\n\n\n\n\n\ndef bfs(graph, start, search):\n    # graph: networkx graph\n\n    visited = set()\n    queue = [start]\n    while queue:\n        node = queue.pop(0)\n        if node in visited:\n            continue\n        visited.add(node)\n        if node == search:\n            return True\n        for neighbour in g.neighbors(node):\n            queue.append(neighbour)\n\n    return False\n\n\n\nFIFO (first in first out)\n\nqueue = []\nqueue.append(1)  # add to end of queue\nqueue.append(2)  # add to end of queue\nqueue.pop(0)  # remove from front of queue\n\n\n\n\n\n\n\nLIFO (last in first out)\n\nstack = []\nstack.append(1)  # add to end of stack\nstack.append(2)  # add to end of stack\nstack.pop()  # remove from end of stack\n\n\n\n\n\nAdjacency List\n\nlists of all the edges in the graph\nspace complexity: O(E)\nstill need to store all the nodes\n\nAdjacency Matrix\n\nmatrix of 0s and 1s with size V x V\ndense matrix space complexity: O(V^2)\nsparse matrix space complexity: O(E)\n\nCan do both directed and undirected graphs\n\n\nnetworkx uses scipy sparse matrix\n\ncreate sparse matrix: scipy.sparse.csr_matrix(x)\nneed to acces using matrix[row, col]\n\nmatrix[row][col] will not work, in np it will work\n\nto sum all the rows: matrix.sum(axis=1)\n\ncannot do np.sum(matrix, axis=1)\nor do matrix.getnnz(axis=1) to get number of non-zero elements in each row\n\nto find vertex: np.argmax(matrix.getnnz(axis=1))\nto find # max edges: np.max(matrix.getnnz(axis=1))\n\n\n\n\n\n\n\n\n\nLinear programming is a method to achieve the best outcome (e.g. maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships\n\n\n\n\nTo specify an optimization problem, we need to specify a few things:\n\nA specification of the space of possible inputs.\nAn objective function which takes an input and computes a score.\n(Optional) A set of constraints, which take an input and return true/false.\n\ncan only get same or worse score than not having constraints\n\nAre we maximizing or minimizing?\n\n\n\n\n\nPuLP is an LP modeler written in Python\ncontinuous problems are easier to solve than discrete problems\n\ndiscrete problems might not be “optimal”\n\nExample of discrete problem: assigning TAs to courses\n\n# Define the problem\nprob = pulp.LpProblem(\"TA-assignments\", pulp.LpMaximize)\n\n# Define the variables\nx = pulp.LpVariable.dicts(\"x\", (TAs, courses), 0, 1, pulp.LpInteger)\n\n# add constraints\nfor course in courses:\n    prob += pulp.lpSum(x[ta][course] for ta in TAs) == 1 # += adds constraint\n\n# add objective\nprob += pulp.lpSum(x[ta][course] * happiness[ta][course] for ta in TAs for course in courses)\n\n# solve\nprob.solve()\n# prob.solve(pulp.apis.PULP_CBC_CMD(msg=0)) # to suppress output\n\n# check status\npulp.LpStatus[prob.status] # 'Optimal'\n\n# print results\nfor ta in TAs:\n    for course in courses:\n        if x[ta][course].value() == 1.0:\n            print(f\"{ta} is assigned to {course}\")\n\n\n\n\nDynamic programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions using a memory-based data structure (array, map, etc).\nDynamic programming only works on problems with optimal substructure and overlapping subproblems.\n\noptimal substructure: optimal solution can be constructed from optimal solutions of its subproblems\noverlapping subproblems: subproblems recur many times\n\nDynamic programming is usually applied to optimization problems."
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#big-o-notation",
    "href": "block_2/512_algs_ds/512_algs_ds.html#big-o-notation",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Big O\nname\nchange in runtime if I double \\(n\\)?\n\n\n\n\n\\(O(1)\\)\nconstant\nsame\n\n\n\\(O(\\log n)\\)\nlogarithmic\nincreased by a constant\n\n\n\\(O(\\sqrt{n})\\)\nsquare root\nincreased by roughly 1.4x\n\n\n\\(O(n)\\)\nlinear\n2x\n\n\n\\(O(n \\log n)\\)\nlinearithmic\nroughly 2x\n\n\n\\(O(n^2)\\)\nquadratic\n4x\n\n\n\\(O(n^3)\\)\ncubic\n8x\n\n\n\\(O(n^k)\\)\npolynomial\nincrease by a factor of \\(2^k\\)\n\n\n\\(O(2^n)\\)\nexponential\nsquared\n\n\n\nsorted from fastest to slowest\nIf mult/ div by contant c: \\(log_c(n)\\) e.g. for (int i = 0; i &lt; n; i *= 2) If add/ sub by constant c: \\(n/c\\) e.g. for (int i = 0; i &lt; n; i += 2)\n\nWe write \\(O(f(n))\\) for some function \\(f(n)\\).\nYou get the doubling time by taking \\(f(2n)/f(n)\\).\nE.g. if \\(f(n)=n^3\\), then \\(f(2n)/f(n)=(2n)^3/n^3=8\\).\n\nSo if you double \\(n\\), the running time goes up 8x.\n\nFor \\(O(2^n)\\), increasing \\(n\\) by 1 causes the runtime to double!\n\nNote: these are common cases of big O, but this list is not exhaustive."
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#space-complexity",
    "href": "block_2/512_algs_ds/512_algs_ds.html#space-complexity",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Space complexity is the amount of memory used by an algorithm.\nWe can use big O notation to describe space complexity.\n\n\n\n\nrange() is a generator, so it doesn’t take up memory\nlist(range()) is a list, so it takes up memory\nnp.arange() is an array, so it takes up memory"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#searching",
    "href": "block_2/512_algs_ds/512_algs_ds.html#searching",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Feature\nLinear Search\nBinary Search\n\n\n\n\nPrinciple\nSequentially checks each element until a match is found or end is reached.\nRepeatedly divides in half the portion of the list that could contain the item until you’ve narrowed down the possible locations to just one.\n\n\nBest-case Time Complexity\n(O(1))\n(O(1))\n\n\nSpace Complexity\n(O(1))\n(O(1))\n\n\nWorks on\nUnsorted and sorted lists\nSorted lists only\n\n\n\n\n\ndef linear_search(arr, x):\n    for i in range(len(arr)):\n        if arr[i] == x:\n            return i\n    return -1  # not found\n\n# Example usage:\narr = [10, 20, 80, 30, 60, 50, 110, 100, 130, 170]\nx = 110\nresult = linear_search(arr, x)\nprint(\"Element is present at index\" if result != -1 else \"Element is not present in array\", result)\n\n\n\ndef binary_search(arr, l, r, x):\n    while l &lt;= r:\n        mid = l + (r - l) // 2\n        # Check if x is present at mid\n        if arr[mid] == x:\n            return mid\n        # If x is greater, ignore left half\n        elif arr[mid] &lt; x:\n            l = mid + 1\n        # If x is smaller, ignore right half\n        else:\n            r = mid - 1\n    # Element was not present\n    return -1\n\n# Example usage:\narr = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nx = 70\nresult = binary_search(arr, 0, len(arr)-1, x)\nprint(\"Element is present at index\" if result != -1 else \"Element is not present in array\", result)"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#sorting",
    "href": "block_2/512_algs_ds/512_algs_ds.html#sorting",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Algorithm\nWorst-case Time Complexity\nSpace Complexity\nDescription\nViz\n\n\n\n\nInsertion Sort\n(O(n^2))\n(O(1))\nBuilds the final sorted list one item at a time. It takes one input element per iteration and finds its correct position in the sorted list.\n\n\n\nSelection Sort\n(O(n^2))\n(O(1))\nDivides the input list into two parts: a sorted and an unsorted sublist. It repeatedly selects the smallest (or largest) element from the unsorted sublist and moves it to the end of the sorted sublist.\n\n\n\nBubble Sort\n(O(n^2))\n(O(1))\nRepeatedly steps through the list, compares adjacent elements and swaps them if they are in the wrong order. The process is repeated for each item.\n\n\n\nMerge Sort\n(O(n n))\n(O(n))\nDivides the unsorted list into n sublists, each containing one element, then repeatedly merges sublists to produce new sorted sublists until there is only one sublist remaining.\n\n\n\nHeap Sort\n(O(n n))\n(O(1))\nConverts the input data into a heap data structure. It then extracts the topmost element (max or min) and reconstructs the heap, repeating this process until the heap is empty.\n\n\n\n\ngifs from https://emre.me/algorithms/sorting-algorithms/"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#hashmap",
    "href": "block_2/512_algs_ds/512_algs_ds.html#hashmap",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Hashing is a technique that is used to uniquely identify a specific object from a group of similar objects.\nin python: hash()\nonly immutable objects can be hashed\n\nlists, sets, and dictionaries are mutable and cannot be hashed\ntuples are immutable and can be hashed\n\n\n\n\n\n\nCreating dictionaries:\n\nx = {}\n\nx = {'a': 1, 'b': 2}\n\nx = dict()\n\nx = dict(a=1, b=2)\nx = dict([('a', 1), ('b', 2)])\nx = dict(zip(['a', 'b'], [1, 2]))\nx = dict({'a': 1, 'b': 2})\n\n\nAccessing values:\n\nx['a']: if key is not found, raises KeyError\nx.get('a', 0): returns 0 if key is not found\n\n\n\n\n\n\ndefaultdict is a subclass of dict that returns a default value when a key is not found\n\nfrom collections import defaultdict\nd = defaultdict(int)\n\nd['a'] returns 0\n\nd = defaultdict(list)\n\nd['a'] returns []\nnot list() because list() is a function that returns an empty list, list is a type\n\nd = defaultdict(set)\n\nd['a'] returns set()\n\nd = defaultdict(lambda: \"hello I am your friendly neighbourhood default value\")\n\nd['a'] returns \"hello I am your friendly neighbourhood default value\"\n\n\n\n\n\n\n\nCounter is a subclass of dict that counts the number of occurrences of an element in a list\n\nfrom collections import Counter\nc = Counter(['a', 'b', 'c', 'a', 'b', 'b'])\n\nc['a'] returns 2\nc['b'] returns 3\nc['c'] returns 1\nc['d'] returns 0\n\nc = Counter({'a': 2, 'b': 3, 'c': 1})\nc = Counter(a=2, b=3, c=1)\n\nother functions:\n\nc.most_common(2) returns the 2 most common elements in the list: [('b', 3), ('a', 2)]"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#graphs",
    "href": "block_2/512_algs_ds/512_algs_ds.html#graphs",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "contains vertices (or nodes) and edges\nuse networkx to create graphs in Python\n\nimport networkx as nx\n\nG = nx.Graph()  # create empty graph\nG.add_node(\"YVR\")  # add node 1\nG.add_node(\"YYZ\")  # add node 2\nG.add_node(\"YUL\")  # add node 3\n\nG.add_edge(\"YVR\", \"YYZ\", weight=4)  # add edge between node 1 and node 2\nG.add_edge(\"YVR\", \"YUL\", weight=5)  # add edge between node 1 and node 3\n\nnx.draw(G, with_labels=True) # draw graph but random layout\nnx.draw(G, with_labels=True, pos=nx.spring_layout(G, seed=5)) # not random layout\n\n\n\ndirected graph: edges have direction\n\nnx.DiGraph()\n\n\n\n\n\n\nDegree: number of edges connected to a node\nPath: sequence of nodes connected by edges\nConnected: graph where there is a path between every pair of nodes\nComponent: connected subgraph"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#graph-searching",
    "href": "block_2/512_algs_ds/512_algs_ds.html#graph-searching",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "def bfs(graph, start, search):\n    # graph: networkx graph\n\n    visited = set()\n    queue = [start]\n    while queue:\n        node = queue.pop(0)\n        if node in visited:\n            continue\n        visited.add(node)\n        if node == search:\n            return True\n        for neighbour in g.neighbors(node):\n            queue.append(neighbour)\n\n    return False\n\n\n\nFIFO (first in first out)\n\nqueue = []\nqueue.append(1)  # add to end of queue\nqueue.append(2)  # add to end of queue\nqueue.pop(0)  # remove from front of queue\n\n\n\n\n\n\n\nLIFO (last in first out)\n\nstack = []\nstack.append(1)  # add to end of stack\nstack.append(2)  # add to end of stack\nstack.pop()  # remove from end of stack\n\n\n\n\n\nAdjacency List\n\nlists of all the edges in the graph\nspace complexity: O(E)\nstill need to store all the nodes\n\nAdjacency Matrix\n\nmatrix of 0s and 1s with size V x V\ndense matrix space complexity: O(V^2)\nsparse matrix space complexity: O(E)\n\nCan do both directed and undirected graphs\n\n\nnetworkx uses scipy sparse matrix\n\ncreate sparse matrix: scipy.sparse.csr_matrix(x)\nneed to acces using matrix[row, col]\n\nmatrix[row][col] will not work, in np it will work\n\nto sum all the rows: matrix.sum(axis=1)\n\ncannot do np.sum(matrix, axis=1)\nor do matrix.getnnz(axis=1) to get number of non-zero elements in each row\n\nto find vertex: np.argmax(matrix.getnnz(axis=1))\nto find # max edges: np.max(matrix.getnnz(axis=1))"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#linear-programming",
    "href": "block_2/512_algs_ds/512_algs_ds.html#linear-programming",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Linear programming is a method to achieve the best outcome (e.g. maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#defining-the-problem",
    "href": "block_2/512_algs_ds/512_algs_ds.html#defining-the-problem",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "To specify an optimization problem, we need to specify a few things:\n\nA specification of the space of possible inputs.\nAn objective function which takes an input and computes a score.\n(Optional) A set of constraints, which take an input and return true/false.\n\ncan only get same or worse score than not having constraints\n\nAre we maximizing or minimizing?"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#using-python-pulp",
    "href": "block_2/512_algs_ds/512_algs_ds.html#using-python-pulp",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "PuLP is an LP modeler written in Python\ncontinuous problems are easier to solve than discrete problems\n\ndiscrete problems might not be “optimal”\n\nExample of discrete problem: assigning TAs to courses\n\n# Define the problem\nprob = pulp.LpProblem(\"TA-assignments\", pulp.LpMaximize)\n\n# Define the variables\nx = pulp.LpVariable.dicts(\"x\", (TAs, courses), 0, 1, pulp.LpInteger)\n\n# add constraints\nfor course in courses:\n    prob += pulp.lpSum(x[ta][course] for ta in TAs) == 1 # += adds constraint\n\n# add objective\nprob += pulp.lpSum(x[ta][course] * happiness[ta][course] for ta in TAs for course in courses)\n\n# solve\nprob.solve()\n# prob.solve(pulp.apis.PULP_CBC_CMD(msg=0)) # to suppress output\n\n# check status\npulp.LpStatus[prob.status] # 'Optimal'\n\n# print results\nfor ta in TAs:\n    for course in courses:\n        if x[ta][course].value() == 1.0:\n            print(f\"{ta} is assigned to {course}\")"
  },
  {
    "objectID": "block_2/512_algs_ds/512_algs_ds.html#dynamic-programming",
    "href": "block_2/512_algs_ds/512_algs_ds.html#dynamic-programming",
    "title": "Algorithms and Data Structures",
    "section": "",
    "text": "Dynamic programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions using a memory-based data structure (array, map, etc).\nDynamic programming only works on problems with optimal substructure and overlapping subproblems.\n\noptimal substructure: optimal solution can be constructed from optimal solutions of its subproblems\noverlapping subproblems: subproblems recur many times\n\nDynamic programming is usually applied to optimization problems."
  },
  {
    "objectID": "block_3/522_workflows/522_workflows.html",
    "href": "block_3/522_workflows/522_workflows.html",
    "title": "Workflows",
    "section": "",
    "text": "Link: https://github.com/UBC-MDS/english-score-predictor\n\n\n\n\n\nhttps://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008316#sec021\n\nBase Your Image on a Minimal Official Docker Image\n\nUse minimal and official base images for simplicity and security.\n\nTag Images with Explicit Versions\n\nExplicitly tag image versions in FROM instructions for reproducibility.\n\nWrite Instructions in the Right Order\n\nOptimize Dockerfile instructions order for Docker’s layer caching.\n\nDocument the Build Context\n\nClearly document the process to rebuild the image.\n\nSpecify Software Versions\n\nUse version pinning for software installations.\n\nUse Version Control\n\nMaintain Dockerfiles in version control systems.\n\nMount Datasets at Run Time\n\nStore large datasets outside the container and mount them at runtime.\n\nMake the Image One-Click Runnable\n\nEnsure the container is easily runnable with sensible defaults.\n\nOrder the Instructions\n\nOrder Dockerfile instructions from least to most likely to change.\n\nRegularly Use and Rebuild Containers\n\nRegularly use and update containers to identify and fix issues early.\n\n\n\n\n\n\ndocker build -t &lt;image_name&gt; .: build an image from a Dockerfile\ndocker run --rm -it &lt;image_name&gt; bash: run a container from an image\n\n-it: interactive mode\n--rm: remove the container after exiting\n\ndocker run --rm -it -v &lt;host_dir&gt;:&lt;container_dir&gt; &lt;image_name&gt; bash: run a container from an image and mount a host directory to a container directory\nexit: exit a container\n\n\n\ndocker images: list all images\ndocker rmi &lt;image_name&gt;: remove an image\ndocker ps -a: list all containers\ndocker rm &lt;container_name&gt;: remove a container\ndocker compose up: run a container from a docker-compose.yml file\n\nsimilar to conda env create -f environment.yml and conda activate &lt;env_name&gt;\n\n\n\n\n\n\nCommon structure:\n\nFROM &lt;base_image&gt;\nRUN &lt;command&gt;\nCOPY &lt;host_dir&gt; &lt;container_dir&gt;\nWORKDIR &lt;container_dir&gt;\nCMD &lt;command&gt;\n\n\nFROM ubuntu:18.04\n\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    cmake \\\n    git \\\n    wget \\\n    unzip \\\n    yasm \\\n    pkg-config \\\n    libswscale-dev \\\n    libtbb2 \\\n    libtbb-dev \\\n    libjpeg-dev \\\n    libpng-dev \\\n    libtiff-dev \\\n    libavformat-dev \\\n    libpq-dev```\ndocker compose run --rm &lt;service_name&gt; bash: run a container from a docker-compose.yml file and mount a host directory to a container directory"
  },
  {
    "objectID": "block_3/522_workflows/522_workflows.html#project",
    "href": "block_3/522_workflows/522_workflows.html#project",
    "title": "Workflows",
    "section": "",
    "text": "Link: https://github.com/UBC-MDS/english-score-predictor"
  },
  {
    "objectID": "block_3/522_workflows/522_workflows.html#docker",
    "href": "block_3/522_workflows/522_workflows.html#docker",
    "title": "Workflows",
    "section": "",
    "text": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008316#sec021\n\nBase Your Image on a Minimal Official Docker Image\n\nUse minimal and official base images for simplicity and security.\n\nTag Images with Explicit Versions\n\nExplicitly tag image versions in FROM instructions for reproducibility.\n\nWrite Instructions in the Right Order\n\nOptimize Dockerfile instructions order for Docker’s layer caching.\n\nDocument the Build Context\n\nClearly document the process to rebuild the image.\n\nSpecify Software Versions\n\nUse version pinning for software installations.\n\nUse Version Control\n\nMaintain Dockerfiles in version control systems.\n\nMount Datasets at Run Time\n\nStore large datasets outside the container and mount them at runtime.\n\nMake the Image One-Click Runnable\n\nEnsure the container is easily runnable with sensible defaults.\n\nOrder the Instructions\n\nOrder Dockerfile instructions from least to most likely to change.\n\nRegularly Use and Rebuild Containers\n\nRegularly use and update containers to identify and fix issues early.\n\n\n\n\n\n\ndocker build -t &lt;image_name&gt; .: build an image from a Dockerfile\ndocker run --rm -it &lt;image_name&gt; bash: run a container from an image\n\n-it: interactive mode\n--rm: remove the container after exiting\n\ndocker run --rm -it -v &lt;host_dir&gt;:&lt;container_dir&gt; &lt;image_name&gt; bash: run a container from an image and mount a host directory to a container directory\nexit: exit a container\n\n\n\ndocker images: list all images\ndocker rmi &lt;image_name&gt;: remove an image\ndocker ps -a: list all containers\ndocker rm &lt;container_name&gt;: remove a container\ndocker compose up: run a container from a docker-compose.yml file\n\nsimilar to conda env create -f environment.yml and conda activate &lt;env_name&gt;\n\n\n\n\n\n\nCommon structure:\n\nFROM &lt;base_image&gt;\nRUN &lt;command&gt;\nCOPY &lt;host_dir&gt; &lt;container_dir&gt;\nWORKDIR &lt;container_dir&gt;\nCMD &lt;command&gt;\n\n\nFROM ubuntu:18.04\n\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    cmake \\\n    git \\\n    wget \\\n    unzip \\\n    yasm \\\n    pkg-config \\\n    libswscale-dev \\\n    libtbb2 \\\n    libtbb-dev \\\n    libjpeg-dev \\\n    libpng-dev \\\n    libtiff-dev \\\n    libavformat-dev \\\n    libpq-dev```\ndocker compose run --rm &lt;service_name&gt; bash: run a container from a docker-compose.yml file and mount a host directory to a container directory"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html",
    "href": "block_3/513_databases/513_databases.html",
    "title": "Relational Databases",
    "section": "",
    "text": "What is a database?\n\nOrganized collection of related data\n\nWhart is a database management system (DBMS)?\n\nCollection of programs that enables users to create and maintain a database\nallows users to create, query, modify, and manage\n\n\nDATABASE != DBMS\n\n\n\nEfficiency: Data is stored efficiently.\nIntegrity: Data is consistent and correct.\nSecurity: Data is safe from unauthorized access.\nConcurrent Access: Multiple users can access the same data at the same time.\nCrash Recovery: Data is safe from crashes.\nIndependence: Data is independent of the programs that use it.\n\n\n\n\n\n\nWorks with entities and relationships\nEntity: a thing or object in the real world that is distinguishable from other objects\n\ne.g. students in a school\n\nRelationship: an association among entities\n\ne.g. a student is enrolled in a course\n\n\n\n\n\n\nCollection of relations\nA relation is an instance of a relation schema (similar to object = instance of a class)\nRelation Schema specifies:\n\nName of relation\nName and domain of each attribute\n\nDomain: set of constraints that determines the type, length, format, range, uniqueness and nullability of values stored for an attribute\n\n\n\n\n\n\nQuery: a request for information from a database. Results in a relation.\nStructured Query Language (SQL): standard programming language for managing and manipulating databases.\nCan be categorized into:\n\nData Definition Language (DDL): used to define the database structure or schema.\nData Manipulation Language (DML): used to read, insert, delete, and modify data.\nData Query Language (DQL): used to retrieve data from a database.\nData Control Language (DCL): used to control access to data stored in a database.\n\n\n\n\n\n\nSpecific flavor of SQL and DBMS\nopen-source, cross-platform DBMS that implements the relational model\nreliable, robust, and feature-rich\n\n\n\n\n\n\nCommand\nUsage\n\n\n\n\n\\l\nList all databases\n\n\n\\c\nConnect to a database\n\n\n\\d\nDescribe tables and views\n\n\n\\dt\nList tables\n\n\n\\dt+\nList tables with additional info\n\n\n\\d+\nList tables and views with additional info\n\n\n\\!\nExecute shell commands\n\n\n\\cd\nChange directory\n\n\n\\i\nExecute commands from a file\n\n\n\\h\nView help on SQL commands\n\n\n\\?\nView help on psql meta commands\n\n\n\\q\nQuit interactive shell\n\n\n\n\n\n\n\nTo install: pip install ipython-sql\nTo load: %load_ext sql\n\n# Login to database\nimport json\nimport urllib.parse\n\n# use credentials.json to login (not included in repo)\nwith open('data/credentials.json') as f:\n    login = json.load(f)\n\nusername = login['user']\npassword = urllib.parse.quote(login['password'])\nhost = login['host']\nport = login['port']\n\nEstablish connection:\n\n%sql postgresql://{username}:{password}@{host}:{port}/world\n\nRun queries:\n\noutput = %sql SELECT name, population FROM country;\nor\n%%sql output &lt;&lt; # Not a pandas dataframe\nSELECT\n  name, population\nFROM\n  country\n;\n\n# convert to pandas dataframe\ndf = output.DataFrame()\n\nSet configurations:\n\n%config SqlMagic.displaylimit = 20\n\n\n\n\n\ntaken from https://www.sqltutorial.org/sql-cheat-sheet/*\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription / Example\n\n\n\n\nSELECT c1, c2FROM t;\nQuery data in columns c1, c2 from a table\n\n\nSELECT *FROM t;\nQuery all rows and columns from a table\n\n\nSELECT c1, c2FROM tWHERE condition;\nQuery data and filter rows with a condition\n\n\nSELECT DISTINCT c1FROM tWHERE condition;\nQuery distinct rows from a table\n\n\nSELECT COUNT(DISTINCT (c1, c2))FROM t;\nCount distinct rows in a table\n\n\nSELECT c1, c2FROM tORDER BY c1 ASC [DESC];\nSort the result set in ascending or descending order\n\n\nSELECT c1, c2FROM tORDER BY c1LIMIT n OFFSET offset;\nSkip offset of rows and return the next n rows\n\n\nSELECT c1, aggregate(c2)FROM tGROUP BY c1;\nGroup rows using an aggregate function\n\n\nSELECT c1, aggregate(c2)FROM tGROUP BY c1HAVING condition;\nFilter groups using HAVING clause\n\n\nSELECT CONCAT(c1, c2)FROM t;\nConcatenate two or more strings\n\n\n\nNotes:\n\nConditions: =, &gt;=, &lt;=, IN ('a','b'), IS NULL, …\nStrings must be enclosed in single quotes '...'\nAggregate functions: AVG, COUNT, MAX, MIN, SUM, ROUND(value, decimal_places)\nNeed decimal point to prevent integer division\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription / Example\n\n\n\n\nSELECT c1, c2FROM t1INNER JOIN t2 ON condition;\nInner join t1 and t2\n\n\nSELECT c1, c2FROM t1LEFT JOIN t2 ON condition;\nLeft join t1 and t2\n\n\nSELECT c1, c2FROM t1RIGHT JOIN t2 ON condition;\nRight join t1 and t2\n\n\nSELECT c1, c2FROM t1FULL OUTER JOIN t2 ON condition;\nPerform full outer join\n\n\nSELECT c1, c2FROM t1CROSS JOIN t2;\nProduce a Cartesian product of rows in tables\n\n\nSELECT c1, c2FROM t1 AINNER JOIN t2 B ON condition;\nJoin t1 to itself using INNER JOIN clause\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription / Example\n\n\n\n\nSELECT c1, c2FROM t1UNION [ALL]SELECT c1, c2 FROM t2;\nCombine rows from two queries\n\n\nSELECT c1, c2FROM t1INTERSECTSELECT c1, c2 FROM t2;\nReturn the intersection of two queries\n\n\nSELECT c1, c2FROM t1MINUSSELECT c1, c2 FROM t2;\nSubtract a result set from another result set\n\n\nSELECT c1, c2FROM tWHERE c1 [NOT] LIKE pattern;\nQuery rows using pattern matching % _\n\n\nSELECT c1, c2FROM tWHERE c1 [NOT] IN value_list;\nQuery rows in a list\n\n\nSELECT c1, c2FROM tWHERE c1 BETWEEN low AND high;\nQuery rows between two values\n\n\nSELECT c1, c2FROM tWHERE c1 IS [NOT] NULL;\nCheck if values in a table is NULL or not\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription / Example\n\n\n\n\nCREATE TABLE t (id INT PRIMARY KEY,name VARCHAR NOT NULL,price INT DEFAULT 0);\nCreate a new table with three columns\n\n\nDROP TABLE t;\nDelete the table from the database\n\n\nALTER TABLE t ADD column;\nAdd a new column to the table\n\n\nALTER TABLE t DROP COLUMN c;\nDrop column c from the\n\n\n\n\n\n\n\nPostgreSQL supports many data types. The most common ones are:\n\nBoolean, BOOLEAN or BOOL\n\nTrue: TRUE, t, true, y, yes, on, 1\n\nCharacters\n\nCHAR(n): string of n characters (pad with spaces)\nVARCHAR(n): string of up to n characters\nTEXT: PostgreSQL-specific type for storing strings of any length\n\nDateTime: date and time\n\nDATE: date (YYYY-MM-DD)\n\nCURRENT_DATE: current date\n\nTIME: time\nTIMESTAMP: date and time\nTIMESTAMPTZ: date and time with timezone\n\nBinary: binary data\nNumbers\n\n\n\n\n\n\" is used for identifiers (e.g. table names, column names)\n' is used for strings.\n\n\n\n\n\n\n\n\n\n\n\n\nName\nStorage Size\nDescription\nRange\n\n\n\n\nsmallint\n2 bytes\nsmall-range integer\n-32,768 to +32,767\n\n\ninteger\n4 bytes\ntypical choice for integer\n-2,147,483,648 to +2,147,483,647\n\n\nbigint\n8 bytes\nlarge-range integer\n-9,223,372,036,854,775,808 to +9,223,372,036,854,775,807\n\n\nserial\n4 bytes\nauto-incrementing integer\n1 to 2,147,483,647\n\n\nbigserial\n8 bytes\nlarge auto-incrementing integer\n1 to 9,223,372,036,854,775,807\n\n\n\nNote: serial and bigserial are not true types, but merely a notational convenience for creating unique identifier columns.\nFloating-Point Numbers\n\n\n\n\n\n\n\n\n\nName\nStorage Size\nDescription\nRange\n\n\n\n\nreal\n4 bytes\nvariable-precision, inexact\nat least 6 decimal digits (implementation dependent)\n\n\ndouble precision\n8 bytes\nvariable-precision, inexact\nat least 15 decimal digits (implementation dependent)\n\n\n\nArbitrary Precision Numbers\n\n\n\n\n\n\n\n\n\nName\nStorage Size\nDescription\nRange\n\n\n\n\nnumeric or decimal\nvariable\nuser-specified precision, exact\n131072 digits before and 16383 digits after the decimal point\n\n\n\nNote: numeric and decimal are the same type in Postgres.\n\n\n\n\nCan do : &lt;col&gt;::&lt;data_type&gt; or CAST(&lt;col&gt; AS &lt;data_type&gt;)\ne.g. SELECT '123'::integer;\ne.g. SELECT CAST('123' AS integer);\n\n\n\n\nSELECT *\nFROM table_name\nWHERE [NOT]\n    condition\n    AND/OR\n    condition;\n\n\n\n\n\n\n\nCondition Example\nDescription\n\n\n\n\nWHERE column = value\nEquals; returns true if the column equals the value.\n\n\nWHERE column &lt;&gt; value\nNot equals; true if the column is not equal to value.\n\n\nWHERE column &gt; value\nGreater than; true if the column is more than value.\n\n\nWHERE column &lt; value\nLess than; true if the column is less than value.\n\n\nWHERE column BETWEEN value1 AND value2\nTrue if the column is within the range of value1 and value2.\n\n\nWHERE column [NOT] IN (value1, value2, ...)\nTrue if the column is equal to any of multiple values.\n\n\nWHERE column [NOT] LIKE pattern\nTrue if the column matches the SQL pattern.\n\n\nWHERE column IS [NOT] NULL\nTrue if the column is NULL.\n\n\n\n\n\n\nLIKE is case-sensitive\nILIKE is case-insensitive\nWildcards:\n\n% (any string of zero or more characters)\n_ (any single character)\n\ne.g. WHERE column LIKE 'abc%'\n\nOther nuances:\n\nusing ESCAPE to identify escape character\n\ne.g. WHERE column LIKE '%$%%' ESCAPE '$': matches strings that contains %\n\n\n\n\n\n\nSELECT column AS alias\nFROM table_name;\n\nCannot use alias in WHERE clause because order of execution is FROM then WHERE\n\n\n\n\n\nFROM and JOIN\nWHERE\nGROUP BY, then HAVING\nSELECT , then DISTINCT\nORDER BY\nLIMIT and OFFSET\n\n\n\n\n\n\n\n\n\nShort Description\nExample/Syntax\n\n\n\n\nAddition\ncol1 + col2\n\n\nSubtraction\ncol1 - col2\n\n\nMultiplication\ncol1 * col2\n\n\nDivision\ncol1 / col2\n\n\nModulus\ncol1 % col2\n\n\nAbsolute Value\nABS(col)\n\n\nRound to n decimal places\nROUND(col, n)\n\n\nRound up\nCEILING(col)\n\n\nRound down\nFLOOR(col)\n\n\nPower of n\nPOWER(col, n)\n\n\nSquare Root\nSQRT(col)\n\n\nTruncate to n decimal places\nTRUNCATE(col, n)\n\n\nGenerate random number\nRAND()\n\n\n\n\n\n\n\n\n\n\n\n\n\nShort Description\nExample/Syntax\n\n\n\n\nConcatenate strings\nCONCAT(str1, str2, ...) or str1 \\|\\| str2\n\n\nLength of string\nCHAR_LENGTH(str)\n\n\nConvert to lower case\nLOWER(str)\n\n\nConvert to upper case\nUPPER(str)\n\n\nExtract substring\nSUBSTRING(str, start, length)\n\n\nTrim spaces\nTRIM(str)\n\n\nReplace substring\nREPLACE(str, from_str, to_str)\n\n\nPosition of substring\nPOSITION(substring IN str)\n\n\n\n\n\n\n\n\n\nShort Description\nExample/Syntax\n\n\n\n\nCurrent date\nCURRENT_DATE\n\n\nCurrent time\nCURRENT_TIME\n\n\nCurrent date and time\nCURRENT_TIMESTAMP\n\n\nExtract part of date/time\nEXTRACT(part FROM date/time)\n\n\nAdd interval to date/time\ndate/time + INTERVAL\n\n\nSubtract interval from date/time\ndate/time - INTERVAL\n\n\nDifference between dates/times\nDATEDIFF(date1, date2)\n\n\nFormat date/time\nFORMAT(date/time, format)\n\n\n\ne.g. SELECT EXTRACT(YEAR FROM CURRENT_DATE);\n\n\n\n\n\n\n\n\n\n\nShort Description\nExample/Syntax\n\n\n\n\nCheck for NULL\ncol IS NULL\n\n\nCheck for non-NULL\ncol IS NOT NULL\n\n\nReplace NULL with specified value\nCOALESCE(col, replace_value)\n\n\nNull-safe equal to operator\ncol1 &lt;=&gt; col2\n\n\nCase statement with NULL handling\nCASE WHEN col IS NULL THEN result ELSE other_result END\n\n\nNull if expression is NULL\nNULLIF(expression, NULL)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nAVG()\nReturns the average value.\n\n\nCOUNT()\nReturns the number of rows.\n\n\nMAX()\nReturns the maximum value.\n\n\nMIN()\nReturns the minimum value.\n\n\nSUM()\nReturns the sum of all or distinct values.\n\n\nVAR()\nReturns the variance of all or distinct values.\n\n\nSTDDEV\nReturns the standard deviation of all or distinct values.\n\n\n\nImportant notes:\n\nCannot use aggregate function with normal column (in SELECT clause) without GROUP BY clause\nAll aggregate functions ignore NULL values except COUNT(*)\nCannot use aggregate function in WHERE clause because order of execution is FROM then WHERE\n\n\n\n\n\nGROUP BY clause is used to group rows with the same values\n\n-- Formal syntax\nSELECT\n    grouping_columns, aggregated_columns\nFROM\n    table1\nWHERE -- filter rows before grouping\n    condition\nGROUP BY -- must be between WHERE and ORDER BY\n    grouping_columns\nHAVING -- Used to filter groups (after grouping)\n    group_condition\nORDER BY\n    grouping_columns\n\n\n\n\nJOIN clause is used to combine rows from two or more tables based on a related column between them\n\nDEFAULT: INNER JOIN\n\n\nSELECT -- columns from both tables\n    t1.column1, t2.column2\nFROM\n    table1 AS t1 -- after using alias, use alias instead of table name\njointype\n    table2 AS t2\nON -- condition to join tables\n    t1.column = t2.column\n\n\n\n\n\n\n\nJoin Type\nDescription\n\n\n\n\nCROSS\nReturns the Cartesian product of the sets of rows from the joined tables (all possible combinations)\n\n\nINNER\nReturns rows when there is a match in both tables. (intersect)\n\n\nNATURAL\nReturns all rows without specifying ON, names have to be the same in both tables.\n\n\nLEFT OUTER\nReturns all rows from the left-hand table, plus any rows in the right-hand table that match the left-hand side.\n\n\nRIGHT OUTER\nReturns all rows from the right-hand table, plus any rows in the left-hand table that match the right-hand side.\n\n\nFULL OUTER\nReturns all rows from both tables, with nulls in place of those rows that have no match in the other table.\n\n\n\n\n\n\n\n\n\nAdd new rows to table, by:\n\ncolumn position:\n    INSERT INTO table_name\n    VALUES\n        (value1, value2, ...),\n        (value1, value2, ...), -- can insert multiple rows at once\n        ...\n\nvalues need to be in the same order as the columns\ndon’t need to specify column names\n\ncolumn name:\nINSERT INTO table_name(column1, column2, ...)\nVALUES (value1, value2, ...)\n\nvalues can be in any order\nneed to specify column names\n\nfrom another table:\nINSERT INTO\n    table_name(column1, column2, ...)\nSELECT *\nFROM other_table\n\n\n\n\n\n\nModify existing rows in table\n\nUPDATE table_name\nSET column1 = value1,\n    column2 = value2,\n    ...\nWHERE condition\n\n\n\n\nRemove rows from table\nremoves all rows but keeps table if no WHERE clause\n\nDELETE FROM table_name\nWHERE condition\n\n\n\n\nRemove all rows from table\nfaster than DELETE because it doesn’t scan every row\n\nalso does not log each row deletion\n\n\nTRUNCATE TABLE table_name\n\n\n\n\n\n\nCREATE TABLE table_name (\n    column1 datatype [constraint] PRIMARY KEY, -- for simple primary key\n    column2 datatype UNIQUE, -- unique constraint\n    column3 TEXT DEFAULT 'default value', -- default value\n    column4 datatype\n        [CONSTRAINT constraint_name] CHECK (condition), -- check constraint\n    -- e.g.\n    name TEXT NOT NULL,\n    phone CHAR(12) CHECK (phone LIKE '___-___-____')\n    ...\n\n    -- table constraints\n    -- if set composite primary key, also can simple\n    [CONSTRAINT constraint_name] PRIMARY KEY (column1, column2, ...),\n);\n\n\n\n\nSimple key: a single column\nComposite key: multiple columns\n\n\n\n\nCan uniquely identify a table row\nMust be minimal: no subset of the candidate key can be a candidate key\nCan have multiple in a table (e.g. id and email)\n\n\n\n\n\nA candidate key that is chosen to be the main identifier of a table\nAutomaticaly unique and not null\nMust be unique and not null\nMust be minimal\n\ngenerally the candidate key with the fewest columns\n\n\n\n\n\n\nA column that references a primary key in another table\nPrevents invalid data from being inserted\nCan be null\nChild table: table with foreign key\nParent table: table with primary key\n\n-- parent table\nCREATE TABLE instructor (\n    id INTEGER PRIMARY KEY,\n    name TEXT NOT NULL,\n    email TEXT UNIQUE,\n);\n\n-- child table\nCREATE TABLE instructor_course (\n    id INTEGER,\n    course_id TEXT,\n    PRIMARY KEY (id, course_id), -- composite primary key\n    FOREIGN KEY (id) REFERENCES instructor (id)\n        ON DELETE CASCADE,\n\n    -- can also specify column name\n    FOREIGN KEY (course_id) REFERENCES instructor (id)\n        ON DELETE CASCADE -- delete child rows when parent row is deleted\n        ON UPDATE CASCADE -- update child rows when child row is updated\n);\n\nForeign key constraint ensures there are no orphaned rows\n\ni.e. all foreign key in child table must exist in parent table\n\n\n\n\n\n\n\n\n\nReferential Action\nDescription\n\n\n\n\nNO ACTION\nDefault. Rejects update or delete of parent row\n\n\nSET NULL\nSets foreign key to null\n\n\nCASCADE\nDeletes or updates child rows when parent row is deleted or updated\n\n\nSET DEFAULT\nSets foreign key to default value\n\n\n\n\n\n\n\n\nUsed to store temporary data\nAutomatically dropped at end of session\nPrivate to current session\nPhysically stored in temp tablespace\n\nCREATE TEMPORARY TABLE table_name (\n    ...\n);\n\n-- can also create from another table\nCREATE TEMPORARY TABLE\n    temp_table_name\nAS\n    SELECT name, department FROM other_table\n;\n\n\n\n\n\nRemove table and all its data\nPostgres does not allow dropping a table if it has a foreign key constraint\n\nneed to drop the foreign key constraint first\nor use CASCADE to drop the table and all its foreign key constraints\n\n\nDROP TABLE table_name [CASCADE];\n\n\n\n\nA transaction is a sequence of SQL statements that are treated as a unit, so either all of them are executed, or none of them are executed.\n\n[{BEGIN|START} [TRANSACTION]]; -- BEGIN more common, can just start  with `BEGIN`\n\n&lt;SQL statements&gt;\n\n{COMMIT|ROLLBACK};\n\nCOMMIT: make all changes made by the transaction permanent\n\nwithout COMMIT, changes are not visible to other users\n\nROLLBACK: undo all changes made by the transaction\n\n\n\nProperties of a transaction:\n\nAtomicity: all or nothing (transaction either completes or is aborted)\nConsistency: database must be in a consistent state before and after the transaction\n\ne.g. no scores &gt; 100, transaction will not be committed if it violates this\n\nIsolation: if two transactions are executed concurrently, the result should be the same as if they were executed one after the other\nDurability: changes made by a committed transaction must be permanent (even if there is a system failure), achieved using transaction log\n\n\n\n\n\n\nA subquery is a query within a query\nNormally used to mix aggregate and non-aggregate queries\nCan only be used if subquery returns ONLY 1 value.\n\n-- Selects countries with a population greater than the average population of all countries\nSELECT\n    name\nFROM\n    country\nWHERE\n    surfacearea &gt; (\n        SELECT AVG(surfacearea) FROM country\n    )\n;\n\n\n\nA correlated subquery is a subquery that uses values from the outer query\n\n-- Selects countries with the largest population in their continent\nSELECT\n    c1.name, c1.continent\nFROM\n    country c1\nWHERE\n    c1.population = (\n        SELECT\n            MAX(c2.population)\n        FROM\n            country c2\n        WHERE\n            c2.continent = c1.continent\n    )\n;\n\n\n\n\nANY and ALL are used to compare a value to a list or subquery\n\n\n\n\nANY\nALL\n\n\n\n\nif one of values true =&gt; true\nif all values true =&gt; true\n\n\n\ne.g.\n-- Find the names of all countries that have a\n-- population greater than all European countries.\nSELECT\n    name\nFROM\n    country\nWHERE\n    continent &lt;&gt; 'Europe'\n    AND\n    population &gt; ALL (\n        SELECT\n            population\n        FROM\n            country\n        WHERE\n            continent = 'Europe'\n    )\n;\n\nEXISTS is used to check if a subquery returns any rows\n\nfaster than IN because it stops as soon as it finds a match\n\n\nSELECT co.name\nFROM country co\nWHERE EXISTS (SELECT * FROM city ci\n        WHERE co.code = ci.countrycode AND ci.population &gt; 5000000);\n        ```\n\n\n\n\n\nIs a named result of a SELECT query\n\nBehaves like a table (called virtual table sometimes)\nCurrent and Dynamic: whenever you query a view, you get the most up-to-date data\nViews persists: they are kept\nNot materialized: they are not stored in the database\nViews can hide complexity\nManage access to data\nDo not support constraints\n\n\nCREATE VIEW view_name AS\n    select_statment\n;\n\nDROP VIEW [IF EXISTS] view_name;\n\n\n\nMaterialized views are stored in the database\nThey are updated periodically\nThey are used for performance reasons (a lot faster)\n\nCREATE MATERIALIZED VIEW my_mat_view AS\n    select_statement\n;\n\nREFRESH MATERIALIZED VIEW [CONCURRENTLY] my_mat_view;\n-- CONCURRENTLY: allows you to query the view while it is being refreshed, no guarantee it is up-to-date\n\nDROP MATERIALIZED VIEW my_mat_view;\n\n\n\n\nDROP TABLE IF EXISTS temp_table_name;\n\nCREATE TEMPORARY TABLE temp_table_name AS (\n    ...\n);\n\n\n\n\nCommon Table Expressions: temporary named result of a query\n\nWITH\n    expression_name [(column_names, ...)]\nAS (\n    query\n)\nquery\n-- SELECT * FROM expression_name;\n-- This query needs to use the column names defined in the CTE after the expression name\n;\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nTemporary Tables\nViews\nMaterialized Views\nCTEs\n\n\n\n\nNature\nPhysical table, session-scoped\nVirtual table\nPhysical storage of query results\nTemporary result set\n\n\nData Storage\nStores data temporarily\nDoes not store data, only stores the query itself\nStores results of a query\nDoes not store data, used within query execution\n\n\nScope\nExists only during a database session\nPermanent, unless dropped\nPermanent, unless dropped\nExists only during the execution of a query\n\n\nAccess\nCan only be accessed by 1 user\nCan be accessed by multiple users\nCan be accessed by multiple users\nCan be accessed by multiple users\n\n\nUsage\nIntermediate data storage, complex queries\nSimplifying access to complex queries, data security\nPerformance improvement for complex queries\nBreaking down complex queries, recursive queries  can manipulate default processing order of SQL clauses\n\n\nPerformance\nDependent on data size and indexes\nExecutes underlying query each time\nFaster access, as data is pre-calculated\nDependent on the complexity of the query\n\n\nUpdates\nData persists for the session, can be updated\nReflects real-time data from base tables\nRequires refresh to update data\nNot applicable (recomputed with each query execution)\n\n\nIndexing\nCan have indexes\nCannot have indexes\nCan have indexes\nNot applicable\n\n\n\n\n\n\n\nWindow functions are used to compute aggregate values over a group of rows\nOnly allowed in SELECT and ORDER BY clauses\nProcessed:\n\nafter WHERE, GROUP BY, and HAVING\nbefore SELECT and ORDER BY\n\n\nSELECT\n    column,\n    window_function_name(expression) OVER (\n        PARTITION BY column\n        ORDER BY column\n        frame_clause\n    )\n    -- e.g.\n    continent,\n    MAX(population)\n        OVER (PARTITION BY continent)\nFROM\n    table\n;\n\n\n\nAggregate functions\n\nAVG, COUNT, MAX, MIN, SUM\n\nRanking functions\n\nCUME_DIST(): returns the cumulative distribution, i.e. the percentage of values less than or equal to the current value.\nNTILE(): given a specified number of buckets, it tells us in which bucket each row goes among other rows in the partition.\nPERCENT_RANK(): similar to CUME_DIST(), but considers only the percentage of values less than (and not equal to) the current value.\nDENSE_RANK(): returns the rank of a row within a partition without jumps after duplicate ranks (e.g. 1, 2, 2, 3, …)\nRANK(): returns the rank of row within a partition but with jumps after duplicates ranks (e.g. 1, 2, 2, 4, …)\nROW_NUMBER(): returns simply the number of a row in a partition, regardless of duplicate values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElements\nRelational\nNon-relational\n\n\n\n\nData Model\ntables\nkey-value pairs, documents, graphs, etc.\n\n\nSchema\nfixed (updates are complicated and time-consuming)\ndynamic (schema-free)\n\n\nQuerying\nsemi standard (SQL)\nproprietary\n\n\nPerformance\nstrong data consistency and integrity\nfaster performance for specific usecases\n\n\nEfficiency\nwrite times and table locks reduce efficiency\nfaster read and write times\n\n\nScalability\nvertical scaling\nhorizontal scaling\n\n\nDevelopment\nrequire more effort\neasier to develop and require fewer resources\n\n\nPrinciples\nACID (Atomicity, Consistency, Isolation, Durability)\nBASE (Basically Available, Soft state, Eventual consistency)\n\n\n\n\n\n\n\nMain goals:\n\nreduce the dependence on fixed schema\nreduce the dependence on a single point of truth\nscaling\n\n\n\n\n\nSchema-free\nAccepting Basic Availability\n\nusually see all tweets, but sometimes a refresh shows new tweets\n\nBASE:\n\nBasically Available\nSoft state\nEventual consistent\n\n\n\n\n\n\nKey-value stores: like a dictionary\n\nRedis, Memcached, Amazon DynamoDB\n\nDocument stores: type of key-value store but with an internal searchable structure.\n\nDocument: contains all the relevant data and has a unique key\nMongoDB, CouchDB\n\nColumn stores: based on Google’s BigTable\n\nCassandra, HBase, Google BigTable\n\nGraph databases: based on graph theory\n\nNeo4j, OrientDB, Amazon Neptune\n\n\n\n\n\n\n\nBased on JSON-like (JavaScript Object Notation) documents for storage:\n\nActually it is BSON (Binary JSON)\nMax BSON document size: 16MB\n\nStructure:\n\nDatabase: client.get_database_names()\nCollection: client[db_name].get_collection_names()\nDocument: client[db_name][collection_name].find_one() or client[db_name][collection_name].find()\n\n\n\n\n\nclient[db_name][collection_name].find()\n\nreturns a cursor\nlist(client[db_name][collection_name].find()) returns a list of documents\nnext(client[db_name][collection_name].find()) returns the next document\n\n\nclient = MongoClient()\nclient[db_name][collection_name].find(\n    filter={...},\n    projection={...},\n    sort=[...],\n    skip=...,\n    limit=...,\n)\n\n\n\nfilter is a dictionary\nfilter = {\"key\": \"value\"} returns all documents where key is value\n\n\n\n\n\n\n\n\n\nOperator\nsymbol\nnotes\n\n\n\n\n$eq or $neq\n= or ~=\n\n\n\n$gt or $gte\n&gt; or &gt;=\n\n\n\n$lt or $lte\n&lt; or &lt;=\n\n\n\n$ne\n!=\n\n\n\n$in\nin\nif any of the values in the list matches the value of the key, the document is returned\n\n\n$nin\nnot in\n\n\n\n$all\nall\nall values in the list must match the value of the keys\n\n\n$exists\nexists\n\n\n\n$regex\nregex\n\n\n\n\n\nAlso can use $not to negate the operator\nusing $or, $nor and $and:\n\nfilter = {\n    \"$or\": [\n        {\"key_1\": \"value_1\"},\n        {\"key_2\": \"value_2\"},\n    ],\n    \"$and\": [\n        {\"key_3\": \"value_3\"},\n        {\"key_4\": \"value_4\"},\n    ],\n    \"key_5\": {\"$gte\": 10, \"$lte\": 20}, # and is implied\n}\n\nfilter = {\n  \"key_1\": \"value_1\",\n  \"key_2\": \"value_2\", # and is implied\n}\n\n\n\n\nprojection is a dictionary\nprojection = {\"key_1\": 1, \"key_2\": 0} returns all documents with only key_1 and without key_2\n\n\n\n\n\nsort is a list of tuples\n\n1: ascending order\n-1: descending order\n\nsort = [(\"key_1\", 1), (\"key_2\", -1)] sorts by key_1 in ascending order and then by key_2 in descending order\n\n\n\n\n\nlimit is an integer\nlimit = 10 returns the first 10 documents\n\n\n\n\n\nskip is an integer\nskip = 10 skips the first 10 documents\n\n\n\n\n\ncount_documents is a method\nclient[db_name][collection_name].count_documents(filter={...}) returns the number of documents that match the filter\n\n\n\n\n\ndistinct is a method\nclient[db_name][collection_name].distinct(\"key_1\") returns a list of distinct values for key_1"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#introduction",
    "href": "block_3/513_databases/513_databases.html#introduction",
    "title": "Relational Databases",
    "section": "",
    "text": "What is a database?\n\nOrganized collection of related data\n\nWhart is a database management system (DBMS)?\n\nCollection of programs that enables users to create and maintain a database\nallows users to create, query, modify, and manage\n\n\nDATABASE != DBMS\n\n\n\nEfficiency: Data is stored efficiently.\nIntegrity: Data is consistent and correct.\nSecurity: Data is safe from unauthorized access.\nConcurrent Access: Multiple users can access the same data at the same time.\nCrash Recovery: Data is safe from crashes.\nIndependence: Data is independent of the programs that use it."
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#relational-data-model",
    "href": "block_3/513_databases/513_databases.html#relational-data-model",
    "title": "Relational Databases",
    "section": "",
    "text": "Works with entities and relationships\nEntity: a thing or object in the real world that is distinguishable from other objects\n\ne.g. students in a school\n\nRelationship: an association among entities\n\ne.g. a student is enrolled in a course"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#relational-database",
    "href": "block_3/513_databases/513_databases.html#relational-database",
    "title": "Relational Databases",
    "section": "",
    "text": "Collection of relations\nA relation is an instance of a relation schema (similar to object = instance of a class)\nRelation Schema specifies:\n\nName of relation\nName and domain of each attribute\n\nDomain: set of constraints that determines the type, length, format, range, uniqueness and nullability of values stored for an attribute"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#query-language-in-a-dbms",
    "href": "block_3/513_databases/513_databases.html#query-language-in-a-dbms",
    "title": "Relational Databases",
    "section": "",
    "text": "Query: a request for information from a database. Results in a relation.\nStructured Query Language (SQL): standard programming language for managing and manipulating databases.\nCan be categorized into:\n\nData Definition Language (DDL): used to define the database structure or schema.\nData Manipulation Language (DML): used to read, insert, delete, and modify data.\nData Query Language (DQL): used to retrieve data from a database.\nData Control Language (DCL): used to control access to data stored in a database."
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#postgresql",
    "href": "block_3/513_databases/513_databases.html#postgresql",
    "title": "Relational Databases",
    "section": "",
    "text": "Specific flavor of SQL and DBMS\nopen-source, cross-platform DBMS that implements the relational model\nreliable, robust, and feature-rich\n\n\n\n\n\n\nCommand\nUsage\n\n\n\n\n\\l\nList all databases\n\n\n\\c\nConnect to a database\n\n\n\\d\nDescribe tables and views\n\n\n\\dt\nList tables\n\n\n\\dt+\nList tables with additional info\n\n\n\\d+\nList tables and views with additional info\n\n\n\\!\nExecute shell commands\n\n\n\\cd\nChange directory\n\n\n\\i\nExecute commands from a file\n\n\n\\h\nView help on SQL commands\n\n\n\\?\nView help on psql meta commands\n\n\n\\q\nQuit interactive shell\n\n\n\n\n\n\n\nTo install: pip install ipython-sql\nTo load: %load_ext sql\n\n# Login to database\nimport json\nimport urllib.parse\n\n# use credentials.json to login (not included in repo)\nwith open('data/credentials.json') as f:\n    login = json.load(f)\n\nusername = login['user']\npassword = urllib.parse.quote(login['password'])\nhost = login['host']\nport = login['port']\n\nEstablish connection:\n\n%sql postgresql://{username}:{password}@{host}:{port}/world\n\nRun queries:\n\noutput = %sql SELECT name, population FROM country;\nor\n%%sql output &lt;&lt; # Not a pandas dataframe\nSELECT\n  name, population\nFROM\n  country\n;\n\n# convert to pandas dataframe\ndf = output.DataFrame()\n\nSet configurations:\n\n%config SqlMagic.displaylimit = 20"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#sql-basic-commands",
    "href": "block_3/513_databases/513_databases.html#sql-basic-commands",
    "title": "Relational Databases",
    "section": "",
    "text": "taken from https://www.sqltutorial.org/sql-cheat-sheet/*\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription / Example\n\n\n\n\nSELECT c1, c2FROM t;\nQuery data in columns c1, c2 from a table\n\n\nSELECT *FROM t;\nQuery all rows and columns from a table\n\n\nSELECT c1, c2FROM tWHERE condition;\nQuery data and filter rows with a condition\n\n\nSELECT DISTINCT c1FROM tWHERE condition;\nQuery distinct rows from a table\n\n\nSELECT COUNT(DISTINCT (c1, c2))FROM t;\nCount distinct rows in a table\n\n\nSELECT c1, c2FROM tORDER BY c1 ASC [DESC];\nSort the result set in ascending or descending order\n\n\nSELECT c1, c2FROM tORDER BY c1LIMIT n OFFSET offset;\nSkip offset of rows and return the next n rows\n\n\nSELECT c1, aggregate(c2)FROM tGROUP BY c1;\nGroup rows using an aggregate function\n\n\nSELECT c1, aggregate(c2)FROM tGROUP BY c1HAVING condition;\nFilter groups using HAVING clause\n\n\nSELECT CONCAT(c1, c2)FROM t;\nConcatenate two or more strings\n\n\n\nNotes:\n\nConditions: =, &gt;=, &lt;=, IN ('a','b'), IS NULL, …\nStrings must be enclosed in single quotes '...'\nAggregate functions: AVG, COUNT, MAX, MIN, SUM, ROUND(value, decimal_places)\nNeed decimal point to prevent integer division\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription / Example\n\n\n\n\nSELECT c1, c2FROM t1INNER JOIN t2 ON condition;\nInner join t1 and t2\n\n\nSELECT c1, c2FROM t1LEFT JOIN t2 ON condition;\nLeft join t1 and t2\n\n\nSELECT c1, c2FROM t1RIGHT JOIN t2 ON condition;\nRight join t1 and t2\n\n\nSELECT c1, c2FROM t1FULL OUTER JOIN t2 ON condition;\nPerform full outer join\n\n\nSELECT c1, c2FROM t1CROSS JOIN t2;\nProduce a Cartesian product of rows in tables\n\n\nSELECT c1, c2FROM t1 AINNER JOIN t2 B ON condition;\nJoin t1 to itself using INNER JOIN clause\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription / Example\n\n\n\n\nSELECT c1, c2FROM t1UNION [ALL]SELECT c1, c2 FROM t2;\nCombine rows from two queries\n\n\nSELECT c1, c2FROM t1INTERSECTSELECT c1, c2 FROM t2;\nReturn the intersection of two queries\n\n\nSELECT c1, c2FROM t1MINUSSELECT c1, c2 FROM t2;\nSubtract a result set from another result set\n\n\nSELECT c1, c2FROM tWHERE c1 [NOT] LIKE pattern;\nQuery rows using pattern matching % _\n\n\nSELECT c1, c2FROM tWHERE c1 [NOT] IN value_list;\nQuery rows in a list\n\n\nSELECT c1, c2FROM tWHERE c1 BETWEEN low AND high;\nQuery rows between two values\n\n\nSELECT c1, c2FROM tWHERE c1 IS [NOT] NULL;\nCheck if values in a table is NULL or not\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription / Example\n\n\n\n\nCREATE TABLE t (id INT PRIMARY KEY,name VARCHAR NOT NULL,price INT DEFAULT 0);\nCreate a new table with three columns\n\n\nDROP TABLE t;\nDelete the table from the database\n\n\nALTER TABLE t ADD column;\nAdd a new column to the table\n\n\nALTER TABLE t DROP COLUMN c;\nDrop column c from the"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#database-data-types",
    "href": "block_3/513_databases/513_databases.html#database-data-types",
    "title": "Relational Databases",
    "section": "",
    "text": "PostgreSQL supports many data types. The most common ones are:\n\nBoolean, BOOLEAN or BOOL\n\nTrue: TRUE, t, true, y, yes, on, 1\n\nCharacters\n\nCHAR(n): string of n characters (pad with spaces)\nVARCHAR(n): string of up to n characters\nTEXT: PostgreSQL-specific type for storing strings of any length\n\nDateTime: date and time\n\nDATE: date (YYYY-MM-DD)\n\nCURRENT_DATE: current date\n\nTIME: time\nTIMESTAMP: date and time\nTIMESTAMPTZ: date and time with timezone\n\nBinary: binary data\nNumbers"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#vs",
    "href": "block_3/513_databases/513_databases.html#vs",
    "title": "Relational Databases",
    "section": "",
    "text": "\" is used for identifiers (e.g. table names, column names)\n' is used for strings.\n\n\n\n\n\n\n\n\n\n\n\n\nName\nStorage Size\nDescription\nRange\n\n\n\n\nsmallint\n2 bytes\nsmall-range integer\n-32,768 to +32,767\n\n\ninteger\n4 bytes\ntypical choice for integer\n-2,147,483,648 to +2,147,483,647\n\n\nbigint\n8 bytes\nlarge-range integer\n-9,223,372,036,854,775,808 to +9,223,372,036,854,775,807\n\n\nserial\n4 bytes\nauto-incrementing integer\n1 to 2,147,483,647\n\n\nbigserial\n8 bytes\nlarge auto-incrementing integer\n1 to 9,223,372,036,854,775,807\n\n\n\nNote: serial and bigserial are not true types, but merely a notational convenience for creating unique identifier columns.\nFloating-Point Numbers\n\n\n\n\n\n\n\n\n\nName\nStorage Size\nDescription\nRange\n\n\n\n\nreal\n4 bytes\nvariable-precision, inexact\nat least 6 decimal digits (implementation dependent)\n\n\ndouble precision\n8 bytes\nvariable-precision, inexact\nat least 15 decimal digits (implementation dependent)\n\n\n\nArbitrary Precision Numbers\n\n\n\n\n\n\n\n\n\nName\nStorage Size\nDescription\nRange\n\n\n\n\nnumeric or decimal\nvariable\nuser-specified precision, exact\n131072 digits before and 16383 digits after the decimal point\n\n\n\nNote: numeric and decimal are the same type in Postgres.\n\n\n\n\nCan do : &lt;col&gt;::&lt;data_type&gt; or CAST(&lt;col&gt; AS &lt;data_type&gt;)\ne.g. SELECT '123'::integer;\ne.g. SELECT CAST('123' AS integer);\n\n\n\n\nSELECT *\nFROM table_name\nWHERE [NOT]\n    condition\n    AND/OR\n    condition;\n\n\n\n\n\n\n\nCondition Example\nDescription\n\n\n\n\nWHERE column = value\nEquals; returns true if the column equals the value.\n\n\nWHERE column &lt;&gt; value\nNot equals; true if the column is not equal to value.\n\n\nWHERE column &gt; value\nGreater than; true if the column is more than value.\n\n\nWHERE column &lt; value\nLess than; true if the column is less than value.\n\n\nWHERE column BETWEEN value1 AND value2\nTrue if the column is within the range of value1 and value2.\n\n\nWHERE column [NOT] IN (value1, value2, ...)\nTrue if the column is equal to any of multiple values.\n\n\nWHERE column [NOT] LIKE pattern\nTrue if the column matches the SQL pattern.\n\n\nWHERE column IS [NOT] NULL\nTrue if the column is NULL.\n\n\n\n\n\n\nLIKE is case-sensitive\nILIKE is case-insensitive\nWildcards:\n\n% (any string of zero or more characters)\n_ (any single character)\n\ne.g. WHERE column LIKE 'abc%'\n\nOther nuances:\n\nusing ESCAPE to identify escape character\n\ne.g. WHERE column LIKE '%$%%' ESCAPE '$': matches strings that contains %\n\n\n\n\n\n\nSELECT column AS alias\nFROM table_name;\n\nCannot use alias in WHERE clause because order of execution is FROM then WHERE\n\n\n\n\n\nFROM and JOIN\nWHERE\nGROUP BY, then HAVING\nSELECT , then DISTINCT\nORDER BY\nLIMIT and OFFSET\n\n\n\n\n\n\n\n\n\nShort Description\nExample/Syntax\n\n\n\n\nAddition\ncol1 + col2\n\n\nSubtraction\ncol1 - col2\n\n\nMultiplication\ncol1 * col2\n\n\nDivision\ncol1 / col2\n\n\nModulus\ncol1 % col2\n\n\nAbsolute Value\nABS(col)\n\n\nRound to n decimal places\nROUND(col, n)\n\n\nRound up\nCEILING(col)\n\n\nRound down\nFLOOR(col)\n\n\nPower of n\nPOWER(col, n)\n\n\nSquare Root\nSQRT(col)\n\n\nTruncate to n decimal places\nTRUNCATE(col, n)\n\n\nGenerate random number\nRAND()\n\n\n\n\n\n\n\n\n\n\n\n\n\nShort Description\nExample/Syntax\n\n\n\n\nConcatenate strings\nCONCAT(str1, str2, ...) or str1 \\|\\| str2\n\n\nLength of string\nCHAR_LENGTH(str)\n\n\nConvert to lower case\nLOWER(str)\n\n\nConvert to upper case\nUPPER(str)\n\n\nExtract substring\nSUBSTRING(str, start, length)\n\n\nTrim spaces\nTRIM(str)\n\n\nReplace substring\nREPLACE(str, from_str, to_str)\n\n\nPosition of substring\nPOSITION(substring IN str)\n\n\n\n\n\n\n\n\n\nShort Description\nExample/Syntax\n\n\n\n\nCurrent date\nCURRENT_DATE\n\n\nCurrent time\nCURRENT_TIME\n\n\nCurrent date and time\nCURRENT_TIMESTAMP\n\n\nExtract part of date/time\nEXTRACT(part FROM date/time)\n\n\nAdd interval to date/time\ndate/time + INTERVAL\n\n\nSubtract interval from date/time\ndate/time - INTERVAL\n\n\nDifference between dates/times\nDATEDIFF(date1, date2)\n\n\nFormat date/time\nFORMAT(date/time, format)\n\n\n\ne.g. SELECT EXTRACT(YEAR FROM CURRENT_DATE);\n\n\n\n\n\n\n\n\n\n\nShort Description\nExample/Syntax\n\n\n\n\nCheck for NULL\ncol IS NULL\n\n\nCheck for non-NULL\ncol IS NOT NULL\n\n\nReplace NULL with specified value\nCOALESCE(col, replace_value)\n\n\nNull-safe equal to operator\ncol1 &lt;=&gt; col2\n\n\nCase statement with NULL handling\nCASE WHEN col IS NULL THEN result ELSE other_result END\n\n\nNull if expression is NULL\nNULLIF(expression, NULL)"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#aggregate-functions",
    "href": "block_3/513_databases/513_databases.html#aggregate-functions",
    "title": "Relational Databases",
    "section": "",
    "text": "Function\nDescription\n\n\n\n\nAVG()\nReturns the average value.\n\n\nCOUNT()\nReturns the number of rows.\n\n\nMAX()\nReturns the maximum value.\n\n\nMIN()\nReturns the minimum value.\n\n\nSUM()\nReturns the sum of all or distinct values.\n\n\nVAR()\nReturns the variance of all or distinct values.\n\n\nSTDDEV\nReturns the standard deviation of all or distinct values.\n\n\n\nImportant notes:\n\nCannot use aggregate function with normal column (in SELECT clause) without GROUP BY clause\nAll aggregate functions ignore NULL values except COUNT(*)\nCannot use aggregate function in WHERE clause because order of execution is FROM then WHERE"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#grouping",
    "href": "block_3/513_databases/513_databases.html#grouping",
    "title": "Relational Databases",
    "section": "",
    "text": "GROUP BY clause is used to group rows with the same values\n\n-- Formal syntax\nSELECT\n    grouping_columns, aggregated_columns\nFROM\n    table1\nWHERE -- filter rows before grouping\n    condition\nGROUP BY -- must be between WHERE and ORDER BY\n    grouping_columns\nHAVING -- Used to filter groups (after grouping)\n    group_condition\nORDER BY\n    grouping_columns"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#joining-tables",
    "href": "block_3/513_databases/513_databases.html#joining-tables",
    "title": "Relational Databases",
    "section": "",
    "text": "JOIN clause is used to combine rows from two or more tables based on a related column between them\n\nDEFAULT: INNER JOIN\n\n\nSELECT -- columns from both tables\n    t1.column1, t2.column2\nFROM\n    table1 AS t1 -- after using alias, use alias instead of table name\njointype\n    table2 AS t2\nON -- condition to join tables\n    t1.column = t2.column\n\n\n\n\n\n\n\nJoin Type\nDescription\n\n\n\n\nCROSS\nReturns the Cartesian product of the sets of rows from the joined tables (all possible combinations)\n\n\nINNER\nReturns rows when there is a match in both tables. (intersect)\n\n\nNATURAL\nReturns all rows without specifying ON, names have to be the same in both tables.\n\n\nLEFT OUTER\nReturns all rows from the left-hand table, plus any rows in the right-hand table that match the left-hand side.\n\n\nRIGHT OUTER\nReturns all rows from the right-hand table, plus any rows in the left-hand table that match the right-hand side.\n\n\nFULL OUTER\nReturns all rows from both tables, with nulls in place of those rows that have no match in the other table."
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#data-manipulation",
    "href": "block_3/513_databases/513_databases.html#data-manipulation",
    "title": "Relational Databases",
    "section": "",
    "text": "Add new rows to table, by:\n\ncolumn position:\n    INSERT INTO table_name\n    VALUES\n        (value1, value2, ...),\n        (value1, value2, ...), -- can insert multiple rows at once\n        ...\n\nvalues need to be in the same order as the columns\ndon’t need to specify column names\n\ncolumn name:\nINSERT INTO table_name(column1, column2, ...)\nVALUES (value1, value2, ...)\n\nvalues can be in any order\nneed to specify column names\n\nfrom another table:\nINSERT INTO\n    table_name(column1, column2, ...)\nSELECT *\nFROM other_table\n\n\n\n\n\n\nModify existing rows in table\n\nUPDATE table_name\nSET column1 = value1,\n    column2 = value2,\n    ...\nWHERE condition\n\n\n\n\nRemove rows from table\nremoves all rows but keeps table if no WHERE clause\n\nDELETE FROM table_name\nWHERE condition\n\n\n\n\nRemove all rows from table\nfaster than DELETE because it doesn’t scan every row\n\nalso does not log each row deletion\n\n\nTRUNCATE TABLE table_name"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#creating-altering-and-dropping-tables",
    "href": "block_3/513_databases/513_databases.html#creating-altering-and-dropping-tables",
    "title": "Relational Databases",
    "section": "",
    "text": "CREATE TABLE table_name (\n    column1 datatype [constraint] PRIMARY KEY, -- for simple primary key\n    column2 datatype UNIQUE, -- unique constraint\n    column3 TEXT DEFAULT 'default value', -- default value\n    column4 datatype\n        [CONSTRAINT constraint_name] CHECK (condition), -- check constraint\n    -- e.g.\n    name TEXT NOT NULL,\n    phone CHAR(12) CHECK (phone LIKE '___-___-____')\n    ...\n\n    -- table constraints\n    -- if set composite primary key, also can simple\n    [CONSTRAINT constraint_name] PRIMARY KEY (column1, column2, ...),\n);\n\n\n\n\nSimple key: a single column\nComposite key: multiple columns\n\n\n\n\nCan uniquely identify a table row\nMust be minimal: no subset of the candidate key can be a candidate key\nCan have multiple in a table (e.g. id and email)\n\n\n\n\n\nA candidate key that is chosen to be the main identifier of a table\nAutomaticaly unique and not null\nMust be unique and not null\nMust be minimal\n\ngenerally the candidate key with the fewest columns\n\n\n\n\n\n\nA column that references a primary key in another table\nPrevents invalid data from being inserted\nCan be null\nChild table: table with foreign key\nParent table: table with primary key\n\n-- parent table\nCREATE TABLE instructor (\n    id INTEGER PRIMARY KEY,\n    name TEXT NOT NULL,\n    email TEXT UNIQUE,\n);\n\n-- child table\nCREATE TABLE instructor_course (\n    id INTEGER,\n    course_id TEXT,\n    PRIMARY KEY (id, course_id), -- composite primary key\n    FOREIGN KEY (id) REFERENCES instructor (id)\n        ON DELETE CASCADE,\n\n    -- can also specify column name\n    FOREIGN KEY (course_id) REFERENCES instructor (id)\n        ON DELETE CASCADE -- delete child rows when parent row is deleted\n        ON UPDATE CASCADE -- update child rows when child row is updated\n);\n\nForeign key constraint ensures there are no orphaned rows\n\ni.e. all foreign key in child table must exist in parent table\n\n\n\n\n\n\n\n\n\nReferential Action\nDescription\n\n\n\n\nNO ACTION\nDefault. Rejects update or delete of parent row\n\n\nSET NULL\nSets foreign key to null\n\n\nCASCADE\nDeletes or updates child rows when parent row is deleted or updated\n\n\nSET DEFAULT\nSets foreign key to default value\n\n\n\n\n\n\n\n\nUsed to store temporary data\nAutomatically dropped at end of session\nPrivate to current session\nPhysically stored in temp tablespace\n\nCREATE TEMPORARY TABLE table_name (\n    ...\n);\n\n-- can also create from another table\nCREATE TEMPORARY TABLE\n    temp_table_name\nAS\n    SELECT name, department FROM other_table\n;"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#drop-table",
    "href": "block_3/513_databases/513_databases.html#drop-table",
    "title": "Relational Databases",
    "section": "",
    "text": "Remove table and all its data\nPostgres does not allow dropping a table if it has a foreign key constraint\n\nneed to drop the foreign key constraint first\nor use CASCADE to drop the table and all its foreign key constraints\n\n\nDROP TABLE table_name [CASCADE];"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#transaction",
    "href": "block_3/513_databases/513_databases.html#transaction",
    "title": "Relational Databases",
    "section": "",
    "text": "A transaction is a sequence of SQL statements that are treated as a unit, so either all of them are executed, or none of them are executed.\n\n[{BEGIN|START} [TRANSACTION]]; -- BEGIN more common, can just start  with `BEGIN`\n\n&lt;SQL statements&gt;\n\n{COMMIT|ROLLBACK};\n\nCOMMIT: make all changes made by the transaction permanent\n\nwithout COMMIT, changes are not visible to other users\n\nROLLBACK: undo all changes made by the transaction\n\n\n\nProperties of a transaction:\n\nAtomicity: all or nothing (transaction either completes or is aborted)\nConsistency: database must be in a consistent state before and after the transaction\n\ne.g. no scores &gt; 100, transaction will not be committed if it violates this\n\nIsolation: if two transactions are executed concurrently, the result should be the same as if they were executed one after the other\nDurability: changes made by a committed transaction must be permanent (even if there is a system failure), achieved using transaction log"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#subqueries",
    "href": "block_3/513_databases/513_databases.html#subqueries",
    "title": "Relational Databases",
    "section": "",
    "text": "A subquery is a query within a query\nNormally used to mix aggregate and non-aggregate queries\nCan only be used if subquery returns ONLY 1 value.\n\n-- Selects countries with a population greater than the average population of all countries\nSELECT\n    name\nFROM\n    country\nWHERE\n    surfacearea &gt; (\n        SELECT AVG(surfacearea) FROM country\n    )\n;\n\n\n\nA correlated subquery is a subquery that uses values from the outer query\n\n-- Selects countries with the largest population in their continent\nSELECT\n    c1.name, c1.continent\nFROM\n    country c1\nWHERE\n    c1.population = (\n        SELECT\n            MAX(c2.population)\n        FROM\n            country c2\n        WHERE\n            c2.continent = c1.continent\n    )\n;\n\n\n\n\nANY and ALL are used to compare a value to a list or subquery\n\n\n\n\nANY\nALL\n\n\n\n\nif one of values true =&gt; true\nif all values true =&gt; true\n\n\n\ne.g.\n-- Find the names of all countries that have a\n-- population greater than all European countries.\nSELECT\n    name\nFROM\n    country\nWHERE\n    continent &lt;&gt; 'Europe'\n    AND\n    population &gt; ALL (\n        SELECT\n            population\n        FROM\n            country\n        WHERE\n            continent = 'Europe'\n    )\n;\n\nEXISTS is used to check if a subquery returns any rows\n\nfaster than IN because it stops as soon as it finds a match\n\n\nSELECT co.name\nFROM country co\nWHERE EXISTS (SELECT * FROM city ci\n        WHERE co.code = ci.countrycode AND ci.population &gt; 5000000);\n        ```"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#views",
    "href": "block_3/513_databases/513_databases.html#views",
    "title": "Relational Databases",
    "section": "",
    "text": "Is a named result of a SELECT query\n\nBehaves like a table (called virtual table sometimes)\nCurrent and Dynamic: whenever you query a view, you get the most up-to-date data\nViews persists: they are kept\nNot materialized: they are not stored in the database\nViews can hide complexity\nManage access to data\nDo not support constraints\n\n\nCREATE VIEW view_name AS\n    select_statment\n;\n\nDROP VIEW [IF EXISTS] view_name;\n\n\n\nMaterialized views are stored in the database\nThey are updated periodically\nThey are used for performance reasons (a lot faster)\n\nCREATE MATERIALIZED VIEW my_mat_view AS\n    select_statement\n;\n\nREFRESH MATERIALIZED VIEW [CONCURRENTLY] my_mat_view;\n-- CONCURRENTLY: allows you to query the view while it is being refreshed, no guarantee it is up-to-date\n\nDROP MATERIALIZED VIEW my_mat_view;"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#temporary-tables",
    "href": "block_3/513_databases/513_databases.html#temporary-tables",
    "title": "Relational Databases",
    "section": "",
    "text": "DROP TABLE IF EXISTS temp_table_name;\n\nCREATE TEMPORARY TABLE temp_table_name AS (\n    ...\n);"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#ctes",
    "href": "block_3/513_databases/513_databases.html#ctes",
    "title": "Relational Databases",
    "section": "",
    "text": "Common Table Expressions: temporary named result of a query\n\nWITH\n    expression_name [(column_names, ...)]\nAS (\n    query\n)\nquery\n-- SELECT * FROM expression_name;\n-- This query needs to use the column names defined in the CTE after the expression name\n;"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#summary-tt-vs-views-vs-ctes",
    "href": "block_3/513_databases/513_databases.html#summary-tt-vs-views-vs-ctes",
    "title": "Relational Databases",
    "section": "",
    "text": "Feature\nTemporary Tables\nViews\nMaterialized Views\nCTEs\n\n\n\n\nNature\nPhysical table, session-scoped\nVirtual table\nPhysical storage of query results\nTemporary result set\n\n\nData Storage\nStores data temporarily\nDoes not store data, only stores the query itself\nStores results of a query\nDoes not store data, used within query execution\n\n\nScope\nExists only during a database session\nPermanent, unless dropped\nPermanent, unless dropped\nExists only during the execution of a query\n\n\nAccess\nCan only be accessed by 1 user\nCan be accessed by multiple users\nCan be accessed by multiple users\nCan be accessed by multiple users\n\n\nUsage\nIntermediate data storage, complex queries\nSimplifying access to complex queries, data security\nPerformance improvement for complex queries\nBreaking down complex queries, recursive queries  can manipulate default processing order of SQL clauses\n\n\nPerformance\nDependent on data size and indexes\nExecutes underlying query each time\nFaster access, as data is pre-calculated\nDependent on the complexity of the query\n\n\nUpdates\nData persists for the session, can be updated\nReflects real-time data from base tables\nRequires refresh to update data\nNot applicable (recomputed with each query execution)\n\n\nIndexing\nCan have indexes\nCannot have indexes\nCan have indexes\nNot applicable"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#window-functions",
    "href": "block_3/513_databases/513_databases.html#window-functions",
    "title": "Relational Databases",
    "section": "",
    "text": "Window functions are used to compute aggregate values over a group of rows\nOnly allowed in SELECT and ORDER BY clauses\nProcessed:\n\nafter WHERE, GROUP BY, and HAVING\nbefore SELECT and ORDER BY\n\n\nSELECT\n    column,\n    window_function_name(expression) OVER (\n        PARTITION BY column\n        ORDER BY column\n        frame_clause\n    )\n    -- e.g.\n    continent,\n    MAX(population)\n        OVER (PARTITION BY continent)\nFROM\n    table\n;\n\n\n\nAggregate functions\n\nAVG, COUNT, MAX, MIN, SUM\n\nRanking functions\n\nCUME_DIST(): returns the cumulative distribution, i.e. the percentage of values less than or equal to the current value.\nNTILE(): given a specified number of buckets, it tells us in which bucket each row goes among other rows in the partition.\nPERCENT_RANK(): similar to CUME_DIST(), but considers only the percentage of values less than (and not equal to) the current value.\nDENSE_RANK(): returns the rank of a row within a partition without jumps after duplicate ranks (e.g. 1, 2, 2, 3, …)\nRANK(): returns the rank of row within a partition but with jumps after duplicates ranks (e.g. 1, 2, 2, 4, …)\nROW_NUMBER(): returns simply the number of a row in a partition, regardless of duplicate values"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#relational-vs.-non-relational-databases",
    "href": "block_3/513_databases/513_databases.html#relational-vs.-non-relational-databases",
    "title": "Relational Databases",
    "section": "",
    "text": "Elements\nRelational\nNon-relational\n\n\n\n\nData Model\ntables\nkey-value pairs, documents, graphs, etc.\n\n\nSchema\nfixed (updates are complicated and time-consuming)\ndynamic (schema-free)\n\n\nQuerying\nsemi standard (SQL)\nproprietary\n\n\nPerformance\nstrong data consistency and integrity\nfaster performance for specific usecases\n\n\nEfficiency\nwrite times and table locks reduce efficiency\nfaster read and write times\n\n\nScalability\nvertical scaling\nhorizontal scaling\n\n\nDevelopment\nrequire more effort\neasier to develop and require fewer resources\n\n\nPrinciples\nACID (Atomicity, Consistency, Isolation, Durability)\nBASE (Basically Available, Soft state, Eventual consistency)"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#nosql-not-only-sql",
    "href": "block_3/513_databases/513_databases.html#nosql-not-only-sql",
    "title": "Relational Databases",
    "section": "",
    "text": "Main goals:\n\nreduce the dependence on fixed schema\nreduce the dependence on a single point of truth\nscaling\n\n\n\n\n\nSchema-free\nAccepting Basic Availability\n\nusually see all tweets, but sometimes a refresh shows new tweets\n\nBASE:\n\nBasically Available\nSoft state\nEventual consistent\n\n\n\n\n\n\nKey-value stores: like a dictionary\n\nRedis, Memcached, Amazon DynamoDB\n\nDocument stores: type of key-value store but with an internal searchable structure.\n\nDocument: contains all the relevant data and has a unique key\nMongoDB, CouchDB\n\nColumn stores: based on Google’s BigTable\n\nCassandra, HBase, Google BigTable\n\nGraph databases: based on graph theory\n\nNeo4j, OrientDB, Amazon Neptune"
  },
  {
    "objectID": "block_3/513_databases/513_databases.html#mongodb",
    "href": "block_3/513_databases/513_databases.html#mongodb",
    "title": "Relational Databases",
    "section": "",
    "text": "Based on JSON-like (JavaScript Object Notation) documents for storage:\n\nActually it is BSON (Binary JSON)\nMax BSON document size: 16MB\n\nStructure:\n\nDatabase: client.get_database_names()\nCollection: client[db_name].get_collection_names()\nDocument: client[db_name][collection_name].find_one() or client[db_name][collection_name].find()\n\n\n\n\n\nclient[db_name][collection_name].find()\n\nreturns a cursor\nlist(client[db_name][collection_name].find()) returns a list of documents\nnext(client[db_name][collection_name].find()) returns the next document\n\n\nclient = MongoClient()\nclient[db_name][collection_name].find(\n    filter={...},\n    projection={...},\n    sort=[...],\n    skip=...,\n    limit=...,\n)\n\n\n\nfilter is a dictionary\nfilter = {\"key\": \"value\"} returns all documents where key is value\n\n\n\n\n\n\n\n\n\nOperator\nsymbol\nnotes\n\n\n\n\n$eq or $neq\n= or ~=\n\n\n\n$gt or $gte\n&gt; or &gt;=\n\n\n\n$lt or $lte\n&lt; or &lt;=\n\n\n\n$ne\n!=\n\n\n\n$in\nin\nif any of the values in the list matches the value of the key, the document is returned\n\n\n$nin\nnot in\n\n\n\n$all\nall\nall values in the list must match the value of the keys\n\n\n$exists\nexists\n\n\n\n$regex\nregex\n\n\n\n\n\nAlso can use $not to negate the operator\nusing $or, $nor and $and:\n\nfilter = {\n    \"$or\": [\n        {\"key_1\": \"value_1\"},\n        {\"key_2\": \"value_2\"},\n    ],\n    \"$and\": [\n        {\"key_3\": \"value_3\"},\n        {\"key_4\": \"value_4\"},\n    ],\n    \"key_5\": {\"$gte\": 10, \"$lte\": 20}, # and is implied\n}\n\nfilter = {\n  \"key_1\": \"value_1\",\n  \"key_2\": \"value_2\", # and is implied\n}\n\n\n\n\nprojection is a dictionary\nprojection = {\"key_1\": 1, \"key_2\": 0} returns all documents with only key_1 and without key_2\n\n\n\n\n\nsort is a list of tuples\n\n1: ascending order\n-1: descending order\n\nsort = [(\"key_1\", 1), (\"key_2\", -1)] sorts by key_1 in ascending order and then by key_2 in descending order\n\n\n\n\n\nlimit is an integer\nlimit = 10 returns the first 10 documents\n\n\n\n\n\nskip is an integer\nskip = 10 skips the first 10 documents\n\n\n\n\n\ncount_documents is a method\nclient[db_name][collection_name].count_documents(filter={...}) returns the number of documents that match the filter\n\n\n\n\n\ndistinct is a method\nclient[db_name][collection_name].distinct(\"key_1\") returns a list of distinct values for key_1"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html",
    "title": "Supervised Learning II",
    "section": "",
    "text": "Infinite amount of numbers but finite amount of bits to represent them\nThese small errors will accumulate and cause problems\n\n\n\n\nlarge datasets with millions of params\nsmall errors can accumulate and cause problems\n\n\n\n\n\nBinary numbers are represented as a sum of powers of 2\ne.g. 104 in binary is 1101000 = \\(1(2^6) + 1(2^5) + 0(2^4) + 1(2^3) + 0(2^2) + 0(2^1) + 0(2^0) = 64 + 32 + 8 = 104\\)\nUnsigned Integers: \\(2^n - 1\\) is the largest number that can be represented with n bits\n\ne.g. 8 bits can represent 0 to 255\nnp.iinfo(np.uint8) gives the min and max values\n\nSigned Integers: \\(2^{n-1} - 1\\) is the largest positive number that can be represented with n bits\n\n\\(-2^{n-1}\\) is the smallest negative number that can be represented with n bits\ne.g. 8 bits can represent -128 to 127 (0 is included in the positive numbers)\n1 bit is used to represent the sign\nnp.iinfo(np.int8) gives the min and max values\n\n\n\n\n\n\n14.75 in binary is 1110.11\n\n\n\n\n2^3\n2^2\n2^1\n2^0\n2^-1\n2^-2\n\n\n\n\n1\n1\n1\n0\n1\n1\n\n\n8\n4\n2\n0\n0.5\n0.25\n\n\n\n$ 8 + 4 + 2 + 0 + 0.5 + 0.25 = 14.75 $\n\n\n\n\nWe typically have a fixed number of bits to represent the fractional part\ne.g. 8 bits total, 4 bits for the integer part and 4 bits for the fractional part\n\nmax value is 15.9375 (\\(2^3 + 2^2 + 2^1 + 2^0 + 2^{-1} + 2^{-2} + 2^{-3} + 2^{-4}\\))\n\noverflow if try a higher value\n\nmin value (bigger than 0) is 0.0625 (\\(2^{-4}\\))\n\nor precision of 0.0625 (any less =&gt; underflow)\n\n\n\n\n\n\n\nRather than having a fixed location for the binary point, we let it “float” around.\n\nlike how we write 0.1234 as 1.234 x 10^-1\n\nFormat: \\[(-1)^S \\times 1. M \\times 2^E\\]\n\nS is the sign bit\nM is the mantissa, always between 1 and 2 (1.0 is implied)\nE is the exponent\n\n\nFloat 64 (double precision) \nFloat 32 (single precision) \n\n\n\n\n\n\nThe spacing changes depending on the floating point number (because of the exponent)\n\n\n\nimport numpy as np\n\nnp.spacing(1e16) # 1.0\n\nnp.nextafter(1e16, 2e16) - 1e16 # 1.0\n\n\n\n\n\n\n1.0 + 2.0 + 3.0 == 6.0 True\n0.1 + 0.2 == 0.3 False\n\n0.1, 0.2, and 0.3 are not exactly representable in binary\n\n1e16 + 1 == 1e16 True\n\n1 is less than the spacing, so it is rounded back\n\n1e16 + 2.0 == 1e16 False\n\n2.0 is greater than the spacing, so it is rounded up\n\n1e16 + 1.0 + 1.0  == 1e16 True\n\n1.0 is less than the spacing, so it is rounded back, then 1.0 is added, which is less than the spacing, so it is rounded back again\n\n\n\n\n\n\n\nIn ML, we want to minimize a loss function\n\ntypically a sum of losses over the training set\n\nCan think of ML as a 3 step process:\n\nChoose model: controls space of possible functions that map X to y\nChoose loss function: measures how well the model fits the data\nChoose optimization algorithm: finds the best model\n\n\n\n\n\nOptimization: process to min/max a function\nObjective Function: function to be optimized\nDomain: set to search for optimal value\nMinimizer: value that minimizes the objective function\n\n\n\n\nCommon loss function is MSE (mean squared error):\n\\[L(w) = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2\\]\nUsing a simple linear regression model \\(y = w_0 + w_1x\\), we can rewrite the loss function as:\n\\[L(w) = \\frac{1}{n} \\sum_{i=1}^n ((w_0 + w_1x_i) - y_i)^2\\]\nSo optimization is finding the values of \\(w_0\\) and \\(w_1\\) that minimize the loss function, \\(L(w)\\).\nIn vector format:\n\\[\\text{MSE} = \\mathcal{L}(\\mathbf{w}) = \\frac{1}{n}\\sum^{n}_{i=1}(\\mathbf{x}_i \\mathbf{w} - y_i)^2\\]\nIn full-matrix format\n\\[\\text{MSE} = \\mathcal{L}(\\mathbf{w}) = \\frac{1}{n}(\\mathbf{X} \\mathbf{w} - \\mathbf{y})^T (\\mathbf{X} \\mathbf{w} - \\mathbf{y}) \\]\n\n\n\n\\[\n\\mathbf{y}=\n\\left[\n\\begin{array}{c} y_1 \\\\\n\\vdots \\\\\ny_i \\\\\n\\vdots\\\\\ny_n\n\\end{array}\n\\right]_{n \\times 1}, \\quad\n\\mathbf{X}=\n\\left[\n\\begin{array}{c} \\mathbf{x}_1 \\\\\n\\vdots \\\\\n\\mathbf{x}_i \\\\\n\\vdots\\\\\n\\mathbf{x}_n\n\\end{array}\n\\right]_{n \\times d}\n= \\left[\\begin{array}{cccc}\nx_{11} & x_{12} & \\cdots & x_{1 d} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{i 1} & x_{i 2} & \\cdots & x_{i d}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n 1} & x_{n 2} & \\cdots & x_{n d}\n\\end{array}\\right]_{n \\times d},\n\\quad\n\\mathbf{w}=\n\\left[\n\\begin{array}{c} w_1 \\\\\n\\vdots\\\\\nw_d\n\\end{array}\n\\right]_{d \\times 1}\n\\]\n\n\\(n\\): number of examples\n\\(d\\): number of input features/dimensions\n\nThe goal is to find the weights \\(\\mathbf{w}\\) that minimize the loss function.\nFormulas:\n\\[\\mathbf{y} = \\mathbf{X} \\mathbf{w}\\]\n\\[\\hat{\\mathbf{y}_i} = \\mathbf{w}^T \\mathbf{x}_i\\]\n\n\n\n\n\nOne of the most important optimization algorithms in ML\nIterative optimization algorithm\nSteps:\n\nstart with some arbitrary \\(\\mathbf{w}\\)\ncalculate the gradient using all training examples\nuse the gradient to adjust \\(\\mathbf{w}\\)\nrepeat for \\(I\\) iterations or until the step-size is sufficiently small\n\nCost: \\(O(ndt)\\) for t iterations, better than brute force search \\(O(nd^2 + d^3)\\)\n\n\\[w_{t+1} = w_t - \\alpha \\nabla= L(w_t)\\]\n\n\\(w_t\\): current value of the weights\n\\(\\alpha\\): learning rate\n\\(\\nabla L(w_t)\\): gradient of the loss function at \\(w_t\\)\n\n\n\n\nLoss function: \\(L(w) = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2\\)\nGradient: \\(\\nabla L(w) = \\frac{d}{dw} L(w) = \\frac{2}{n} \\sum_{i=1}^n x_{i1} (x_{i1} w_1 - y_i)\\)\n\nOr in Matrix form: \\(\\nabla L(w) = \\frac{2}{n} \\mathbf{X}^T (\\mathbf{X} \\mathbf{w} - \\mathbf{y})\\)\n\n\n\n\n\n\nNeed to scale for the contour plot to be more “round”\n\nbetter for gradient descent\n\n\n\n\nIn real life, contour plots are not so nice\n\n\n\n\n\n\nInitialization: Start with an initial set of parameters, often randomly chosen.\nForward pass: Generate predictions using the current values of the parameters. (E.g., \\(\\hat{y_i} = x_{1}w_1 + Bias\\) in the toy example above)\nLoss calculation: Evaluate the loss, which quantifies the discrepancy between the model’s predictions and the actual target values.\nGradient calculation: Compute the gradient of the loss function with respect to each parameter either on a batch or the full dataset. This gradient indicates the direction in which the loss is increasing and its magnitude.\nParameter Update: Adjust the parameters in the opposite direction of the calculated gradient, scaled by the learning rate. This step aims to reduce the loss by moving the parameters toward values that minimize it.\n\n\n\n\n\nUse minimize function from scipy.optimize\n\nfrom scipy.optimize import minimize\n\ndef mse(w, X, y):\n    \"\"\"Mean squared error.\"\"\"\n    return np.mean((X @ w - y) ** 2)\n\ndef mse_grad(w, X, y):\n    \"\"\"Gradient of mean squared error.\"\"\"\n    n = len(y)\n    return (2/n) * X.T @ (X @ w - y)\n\nout = minimize(mse, w, jac=mse_grad, args=(X_scaled_ones, toy_y), method=\"BFGS\")\n# jac: function to compute the gradient (optional)\n# - will use finite difference approximation if not provided\n\nOther methods:\n\nBFGS: Broyden–Fletcher–Goldfarb–Shanno algorithm\nCG: Conjugate gradient algorithm\nL-BFGS-B: Limited-memory BFGS with bounds on the variables\nSLSQP: Sequential Least SQuares Programming\nTNC: Truncated Newton algorithm\n\n\n\n\n\n\n\nInstead of updating our parameters based on a gradient calculated using all training data, we simply use one of our data points (the \\(i\\)-th one)\n\nGradient Descent\nLoss function:\n\\[\\text{MSE} = \\mathcal{L}(\\mathbf{w}) = \\frac{1}{n}\\sum^{n}_{i=1} (\\mathbf{x}_i \\mathbf{w} - y_i)^2\\]\nUpdate procedure:\n\\[\\mathbf{w}^{j+1} = \\mathbf{w}^{j} - \\alpha \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}^{j})\\]\nStochastic Gradient Descent\nLoss function:\n\\[\\text{MSE}_i = \\mathcal{L}_i(\\mathbf{w}) = (\\mathbf{x}_i \\mathbf{w} - y_i)^2\\]\nUpdate procedure: \\[\\mathbf{w}^{j+1} = \\mathbf{w}^{j} - \\alpha \\nabla_\\mathbf{w} \\mathcal{L}_i(\\mathbf{w}^{j})\\]\n\n\n\n\n\nGradient Descent\nStochastic Gradient Descent\n\n\n\n\nUse all data points\nUse one data point\n\n\nSlow\nFast\n\n\nAccurate\nLess Accurate\n\n\n\n\nMini-batch Gradient Descent is a (in-between) compromise between the two\nInstead of using a single data point, we use a small batch of data points d\n\n\n\n\nShuffle and divide all data into \\(k\\) batches, every example is used once\n\nDefault in PyTorch\nAn example will only show up in one batch\n\nChoose some examples for each batch without replacement\n\nAn example may show up in multiple batches\nThe same example cannot show up in the same batch more than once\n\nChoose some examples for each batch with replacement\n\nAn example may show up in multiple batches\nThe same example may show up in the same batch more than once\n\n\n\n\n\n\nAssume we have a dataset of \\(n\\) observations (also known as rows, samples, examples, data points, or points)\n\nIteration: each time you update model weights\nBatch: a subset of data used in an iteration\nEpoch: One full pass through the dataset to look at all \\(n\\) observations\n\nIn other words,\n\nIn GD, each iteration involves computing the gradient over all examples, so\n\n\\[1 \\: \\text{iteration} = 1 \\: \\text{epoch}\\]\n\nIn SGD, each iteration involves one data point, so\n\n\\[n \\text{ iterations} = 1 \\: \\text{epoch}\\]\n\nIn MGD, each iteration involves a batch of data, so\n\n\\[\n\\begin{align}\n\\frac{n}{\\text{batch size}} \\text{iterations} &= 1 \\text{ epoch}\\\\\n\\end{align}\n\\]\n*Note: nobody really says “minibatch SGD”, we just say SGD: in SGD you can specify a batch size of anything between 1 and \\(n\\)\n\n\n\n\n\nModels that does a good job of approximating complex non-linear functions\nIt is a sequence of layers, each of which is a linear transformation followed by a non-linear transformation\n\n\n\n\nNode (or neuron): a single unit in a layer\nInput layer: the features of the data\nHidden layer: the layer(s) between the input and output layers\nOutput layer: the prediction(s) of the model\nWeights: the parameters of the model\nActivation function: the non-linear transformation (e.g. ReLU, Sigmoid, Tanh, etc.)\n\n\nX : (n x d), W : (h x d), b : (n x h), where h is the number of hidden nodes b is actually 1 x hs, but we can think of it as n x hs because it is broadcasted\n\\[\\mathbf{H}^{(1)} = \\phi^{(1)} (\\mathbf{X}\\mathbf{W}^{(1)\\text{T}} + \\mathbf{b}^{(1)})\\]\n\\[\\mathbf{H}^{(2)} = \\phi^{(2)} (\\mathbf{H}^{(1)}\\mathbf{W}^{(2)\\text{T}} + \\mathbf{b}^{(2)})\\]\n\\[\\mathbf{Y} = (\\mathbf{H}^{(2)}\\mathbf{W}^{(3)\\text{T}} + \\mathbf{b}^{(3)})\\]\n\nIn a layer, \\[\\text{ num of weights} = \\text{num of nodes in previous layer} \\times \\text{num of nodes in current layer}\\]\n\n\\[\\text{num of biases} = \\text{num of nodes in current layer}\\]\n\\[\\text{num of parameters} = \\text{num of weights} + \\text{num of biases}\\]\n\n\n\n\n\n\n\nBackpropagation: a method to calculate the gradient of the loss function with respect to the weights\nChain rule: a method to calculate the gradient of a function composed of multiple functions\nIt is pretty complicated, but PyTorch does it for us\n\n\n\n\n\n\nNeural networks with &gt; 1 hidden layer\n\nNN with 1 hidden layer: shallow neural network\n\n\n\n\n\n\n\nPyTorch is a popular open-source machine learning library by Facebook based on Torch\nIt is a Python package that provides two high-level features:\n\nTensor computation (like NumPy) with strong GPU acceleration\nGradient computation through automatic differentiation\n\n\n\n\n\nSimilar to ndarray in NumPy\n\nimport torch\n\n# Create a tensor\nx = torch.tensor([1, 2, 3, 4, 5]) # int\nx = torch.tensor([1, 2, 3, 4, 5.]) # float\nx = torch.tensor([[1, 2], [3, 4], [5, 6]])\n\ny = torch.zeros(3, 2)\ny = torch.ones(3, 2)\ny = torch.rand(3, 2)\n\n# Check the shape, dimensions, and data type\nx.shape\nx.ndim\nx.dtype\n\n# Operations\na = torch.rand(1, 3)\nb = torch.rand(3, 1)\n\na + b # broadcasting\na * b # element-wise multiplication\na @ b # matrix multiplication\na.mean()\na.sum()\n\n# Indexing\na[0,:] # first row\na[0] # first row\na[:,0] # first column\n\n# Convert to NumPy\nx.numpy()\n\n\n\n# Check if GPU is available\ntorch.backends.mps.is_available() # mac M chips\ntorch.cuda.is_available() # Nvidia GPU\n\n# To activate GPU\ndevice = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nx.to('cpu') # move tensor to cpu\n\n\n\nuse backward() to compute the gradient, backpropagation\n\nX = torch.tensor([1.0, 2.0, 3.0], requires_grad=False)\nw = torch.tensor([1.0], requires_grad=True)  # Random initial weight\ny = torch.tensor([2.0, 4.0, 6.0], requires_grad=False)  # Target values\nmse = ((X * w - y)**2).mean()\nmse.backward()\nw.grad\n\n\n\n\n\nEvery NN model has to inherit from torch.nn.Module\n\nfrom torch import nn\n\nclass linearRegression(nn.Module):  # inherit from nn.Module\n\n    def __init__(self, input_size, output_size):\n        super().__init__()  # call the constructor of the parent class\n\n        self.linear = nn.Linear(input_size, output_size,)  # wX + b\n\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\n# Create a model\nmodel = linearRegression(1, 1) # input size, output size\n\n# View model\nsummary(model)\n\n## Train the model\nLEARNING_RATE = 0.02\ncriterion = nn.MSELoss()  # loss function\noptimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)  # optimization algorithm is SGD\n\n# DataLoader for mini-batch\nfrom torch.utils.data import DataLoader, TensorDataset\n\nBATCH_SIZE = 50\ndataset = TensorDataset(X_t, y_t)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n# Training\ndef trainer(model, criterion, optimizer, dataloader, epochs=5, verbose=True):\n    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n\n    for epoch in range(epochs):\n        losses = 0\n\n        for X, y in dataloader:\n\n            optimizer.zero_grad()       # Clear gradients w.r.t. parameters\n            y_hat = model(X).flatten()  # Forward pass to get output\n            loss = criterion(y_hat, y)  # Calculate loss\n            loss.backward()             # Getting gradients w.r.t. parameters\n            optimizer.step()            # Update parameters\n            losses += loss.item()       # Add loss for this batch to running total\n\n        if verbose: print(f\"epoch: {epoch + 1}, loss: {losses / len(dataloader):.4f}\")\n\ntrainer(model, criterion, optimizer, dataloader, epochs=30, verbose=True)\n\n\n\n\nuse torch.nn.Sequential to create a model\n\nclass nonlinRegression(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n\n        self.main = torch.nn.Sequential(\n            nn.Linear(input_size, hidden_size),  # input -&gt; hidden layer\n            nn.Sigmoid(),                        # sigmoid activation function in hidden layer\n            nn.Linear(hidden_size, output_size)  # hidden -&gt; output layer\n        )\n\n    def forward(self, x):\n        x = self.main(x)\n        return x\n\n\n\n\n\n\nTask\nCriterion (Loss)\nOptimizer\n\n\n\n\nRegression\nMSELoss\nSGD\n\n\nBinary Classification\nBCELoss\nAdam\n\n\nMulti-class Classification\nCrossEntropyLoss\nAdam\n\n\n\n\nInput of CrossEntropyLoss doesn’t need to be normalized (i.e. no need to sum to 1/ no need to use nn.Softmax)\n\n# criterions\nfrom torch import nn\nreg_criterion = torch.nn.MSELoss()\nbc_criterion = torch.nn.BCEWithLogitsLoss()\nmse_criterion = torch.nn.CrossEntropyLoss()\n\n# optimizers\nfrom torch import optim\nreg_optim = torch.optim.SGD(model.parameters(), lr=0.2)\nclass_optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n\n\n\n\n\n\nIt is to calculate the gradient of the loss function with respect to the weights\nIt is a special case of the chain rule of calculus\nProcess:\n\nDo “forward pass” to calculate the output of the network (prediction and loss)\n\n\n\nDo “backward pass” to calculate the gradients of the loss function with respect to the weights. Below is an example of reverse-mode autmatic differentiation (backpropagation):\n\n\n\n\n\n\n\ntorch.autograd is PyTorch’s automatic differentiation engine that powers neural network training\n\nimport torch\n\n# Create model\nclass network(torch.nn.Module):\n    def __init__(self):\n        super(network, self).__init__()\n        self.layer1 = torch.nn.Linear(1, 6)\n        self.dropout = torch.nn.Dropout(0.2) # dropout layer\n        ...\n\n    def forward(self, x):\n        x = self.layer1(x)\n        ...\n        return x\n\nmodel = network()\ncriterion = torch.nn.MSELoss()\n\n# Forward pass\nloss = criterion(model(x), y)\n# Backward pass\nloss.backward()\n\n# Access gradients\nprint(model.layer1.weight.grad) # or model.layer1.weight.bias.grad\n\n# Update weights\nmodel.state_dict() # get the current weights\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\noptimizer.step() # update weights\n\n\n\n\nBackpropagation can suffer from two problems because of multiple chain rule applications:\n\nVanishing gradients: the gradients of the loss function with respect to the weights become very small\n\n0 gradients because of underflow\n\nExploding gradients: the gradients of the loss function with respect to the weights become very large\n\nPossible solutions:\n\nUse ReLU activation function: but it can also suffer from the dying ReLU problem (gradients are 0)\nWeight initialization: initialize the weights with small values\nBatch normalization: normalize the input layer by adjusting and scaling the activations\nSkip connections: add connections that skip one or more layers\nGradient clipping: clip the gradients during backpropagation\n\n\n\n\n\n\n\n\nAdd validation loss to the training loop\nEarly stopping: if we see the validation loss is increasing, we stop training\n\nDefine a patience parameter: if the validation loss increases for patience epochs, we stop training\n\nRegularization: add a penalty term to the loss function to prevent overfitting\n\nSee 573 notes for more details\nweight_decay parameter in the optimizer\n\nDropout: randomly set some neurons to 0 during training\n\nIt prevents overfitting by reducing the complexity of the model\ntorch.nn.Dropout(0.2)\n\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\ndef trainer(model, criterion, optimizer, trainloader, validloader, epochs=5, patience=5):\n    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n\n    train_loss = []\n    valid_loss = []\n\n    for epoch in range(epochs):  # for each epoch\n        train_batch_loss = 0\n        valid_batch_loss = 0\n\n        # Training\n        for X, y in trainloader:\n\n            optimizer.zero_grad()       # Zero all the gradients w.r.t. parameters\n\n            y_hat = model(X).flatten()  # Forward pass to get output\n            loss = criterion(y_hat, y)  # Calculate loss based on output\n            loss.backward()             # Calculate gradients w.r.t. parameters\n            optimizer.step()            # Update parameters\n\n            train_batch_loss += loss.item()  # Add loss for this batch to running total\n\n        train_loss.append(train_batch_loss / len(trainloader))\n\n        # Validation\n        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n\n            for X_valid, y_valid in validloader:\n\n                y_hat = model(X_valid).flatten()  # Forward pass to get output\n                loss = criterion(y_hat, y_valid)  # Calculate loss based on output\n\n                valid_batch_loss += loss.item()\n\n        valid_loss.append(valid_batch_loss / len(validloader))\n\n        # Early stopping\n        if epoch &gt; 0 and valid_loss[-1] &gt; valid_loss[-2]:\n            consec_increases += 1\n        else:\n            consec_increases = 0\n        if consec_increases == patience:\n            print(f\"Stopped early at epoch {epoch + 1} - val loss increased for {consec_increases} consecutive epochs!\")\n            break\n\n    return train_loss, valid_loss\n\nUsing the trainer function:\n\nimport torch\nimport torch.nn\nimport torch.optim\n\ntorch.manual_seed(1)\n\nmodel = network(1, 6, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05) # weight_decay=0.01 for L2 regularization\ntrain_loss, valid_loss = trainer(model, criterion, optimizer, trainloader, validloader, epochs=201, patience=3)\n\nplot_loss(train_loss, valid_loss)\n\n\n\n\nAny continuous function can be approximated arbitrarily well by a neural network with a single hidden layer\n\nIn other words, NN are universal function approximators\n\n\n\n\n\n\n\n\nDrastically reduces the number of params (compared to NN):\n\nhave activations depend on small number of inputs\nsame parameters (convolutional filter) are used for different parts of the image\n\nCan capture spatial information (preserves the structure of the image)\n\n\n\n\nIdea: use a small filter/kernel to extract features from the image\n\nFilter: a small matrix of weights (normally odd dimensioned -&gt; for symmetry)\n\n\n\n\nNotice that the filter results in a smaller output image\n\nThis is because we are not padding the image\nWe can add padding to the image to keep the same size\n\nPadding: add zeros around the image\n\nCan also add stride to move the filter more than 1 pixel at a time\n\n\n\n\n\n\nimg src\n\n\n\n\n\nconv_1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=(3,3))\n\nArguments:\n\nin_channels: number of input channels (gray scale image has 1 channel, RGB has 3)\nout_channels: number of output channels (similar to hidden nodes in NN)\nkernel_size: size of the filter\nstride: how many pixels to move the filter each time\npadding: how many pixels to add around the image\n\n\n\n\nSize of input image (e.g. 256x256) doesn’t matter, what matters is: in_channels, out_channels, kernel_size\n\n\\[\\text{total params} = (\\text{out channels} \\times \\text{in channels} \\times \\text{kernel size}^2) + \\text{out channels}\\]\n\\[\\text{output size} = \\frac{\\text{input size} - \\text{kernel size} + 2 \\times \\text{padding}}{\\text{stride}} + 1\\]\n\n\n\nImages: [batch_size, channels, height, width]\nKernel: [out_channels, in_channels, kernel_height, kernel_width]\n\nNote: before passing the image to the convolutional layer, we need to reshape it to the correct dimensions. Also if you want to plt.imshow() the image, you need to reshape it back to [height, width, channels].\n\n\n\n\n\nfeature learning -&gt; classification\nUse torch.nn.Flatten() to flatten the image\nAt the end need to either do regression or classification\n\n\n\n\n\nIdea: reduce the size of the image\n\nless params\nless overfitting\n\nCommon types:\n\nMax pooling: take the max value in each region\n\nWorks well since it takes the sharpest features\n\nAverage pooling: take the average value in each region\n\n\n\n\n\nclass CNN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.main = torch.nn.Sequential(\n\n            torch.nn.Conv2d(in_channels=1,\n                out_channels=3,\n                kernel_size=(3, 3),\n                padding=1),\n            torch.nn.ReLU(), # activation function\n            torch.nn.MaxPool2d((2, 2)),\n\n            torch.nn.Conv2d(in_channels=3,\n                out_channels=2,\n                kernel_size=(3, 3),\n                padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d((2, 2)),\n\n            torch.nn.Flatten(),\n            torch.nn.Linear(1250, 1)\n        )\n\n    def forward(self, x):\n        out = self.main(x)\n        return out\n# Trainer code\ndef trainer(\n    model, criterion, optimizer, trainloader, validloader, epochs=5, verbose=True\n):\n    train_loss, train_accuracy, valid_loss, valid_accuracy = [], [], [], []\n    for epoch in range(epochs):  # for each epoch\n        train_batch_loss = 0\n        train_batch_acc = 0\n        valid_batch_loss = 0\n        valid_batch_acc = 0\n\n        # Training\n        for X, y in trainloader:\n            if device.type in ['cuda', 'mps']:\n                    X, y = X.to(device), y.to(device)\n            optimizer.zero_grad()  # Zero all the gradients w.r.t. parameters\n            y_hat = model(X)  # Forward pass to get output\n            idx = torch.softmax(y_hat, dim=1).argmax(dim=1) # Multiclass classification\n            loss = criterion(y_hat, y)\n            loss.backward()  # Calculate gradients w.r.t. parameters\n            optimizer.step()  # Update parameters\n            train_batch_loss += loss.item()  # Add loss for this batch to running total\n            train_batch_acc += (\n                    (idx.squeeze() == y).type(\n                        torch.float32).mean().item()\n                )\n        train_loss.append(train_batch_loss / len(trainloader))\n        train_accuracy.append(train_batch_acc / len(trainloader))\n\n        # Validation\n        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n            for X, y in validloader:\n                if device.type in ['cuda', 'mps']:\n                    X, y = X.to(device), y.to(device)\n                y_hat = model(X)\n                idx = torch.softmax(y_hat, dim=1).argmax(dim=1)\n                loss = criterion(y_hat, y)\n                valid_batch_loss += loss.item()\n                valid_batch_acc += (\n                    (idx.squeeze() == y).type(\n                        torch.float32).mean().item()\n                )\n        valid_loss.append(valid_batch_loss / len(validloader))\n        valid_accuracy.append(valid_batch_acc / len(validloader))  # accuracy\n\n        # Print progress\n        if verbose:\n            print(\n                f\"Epoch {epoch + 1}:\",\n                f\"Train Loss: {train_loss[-1]:.3f}.\",\n                f\"Valid Loss: {valid_loss[-1]:.3f}.\",\n                f\"Train Accuracy: {train_accuracy[-1]:.2f}.\",\n                f\"Valid Accuracy: {valid_accuracy[-1]:.2f}.\",\n            )\n\n    results = {\n        \"train_loss\": train_loss,\n        \"train_accuracy\": train_accuracy,\n        \"valid_loss\": valid_loss,\n        \"valid_accuracy\": valid_accuracy,\n    }\n    return results\n\n\n\n\nTo get a summary of the model\n\nNo need to manually calculate the output size of each layer\n\n\nfrom torchsummary import summary\n\nmodel = CNN()\nsummary(model, (1, 256, 256))\n\n\n\n\n\n\n\nNormally there are 2 steps:\n\ncreate a dataset object: the raw data\ncreate a dataloader object: batches the data, shuffles, etc.\n\nUse torchvision to load the data\n\ntorchvision.datasets.ImageFolder: loads images from folders\nAssumes structure: root/class_1/xxx.png, root/class_2/xxx.png, …\n\n\nimport torch\nfrom torchvision import datasets, transforms\n\nIMAGE_SIZE = (256, 256)\nBATCH_SIZE = 32\n\n# create transform object\ndata_transforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.ToTensor()\n])\n\n# create dataset object\ntrain_dataset = datasets.ImageFolder(root='path/to/data', transform=data_transforms)\n\n# check out the data\ntrain_dataset.classes # list of classes\ntrain_dataset.targets # list of labels\ntrain_dataset.samples # list of (path, label) tuples\n\n# create dataloader object\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,          # our raw data\n    batch_size=BATCH_SIZE,  # the size of batches we want the dataloader to return\n    shuffle=True,           # shuffle our data before batching\n    drop_last=False         # don't drop the last batch even if it's smaller than batch_size\n)\n\n# get a batch of data\nimages, labels = next(iter(train_loader))\n\n\n\n\nPyTorch documentation\nConvention: .pt or .pth file extension\n\nPATH = \"models/my_cnn.pt\"\n\n# load model\nmodel = bitmoji_CNN() # must have defined the model class\nmodel.load_state_dict(torch.load(PATH))\nmodel.eval() # set model to evaluation mode (not training mode)\n\n# save model\ntorch.save(model.state_dict(), PATH)\n\n\n\n\nTo make CNN more robust to different images + increase the size of the dataset\nCommon augmentations:\n\nCrop\nRotate\nFlip\nColor jitter\n\n\ndata_transforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.RandomVerticalFlip(p=0.5), # p=0.5 means 50% chance of applying this augmentation\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor()\n])\n\n\n\n\n\nNN has a lot of hyperparameters\n\nGrid search will take a long time\nNeed a smarter approach: Optimization Algorithms\n\nExamples: Ax (we will use this), Raytune, Neptune, skorch.\n\n\n\n\n\nIdea: use a pre-trained model and fine-tune it to our specific task\nInstall from torchvision.models\n\nAll models have been trained on ImageNet dataset (224x224 images)\n\nSee here for code\n\n\n\ndensenet = models.densenet121(weights='DenseNet121_Weights.DEFAULT')\n\nfor param in densenet.parameters():  # Freeze parameters so we don't update them\n    param.requires_grad = False\n# can fine-tune to freeze only some layers\n\nlist(densenet.named_children())[-1] # check the last layer\n\n# update the last layer\nnew_layers = nn.Sequential(\n    nn.Linear(1024, 500),\n    nn.ReLU(),\n    nn.Linear(500, 1)\n)\ndensenet.classifier = new_layers\nThen train the model as usual.\ndensenet.to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(densenet.parameters(), lr=2e-3)\nresults = trainer(densenet, criterion, optimizer, train_loader, valid_loader, device, epochs=10)\n\n\n\n\nIdea:\n\nTake output from pre-trained model\nFeed output to a new model\n\n\ndef get_features(model, train_loader, valid_loader):\n    \"\"\"\n    Extract features from both training and validation datasets using the provided model.\n\n    This function passes data through a given neural network model to extract features. It's designed\n    to work with datasets loaded using PyTorch's DataLoader. The function operates under the assumption\n    that gradients are not required, optimizing memory and computation for inference tasks.\n    \"\"\"\n\n    # Disable gradient computation for efficiency during inference\n    with torch.no_grad():\n        # Initialize empty tensors for training features and labels\n        Z_train = torch.empty((0, 1024))  # Assuming each feature vector has 1024 elements\n        y_train = torch.empty((0))\n\n        # Initialize empty tensors for validation features and labels\n        Z_valid = torch.empty((0, 1024))\n        y_valid = torch.empty((0))\n\n        # Process training data\n        for X, y in train_loader:\n            # Extract features and concatenate them to the corresponding tensors\n            Z_train = torch.cat((Z_train, model(X)), dim=0)\n            y_train = torch.cat((y_train, y))\n\n        # Process validation data\n        for X, y in valid_loader:\n            # Extract features and concatenate them to the corresponding tensors\n            Z_valid = torch.cat((Z_valid, model(X)), dim=0)\n            y_valid = torch.cat((y_valid, y))\n\n    # Return the feature and label tensors\n    return Z_train, y_train, Z_valid, y_valid\nNow we can use the extracted features to train a new model.\n# Extract features from the pre-trained model\ndensenet = models.densenet121(weights='DenseNet121_Weights.DEFAULT')\ndensenet.classifier = nn.Identity()  # remove that last \"classification\" layer\nZ_train, y_train, Z_valid, y_valid = get_features(densenet, train_loader, valid_loader)\n\n# Train a new model using the extracted features\n# Let's scale our data\nscaler = StandardScaler()\nZ_train = scaler.fit_transform(Z_train)\nZ_valid = scaler.transform(Z_valid)\n\n# Fit a model\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(Z_train, y_train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative Models\nDiscriminative Models\n\n\n\n\nDirectly model the joint probability distribution of the input and output\nModel the conditional probability of the output given the input\n\n\nDirectly model \\(P(y\\|x)\\)\nEstimate \\(P(x\\|y)\\) to then deduce \\(P(y\\|x)\\)\n\n\nBuild model for each class\nMake boundary between classes\n\n\n“Generate or draw a cat”\n“Distinquish between cats and dogs”\n\n\nExamples: Naibe bayes, ChatGPT\nExamples: Logistic Regression, SVM, Tree based models, CNN\n\n\n\n\n\n\n\n\nDesigned to reconstruct the input\nEncoder and a decoder\nWhy do we need autoencoders?\n\nDimensionality reduction\nDenoising\n\n\n\n\n\nMaybe the z axis is unimportant in the input space for classification\n\nfrom torch import nn\n\nclass autoencoder(torch.nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, 2),\n            nn.Sigmoid()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(2, input_size),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n# Set up the training\nBATCH_SIZE = 100\ntorch.manual_seed(1)\nX_tensor = torch.tensor(X, dtype=torch.float32)\ndataloader = DataLoader(X_tensor,\n                        batch_size=BATCH_SIZE)\nmodel = autoencoder(3, 2)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters())\n\n# Train the model\nEPOCHS = 5\n\nfor epoch in range(EPOCHS):\n    for batch in dataloader:\n        optimizer.zero_grad()           # Clear gradients w.r.t. parameters\n        y_hat = model(batch)            # Forward pass to get output\n        loss = criterion(y_hat, batch)  # Calculate loss\n        loss.backward()                 # Getting gradients w.r.t. parameters\n        optimizer.step()                # Update parameters\n\n# Use encoder\nmodel.eval()\nX_encoded = model.encoder(X_tensor)\n\n\n\n\nRemove noise from the input\nUse Transposed Convolution Layers to upsample the input\n\nNormal convolution: downsample (output is smaller than input)\nTransposed convolution: upsample (output is larger than input)\n\n\ndef conv_block(input_channels, output_channels):\n    return nn.Sequential(\n        nn.Conv2d(input_channels, output_channels, 3, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(2)  # reduce x-y dims by two; window and stride of 2\n    )\n\ndef deconv_block(input_channels, output_channels, kernel_size):\n    return nn.Sequential(\n        nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride=2),\n        nn.ReLU()\n    )\n\nclass autoencoder(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            conv_block(1, 32),\n            conv_block(32, 16),\n            conv_block(16, 8)\n        )\n        self.decoder = nn.Sequential(\n            deconv_block(8, 8, 3),\n            deconv_block(8, 16, 2),\n            deconv_block(16, 32, 2),\n            nn.Conv2d(32, 1, 3, padding=1)  # final conv layer to decrease channel back to 1\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        x = torch.sigmoid(x)  # get pixels between 0 and 1\n        return x\n# Set up the training\nEPOCHS = 20\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters())\nimg_list = []\n\nfor epoch in range(EPOCHS):\n    losses = 0\n    for batch, _ in trainloader:\n        noisy_batch = batch + noise * torch.randn(*batch.shape)\n        noisy_batch = torch.clip(noisy_batch, 0.0, 1.0)\n        optimizer.zero_grad()\n        y_hat = model(noisy_batch)\n        loss = criterion(y_hat, batch)\n        loss.backward()\n        optimizer.step()\n        losses += loss.item()\n    print(f\"epoch: {epoch + 1}, loss: {losses / len(trainloader):.4f}\")\n    # Save example results each epoch so we can see what's going on\n    with torch.no_grad():\n        noisy_8 = noisy_batch[:1, :1, :, :]\n        model_8 = model(input_8)\n        real_8 = batch[:1, :1, :, :]\n    img_list.append(utils.make_grid([noisy_8[0], model_8[0], real_8[0]], padding=1))```\n\n\n\n\n\n\nModel used to generate new data (indistinguishable from real data)\nNo need for labels (unsupervised learning)\nSee here\n\n\n\nTwo networks:\n\nGenerator: creates new data\nDiscriminator: tries to distinguish between real and fake data\n\nBoth are battling each other:\n\nGenerator tries to create data that the discriminator can’t distinguish from real data\nDiscriminator tries to distinguish between real and fake data\n\n\n\n\n\nTrain the discriminator (simple binary classification)\n\nTrain the discriminator on real data\nTrain the discriminator on fake data (generated by the generator)\n\nTrain the generator\n\nGenerate fake images with the generator and label them as real\nPass to discriminator and ask it to classify them (real or fake)\nPass judgement to a loss function (see how far it is from the ideal output)\n\nideal output: all fake images are classified as real\n\nDo backpropagation and update the generator\n\nRepeat\n\n\n\n\n\nCreating the data loader\nDATA_DIR = \"../input/face-recognition-dataset/Extracted Faces\"\n\nBATCH_SIZE = 64\nIMAGE_SIZE = (128, 128)\n\ndata_transforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE), # uses CPU (bottleneck)\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ndataset = datasets.ImageFolder(root=DATA_DIR, transform=data_transforms)\ndata_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\nCreating the generator\n\nclass Generator(nn.Module):\n\n def __init__(self, LATENT_SIZE):\n     super(Generator, self).__init__()\n\n     self.main = nn.Sequential(\n         nn.ConvTranspose2d(LATENT_SIZE, 1024, kernel_size=4, stride=1, padding=0, bias=False),\n         nn.BatchNorm2d(1024),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(512),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(128),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(3),\n\n         nn.Tanh()\n     )\n\n def forward(self, input):\n     return self.main(input)\nCreating the discriminator\n\nclass Discriminator(nn.Module):\n\n def __init__(self):\n     super(Discriminator, self).__init__()\n\n     self.main = nn.Sequential(\n         nn.Conv2d(3, 128, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(128),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.Conv2d(128, 512, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(512),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n         nn.Flatten(),\n         nn.Sigmoid()\n     )\n\n def forward(self, input):\n     return self.main(input)\nInstantiating the models\ndevice = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n\nLATENT_SIZE = 100\ngenerator = Generator(LATENT_SIZE).to(device)\ndiscriminator = Discriminator().to(device)\n\ncriterion = nn.BCELoss()\n\noptimizerG = optim.Adam(generator.parameters(), lr=0.001, betas=(0.5, 0.999))\noptimizerD = optim.Adam(discriminator.parameters(), lr=0.001, betas=(0.5, 0.999))\n\n def weights_init(m):\n     if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n         nn.init.normal_(m.weight.data, 0.0, 0.02)\n     elif isinstance(m, nn.BatchNorm2d):\n         nn.init.normal_(m.weight.data, 1.0, 0.02)\n         nn.init.constant_(m.bias.data, 0)\n\n\n generator.apply(weights_init)\n discriminator.apply(weights_init);\nTraining the GAN\n img_list = []\nfixed_noise = torch.randn(BATCH_SIZE, LATENT_SIZE, 1, 1).to(device)\n\n NUM_EPOCHS = 50\nfrom statistics import mean\nprint('Training started:\\n')\n\n D_real_epoch, D_fake_epoch, loss_dis_epoch, loss_gen_epoch = [], [], [], []\n\n for epoch in range(NUM_EPOCHS):\n     D_real_iter, D_fake_iter, loss_dis_iter, loss_gen_iter = [], [], [], []\n\n     for real_batch, _ in data_loader:\n\n         # STEP 1: train discriminator\n         # ==================================\n         optimizerD.zero_grad()\n\n         real_batch = real_batch.to(device)\n         real_labels = torch.ones((real_batch.shape[0],), dtype=torch.float).to(device)\n\n         output = discriminator(real_batch).view(-1)\n         loss_real = criterion(output, real_labels)\n\n         # Iteration book-keeping\n         D_real_iter.append(output.mean().item())\n\n         # Train with fake data\n         noise = torch.randn(real_batch.shape[0], LATENT_SIZE, 1, 1).to(device)\n\n         fake_batch = generator(noise)\n         fake_labels = torch.zeros_like(real_labels)\n\n         output = discriminator(fake_batch.detach()).view(-1)\n         loss_fake = criterion(output, fake_labels)\n\n         # Update discriminator weights\n         loss_dis = loss_real + loss_fake\n         loss_dis.backward()\n         optimizerD.step()\n\n         # Iteration book-keeping\n         loss_dis_iter.append(loss_dis.mean().item())\n         D_fake_iter.append(output.mean().item())\n\n         # STEP 2: train generator\n         # ==================================\n         optimizerG.zero_grad()\n\n         # Calculate the output with the updated weights of the discriminator\n         output = discriminator(fake_batch).view(-1)\n         loss_gen = criterion(output, real_labels)\n         loss_gen.backward()\n\n         # Book-keeping\n         loss_gen_iter.append(loss_gen.mean().item())\n\n         # Update generator weights and store loss\n         optimizerG.step()\n\n     print(f\"Epoch ({epoch + 1}/{NUM_EPOCHS})\\t\",\n         f\"Loss_G: {mean(loss_gen_iter):.4f}\",\n         f\"Loss_D: {mean(loss_dis_iter):.4f}\\t\",\n         f\"D_real: {mean(D_real_iter):.4f}\",\n         f\"D_fake: {mean(D_fake_iter):.4f}\")\n\n     # Epoch book-keeping\n     loss_gen_epoch.append(mean(loss_gen_iter))\n     loss_dis_epoch.append(mean(loss_dis_iter))\n     D_real_epoch.append(mean(D_real_iter))\n     D_fake_epoch.append(mean(D_fake_iter))\n\n     # Keeping track of the evolution of a fixed noise latent vector\n     with torch.no_grad():\n         fake_images = generator(fixed_noise).detach().cpu()\n         #img_list.append(utils.make_grid(fake_images, normalize=True, nrows=10))\n\n print(\"\\nTraining ended.\")\nVisualize training process\n plt.plot(np.array(loss_gen_epoch), label='loss_gen')\n plt.plot(np.array(loss_dis_epoch), label='loss_dis')\n plt.xlabel(\"Epoch\")\n plt.ylabel(\"Loss\")\n plt.legend();\n plt.plot(np.array(D_real_epoch), label='D_real')\n plt.plot(np.array(D_fake_epoch), label='D_fake')\n plt.xlabel(\"Epoch\")\n plt.ylabel(\"Probability\")\n plt.legend();\n\n\n\n\nclass multiModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        ...\n\n    def forward(self, image, data):\n        x_cnn = self.cnn(image) # 1st model: CNN\n        x_fc = self.fc(data) # 2nd model: Fully connected\n        return torch.cat((x_cnn, x_fc), dim=1) # concatenate the two outputs"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#rounding-errors-in-programming",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#rounding-errors-in-programming",
    "title": "Supervised Learning II",
    "section": "",
    "text": "Infinite amount of numbers but finite amount of bits to represent them\nThese small errors will accumulate and cause problems\n\n\n\n\nlarge datasets with millions of params\nsmall errors can accumulate and cause problems\n\n\n\n\n\nBinary numbers are represented as a sum of powers of 2\ne.g. 104 in binary is 1101000 = \\(1(2^6) + 1(2^5) + 0(2^4) + 1(2^3) + 0(2^2) + 0(2^1) + 0(2^0) = 64 + 32 + 8 = 104\\)\nUnsigned Integers: \\(2^n - 1\\) is the largest number that can be represented with n bits\n\ne.g. 8 bits can represent 0 to 255\nnp.iinfo(np.uint8) gives the min and max values\n\nSigned Integers: \\(2^{n-1} - 1\\) is the largest positive number that can be represented with n bits\n\n\\(-2^{n-1}\\) is the smallest negative number that can be represented with n bits\ne.g. 8 bits can represent -128 to 127 (0 is included in the positive numbers)\n1 bit is used to represent the sign\nnp.iinfo(np.int8) gives the min and max values\n\n\n\n\n\n\n14.75 in binary is 1110.11\n\n\n\n\n2^3\n2^2\n2^1\n2^0\n2^-1\n2^-2\n\n\n\n\n1\n1\n1\n0\n1\n1\n\n\n8\n4\n2\n0\n0.5\n0.25\n\n\n\n$ 8 + 4 + 2 + 0 + 0.5 + 0.25 = 14.75 $\n\n\n\n\nWe typically have a fixed number of bits to represent the fractional part\ne.g. 8 bits total, 4 bits for the integer part and 4 bits for the fractional part\n\nmax value is 15.9375 (\\(2^3 + 2^2 + 2^1 + 2^0 + 2^{-1} + 2^{-2} + 2^{-3} + 2^{-4}\\))\n\noverflow if try a higher value\n\nmin value (bigger than 0) is 0.0625 (\\(2^{-4}\\))\n\nor precision of 0.0625 (any less =&gt; underflow)\n\n\n\n\n\n\n\nRather than having a fixed location for the binary point, we let it “float” around.\n\nlike how we write 0.1234 as 1.234 x 10^-1\n\nFormat: \\[(-1)^S \\times 1. M \\times 2^E\\]\n\nS is the sign bit\nM is the mantissa, always between 1 and 2 (1.0 is implied)\nE is the exponent\n\n\nFloat 64 (double precision) \nFloat 32 (single precision) \n\n\n\n\n\n\nThe spacing changes depending on the floating point number (because of the exponent)\n\n\n\nimport numpy as np\n\nnp.spacing(1e16) # 1.0\n\nnp.nextafter(1e16, 2e16) - 1e16 # 1.0\n\n\n\n\n\n\n1.0 + 2.0 + 3.0 == 6.0 True\n0.1 + 0.2 == 0.3 False\n\n0.1, 0.2, and 0.3 are not exactly representable in binary\n\n1e16 + 1 == 1e16 True\n\n1 is less than the spacing, so it is rounded back\n\n1e16 + 2.0 == 1e16 False\n\n2.0 is greater than the spacing, so it is rounded up\n\n1e16 + 1.0 + 1.0  == 1e16 True\n\n1.0 is less than the spacing, so it is rounded back, then 1.0 is added, which is less than the spacing, so it is rounded back again"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#optimization",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#optimization",
    "title": "Supervised Learning II",
    "section": "",
    "text": "In ML, we want to minimize a loss function\n\ntypically a sum of losses over the training set\n\nCan think of ML as a 3 step process:\n\nChoose model: controls space of possible functions that map X to y\nChoose loss function: measures how well the model fits the data\nChoose optimization algorithm: finds the best model\n\n\n\n\n\nOptimization: process to min/max a function\nObjective Function: function to be optimized\nDomain: set to search for optimal value\nMinimizer: value that minimizes the objective function\n\n\n\n\nCommon loss function is MSE (mean squared error):\n\\[L(w) = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2\\]\nUsing a simple linear regression model \\(y = w_0 + w_1x\\), we can rewrite the loss function as:\n\\[L(w) = \\frac{1}{n} \\sum_{i=1}^n ((w_0 + w_1x_i) - y_i)^2\\]\nSo optimization is finding the values of \\(w_0\\) and \\(w_1\\) that minimize the loss function, \\(L(w)\\).\nIn vector format:\n\\[\\text{MSE} = \\mathcal{L}(\\mathbf{w}) = \\frac{1}{n}\\sum^{n}_{i=1}(\\mathbf{x}_i \\mathbf{w} - y_i)^2\\]\nIn full-matrix format\n\\[\\text{MSE} = \\mathcal{L}(\\mathbf{w}) = \\frac{1}{n}(\\mathbf{X} \\mathbf{w} - \\mathbf{y})^T (\\mathbf{X} \\mathbf{w} - \\mathbf{y}) \\]\n\n\n\n\\[\n\\mathbf{y}=\n\\left[\n\\begin{array}{c} y_1 \\\\\n\\vdots \\\\\ny_i \\\\\n\\vdots\\\\\ny_n\n\\end{array}\n\\right]_{n \\times 1}, \\quad\n\\mathbf{X}=\n\\left[\n\\begin{array}{c} \\mathbf{x}_1 \\\\\n\\vdots \\\\\n\\mathbf{x}_i \\\\\n\\vdots\\\\\n\\mathbf{x}_n\n\\end{array}\n\\right]_{n \\times d}\n= \\left[\\begin{array}{cccc}\nx_{11} & x_{12} & \\cdots & x_{1 d} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{i 1} & x_{i 2} & \\cdots & x_{i d}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n 1} & x_{n 2} & \\cdots & x_{n d}\n\\end{array}\\right]_{n \\times d},\n\\quad\n\\mathbf{w}=\n\\left[\n\\begin{array}{c} w_1 \\\\\n\\vdots\\\\\nw_d\n\\end{array}\n\\right]_{d \\times 1}\n\\]\n\n\\(n\\): number of examples\n\\(d\\): number of input features/dimensions\n\nThe goal is to find the weights \\(\\mathbf{w}\\) that minimize the loss function.\nFormulas:\n\\[\\mathbf{y} = \\mathbf{X} \\mathbf{w}\\]\n\\[\\hat{\\mathbf{y}_i} = \\mathbf{w}^T \\mathbf{x}_i\\]"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#gradient-descent",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#gradient-descent",
    "title": "Supervised Learning II",
    "section": "",
    "text": "One of the most important optimization algorithms in ML\nIterative optimization algorithm\nSteps:\n\nstart with some arbitrary \\(\\mathbf{w}\\)\ncalculate the gradient using all training examples\nuse the gradient to adjust \\(\\mathbf{w}\\)\nrepeat for \\(I\\) iterations or until the step-size is sufficiently small\n\nCost: \\(O(ndt)\\) for t iterations, better than brute force search \\(O(nd^2 + d^3)\\)\n\n\\[w_{t+1} = w_t - \\alpha \\nabla= L(w_t)\\]\n\n\\(w_t\\): current value of the weights\n\\(\\alpha\\): learning rate\n\\(\\nabla L(w_t)\\): gradient of the loss function at \\(w_t\\)\n\n\n\n\nLoss function: \\(L(w) = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2\\)\nGradient: \\(\\nabla L(w) = \\frac{d}{dw} L(w) = \\frac{2}{n} \\sum_{i=1}^n x_{i1} (x_{i1} w_1 - y_i)\\)\n\nOr in Matrix form: \\(\\nabla L(w) = \\frac{2}{n} \\mathbf{X}^T (\\mathbf{X} \\mathbf{w} - \\mathbf{y})\\)\n\n\n\n\n\n\nNeed to scale for the contour plot to be more “round”\n\nbetter for gradient descent\n\n\n\n\nIn real life, contour plots are not so nice\n\n\n\n\n\n\nInitialization: Start with an initial set of parameters, often randomly chosen.\nForward pass: Generate predictions using the current values of the parameters. (E.g., \\(\\hat{y_i} = x_{1}w_1 + Bias\\) in the toy example above)\nLoss calculation: Evaluate the loss, which quantifies the discrepancy between the model’s predictions and the actual target values.\nGradient calculation: Compute the gradient of the loss function with respect to each parameter either on a batch or the full dataset. This gradient indicates the direction in which the loss is increasing and its magnitude.\nParameter Update: Adjust the parameters in the opposite direction of the calculated gradient, scaled by the learning rate. This step aims to reduce the loss by moving the parameters toward values that minimize it.\n\n\n\n\n\nUse minimize function from scipy.optimize\n\nfrom scipy.optimize import minimize\n\ndef mse(w, X, y):\n    \"\"\"Mean squared error.\"\"\"\n    return np.mean((X @ w - y) ** 2)\n\ndef mse_grad(w, X, y):\n    \"\"\"Gradient of mean squared error.\"\"\"\n    n = len(y)\n    return (2/n) * X.T @ (X @ w - y)\n\nout = minimize(mse, w, jac=mse_grad, args=(X_scaled_ones, toy_y), method=\"BFGS\")\n# jac: function to compute the gradient (optional)\n# - will use finite difference approximation if not provided\n\nOther methods:\n\nBFGS: Broyden–Fletcher–Goldfarb–Shanno algorithm\nCG: Conjugate gradient algorithm\nL-BFGS-B: Limited-memory BFGS with bounds on the variables\nSLSQP: Sequential Least SQuares Programming\nTNC: Truncated Newton algorithm"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#stochastic-gradient-descent",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#stochastic-gradient-descent",
    "title": "Supervised Learning II",
    "section": "",
    "text": "Instead of updating our parameters based on a gradient calculated using all training data, we simply use one of our data points (the \\(i\\)-th one)\n\nGradient Descent\nLoss function:\n\\[\\text{MSE} = \\mathcal{L}(\\mathbf{w}) = \\frac{1}{n}\\sum^{n}_{i=1} (\\mathbf{x}_i \\mathbf{w} - y_i)^2\\]\nUpdate procedure:\n\\[\\mathbf{w}^{j+1} = \\mathbf{w}^{j} - \\alpha \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}^{j})\\]\nStochastic Gradient Descent\nLoss function:\n\\[\\text{MSE}_i = \\mathcal{L}_i(\\mathbf{w}) = (\\mathbf{x}_i \\mathbf{w} - y_i)^2\\]\nUpdate procedure: \\[\\mathbf{w}^{j+1} = \\mathbf{w}^{j} - \\alpha \\nabla_\\mathbf{w} \\mathcal{L}_i(\\mathbf{w}^{j})\\]\n\n\n\n\n\nGradient Descent\nStochastic Gradient Descent\n\n\n\n\nUse all data points\nUse one data point\n\n\nSlow\nFast\n\n\nAccurate\nLess Accurate\n\n\n\n\nMini-batch Gradient Descent is a (in-between) compromise between the two\nInstead of using a single data point, we use a small batch of data points d\n\n\n\n\nShuffle and divide all data into \\(k\\) batches, every example is used once\n\nDefault in PyTorch\nAn example will only show up in one batch\n\nChoose some examples for each batch without replacement\n\nAn example may show up in multiple batches\nThe same example cannot show up in the same batch more than once\n\nChoose some examples for each batch with replacement\n\nAn example may show up in multiple batches\nThe same example may show up in the same batch more than once\n\n\n\n\n\n\nAssume we have a dataset of \\(n\\) observations (also known as rows, samples, examples, data points, or points)\n\nIteration: each time you update model weights\nBatch: a subset of data used in an iteration\nEpoch: One full pass through the dataset to look at all \\(n\\) observations\n\nIn other words,\n\nIn GD, each iteration involves computing the gradient over all examples, so\n\n\\[1 \\: \\text{iteration} = 1 \\: \\text{epoch}\\]\n\nIn SGD, each iteration involves one data point, so\n\n\\[n \\text{ iterations} = 1 \\: \\text{epoch}\\]\n\nIn MGD, each iteration involves a batch of data, so\n\n\\[\n\\begin{align}\n\\frac{n}{\\text{batch size}} \\text{iterations} &= 1 \\text{ epoch}\\\\\n\\end{align}\n\\]\n*Note: nobody really says “minibatch SGD”, we just say SGD: in SGD you can specify a batch size of anything between 1 and \\(n\\)"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#neural-networks",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#neural-networks",
    "title": "Supervised Learning II",
    "section": "",
    "text": "Models that does a good job of approximating complex non-linear functions\nIt is a sequence of layers, each of which is a linear transformation followed by a non-linear transformation\n\n\n\n\nNode (or neuron): a single unit in a layer\nInput layer: the features of the data\nHidden layer: the layer(s) between the input and output layers\nOutput layer: the prediction(s) of the model\nWeights: the parameters of the model\nActivation function: the non-linear transformation (e.g. ReLU, Sigmoid, Tanh, etc.)\n\n\nX : (n x d), W : (h x d), b : (n x h), where h is the number of hidden nodes b is actually 1 x hs, but we can think of it as n x hs because it is broadcasted\n\\[\\mathbf{H}^{(1)} = \\phi^{(1)} (\\mathbf{X}\\mathbf{W}^{(1)\\text{T}} + \\mathbf{b}^{(1)})\\]\n\\[\\mathbf{H}^{(2)} = \\phi^{(2)} (\\mathbf{H}^{(1)}\\mathbf{W}^{(2)\\text{T}} + \\mathbf{b}^{(2)})\\]\n\\[\\mathbf{Y} = (\\mathbf{H}^{(2)}\\mathbf{W}^{(3)\\text{T}} + \\mathbf{b}^{(3)})\\]\n\nIn a layer, \\[\\text{ num of weights} = \\text{num of nodes in previous layer} \\times \\text{num of nodes in current layer}\\]\n\n\\[\\text{num of biases} = \\text{num of nodes in current layer}\\]\n\\[\\text{num of parameters} = \\text{num of weights} + \\text{num of biases}\\]\n\n\n\n\n\n\n\nBackpropagation: a method to calculate the gradient of the loss function with respect to the weights\nChain rule: a method to calculate the gradient of a function composed of multiple functions\nIt is pretty complicated, but PyTorch does it for us\n\n\n\n\n\n\nNeural networks with &gt; 1 hidden layer\n\nNN with 1 hidden layer: shallow neural network"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#pytorch-for-neural-networks",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#pytorch-for-neural-networks",
    "title": "Supervised Learning II",
    "section": "",
    "text": "PyTorch is a popular open-source machine learning library by Facebook based on Torch\nIt is a Python package that provides two high-level features:\n\nTensor computation (like NumPy) with strong GPU acceleration\nGradient computation through automatic differentiation\n\n\n\n\n\nSimilar to ndarray in NumPy\n\nimport torch\n\n# Create a tensor\nx = torch.tensor([1, 2, 3, 4, 5]) # int\nx = torch.tensor([1, 2, 3, 4, 5.]) # float\nx = torch.tensor([[1, 2], [3, 4], [5, 6]])\n\ny = torch.zeros(3, 2)\ny = torch.ones(3, 2)\ny = torch.rand(3, 2)\n\n# Check the shape, dimensions, and data type\nx.shape\nx.ndim\nx.dtype\n\n# Operations\na = torch.rand(1, 3)\nb = torch.rand(3, 1)\n\na + b # broadcasting\na * b # element-wise multiplication\na @ b # matrix multiplication\na.mean()\na.sum()\n\n# Indexing\na[0,:] # first row\na[0] # first row\na[:,0] # first column\n\n# Convert to NumPy\nx.numpy()\n\n\n\n# Check if GPU is available\ntorch.backends.mps.is_available() # mac M chips\ntorch.cuda.is_available() # Nvidia GPU\n\n# To activate GPU\ndevice = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nx.to('cpu') # move tensor to cpu\n\n\n\nuse backward() to compute the gradient, backpropagation\n\nX = torch.tensor([1.0, 2.0, 3.0], requires_grad=False)\nw = torch.tensor([1.0], requires_grad=True)  # Random initial weight\ny = torch.tensor([2.0, 4.0, 6.0], requires_grad=False)  # Target values\nmse = ((X * w - y)**2).mean()\nmse.backward()\nw.grad\n\n\n\n\n\nEvery NN model has to inherit from torch.nn.Module\n\nfrom torch import nn\n\nclass linearRegression(nn.Module):  # inherit from nn.Module\n\n    def __init__(self, input_size, output_size):\n        super().__init__()  # call the constructor of the parent class\n\n        self.linear = nn.Linear(input_size, output_size,)  # wX + b\n\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\n# Create a model\nmodel = linearRegression(1, 1) # input size, output size\n\n# View model\nsummary(model)\n\n## Train the model\nLEARNING_RATE = 0.02\ncriterion = nn.MSELoss()  # loss function\noptimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)  # optimization algorithm is SGD\n\n# DataLoader for mini-batch\nfrom torch.utils.data import DataLoader, TensorDataset\n\nBATCH_SIZE = 50\ndataset = TensorDataset(X_t, y_t)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n# Training\ndef trainer(model, criterion, optimizer, dataloader, epochs=5, verbose=True):\n    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n\n    for epoch in range(epochs):\n        losses = 0\n\n        for X, y in dataloader:\n\n            optimizer.zero_grad()       # Clear gradients w.r.t. parameters\n            y_hat = model(X).flatten()  # Forward pass to get output\n            loss = criterion(y_hat, y)  # Calculate loss\n            loss.backward()             # Getting gradients w.r.t. parameters\n            optimizer.step()            # Update parameters\n            losses += loss.item()       # Add loss for this batch to running total\n\n        if verbose: print(f\"epoch: {epoch + 1}, loss: {losses / len(dataloader):.4f}\")\n\ntrainer(model, criterion, optimizer, dataloader, epochs=30, verbose=True)\n\n\n\n\nuse torch.nn.Sequential to create a model\n\nclass nonlinRegression(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n\n        self.main = torch.nn.Sequential(\n            nn.Linear(input_size, hidden_size),  # input -&gt; hidden layer\n            nn.Sigmoid(),                        # sigmoid activation function in hidden layer\n            nn.Linear(hidden_size, output_size)  # hidden -&gt; output layer\n        )\n\n    def forward(self, x):\n        x = self.main(x)\n        return x\n\n\n\n\n\n\nTask\nCriterion (Loss)\nOptimizer\n\n\n\n\nRegression\nMSELoss\nSGD\n\n\nBinary Classification\nBCELoss\nAdam\n\n\nMulti-class Classification\nCrossEntropyLoss\nAdam\n\n\n\n\nInput of CrossEntropyLoss doesn’t need to be normalized (i.e. no need to sum to 1/ no need to use nn.Softmax)\n\n# criterions\nfrom torch import nn\nreg_criterion = torch.nn.MSELoss()\nbc_criterion = torch.nn.BCEWithLogitsLoss()\nmse_criterion = torch.nn.CrossEntropyLoss()\n\n# optimizers\nfrom torch import optim\nreg_optim = torch.optim.SGD(model.parameters(), lr=0.2)\nclass_optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#backpropagation",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#backpropagation",
    "title": "Supervised Learning II",
    "section": "",
    "text": "It is to calculate the gradient of the loss function with respect to the weights\nIt is a special case of the chain rule of calculus\nProcess:\n\nDo “forward pass” to calculate the output of the network (prediction and loss)\n\n\n\nDo “backward pass” to calculate the gradients of the loss function with respect to the weights. Below is an example of reverse-mode autmatic differentiation (backpropagation):\n\n\n\n\n\n\n\ntorch.autograd is PyTorch’s automatic differentiation engine that powers neural network training\n\nimport torch\n\n# Create model\nclass network(torch.nn.Module):\n    def __init__(self):\n        super(network, self).__init__()\n        self.layer1 = torch.nn.Linear(1, 6)\n        self.dropout = torch.nn.Dropout(0.2) # dropout layer\n        ...\n\n    def forward(self, x):\n        x = self.layer1(x)\n        ...\n        return x\n\nmodel = network()\ncriterion = torch.nn.MSELoss()\n\n# Forward pass\nloss = criterion(model(x), y)\n# Backward pass\nloss.backward()\n\n# Access gradients\nprint(model.layer1.weight.grad) # or model.layer1.weight.bias.grad\n\n# Update weights\nmodel.state_dict() # get the current weights\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\noptimizer.step() # update weights\n\n\n\n\nBackpropagation can suffer from two problems because of multiple chain rule applications:\n\nVanishing gradients: the gradients of the loss function with respect to the weights become very small\n\n0 gradients because of underflow\n\nExploding gradients: the gradients of the loss function with respect to the weights become very large\n\nPossible solutions:\n\nUse ReLU activation function: but it can also suffer from the dying ReLU problem (gradients are 0)\nWeight initialization: initialize the weights with small values\nBatch normalization: normalize the input layer by adjusting and scaling the activations\nSkip connections: add connections that skip one or more layers\nGradient clipping: clip the gradients during backpropagation\n\n\n\n\n\n\n\n\nAdd validation loss to the training loop\nEarly stopping: if we see the validation loss is increasing, we stop training\n\nDefine a patience parameter: if the validation loss increases for patience epochs, we stop training\n\nRegularization: add a penalty term to the loss function to prevent overfitting\n\nSee 573 notes for more details\nweight_decay parameter in the optimizer\n\nDropout: randomly set some neurons to 0 during training\n\nIt prevents overfitting by reducing the complexity of the model\ntorch.nn.Dropout(0.2)\n\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\ndef trainer(model, criterion, optimizer, trainloader, validloader, epochs=5, patience=5):\n    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n\n    train_loss = []\n    valid_loss = []\n\n    for epoch in range(epochs):  # for each epoch\n        train_batch_loss = 0\n        valid_batch_loss = 0\n\n        # Training\n        for X, y in trainloader:\n\n            optimizer.zero_grad()       # Zero all the gradients w.r.t. parameters\n\n            y_hat = model(X).flatten()  # Forward pass to get output\n            loss = criterion(y_hat, y)  # Calculate loss based on output\n            loss.backward()             # Calculate gradients w.r.t. parameters\n            optimizer.step()            # Update parameters\n\n            train_batch_loss += loss.item()  # Add loss for this batch to running total\n\n        train_loss.append(train_batch_loss / len(trainloader))\n\n        # Validation\n        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n\n            for X_valid, y_valid in validloader:\n\n                y_hat = model(X_valid).flatten()  # Forward pass to get output\n                loss = criterion(y_hat, y_valid)  # Calculate loss based on output\n\n                valid_batch_loss += loss.item()\n\n        valid_loss.append(valid_batch_loss / len(validloader))\n\n        # Early stopping\n        if epoch &gt; 0 and valid_loss[-1] &gt; valid_loss[-2]:\n            consec_increases += 1\n        else:\n            consec_increases = 0\n        if consec_increases == patience:\n            print(f\"Stopped early at epoch {epoch + 1} - val loss increased for {consec_increases} consecutive epochs!\")\n            break\n\n    return train_loss, valid_loss\n\nUsing the trainer function:\n\nimport torch\nimport torch.nn\nimport torch.optim\n\ntorch.manual_seed(1)\n\nmodel = network(1, 6, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05) # weight_decay=0.01 for L2 regularization\ntrain_loss, valid_loss = trainer(model, criterion, optimizer, trainloader, validloader, epochs=201, patience=3)\n\nplot_loss(train_loss, valid_loss)\n\n\n\n\nAny continuous function can be approximated arbitrarily well by a neural network with a single hidden layer\n\nIn other words, NN are universal function approximators"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#convolutional-neural-networks-cnn",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#convolutional-neural-networks-cnn",
    "title": "Supervised Learning II",
    "section": "",
    "text": "Drastically reduces the number of params (compared to NN):\n\nhave activations depend on small number of inputs\nsame parameters (convolutional filter) are used for different parts of the image\n\nCan capture spatial information (preserves the structure of the image)\n\n\n\n\nIdea: use a small filter/kernel to extract features from the image\n\nFilter: a small matrix of weights (normally odd dimensioned -&gt; for symmetry)\n\n\n\n\nNotice that the filter results in a smaller output image\n\nThis is because we are not padding the image\nWe can add padding to the image to keep the same size\n\nPadding: add zeros around the image\n\nCan also add stride to move the filter more than 1 pixel at a time\n\n\n\n\n\n\nimg src\n\n\n\n\n\nconv_1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=(3,3))\n\nArguments:\n\nin_channels: number of input channels (gray scale image has 1 channel, RGB has 3)\nout_channels: number of output channels (similar to hidden nodes in NN)\nkernel_size: size of the filter\nstride: how many pixels to move the filter each time\npadding: how many pixels to add around the image\n\n\n\n\nSize of input image (e.g. 256x256) doesn’t matter, what matters is: in_channels, out_channels, kernel_size\n\n\\[\\text{total params} = (\\text{out channels} \\times \\text{in channels} \\times \\text{kernel size}^2) + \\text{out channels}\\]\n\\[\\text{output size} = \\frac{\\text{input size} - \\text{kernel size} + 2 \\times \\text{padding}}{\\text{stride}} + 1\\]\n\n\n\nImages: [batch_size, channels, height, width]\nKernel: [out_channels, in_channels, kernel_height, kernel_width]\n\nNote: before passing the image to the convolutional layer, we need to reshape it to the correct dimensions. Also if you want to plt.imshow() the image, you need to reshape it back to [height, width, channels].\n\n\n\n\n\nfeature learning -&gt; classification\nUse torch.nn.Flatten() to flatten the image\nAt the end need to either do regression or classification\n\n\n\n\n\nIdea: reduce the size of the image\n\nless params\nless overfitting\n\nCommon types:\n\nMax pooling: take the max value in each region\n\nWorks well since it takes the sharpest features\n\nAverage pooling: take the average value in each region\n\n\n\n\n\nclass CNN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.main = torch.nn.Sequential(\n\n            torch.nn.Conv2d(in_channels=1,\n                out_channels=3,\n                kernel_size=(3, 3),\n                padding=1),\n            torch.nn.ReLU(), # activation function\n            torch.nn.MaxPool2d((2, 2)),\n\n            torch.nn.Conv2d(in_channels=3,\n                out_channels=2,\n                kernel_size=(3, 3),\n                padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d((2, 2)),\n\n            torch.nn.Flatten(),\n            torch.nn.Linear(1250, 1)\n        )\n\n    def forward(self, x):\n        out = self.main(x)\n        return out\n# Trainer code\ndef trainer(\n    model, criterion, optimizer, trainloader, validloader, epochs=5, verbose=True\n):\n    train_loss, train_accuracy, valid_loss, valid_accuracy = [], [], [], []\n    for epoch in range(epochs):  # for each epoch\n        train_batch_loss = 0\n        train_batch_acc = 0\n        valid_batch_loss = 0\n        valid_batch_acc = 0\n\n        # Training\n        for X, y in trainloader:\n            if device.type in ['cuda', 'mps']:\n                    X, y = X.to(device), y.to(device)\n            optimizer.zero_grad()  # Zero all the gradients w.r.t. parameters\n            y_hat = model(X)  # Forward pass to get output\n            idx = torch.softmax(y_hat, dim=1).argmax(dim=1) # Multiclass classification\n            loss = criterion(y_hat, y)\n            loss.backward()  # Calculate gradients w.r.t. parameters\n            optimizer.step()  # Update parameters\n            train_batch_loss += loss.item()  # Add loss for this batch to running total\n            train_batch_acc += (\n                    (idx.squeeze() == y).type(\n                        torch.float32).mean().item()\n                )\n        train_loss.append(train_batch_loss / len(trainloader))\n        train_accuracy.append(train_batch_acc / len(trainloader))\n\n        # Validation\n        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n            for X, y in validloader:\n                if device.type in ['cuda', 'mps']:\n                    X, y = X.to(device), y.to(device)\n                y_hat = model(X)\n                idx = torch.softmax(y_hat, dim=1).argmax(dim=1)\n                loss = criterion(y_hat, y)\n                valid_batch_loss += loss.item()\n                valid_batch_acc += (\n                    (idx.squeeze() == y).type(\n                        torch.float32).mean().item()\n                )\n        valid_loss.append(valid_batch_loss / len(validloader))\n        valid_accuracy.append(valid_batch_acc / len(validloader))  # accuracy\n\n        # Print progress\n        if verbose:\n            print(\n                f\"Epoch {epoch + 1}:\",\n                f\"Train Loss: {train_loss[-1]:.3f}.\",\n                f\"Valid Loss: {valid_loss[-1]:.3f}.\",\n                f\"Train Accuracy: {train_accuracy[-1]:.2f}.\",\n                f\"Valid Accuracy: {valid_accuracy[-1]:.2f}.\",\n            )\n\n    results = {\n        \"train_loss\": train_loss,\n        \"train_accuracy\": train_accuracy,\n        \"valid_loss\": valid_loss,\n        \"valid_accuracy\": valid_accuracy,\n    }\n    return results\n\n\n\n\nTo get a summary of the model\n\nNo need to manually calculate the output size of each layer\n\n\nfrom torchsummary import summary\n\nmodel = CNN()\nsummary(model, (1, 256, 256))\n\n\n\n\n\n\n\nNormally there are 2 steps:\n\ncreate a dataset object: the raw data\ncreate a dataloader object: batches the data, shuffles, etc.\n\nUse torchvision to load the data\n\ntorchvision.datasets.ImageFolder: loads images from folders\nAssumes structure: root/class_1/xxx.png, root/class_2/xxx.png, …\n\n\nimport torch\nfrom torchvision import datasets, transforms\n\nIMAGE_SIZE = (256, 256)\nBATCH_SIZE = 32\n\n# create transform object\ndata_transforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.ToTensor()\n])\n\n# create dataset object\ntrain_dataset = datasets.ImageFolder(root='path/to/data', transform=data_transforms)\n\n# check out the data\ntrain_dataset.classes # list of classes\ntrain_dataset.targets # list of labels\ntrain_dataset.samples # list of (path, label) tuples\n\n# create dataloader object\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,          # our raw data\n    batch_size=BATCH_SIZE,  # the size of batches we want the dataloader to return\n    shuffle=True,           # shuffle our data before batching\n    drop_last=False         # don't drop the last batch even if it's smaller than batch_size\n)\n\n# get a batch of data\nimages, labels = next(iter(train_loader))\n\n\n\n\nPyTorch documentation\nConvention: .pt or .pth file extension\n\nPATH = \"models/my_cnn.pt\"\n\n# load model\nmodel = bitmoji_CNN() # must have defined the model class\nmodel.load_state_dict(torch.load(PATH))\nmodel.eval() # set model to evaluation mode (not training mode)\n\n# save model\ntorch.save(model.state_dict(), PATH)\n\n\n\n\nTo make CNN more robust to different images + increase the size of the dataset\nCommon augmentations:\n\nCrop\nRotate\nFlip\nColor jitter\n\n\ndata_transforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.RandomVerticalFlip(p=0.5), # p=0.5 means 50% chance of applying this augmentation\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor()\n])\n\n\n\n\n\nNN has a lot of hyperparameters\n\nGrid search will take a long time\nNeed a smarter approach: Optimization Algorithms\n\nExamples: Ax (we will use this), Raytune, Neptune, skorch.\n\n\n\n\n\nIdea: use a pre-trained model and fine-tune it to our specific task\nInstall from torchvision.models\n\nAll models have been trained on ImageNet dataset (224x224 images)\n\nSee here for code\n\n\n\ndensenet = models.densenet121(weights='DenseNet121_Weights.DEFAULT')\n\nfor param in densenet.parameters():  # Freeze parameters so we don't update them\n    param.requires_grad = False\n# can fine-tune to freeze only some layers\n\nlist(densenet.named_children())[-1] # check the last layer\n\n# update the last layer\nnew_layers = nn.Sequential(\n    nn.Linear(1024, 500),\n    nn.ReLU(),\n    nn.Linear(500, 1)\n)\ndensenet.classifier = new_layers\nThen train the model as usual.\ndensenet.to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(densenet.parameters(), lr=2e-3)\nresults = trainer(densenet, criterion, optimizer, train_loader, valid_loader, device, epochs=10)\n\n\n\n\nIdea:\n\nTake output from pre-trained model\nFeed output to a new model\n\n\ndef get_features(model, train_loader, valid_loader):\n    \"\"\"\n    Extract features from both training and validation datasets using the provided model.\n\n    This function passes data through a given neural network model to extract features. It's designed\n    to work with datasets loaded using PyTorch's DataLoader. The function operates under the assumption\n    that gradients are not required, optimizing memory and computation for inference tasks.\n    \"\"\"\n\n    # Disable gradient computation for efficiency during inference\n    with torch.no_grad():\n        # Initialize empty tensors for training features and labels\n        Z_train = torch.empty((0, 1024))  # Assuming each feature vector has 1024 elements\n        y_train = torch.empty((0))\n\n        # Initialize empty tensors for validation features and labels\n        Z_valid = torch.empty((0, 1024))\n        y_valid = torch.empty((0))\n\n        # Process training data\n        for X, y in train_loader:\n            # Extract features and concatenate them to the corresponding tensors\n            Z_train = torch.cat((Z_train, model(X)), dim=0)\n            y_train = torch.cat((y_train, y))\n\n        # Process validation data\n        for X, y in valid_loader:\n            # Extract features and concatenate them to the corresponding tensors\n            Z_valid = torch.cat((Z_valid, model(X)), dim=0)\n            y_valid = torch.cat((y_valid, y))\n\n    # Return the feature and label tensors\n    return Z_train, y_train, Z_valid, y_valid\nNow we can use the extracted features to train a new model.\n# Extract features from the pre-trained model\ndensenet = models.densenet121(weights='DenseNet121_Weights.DEFAULT')\ndensenet.classifier = nn.Identity()  # remove that last \"classification\" layer\nZ_train, y_train, Z_valid, y_valid = get_features(densenet, train_loader, valid_loader)\n\n# Train a new model using the extracted features\n# Let's scale our data\nscaler = StandardScaler()\nZ_train = scaler.fit_transform(Z_train)\nZ_valid = scaler.transform(Z_valid)\n\n# Fit a model\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(Z_train, y_train)"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#advanced-cnn",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#advanced-cnn",
    "title": "Supervised Learning II",
    "section": "",
    "text": "Generative Models\nDiscriminative Models\n\n\n\n\nDirectly model the joint probability distribution of the input and output\nModel the conditional probability of the output given the input\n\n\nDirectly model \\(P(y\\|x)\\)\nEstimate \\(P(x\\|y)\\) to then deduce \\(P(y\\|x)\\)\n\n\nBuild model for each class\nMake boundary between classes\n\n\n“Generate or draw a cat”\n“Distinquish between cats and dogs”\n\n\nExamples: Naibe bayes, ChatGPT\nExamples: Logistic Regression, SVM, Tree based models, CNN\n\n\n\n\n\n\n\n\nDesigned to reconstruct the input\nEncoder and a decoder\nWhy do we need autoencoders?\n\nDimensionality reduction\nDenoising\n\n\n\n\n\nMaybe the z axis is unimportant in the input space for classification\n\nfrom torch import nn\n\nclass autoencoder(torch.nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, 2),\n            nn.Sigmoid()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(2, input_size),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n# Set up the training\nBATCH_SIZE = 100\ntorch.manual_seed(1)\nX_tensor = torch.tensor(X, dtype=torch.float32)\ndataloader = DataLoader(X_tensor,\n                        batch_size=BATCH_SIZE)\nmodel = autoencoder(3, 2)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters())\n\n# Train the model\nEPOCHS = 5\n\nfor epoch in range(EPOCHS):\n    for batch in dataloader:\n        optimizer.zero_grad()           # Clear gradients w.r.t. parameters\n        y_hat = model(batch)            # Forward pass to get output\n        loss = criterion(y_hat, batch)  # Calculate loss\n        loss.backward()                 # Getting gradients w.r.t. parameters\n        optimizer.step()                # Update parameters\n\n# Use encoder\nmodel.eval()\nX_encoded = model.encoder(X_tensor)\n\n\n\n\nRemove noise from the input\nUse Transposed Convolution Layers to upsample the input\n\nNormal convolution: downsample (output is smaller than input)\nTransposed convolution: upsample (output is larger than input)\n\n\ndef conv_block(input_channels, output_channels):\n    return nn.Sequential(\n        nn.Conv2d(input_channels, output_channels, 3, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(2)  # reduce x-y dims by two; window and stride of 2\n    )\n\ndef deconv_block(input_channels, output_channels, kernel_size):\n    return nn.Sequential(\n        nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride=2),\n        nn.ReLU()\n    )\n\nclass autoencoder(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            conv_block(1, 32),\n            conv_block(32, 16),\n            conv_block(16, 8)\n        )\n        self.decoder = nn.Sequential(\n            deconv_block(8, 8, 3),\n            deconv_block(8, 16, 2),\n            deconv_block(16, 32, 2),\n            nn.Conv2d(32, 1, 3, padding=1)  # final conv layer to decrease channel back to 1\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        x = torch.sigmoid(x)  # get pixels between 0 and 1\n        return x\n# Set up the training\nEPOCHS = 20\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters())\nimg_list = []\n\nfor epoch in range(EPOCHS):\n    losses = 0\n    for batch, _ in trainloader:\n        noisy_batch = batch + noise * torch.randn(*batch.shape)\n        noisy_batch = torch.clip(noisy_batch, 0.0, 1.0)\n        optimizer.zero_grad()\n        y_hat = model(noisy_batch)\n        loss = criterion(y_hat, batch)\n        loss.backward()\n        optimizer.step()\n        losses += loss.item()\n    print(f\"epoch: {epoch + 1}, loss: {losses / len(trainloader):.4f}\")\n    # Save example results each epoch so we can see what's going on\n    with torch.no_grad():\n        noisy_8 = noisy_batch[:1, :1, :, :]\n        model_8 = model(input_8)\n        real_8 = batch[:1, :1, :, :]\n    img_list.append(utils.make_grid([noisy_8[0], model_8[0], real_8[0]], padding=1))```"
  },
  {
    "objectID": "block_4/572_sup_learn_2/572_sup_learn_2.html#generative-adversarial-networks-gans",
    "href": "block_4/572_sup_learn_2/572_sup_learn_2.html#generative-adversarial-networks-gans",
    "title": "Supervised Learning II",
    "section": "",
    "text": "Model used to generate new data (indistinguishable from real data)\nNo need for labels (unsupervised learning)\nSee here\n\n\n\nTwo networks:\n\nGenerator: creates new data\nDiscriminator: tries to distinguish between real and fake data\n\nBoth are battling each other:\n\nGenerator tries to create data that the discriminator can’t distinguish from real data\nDiscriminator tries to distinguish between real and fake data\n\n\n\n\n\nTrain the discriminator (simple binary classification)\n\nTrain the discriminator on real data\nTrain the discriminator on fake data (generated by the generator)\n\nTrain the generator\n\nGenerate fake images with the generator and label them as real\nPass to discriminator and ask it to classify them (real or fake)\nPass judgement to a loss function (see how far it is from the ideal output)\n\nideal output: all fake images are classified as real\n\nDo backpropagation and update the generator\n\nRepeat\n\n\n\n\n\nCreating the data loader\nDATA_DIR = \"../input/face-recognition-dataset/Extracted Faces\"\n\nBATCH_SIZE = 64\nIMAGE_SIZE = (128, 128)\n\ndata_transforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE), # uses CPU (bottleneck)\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ndataset = datasets.ImageFolder(root=DATA_DIR, transform=data_transforms)\ndata_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\nCreating the generator\n\nclass Generator(nn.Module):\n\n def __init__(self, LATENT_SIZE):\n     super(Generator, self).__init__()\n\n     self.main = nn.Sequential(\n         nn.ConvTranspose2d(LATENT_SIZE, 1024, kernel_size=4, stride=1, padding=0, bias=False),\n         nn.BatchNorm2d(1024),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(512),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(128),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(3),\n\n         nn.Tanh()\n     )\n\n def forward(self, input):\n     return self.main(input)\nCreating the discriminator\n\nclass Discriminator(nn.Module):\n\n def __init__(self):\n     super(Discriminator, self).__init__()\n\n     self.main = nn.Sequential(\n         nn.Conv2d(3, 128, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(128),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.Conv2d(128, 512, kernel_size=4, stride=2, padding=1, bias=False),\n         nn.BatchNorm2d(512),\n         nn.LeakyReLU(0.2, inplace=True),\n\n         nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n         nn.Flatten(),\n         nn.Sigmoid()\n     )\n\n def forward(self, input):\n     return self.main(input)\nInstantiating the models\ndevice = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n\nLATENT_SIZE = 100\ngenerator = Generator(LATENT_SIZE).to(device)\ndiscriminator = Discriminator().to(device)\n\ncriterion = nn.BCELoss()\n\noptimizerG = optim.Adam(generator.parameters(), lr=0.001, betas=(0.5, 0.999))\noptimizerD = optim.Adam(discriminator.parameters(), lr=0.001, betas=(0.5, 0.999))\n\n def weights_init(m):\n     if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n         nn.init.normal_(m.weight.data, 0.0, 0.02)\n     elif isinstance(m, nn.BatchNorm2d):\n         nn.init.normal_(m.weight.data, 1.0, 0.02)\n         nn.init.constant_(m.bias.data, 0)\n\n\n generator.apply(weights_init)\n discriminator.apply(weights_init);\nTraining the GAN\n img_list = []\nfixed_noise = torch.randn(BATCH_SIZE, LATENT_SIZE, 1, 1).to(device)\n\n NUM_EPOCHS = 50\nfrom statistics import mean\nprint('Training started:\\n')\n\n D_real_epoch, D_fake_epoch, loss_dis_epoch, loss_gen_epoch = [], [], [], []\n\n for epoch in range(NUM_EPOCHS):\n     D_real_iter, D_fake_iter, loss_dis_iter, loss_gen_iter = [], [], [], []\n\n     for real_batch, _ in data_loader:\n\n         # STEP 1: train discriminator\n         # ==================================\n         optimizerD.zero_grad()\n\n         real_batch = real_batch.to(device)\n         real_labels = torch.ones((real_batch.shape[0],), dtype=torch.float).to(device)\n\n         output = discriminator(real_batch).view(-1)\n         loss_real = criterion(output, real_labels)\n\n         # Iteration book-keeping\n         D_real_iter.append(output.mean().item())\n\n         # Train with fake data\n         noise = torch.randn(real_batch.shape[0], LATENT_SIZE, 1, 1).to(device)\n\n         fake_batch = generator(noise)\n         fake_labels = torch.zeros_like(real_labels)\n\n         output = discriminator(fake_batch.detach()).view(-1)\n         loss_fake = criterion(output, fake_labels)\n\n         # Update discriminator weights\n         loss_dis = loss_real + loss_fake\n         loss_dis.backward()\n         optimizerD.step()\n\n         # Iteration book-keeping\n         loss_dis_iter.append(loss_dis.mean().item())\n         D_fake_iter.append(output.mean().item())\n\n         # STEP 2: train generator\n         # ==================================\n         optimizerG.zero_grad()\n\n         # Calculate the output with the updated weights of the discriminator\n         output = discriminator(fake_batch).view(-1)\n         loss_gen = criterion(output, real_labels)\n         loss_gen.backward()\n\n         # Book-keeping\n         loss_gen_iter.append(loss_gen.mean().item())\n\n         # Update generator weights and store loss\n         optimizerG.step()\n\n     print(f\"Epoch ({epoch + 1}/{NUM_EPOCHS})\\t\",\n         f\"Loss_G: {mean(loss_gen_iter):.4f}\",\n         f\"Loss_D: {mean(loss_dis_iter):.4f}\\t\",\n         f\"D_real: {mean(D_real_iter):.4f}\",\n         f\"D_fake: {mean(D_fake_iter):.4f}\")\n\n     # Epoch book-keeping\n     loss_gen_epoch.append(mean(loss_gen_iter))\n     loss_dis_epoch.append(mean(loss_dis_iter))\n     D_real_epoch.append(mean(D_real_iter))\n     D_fake_epoch.append(mean(D_fake_iter))\n\n     # Keeping track of the evolution of a fixed noise latent vector\n     with torch.no_grad():\n         fake_images = generator(fixed_noise).detach().cpu()\n         #img_list.append(utils.make_grid(fake_images, normalize=True, nrows=10))\n\n print(\"\\nTraining ended.\")\nVisualize training process\n plt.plot(np.array(loss_gen_epoch), label='loss_gen')\n plt.plot(np.array(loss_dis_epoch), label='loss_dis')\n plt.xlabel(\"Epoch\")\n plt.ylabel(\"Loss\")\n plt.legend();\n plt.plot(np.array(D_real_epoch), label='D_real')\n plt.plot(np.array(D_fake_epoch), label='D_fake')\n plt.xlabel(\"Epoch\")\n plt.ylabel(\"Probability\")\n plt.legend();\n\n\n\n\nclass multiModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        ...\n\n    def forward(self, image, data):\n        x_cnn = self.cnn(image) # 1st model: CNN\n        x_fc = self.fc(data) # 2nd model: Fully connected\n        return torch.cat((x_cnn, x_fc), dim=1) # concatenate the two outputs"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html",
    "href": "block_4/562_regression_2/562_regression_2.html",
    "title": "Regression II",
    "section": "",
    "text": "General stages of the workflow:\n\nStudy design\nData collection and wrangling\nExploratory data analysis (EDA)\nData modeling\nEstimation\nResults\nStorytelling\n\n\n\nFrom: https://pages.github.ubc.ca/MDS-2023-24/DSCI_562_regr-2_students/notes/lecture1-glm-link-functions-and-count-regression.html\n\nChoose a proper workflow according to either:\n\nInferential\nPredictive\n\nAlso choose correct regression model\n\n\n\n\n\n\n\nResponse of continuous nature (hence the “ordinary”)\nResponse is subject to regressors (or explanatory variables/ features/ independent variables)\n\nMore than 1 regressor =&gt; multiple linear regression\n\n\n\\[Response = Systematic + Random\\]\n\\[Y_i = \\beta_0 + \\beta_1 g_1(x_{i1}) + \\beta_2 g_2(x_{i2}) + \\dots + \\beta_p g_p(x_{ip}) + \\epsilon_i\\]\n\nrandom is the \\(\\epsilon_i\\) term\n\n\n\n\nLinearity: the relationship between the response and functions of the regressors is linear\nErrors are independent of each other and are normally distributed with mean 0 and variance \\(\\sigma^2\\)\n\nHence, each \\(Y_i\\) is assumed to be independent and normally distributed.\n\n\n\n\nTo fit will need \\(k+2\\) parameters: \\(\\beta_0, \\beta_1, \\dots, \\beta_k, \\sigma^2\\)\nMinimize the sum of squared errors (SSE) OR maximize the likelihood of the observed data\nMaximum Likelihood Estimation (MLE): find the parameters that maximize the likelihood of the observed data\n\nLikelihood: the probability of observing the data given the parameters\nLog-likelihood: the log of the likelihood\n\n\n\n\n\n\nDo a t-test on the parameters to see if they are statistically significant\n\n\n\n\n\n\nOLS allows response to take any real number.\nExamples of non-suitable responses:\n\nNon-negative values\nBinary values (success/failure)\nCount data\n\n\n\n\n\n\n\nRecall: OLS models a continuous response via its conditional mean\n\n\\[\\mu_i = E(Y_i | X_i) = \\beta_0 + \\beta_1 g_1(x_{i1}) + \\beta_2 g_2(x_{i2}) + \\dots + \\beta_p g_p(x_{ip}) \\]\n\nBUT this is not suitable for non-continuous responses (e.g. binary, count, non-negative).\nSolution: use a link function \\(h(\\mu_i)\\) to map the conditional mean to the real line\n\n\n\nLink function: relate the systematic component, \\(\\eta_i\\), with the response’s mean\n\n\\[h(\\mu_i) = \\eta_i = \\beta_0 + \\beta_1 g_1(x_{i1}) + \\beta_2 g_2(x_{i2}) + \\dots + \\beta_p g_p(x_{ip}) \\]\n\nMonotonic: allows for a one-to-one mapping between the mean of the response variable and the linear predictor\n\n\\[\\mu_i = h^{-1}(\\eta_i)\\]\n\nDifferentiable: to allow for maximum likelihood estimation (MLE), used to obtain \\(\\hat{\\beta}\\) $$\n\n\n\n\n\nGeneralized Linear Models (GLM): a generalization of OLS regression that allows for non-continuous responses\n\nGLM = link function + error distribution\n\n\n\n\n\nPoisson regression: a GLM for count data (Equidispersed)\n\nEquidispersed: the variance of the response is equal to its mean (i.e. \\(Var(Y_i) = E(Y_i) = \\lambda_i\\))\n\nIt assumes a random sample of \\(n\\) count observations \\(Y_i\\)s\n\nIndependent\nNot Identically Distributed: Each \\(Y_i\\) has its own mean \\(E(Y_i) = \\lambda_i &gt; 0\\) and variance \\(Var(Y_i) = \\lambda_i &gt; 0\\)\n\n\n\\[Y_i \\sim Poisson(\\lambda_i)\\]\n\n\\(\\lambda_i\\) is the risk of event occurance in a given timeframe or area (definition of Poisson distribution)\n\n\n\n\nLog link function: the log of the mean of the response variable is linearly related to the regressors\n\n\\[h(\\mu_i) = log(\\mu_i) = \\eta_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ik} \\]\nHence,\n\\[\\lambda_i = e^{\\eta_i} = e^{\\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ik}}\\]\n\nThis is good since \\(\\lambda_i\\) (mean count) is always positive\n\n\n\n\nglm(Y ~ X, family = poisson, data = dataset)\n\n# view each regression coefficient\ntidy(glm_model)\ntidy(glm_model, conf.int = TRUE) # for 95% confidence interval\n\n# view model summary\nglance(glm_model)\n\n\ne.g. \\(\\beta_1 = 0.5\\)\n\n\\(\\beta_1\\) is the expected change in the log of the mean count for a one-unit increase in \\(X_1\\) holding all other variables constant\na one-unit increase in \\(X_1\\) will increase the mean count by \\(e^{0.5} = 1.65\\) times.\n\n\n\n\n\n\nTo determine the significance of the parameters \\(\\beta_1, \\beta_2, \\dots, \\beta_p\\), we can do a Wald statistic\n\n\\[z_j = \\frac{\\hat{\\beta_j}}{SE(\\hat{\\beta_j})}\\]\n\nTo test the hypothesis:\n\n\\(H_0: \\beta_j = 0\\)\n\\(H_1: \\beta_j \\neq 0\\)\n\n\n\n\n\n\n\nNegative Binomial Regression: a GLM for count data (Overdispersed)\n\nOverdispersed: the variance of the response is greater than its mean (i.e. \\(Var(Y_i) &gt; E(Y_i) = \\lambda_i\\))\n\n\n\nCheck for Overdispersion\ndispersiontest(glm_model)\n\nif p-value &lt; 0.05, then there is overdispersion (reject null hypothesis)\n\nIf use Poisson regression on overdispersed data, then the standard errors will be underestimated =&gt; Type I error (false positive) increases\n\n\nRecall PMF of Negative Binomial Distribution:\n\n\\[P(Y_i | m,p_i) = (^{y_i + m - 1}_{y_i}) p_i^{y_i} (1 - p_i)^m\\]\n\n\\(y_i\\) is the number of failures before experiencing \\(m\\) successes where probability of success is \\(p_i\\)\n\n\\[E(Y_i) = \\frac{m(1-p_i)}{p_i}\\]\n\\[Var(Y_i) = \\frac{m(1-p_i)}{p_i^2}\\]\n\nRearranging the above equations, we get:\n\n\\[E(Y_i) = \\lambda_i\\]\n\\[Var(Y_i) = \\lambda_i (1 + \\frac{\\lambda_i}{m})\\]\n\nInteresting information:\n\n\\[X \\sim Poisson(\\lambda) = lim_{m \\to \\infty} Negative Binomial(m, p_i)\\]\n\n\nglm.nb(Y ~ X, data = dataset)\nSince negative binomial has the same link function as Poisson, we can interpret the coefficients the same way.\n\n\n\n\n\n\n\nThe deviance (\\(D_k\\)) is used to compare a given model with k regressors (\\(l_k\\)) with the baseline/ saturated model (\\(l_0\\)).\n\nThe baseline model is the “perfect” fit to the data (overfitted), it has a distinct poisson mean (\\(\\lambda_i\\)) for each \\(i\\)th observation.\n\n\n\\[D_k = 2 log \\frac{\\hat{l}_k}{\\hat{l}_0}\\]\n\nInterpretation of \\(D_k\\)\n\nLarge value of \\(D_k\\) =&gt; poor fit compared to baseline model\nSmall value of \\(D_k\\) =&gt; good fit compared to baseline model\n\n\n\n\n\\[D_k = 2 \\sum_{i=1}^n \\left[ y_i log \\left( \\frac{y_i}{\\hat{\\lambda}_i} \\right) - (y_i - \\hat{\\lambda}_i) \\right]\\]\n*note: when \\(y_i = 0\\), log term is defined to be 0.\n\nHypothesises are as follows (opposite of normal hypothesis):\n\n\\(H_0\\): Our model with k regressors fits the data better than the saturated model.\n\\(H_A\\): Otherwise\n\n\nglance(model) # D_k is \"deviance\" col\n\n# to get p-value\npchisq(summary_poisson_model_2$deviance,\n  df = summary_poisson_model_2$df.residual,\n  lower.tail = FALSE\n)\n\nFormally deviance is residual deviance, this is a test statistic.\nAsmptomatically, it has a null distribution of:\n\n\\[D_k \\sim \\chi^2_{n-k-1}\\]\n\ndof: \\(n-k-1\\)\n\n\\(n\\) is the number of observations\n\\(k\\) is the number of regressors (including intercept)\n\n\n\n\n\nanova(model_1, model_2, test = \"Chisq\")\n# deviance column is \\delta D_k\n\nmodel_1 is nested in model_2\n\\(H_0\\): model_1 fits the data better as model_2\n\\(H_A\\): model_2 fits the data better as model_1\n\n\\[\\Delta D_k = D_{k_1} - D_{k_2} \\sim \\chi^2_{k_2 - k_1}\\]\n\n\n\n\n\\[AIC_k = D_k + 2k\\]\n\nAIC can be used to compare models that are not nested.\nSmaller AIC is better (means better fit)\nCan get from glance() function\n\n\n\n\n\\[BIC_k = D_k + k log(n)\\]\n\nBIC tends to select models with fewer regressors than AIC.\nsmaller BIC is better (means better fit)\nCan get from glance() function\n\n\n\n\n\n\nIs a MLE-based GLM for when the response is categorical and nominal.\n\nNominal: unordered categories\n\ne.g. red, green, blue\n\nOrdinal: ordered categories\n\ne.g. low, medium, high\n\n\nSimilar to binomial logistic regression, but with more than 2 categories.\nLink function is the logit function.\n\nneed more than 1 logit function to model the probabilities of each category.\nOne category is the baseline category, the other categories are compared to the baseline category.\n\n\n\\[\\eta_i^{(model 2, model 1)} = \\log\\left[\\frac{P(Y_i = \\texttt{model 2} \\mid X_{i, 1}, X_{i,2}, X_{i,3})}{P(Y_i = \\texttt{model 1} \\mid X_{i, 1}, X_{i,2}, X_{i,3})}\\right]\\]\n\\[= \\beta_0^{(\\texttt{model 2},\\texttt{model 1})} + \\beta_1^{(\\texttt{model 2},\\texttt{model 1})} X_{i, 1} + \\beta_2^{(\\texttt{model 2},\\texttt{model 1})} X_{i, 2} + \\beta_3^{(\\texttt{model 2},\\texttt{model 1})} X_{i, 3}\\] \\[\\eta_i^{(model 3, model 1)} = \\log\\left[\\frac{P(Y_i = \\texttt{model 3} \\mid X_{i, 1}, X_{i,2}, X_{i,3})}{P(Y_i = \\texttt{model 1} \\mid X_{i, 1}, X_{i,2}, X_{i,3})}\\right]\\]\n\\[=\\beta_0^{(\\texttt{model 3},\\texttt{model 1})} + \\beta_1^{(\\texttt{model 3},\\texttt{model 1})} X_{i, 1} + \\beta_2^{(\\texttt{model 3},\\texttt{model 1})} X_{i, 2} + \\beta_3^{(\\texttt{model 3},\\texttt{model 1})} X_{i, 3}\\]\nWith some algebra, we can get the following (For m categories):\n\\[p_{i, \\texttt{model 1}} = \\frac{1}{1 + \\sum_{j=2}^m e^{\\eta_i^{(\\texttt{model j}, \\texttt{model 1})}}}\\]\n\\[p_{i, \\texttt{model 2}} = \\frac{e^{\\eta_i^{(\\texttt{model 2}, \\texttt{model 1})}}}{1 + \\sum_{j=2}^m e^{\\eta_i^{(\\texttt{model j}, \\texttt{model 1})}}}\\]\n\nAll probabilities sum to 1.\n\n\n\n\nThe baseline level is the level that is not included in the model.\n\ncan find using levels() function, the first level is the baseline level.\n\n\nlevels(data$response) # to check levels\n\n# to change levels\ndata$response &lt;- recode_factor(data$response,\n  \"0\" = \"new_level_0\",\n  \"1\" = \"new_level_1\",\n)\n\n\n\nmodel &lt;- multinom(response ~ regressor_1 + regressor_2 + regressor_3,\n  data = data)\n\n# to get test statistics\nmlr_output &lt;- tidy(model,\n  conf.int = TRUE, # to get confidence intervals (default is 95%)\n  exponentiate = TRUE) # to get odds ratios\n# default result is log odds ratios\n\n# can filter p-values\nmlr_output |&gt; filter(p.value &lt; 0.05)\n\n# predict\npredict(model, newdata = data, type = \"probs\")\n# sum of all probabilities is 1\n\n\n\n\nCheck if regressor is significant using Wald test.\n\n\\[z_j^{(u,v)} = \\frac{\\hat{\\beta}_j^{(u,v)}}{SE(\\hat{\\beta}_j^{(u,v)})}\\]\n\nFor large sample sizes, \\(z_j^{(u,v)} \\sim N(0,1)\\)\nTo test the hypothesis:\n\n\\(H_0\\): \\(\\beta_j^{(u,v)} = 0\\)\n\\(H_A\\): \\(\\beta_j^{(u,v)} \\neq 0\\)\n\n\n\n\n\ne.g. \\(\\beta_1^{(b,a)} = 0.5\\)\n\nFor a 1 unit increase in \\(X_1\\), the odds of being in category \\(b\\) is \\(e^{0.5} = 1.65\\) times the odds of being in category \\(a\\).\n\ne.g. \\(\\beta_2^{(c,a)} = -0.5\\)\n\nFor a 1 unit increase in \\(X_2\\), the odds of being in category \\(c\\) decrease by \\(39\\%\\) [$ 1 - (e^{-0.5}) = 1 - 0.61 = 0.39$] less than being in category \\(a\\).\n\n\n\n\n\n\nOrdinal: has a natural ordering\nThere might be loss of information when using MLR for ordinal data\nWe are going to use the proportional odds model for ordinal data\n\nIt is a cumulative logit model\n\n\n\n\n\nReorder the levels of the response variable\n\ndata$response &lt;- as.ordered(data$response)\ndata$response &lt;- fct_relevel(\n  data$response,\n  c(\"unlikely\", \"somewhat likely\", \"very likely\")\n)\nlevels(data$response)\n\n\n\n\nFor a response with \\(m\\) responses and \\(k\\) regressors, the model is:\nWe will have:\n\n\\(m-1\\) equations (link functions: logit)\n\\(m-1\\) intercepts\n\\(k\\) regression coefficients\n\n\n\n\n\\[\n\\begin{gather*}\n\\text{Level } m - 1 \\text{ or any lesser degree versus level } m\\\\\n\\text{Level } m - 2 \\text{ or any lesser degree versus level } m - 1 \\text{ or any higher degree}\\\\\n\\vdots \\\\\n\\text{Level } 1 \\text{ versus level } 2 \\text{ or any higher degree}\\\\\n\\end{gather*}\n\\]\n\\[\n\\begin{gather*}\n\\eta_i^{(m - 1)} = \\log\\left[\\frac{P(Y_i \\leq m - 1 \\mid X_{i,1}, \\ldots, X_{i,k})}{P(Y_i = m \\mid X_{i,1}, \\ldots, X_{i,k})}\\right] = \\beta_0^{(m - 1)} - \\beta_1 X_{i, 1} - \\beta_2 X_{i, 2} - \\ldots - \\beta_k X_{i, k} \\\\\n\\eta_i^{(m - 2)} = \\log\\left[\\frac{P(Y_i \\leq m - 2 \\mid X_{i,1}, \\ldots, X_{i,k})}{P(Y_i &gt; m - 2 \\mid X_{i,1}, \\ldots, X_{i,k})}\\right] = \\beta_0^{(m - 2)} - \\beta_1 X_{i, 1} - \\beta_2 X_{i, 2} - \\ldots - \\beta_k X_{i, k} \\\\\n\\vdots \\\\\n\\eta_i^{(1)} = \\log\\left[\\frac{P(Y_i = 1 \\mid X_{i,1}, \\ldots, X_{i,k})}{P(Y_i &gt; 1 \\mid X_{i,1}, \\ldots, X_{i,k})}\\right] = \\beta_0^{(1)} - \\beta_1 X_{i, 1} - \\beta_2 X_{i, 2} - \\ldots - \\beta_k X_{i, k}.\n\\end{gather*}\n\\]\n\n\n\n\\[p_{i,j} = P(Y_i = j \\mid X_{i,1}, \\ldots, X_{i,k}) = P(Y_i \\leq j \\mid ...) - P(Y_i \\leq j - 1 \\mid ...)\\]\n\n\\(i\\) is the index of the observation\n\\(j\\) is the level of the response variable\n\n\\[\\sum_{j = 1}^{m} p_{i,j} = 1\\]\n\n\n\n\n\nuse MASS::polr function\n\nordinal_model &lt;- polr(\n  formula = response ~ regressor_1 + regressor_2,\n  data = data,\n  Hess = TRUE # Hessian matrix of log-likelihood\n)\n\n\n\n\nSimilar to MLR using Wald test\n\ncbind(\n  tidy(ordinal_model),\n  p.value = pnorm(abs(tidy(ordinal_model)$statistic),\n    lower.tail = FALSE\n  ) * 2\n)\n# confidence intervals\n\nconfint(ordinal_model) # default is 95%\n\n\n\n\ne.g. \\(\\beta_1 = 0.6\\)\n\nFor a one unit increase in \\(X_1\\), the odds of being in a higher category is \\(e^{0.6} = 1.82\\) times the odds of being in a lower category, holding all other variables constant.\n\n\n\n\n\npredict(ordinal_model, newdata = data, type = \"probs\")\n# returns probabilities for each level\n\nTo get the corresponding predicted cumulative odds for a new observation, use VGAM::vglm function\n\nolr &lt;- vglm(\n  response ~ regressor_1 + regressor_2,\n  propodds, # for proportional odds model\n  data,\n)\n\n# can also predict using this model, same as code block above\npredict(olr, newdata = data, type = \"response\")\n\n# get predicted cumulative odds\npredict(olr, newdata = data, type = \"link\") |&gt;\n  exp() # to get odds instead of log odds\n\nInterpret the predicted cumulative odds as:\n\ne.g. \\(logitlink(P[Y_i \\geq j]) = 2.68\\)\n\nA student with [data for \\(X_i\\)] is 2.68 times more likely to be in \\(j\\) or higher category than in category \\(j - 1\\) or lower, holding all other variables constant.\n\ne.g. \\(logitlink(P[Y_i \\geq 2]) = 0.33\\)\n\nA student with [data for \\(X_i\\)] is 3.03 (1/0.33) times more likely to be in \\(j\\) category or lower than in category j or higher, holding all other variables constant.\n\n\n\n\n\n\n\nIf the proportional odds assumption is not met, we can use the partial proportional odds model.\nTest for proportional odds assumption using the Brant-Wald test.\n\n\\(H_0\\): Our OLR model globally fulfills the proportional odds assumption.\n\\(H_A\\): Our OLR model does not globally fulfill the proportional odds assumption.\n\n\nbrant(ordinal_model)\n\nIf the proportional odds assumption is not met, we can use the generalized ordinal logistic regression model.\n\nBasically all \\(\\beta\\)’s are allowed to vary across the different levels of the response variable.\n\n\n\n\n\n\n\nLinear Fixed Effects Model (LFE) is a generalization of the linear regression model\nFixed Effects: the parameters of the model\n\nconstant for all observations\n\n\n\n\n\nData Hierarchy: the data is organized in a hierarchy\n\nCan be due to sampling levels\ne.g. investmests in different firms, students in different schools (sampling schemes may be different in different schools)\n\nMight have some correlation between datapoints in firms/ schools\n\nviolates the independence assumption (i.i.d. observations)\n\n\n\n\n\nGoal: assessing the association of gross investment with market_value and capital in the population of American firms.\nData: 11 firms, 20 observations per firm\n\n2 heirachical levels: firm and observation\n\n\n\nTrial 1: ignore firm\n\nordinary_model &lt;- lm(\n  formula = investment ~ market_value + capital,\n  data = Grunfeld)\n\nTrial 2: Different intercepts for different firms\n\nmodel_varying_intercept &lt;- lm(\n  # -1: so that baseline is not included as first intercept\n    formula = investment ~ market_value + capital + firm - 1,\n    data = Grunfeld)\n\nTrial 3: OLS regeression for each firm\n\n\nThis does NOT solve our goal.\nWe want to find out among all firms, not one specific firm.\n\nmodel_by_firm &lt;- lm(\n  investment ~ market_value * firm + capital * firm,\n  data = Grunfeld)\n\n\n\n\n\n\nFundamental idea:\n\ndata subsets of elements share a correlation structure\ni.e. all n rows of training data are not independent\n\n\n\\[ \\text{mixed effect} = \\text{fixed effect} + \\text{random effect} \\]\n\\[\\beta_{0j} = \\beta_0 + b_{0j}\\]\n\n\\(\\beta_{0j}\\)/ mixed effect: the intercept for the \\(j\\)th school/ firm\n\\(\\beta_0\\)/ fixed effect: the average intercept\n\\(b_{0j}\\)/ random effect: the deviation of the \\(j\\)th school/ firm from the average intercept\n\n\\(b_{0j} \\sim N(0, \\sigma^2_{0})\\)\nindependent of the error term \\(\\epsilon\\)\n\nVariance of the \\(i\\)th observation:\n\n\\(\\sigma^2_{0} + \\sigma^2_{\\epsilon}\\)\n\n\n\n\n\\[\ny_{ij} = \\beta_{0j} + \\beta_{1j}x_{1ij} + \\beta_{2j}x_{2ij} + \\epsilon_{ij} \\\\ = (\\beta_0 + b_{0j}) + (\\beta_1 + b_{1j})x_{1ij} + (\\beta_2 + b_{2j})x_{2ij} + \\epsilon_{ij}\n\\]\nFor \\(i\\) in \\(1, 2, \\ldots, n_j\\) and \\(j\\) in \\(1, 2, \\ldots, J\\)\nNote: \\((b_{0j}, b_{1j}, b_{2j}) \\sim N(\\textbf{0}, \\textbf{D})\\)\n\n\\(\\textbf{0}\\): vector of zero, e.g. \\((0, 0, 0)^T\\)\n\\(\\textbf{D}\\): generic covariance matrix\n\n\\[\\textbf{D} = \\begin{bmatrix} \\sigma^2_{0} & \\sigma_{01} & \\sigma_{02} \\\\ \\sigma_{10} & \\sigma^2_{1} & \\sigma_{12} \\\\ \\sigma_{20} & \\sigma_{21} & \\sigma^2_{2} \\end{bmatrix} = \\begin{bmatrix} \\sigma^2_{0} & \\rho_{01}\\sigma_{0}\\sigma_{1} & \\rho_{02}\\sigma_{0}\\sigma_{2} \\\\ \\rho_{10}\\sigma_{0}\\sigma_{1} & \\sigma^2_{1} & \\rho_{12}\\sigma_{1}\\sigma_{2} \\\\ \\rho_{20}\\sigma_{0}\\sigma_{2} & \\rho_{21}\\sigma_{1}\\sigma_{2} & \\sigma^2_{2} \\end{bmatrix}\\]\n\n\\(\\rho_{uv}\\): pearson correlation between uth and vth random effects\n\n\n\n\n\nuse the lmer function from the lme4 package\n\nmixed_intercept_model &lt;- lmer(\n  response ~ regressor_1 + regressor_2 +\n    (1 | school), # random intercept by firm\n  data\n)\n\nfull_model &lt;- lmer(\n  response ~ regressor_1 + regressor_2 +\n    (regressor_1 + regressor_2| school),\n    # random intercept and slope by firm\n  data\n)\n\nEquation for mixed intercept model:\n\n\\[y_{ij} = (\\beta_0 + b_{0j}) + \\beta_1x_{1ij} + \\beta_2x_{2ij} + \\epsilon_{ij}\\]\n\nEquation for full model:\n\n\\[y_{ij} = (\\beta_0 + b_{0j}) + (\\beta_1 + b_{1j})x_{1ij} + (\\beta_2 + b_{2j})x_{2ij} + \\epsilon_{ij}\\]\n\n\n\n\nCannot do inference using normal t-test\n\nsummary(mixed_intercept_model)\nsummary(full_model)\n\n# obtain coefficients\ncoef(mixed_intercept_model)$firm\ncoef(full_mixed_model)$firm\n\n\n\n\nPredict on existing group\nPredict on new group\n\npredict(full_model,\n  newdata = tibble(school = \"new_school\", regressor_1 = 1, regressor_2 = 2))\n\n\n\n\n\nRecall that classical linear regression (parametric) favours interpreatbility when aiming to make inference\nIf goal is accurate prediction, then we can use local regression (non-linear)\n\n\n\n\nUse step function to fit a piecewise constant function\n\n\\[\nC_0(X_i) = I(X_i &lt; c_1) = \\begin{cases} 1 & \\text{if } X_i &lt; c_1 \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\n\\[\nC_1(X_i) = I(c_1 \\leq X_i &lt; c_2) = \\begin{cases} 1 & \\text{if } c_1 \\leq X_i &lt; c_2 \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\n\\[\n\\vdots\n\\]\n\\[\nC_{k-1}(X_i) = I(c_{k-1} \\leq X_i &lt; c_k) = \\begin{cases} 1 & \\text{if } c_{k-1} \\leq X_i &lt; c_k \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\n\\[\nC_k(X_i) = I(X_i \\leq c_k) = \\begin{cases} 1 & \\text{if } X_i \\leq c_k \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\n\\[Y_i = \\beta_0 + \\beta_1 C_1(X_i) + \\beta_2 C_2(X_i) + \\cdots + \\beta_k C_k(X_i) + \\epsilon_i\\]\n\nNo need \\(C_0\\) just by definition of \\(C_0\\)\n\nbreakpoints &lt;- c(10, 20, 30, 40, 50) # or 5 (number of breakpoints)\n\n# create steps\ndata &lt;- data |&gt; mutate(\n    steps = cut(data$var_to_split,\n                breaks = breakpoints,\n                right = FALSE))\nlevels(data$steps) # check levels\n\nmodel &lt;- lm(Y ~ steps, data = data)\n\n\n\n\nAdd interaction terms to the model\n\n\\[Y_i = \\beta_0 + \\beta_1C_1(X_i) + \\cdots + \\beta_kC_k(X_i) \\\\ + \\beta_{k+1}X_i + \\beta_{k+2}X_iC_1(X_i) + \\cdots + \\beta_{2k}X_iC_k(X_i) + \\epsilon_i\\]\nmodel_piecewise_linear &lt;- lm(Y ~ steps * var_to_split, data = data)\n\n\n\n\\[Y_i = \\beta_0 + \\beta_1X_i + \\beta_2(X_i - c_1)_+ + \\cdots + \\beta_k(X_i - c_{k-1})_+ + \\epsilon_i\\]\nWhere:\n\\[(X_i - c_j)_+ = \\begin{cases} X_i - c_j & \\text{if } X_i &gt; c_j \\\\ 0 & \\text{otherwise} \\end{cases}\\]\nmodel_piecewise_cont_linear &lt;- lm(Y ~ var_to_split +\n    I(var_to_split - breakpoint[2]) * I(var_to_split &gt;= breakpoint[2]) +\n    I(var_to_split - breakpoint[3]) * I(var_to_split &gt;= breakpoint[3]) +\n    I(var_to_split - breakpoint[4]) * I(var_to_split &gt;= breakpoint[4]) +\n    I(var_to_split - breakpoint[5]) * I(var_to_split &gt;= breakpoint[5]),\n    data = data)\n\n\n\n\n\nIn this section:\n\n\\(k\\): number of neighbours\n\\(p\\): number of regressors\n\nkNN is a non-parametric method\nno training phase (lazy learner)\nfinds \\(k\\) closest neighbours to the query point \\(x_0\\) and predicts the average of the neighbours’ responses\n\\(k=1\\) means no training error but overfitting\n\nmodel_knn &lt;- knnreg(Y ~ X, data = data, k = 5)\n\n\n\n\nIdea:\n\nFind closest points to \\(x_i\\) (query point)\nassign weights based on distance\n\ncloser -&gt; more weight\n\nuse weighted least squares for second degree polynomial fit\n\nMinimize sum of squares of weighted residuals\n\n\\[\\sum_{i=1}^n w_i(y_i - \\beta_0 - \\beta_1x_i - \\beta_2x_i^2)^2\\]\n\nThis model can deal with heteroscedasticity (non-constant variance)\n\nWeighted least squares allows different variance for each observation\n\nThings to consider:\n\nspan: between 0 and 1, specifies the proportion of points considered as neighbours (more neighbours -&gt; smoother fit)\ndegree: degree of polynomial to fit\n\n\nmodel_lowess &lt;- lowess(Y ~ X, data = data, span = 0.5, degree = 2)\n\n\n\n\n\n\nThe quantile \\(Q(\\tau)\\) is the observed value of \\(X\\) such that \\(\\tau * 100\\%\\) of the data is less than or equal to \\(X\\).\n\ni.e. on the left side of the distribution\n\n\n \n\n\n\n\nQuantile regression is a form of regression analysis used to estimate the conditional median or other quantiles of a response variable.\nTypes of questions that can be answered with quantile regression:\n\nFor baseball teams in the upper 75% threshold in runs, are these runs largely associated with a large number of hits?\nFor any given team that scores 1000 hits in future tournaments, how many runs can this team score with 50% chance?\n\n\n\n\n\n\nRecall OLS: \\[\n\\mathbb{E}(Y_i \\mid X_{i,j} = x_{i,j}) = \\beta_0 + \\beta_1 x_{i,1} + \\ldots + \\beta_k x_{i,k};\n\\]\n\n\\[\n\\text{loss} = \\sum_{i = 1}^n (y_i - \\beta_0 - \\beta_1 x_{i,1} - \\ldots - \\beta_k x_{i,k})^2.\n\\]\n\nParametric Quantile Regression:\n\n\\[\nQ_i( \\tau \\mid X_{i,j} = x_{i,j}) = \\beta_0(\\tau) + \\beta_1(\\tau) x_{i,1} + \\ldots + \\beta_k(\\tau) x_{i,k}\n\\]\n(notice the \\(\\beta\\)s are now functions of \\(\\tau\\))\n\nError term: Fidelity\n\n\\[\n\\text{loss} = \\sum_{i} e_i[\\tau - I(e_i &lt; 0)] = \\sum_{i: e_i \\geq 0} \\tau|e_i|+\\sum_{i: e_i &lt; 0}(1-\\tau)|e_i|\n\\]\nWhere\n\\[I(e_i &lt; 0) = \\begin{cases} 1 & \\text{if } e_i &lt; 0 \\\\ 0 & \\text{otherwise} \\end{cases}\\]\n\n\n# Plotting the quantiles\nggplot(data, aes(x, y)) +\n    geom_point() +\n    geom_quantile(quantiles = c(0.25, 0.5, 0.75),\n                    formula = y ~ x)\n\n# fit the model\npqr_model &lt;- rq(y ~ x,\n                tau = c(0.25, 0.5, 0.75),\n                data = data)\n\n\n\n\nTo justify the relationship b/w the \\(\\tau\\)th quantile in response and regressors.\nUse test statistics to test the null hypothesis that the \\(\\tau\\)th quantile is not related to the regressors.\n\n\\(t\\)-value with \\(n - k - 1\\) degrees of freedom:\n\n\n\\[t_j = \\frac{\\hat{\\beta}_j(\\tau)}{\\text{SE}(\\hat{\\beta}_j(\\tau))}\\]\n\nNull hypothesis: \\(H_0 : \\beta_j(\\tau) = 0\\).\nCheck p-value: summary(pqr_model)[1] for \\(\\tau = 0.25\\), summary(pqr_model)[2] for \\(\\tau = 0.5\\), summary(pqr_model)[3] for \\(\\tau = 0.75\\).\n\n\n\n\n\nSimilar to OLS\n\\(\\beta_1(\\tau)\\) is the change in the \\(\\tau\\)th quantile of \\(Y\\) for a unit increase in \\(X_1\\).\ne.g. \\(\\beta_1(0.75) = 0.5\\) means that for a unit increase in \\(X_1\\), the 75th quantile of \\(Y\\) increases by 0.5.\n\n\n\n\npredict(pqr_model, newdata = data.frame(...))\n\n\n\n\n\nImplicates no distributional assumptions and no model function specification.\n\\(\\lambda\\) is the penalty parameter.\n\nChoosing how local the estimation is.\nsmall \\(\\lambda\\): better approx, but more variance (model not smooth)\nlarge \\(\\lambda\\): lose local info, favouring smoothness (global info)\n\n\nmedian_rqss &lt;- rqss(y ~ qss(x, lambda = 0.5),\n            tau = 0.5, # cannot do multiple quantiles\n            data = data)\n\nsummary(median_rqss)\n\npredict(median_rqss, newdata = data.frame(...))\n\n\n\n\n\n\n\n\n\nThe probability of missing data is the same for all observations\nMissingness is independent of data (Ideal case because there is no pattern)\nNo systematic differences between missing and non-missing data\n\n\n\n\n\nThe probability of missing data depends on observed data\nCan use imputation to fill in missing data:\n\nHot deck: Replace missing value with a value from the same dataset\nCold deck: Replace missing value with a value from a different dataset\n\n\n\n\n\n\nThe probability of missing data depends on unobservable quantities\nE.g. missing data on income for people who are unemployed\n\n\n\n\n\n\n\n\nRemove all observations with missing data\nIf data is MCAR, this is unbiased\n\nAlso increases standard errors since we’re using less data\n\nIf data is MAR or MNAR, this is biased (CAREFUL)\n\ne.g. if missing data is related to income, lower income will omit telling us their income, so removing them will bias our data to higher income.\n\n\n\n\n\n\nReplace missing data with the mean of the observed data\n\nCan only be used on continuous/ count data\n\nArtificially reduces standard errors (drawback)\n\nAlso reduces variance, which is not good\n\n\nlibrary(mice)\n\ndata &lt;- mice(data, seed = 1, method = \"mean\")\ncomplete(data)\n\n\n\n\nUse a regression model to predict missing data\nUse the predicted value as the imputed value\nWe will reinforce the relationship between the predictor and the variable with missing data\n\nThis is not good if the relationship is not strong\nWill change inference results\n\n\ndata &lt;- mice(data, seed = 1, method = \"norm.predict\")\ncomplete(data)\n\n\n\n\nIdea: Impute missing data multiple times to account for uncertainty\nUse mice package in R. Stands for Multivariate Imputation by Chained Equations\nSteps:\n\nCreate m copies of the dataset\nIn each copy, impute missing data (different values)\nCarry out analysis on each dataset\nCombine models to one pooled model\n\n\nimp_data &lt;- mice(data, seed = 1, m = 15, printFlag = FALSE)\n\ncomplete(data, 3) # Get the third imputed dataset\n\n# estimate OLS regression on each dataset\nmodels &lt;- with(imp_data, lm(y ~ x1 + x2))\n\n# get third model\nmodels$analyses[[3]]\n\n# combine models\npooled_model &lt;- pool(models)\n\nsummary(pooled_model) # remember to exp if using log model"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#data-science-workflow",
    "href": "block_4/562_regression_2/562_regression_2.html#data-science-workflow",
    "title": "Regression II",
    "section": "",
    "text": "General stages of the workflow:\n\nStudy design\nData collection and wrangling\nExploratory data analysis (EDA)\nData modeling\nEstimation\nResults\nStorytelling\n\n\n\nFrom: https://pages.github.ubc.ca/MDS-2023-24/DSCI_562_regr-2_students/notes/lecture1-glm-link-functions-and-count-regression.html\n\nChoose a proper workflow according to either:\n\nInferential\nPredictive\n\nAlso choose correct regression model"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#review-of-regression-i",
    "href": "block_4/562_regression_2/562_regression_2.html#review-of-regression-i",
    "title": "Regression II",
    "section": "",
    "text": "Response of continuous nature (hence the “ordinary”)\nResponse is subject to regressors (or explanatory variables/ features/ independent variables)\n\nMore than 1 regressor =&gt; multiple linear regression\n\n\n\\[Response = Systematic + Random\\]\n\\[Y_i = \\beta_0 + \\beta_1 g_1(x_{i1}) + \\beta_2 g_2(x_{i2}) + \\dots + \\beta_p g_p(x_{ip}) + \\epsilon_i\\]\n\nrandom is the \\(\\epsilon_i\\) term\n\n\n\n\nLinearity: the relationship between the response and functions of the regressors is linear\nErrors are independent of each other and are normally distributed with mean 0 and variance \\(\\sigma^2\\)\n\nHence, each \\(Y_i\\) is assumed to be independent and normally distributed.\n\n\n\n\nTo fit will need \\(k+2\\) parameters: \\(\\beta_0, \\beta_1, \\dots, \\beta_k, \\sigma^2\\)\nMinimize the sum of squared errors (SSE) OR maximize the likelihood of the observed data\nMaximum Likelihood Estimation (MLE): find the parameters that maximize the likelihood of the observed data\n\nLikelihood: the probability of observing the data given the parameters\nLog-likelihood: the log of the likelihood\n\n\n\n\n\n\nDo a t-test on the parameters to see if they are statistically significant\n\n\n\n\n\n\nOLS allows response to take any real number.\nExamples of non-suitable responses:\n\nNon-negative values\nBinary values (success/failure)\nCount data"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#link-function",
    "href": "block_4/562_regression_2/562_regression_2.html#link-function",
    "title": "Regression II",
    "section": "",
    "text": "Recall: OLS models a continuous response via its conditional mean\n\n\\[\\mu_i = E(Y_i | X_i) = \\beta_0 + \\beta_1 g_1(x_{i1}) + \\beta_2 g_2(x_{i2}) + \\dots + \\beta_p g_p(x_{ip}) \\]\n\nBUT this is not suitable for non-continuous responses (e.g. binary, count, non-negative).\nSolution: use a link function \\(h(\\mu_i)\\) to map the conditional mean to the real line\n\n\n\nLink function: relate the systematic component, \\(\\eta_i\\), with the response’s mean\n\n\\[h(\\mu_i) = \\eta_i = \\beta_0 + \\beta_1 g_1(x_{i1}) + \\beta_2 g_2(x_{i2}) + \\dots + \\beta_p g_p(x_{ip}) \\]\n\nMonotonic: allows for a one-to-one mapping between the mean of the response variable and the linear predictor\n\n\\[\\mu_i = h^{-1}(\\eta_i)\\]\n\nDifferentiable: to allow for maximum likelihood estimation (MLE), used to obtain \\(\\hat{\\beta}\\) $$"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#generalized-linear-models-glm",
    "href": "block_4/562_regression_2/562_regression_2.html#generalized-linear-models-glm",
    "title": "Regression II",
    "section": "",
    "text": "Generalized Linear Models (GLM): a generalization of OLS regression that allows for non-continuous responses\n\nGLM = link function + error distribution"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#poisson-regression",
    "href": "block_4/562_regression_2/562_regression_2.html#poisson-regression",
    "title": "Regression II",
    "section": "",
    "text": "Poisson regression: a GLM for count data (Equidispersed)\n\nEquidispersed: the variance of the response is equal to its mean (i.e. \\(Var(Y_i) = E(Y_i) = \\lambda_i\\))\n\nIt assumes a random sample of \\(n\\) count observations \\(Y_i\\)s\n\nIndependent\nNot Identically Distributed: Each \\(Y_i\\) has its own mean \\(E(Y_i) = \\lambda_i &gt; 0\\) and variance \\(Var(Y_i) = \\lambda_i &gt; 0\\)\n\n\n\\[Y_i \\sim Poisson(\\lambda_i)\\]\n\n\\(\\lambda_i\\) is the risk of event occurance in a given timeframe or area (definition of Poisson distribution)\n\n\n\n\nLog link function: the log of the mean of the response variable is linearly related to the regressors\n\n\\[h(\\mu_i) = log(\\mu_i) = \\eta_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ik} \\]\nHence,\n\\[\\lambda_i = e^{\\eta_i} = e^{\\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ik}}\\]\n\nThis is good since \\(\\lambda_i\\) (mean count) is always positive\n\n\n\n\nglm(Y ~ X, family = poisson, data = dataset)\n\n# view each regression coefficient\ntidy(glm_model)\ntidy(glm_model, conf.int = TRUE) # for 95% confidence interval\n\n# view model summary\nglance(glm_model)\n\n\ne.g. \\(\\beta_1 = 0.5\\)\n\n\\(\\beta_1\\) is the expected change in the log of the mean count for a one-unit increase in \\(X_1\\) holding all other variables constant\na one-unit increase in \\(X_1\\) will increase the mean count by \\(e^{0.5} = 1.65\\) times.\n\n\n\n\n\n\nTo determine the significance of the parameters \\(\\beta_1, \\beta_2, \\dots, \\beta_p\\), we can do a Wald statistic\n\n\\[z_j = \\frac{\\hat{\\beta_j}}{SE(\\hat{\\beta_j})}\\]\n\nTo test the hypothesis:\n\n\\(H_0: \\beta_j = 0\\)\n\\(H_1: \\beta_j \\neq 0\\)"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#negative-binomial-regression",
    "href": "block_4/562_regression_2/562_regression_2.html#negative-binomial-regression",
    "title": "Regression II",
    "section": "",
    "text": "Negative Binomial Regression: a GLM for count data (Overdispersed)\n\nOverdispersed: the variance of the response is greater than its mean (i.e. \\(Var(Y_i) &gt; E(Y_i) = \\lambda_i\\))\n\n\n\nCheck for Overdispersion\ndispersiontest(glm_model)\n\nif p-value &lt; 0.05, then there is overdispersion (reject null hypothesis)\n\nIf use Poisson regression on overdispersed data, then the standard errors will be underestimated =&gt; Type I error (false positive) increases\n\n\nRecall PMF of Negative Binomial Distribution:\n\n\\[P(Y_i | m,p_i) = (^{y_i + m - 1}_{y_i}) p_i^{y_i} (1 - p_i)^m\\]\n\n\\(y_i\\) is the number of failures before experiencing \\(m\\) successes where probability of success is \\(p_i\\)\n\n\\[E(Y_i) = \\frac{m(1-p_i)}{p_i}\\]\n\\[Var(Y_i) = \\frac{m(1-p_i)}{p_i^2}\\]\n\nRearranging the above equations, we get:\n\n\\[E(Y_i) = \\lambda_i\\]\n\\[Var(Y_i) = \\lambda_i (1 + \\frac{\\lambda_i}{m})\\]\n\nInteresting information:\n\n\\[X \\sim Poisson(\\lambda) = lim_{m \\to \\infty} Negative Binomial(m, p_i)\\]\n\n\nglm.nb(Y ~ X, data = dataset)\nSince negative binomial has the same link function as Poisson, we can interpret the coefficients the same way."
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#likelihood-based-model-selection",
    "href": "block_4/562_regression_2/562_regression_2.html#likelihood-based-model-selection",
    "title": "Regression II",
    "section": "",
    "text": "The deviance (\\(D_k\\)) is used to compare a given model with k regressors (\\(l_k\\)) with the baseline/ saturated model (\\(l_0\\)).\n\nThe baseline model is the “perfect” fit to the data (overfitted), it has a distinct poisson mean (\\(\\lambda_i\\)) for each \\(i\\)th observation.\n\n\n\\[D_k = 2 log \\frac{\\hat{l}_k}{\\hat{l}_0}\\]\n\nInterpretation of \\(D_k\\)\n\nLarge value of \\(D_k\\) =&gt; poor fit compared to baseline model\nSmall value of \\(D_k\\) =&gt; good fit compared to baseline model\n\n\n\n\n\\[D_k = 2 \\sum_{i=1}^n \\left[ y_i log \\left( \\frac{y_i}{\\hat{\\lambda}_i} \\right) - (y_i - \\hat{\\lambda}_i) \\right]\\]\n*note: when \\(y_i = 0\\), log term is defined to be 0.\n\nHypothesises are as follows (opposite of normal hypothesis):\n\n\\(H_0\\): Our model with k regressors fits the data better than the saturated model.\n\\(H_A\\): Otherwise\n\n\nglance(model) # D_k is \"deviance\" col\n\n# to get p-value\npchisq(summary_poisson_model_2$deviance,\n  df = summary_poisson_model_2$df.residual,\n  lower.tail = FALSE\n)\n\nFormally deviance is residual deviance, this is a test statistic.\nAsmptomatically, it has a null distribution of:\n\n\\[D_k \\sim \\chi^2_{n-k-1}\\]\n\ndof: \\(n-k-1\\)\n\n\\(n\\) is the number of observations\n\\(k\\) is the number of regressors (including intercept)\n\n\n\n\n\nanova(model_1, model_2, test = \"Chisq\")\n# deviance column is \\delta D_k\n\nmodel_1 is nested in model_2\n\\(H_0\\): model_1 fits the data better as model_2\n\\(H_A\\): model_2 fits the data better as model_1\n\n\\[\\Delta D_k = D_{k_1} - D_{k_2} \\sim \\chi^2_{k_2 - k_1}\\]\n\n\n\n\n\\[AIC_k = D_k + 2k\\]\n\nAIC can be used to compare models that are not nested.\nSmaller AIC is better (means better fit)\nCan get from glance() function\n\n\n\n\n\\[BIC_k = D_k + k log(n)\\]\n\nBIC tends to select models with fewer regressors than AIC.\nsmaller BIC is better (means better fit)\nCan get from glance() function"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#multinomial-logistic-regression",
    "href": "block_4/562_regression_2/562_regression_2.html#multinomial-logistic-regression",
    "title": "Regression II",
    "section": "",
    "text": "Is a MLE-based GLM for when the response is categorical and nominal.\n\nNominal: unordered categories\n\ne.g. red, green, blue\n\nOrdinal: ordered categories\n\ne.g. low, medium, high\n\n\nSimilar to binomial logistic regression, but with more than 2 categories.\nLink function is the logit function.\n\nneed more than 1 logit function to model the probabilities of each category.\nOne category is the baseline category, the other categories are compared to the baseline category.\n\n\n\\[\\eta_i^{(model 2, model 1)} = \\log\\left[\\frac{P(Y_i = \\texttt{model 2} \\mid X_{i, 1}, X_{i,2}, X_{i,3})}{P(Y_i = \\texttt{model 1} \\mid X_{i, 1}, X_{i,2}, X_{i,3})}\\right]\\]\n\\[= \\beta_0^{(\\texttt{model 2},\\texttt{model 1})} + \\beta_1^{(\\texttt{model 2},\\texttt{model 1})} X_{i, 1} + \\beta_2^{(\\texttt{model 2},\\texttt{model 1})} X_{i, 2} + \\beta_3^{(\\texttt{model 2},\\texttt{model 1})} X_{i, 3}\\] \\[\\eta_i^{(model 3, model 1)} = \\log\\left[\\frac{P(Y_i = \\texttt{model 3} \\mid X_{i, 1}, X_{i,2}, X_{i,3})}{P(Y_i = \\texttt{model 1} \\mid X_{i, 1}, X_{i,2}, X_{i,3})}\\right]\\]\n\\[=\\beta_0^{(\\texttt{model 3},\\texttt{model 1})} + \\beta_1^{(\\texttt{model 3},\\texttt{model 1})} X_{i, 1} + \\beta_2^{(\\texttt{model 3},\\texttt{model 1})} X_{i, 2} + \\beta_3^{(\\texttt{model 3},\\texttt{model 1})} X_{i, 3}\\]\nWith some algebra, we can get the following (For m categories):\n\\[p_{i, \\texttt{model 1}} = \\frac{1}{1 + \\sum_{j=2}^m e^{\\eta_i^{(\\texttt{model j}, \\texttt{model 1})}}}\\]\n\\[p_{i, \\texttt{model 2}} = \\frac{e^{\\eta_i^{(\\texttt{model 2}, \\texttt{model 1})}}}{1 + \\sum_{j=2}^m e^{\\eta_i^{(\\texttt{model j}, \\texttt{model 1})}}}\\]\n\nAll probabilities sum to 1.\n\n\n\n\nThe baseline level is the level that is not included in the model.\n\ncan find using levels() function, the first level is the baseline level.\n\n\nlevels(data$response) # to check levels\n\n# to change levels\ndata$response &lt;- recode_factor(data$response,\n  \"0\" = \"new_level_0\",\n  \"1\" = \"new_level_1\",\n)\n\n\n\nmodel &lt;- multinom(response ~ regressor_1 + regressor_2 + regressor_3,\n  data = data)\n\n# to get test statistics\nmlr_output &lt;- tidy(model,\n  conf.int = TRUE, # to get confidence intervals (default is 95%)\n  exponentiate = TRUE) # to get odds ratios\n# default result is log odds ratios\n\n# can filter p-values\nmlr_output |&gt; filter(p.value &lt; 0.05)\n\n# predict\npredict(model, newdata = data, type = \"probs\")\n# sum of all probabilities is 1\n\n\n\n\nCheck if regressor is significant using Wald test.\n\n\\[z_j^{(u,v)} = \\frac{\\hat{\\beta}_j^{(u,v)}}{SE(\\hat{\\beta}_j^{(u,v)})}\\]\n\nFor large sample sizes, \\(z_j^{(u,v)} \\sim N(0,1)\\)\nTo test the hypothesis:\n\n\\(H_0\\): \\(\\beta_j^{(u,v)} = 0\\)\n\\(H_A\\): \\(\\beta_j^{(u,v)} \\neq 0\\)\n\n\n\n\n\ne.g. \\(\\beta_1^{(b,a)} = 0.5\\)\n\nFor a 1 unit increase in \\(X_1\\), the odds of being in category \\(b\\) is \\(e^{0.5} = 1.65\\) times the odds of being in category \\(a\\).\n\ne.g. \\(\\beta_2^{(c,a)} = -0.5\\)\n\nFor a 1 unit increase in \\(X_2\\), the odds of being in category \\(c\\) decrease by \\(39\\%\\) [$ 1 - (e^{-0.5}) = 1 - 0.61 = 0.39$] less than being in category \\(a\\)."
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#ordinal-logistic-regression",
    "href": "block_4/562_regression_2/562_regression_2.html#ordinal-logistic-regression",
    "title": "Regression II",
    "section": "",
    "text": "Ordinal: has a natural ordering\nThere might be loss of information when using MLR for ordinal data\nWe are going to use the proportional odds model for ordinal data\n\nIt is a cumulative logit model\n\n\n\n\n\nReorder the levels of the response variable\n\ndata$response &lt;- as.ordered(data$response)\ndata$response &lt;- fct_relevel(\n  data$response,\n  c(\"unlikely\", \"somewhat likely\", \"very likely\")\n)\nlevels(data$response)\n\n\n\n\nFor a response with \\(m\\) responses and \\(k\\) regressors, the model is:\nWe will have:\n\n\\(m-1\\) equations (link functions: logit)\n\\(m-1\\) intercepts\n\\(k\\) regression coefficients\n\n\n\n\n\\[\n\\begin{gather*}\n\\text{Level } m - 1 \\text{ or any lesser degree versus level } m\\\\\n\\text{Level } m - 2 \\text{ or any lesser degree versus level } m - 1 \\text{ or any higher degree}\\\\\n\\vdots \\\\\n\\text{Level } 1 \\text{ versus level } 2 \\text{ or any higher degree}\\\\\n\\end{gather*}\n\\]\n\\[\n\\begin{gather*}\n\\eta_i^{(m - 1)} = \\log\\left[\\frac{P(Y_i \\leq m - 1 \\mid X_{i,1}, \\ldots, X_{i,k})}{P(Y_i = m \\mid X_{i,1}, \\ldots, X_{i,k})}\\right] = \\beta_0^{(m - 1)} - \\beta_1 X_{i, 1} - \\beta_2 X_{i, 2} - \\ldots - \\beta_k X_{i, k} \\\\\n\\eta_i^{(m - 2)} = \\log\\left[\\frac{P(Y_i \\leq m - 2 \\mid X_{i,1}, \\ldots, X_{i,k})}{P(Y_i &gt; m - 2 \\mid X_{i,1}, \\ldots, X_{i,k})}\\right] = \\beta_0^{(m - 2)} - \\beta_1 X_{i, 1} - \\beta_2 X_{i, 2} - \\ldots - \\beta_k X_{i, k} \\\\\n\\vdots \\\\\n\\eta_i^{(1)} = \\log\\left[\\frac{P(Y_i = 1 \\mid X_{i,1}, \\ldots, X_{i,k})}{P(Y_i &gt; 1 \\mid X_{i,1}, \\ldots, X_{i,k})}\\right] = \\beta_0^{(1)} - \\beta_1 X_{i, 1} - \\beta_2 X_{i, 2} - \\ldots - \\beta_k X_{i, k}.\n\\end{gather*}\n\\]\n\n\n\n\\[p_{i,j} = P(Y_i = j \\mid X_{i,1}, \\ldots, X_{i,k}) = P(Y_i \\leq j \\mid ...) - P(Y_i \\leq j - 1 \\mid ...)\\]\n\n\\(i\\) is the index of the observation\n\\(j\\) is the level of the response variable\n\n\\[\\sum_{j = 1}^{m} p_{i,j} = 1\\]\n\n\n\n\n\nuse MASS::polr function\n\nordinal_model &lt;- polr(\n  formula = response ~ regressor_1 + regressor_2,\n  data = data,\n  Hess = TRUE # Hessian matrix of log-likelihood\n)\n\n\n\n\nSimilar to MLR using Wald test\n\ncbind(\n  tidy(ordinal_model),\n  p.value = pnorm(abs(tidy(ordinal_model)$statistic),\n    lower.tail = FALSE\n  ) * 2\n)\n# confidence intervals\n\nconfint(ordinal_model) # default is 95%\n\n\n\n\ne.g. \\(\\beta_1 = 0.6\\)\n\nFor a one unit increase in \\(X_1\\), the odds of being in a higher category is \\(e^{0.6} = 1.82\\) times the odds of being in a lower category, holding all other variables constant.\n\n\n\n\n\npredict(ordinal_model, newdata = data, type = \"probs\")\n# returns probabilities for each level\n\nTo get the corresponding predicted cumulative odds for a new observation, use VGAM::vglm function\n\nolr &lt;- vglm(\n  response ~ regressor_1 + regressor_2,\n  propodds, # for proportional odds model\n  data,\n)\n\n# can also predict using this model, same as code block above\npredict(olr, newdata = data, type = \"response\")\n\n# get predicted cumulative odds\npredict(olr, newdata = data, type = \"link\") |&gt;\n  exp() # to get odds instead of log odds\n\nInterpret the predicted cumulative odds as:\n\ne.g. \\(logitlink(P[Y_i \\geq j]) = 2.68\\)\n\nA student with [data for \\(X_i\\)] is 2.68 times more likely to be in \\(j\\) or higher category than in category \\(j - 1\\) or lower, holding all other variables constant.\n\ne.g. \\(logitlink(P[Y_i \\geq 2]) = 0.33\\)\n\nA student with [data for \\(X_i\\)] is 3.03 (1/0.33) times more likely to be in \\(j\\) category or lower than in category j or higher, holding all other variables constant.\n\n\n\n\n\n\n\nIf the proportional odds assumption is not met, we can use the partial proportional odds model.\nTest for proportional odds assumption using the Brant-Wald test.\n\n\\(H_0\\): Our OLR model globally fulfills the proportional odds assumption.\n\\(H_A\\): Our OLR model does not globally fulfill the proportional odds assumption.\n\n\nbrant(ordinal_model)\n\nIf the proportional odds assumption is not met, we can use the generalized ordinal logistic regression model.\n\nBasically all \\(\\beta\\)’s are allowed to vary across the different levels of the response variable."
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#linear-fixed-effects-model",
    "href": "block_4/562_regression_2/562_regression_2.html#linear-fixed-effects-model",
    "title": "Regression II",
    "section": "",
    "text": "Linear Fixed Effects Model (LFE) is a generalization of the linear regression model\nFixed Effects: the parameters of the model\n\nconstant for all observations\n\n\n\n\n\nData Hierarchy: the data is organized in a hierarchy\n\nCan be due to sampling levels\ne.g. investmests in different firms, students in different schools (sampling schemes may be different in different schools)\n\nMight have some correlation between datapoints in firms/ schools\n\nviolates the independence assumption (i.i.d. observations)\n\n\n\n\n\nGoal: assessing the association of gross investment with market_value and capital in the population of American firms.\nData: 11 firms, 20 observations per firm\n\n2 heirachical levels: firm and observation\n\n\n\nTrial 1: ignore firm\n\nordinary_model &lt;- lm(\n  formula = investment ~ market_value + capital,\n  data = Grunfeld)\n\nTrial 2: Different intercepts for different firms\n\nmodel_varying_intercept &lt;- lm(\n  # -1: so that baseline is not included as first intercept\n    formula = investment ~ market_value + capital + firm - 1,\n    data = Grunfeld)\n\nTrial 3: OLS regeression for each firm\n\n\nThis does NOT solve our goal.\nWe want to find out among all firms, not one specific firm.\n\nmodel_by_firm &lt;- lm(\n  investment ~ market_value * firm + capital * firm,\n  data = Grunfeld)"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#linear-mixed-effects-model",
    "href": "block_4/562_regression_2/562_regression_2.html#linear-mixed-effects-model",
    "title": "Regression II",
    "section": "",
    "text": "Fundamental idea:\n\ndata subsets of elements share a correlation structure\ni.e. all n rows of training data are not independent\n\n\n\\[ \\text{mixed effect} = \\text{fixed effect} + \\text{random effect} \\]\n\\[\\beta_{0j} = \\beta_0 + b_{0j}\\]\n\n\\(\\beta_{0j}\\)/ mixed effect: the intercept for the \\(j\\)th school/ firm\n\\(\\beta_0\\)/ fixed effect: the average intercept\n\\(b_{0j}\\)/ random effect: the deviation of the \\(j\\)th school/ firm from the average intercept\n\n\\(b_{0j} \\sim N(0, \\sigma^2_{0})\\)\nindependent of the error term \\(\\epsilon\\)\n\nVariance of the \\(i\\)th observation:\n\n\\(\\sigma^2_{0} + \\sigma^2_{\\epsilon}\\)\n\n\n\n\n\\[\ny_{ij} = \\beta_{0j} + \\beta_{1j}x_{1ij} + \\beta_{2j}x_{2ij} + \\epsilon_{ij} \\\\ = (\\beta_0 + b_{0j}) + (\\beta_1 + b_{1j})x_{1ij} + (\\beta_2 + b_{2j})x_{2ij} + \\epsilon_{ij}\n\\]\nFor \\(i\\) in \\(1, 2, \\ldots, n_j\\) and \\(j\\) in \\(1, 2, \\ldots, J\\)\nNote: \\((b_{0j}, b_{1j}, b_{2j}) \\sim N(\\textbf{0}, \\textbf{D})\\)\n\n\\(\\textbf{0}\\): vector of zero, e.g. \\((0, 0, 0)^T\\)\n\\(\\textbf{D}\\): generic covariance matrix\n\n\\[\\textbf{D} = \\begin{bmatrix} \\sigma^2_{0} & \\sigma_{01} & \\sigma_{02} \\\\ \\sigma_{10} & \\sigma^2_{1} & \\sigma_{12} \\\\ \\sigma_{20} & \\sigma_{21} & \\sigma^2_{2} \\end{bmatrix} = \\begin{bmatrix} \\sigma^2_{0} & \\rho_{01}\\sigma_{0}\\sigma_{1} & \\rho_{02}\\sigma_{0}\\sigma_{2} \\\\ \\rho_{10}\\sigma_{0}\\sigma_{1} & \\sigma^2_{1} & \\rho_{12}\\sigma_{1}\\sigma_{2} \\\\ \\rho_{20}\\sigma_{0}\\sigma_{2} & \\rho_{21}\\sigma_{1}\\sigma_{2} & \\sigma^2_{2} \\end{bmatrix}\\]\n\n\\(\\rho_{uv}\\): pearson correlation between uth and vth random effects\n\n\n\n\n\nuse the lmer function from the lme4 package\n\nmixed_intercept_model &lt;- lmer(\n  response ~ regressor_1 + regressor_2 +\n    (1 | school), # random intercept by firm\n  data\n)\n\nfull_model &lt;- lmer(\n  response ~ regressor_1 + regressor_2 +\n    (regressor_1 + regressor_2| school),\n    # random intercept and slope by firm\n  data\n)\n\nEquation for mixed intercept model:\n\n\\[y_{ij} = (\\beta_0 + b_{0j}) + \\beta_1x_{1ij} + \\beta_2x_{2ij} + \\epsilon_{ij}\\]\n\nEquation for full model:\n\n\\[y_{ij} = (\\beta_0 + b_{0j}) + (\\beta_1 + b_{1j})x_{1ij} + (\\beta_2 + b_{2j})x_{2ij} + \\epsilon_{ij}\\]\n\n\n\n\nCannot do inference using normal t-test\n\nsummary(mixed_intercept_model)\nsummary(full_model)\n\n# obtain coefficients\ncoef(mixed_intercept_model)$firm\ncoef(full_mixed_model)$firm\n\n\n\n\nPredict on existing group\nPredict on new group\n\npredict(full_model,\n  newdata = tibble(school = \"new_school\", regressor_1 = 1, regressor_2 = 2))"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#piecewise-local-regression",
    "href": "block_4/562_regression_2/562_regression_2.html#piecewise-local-regression",
    "title": "Regression II",
    "section": "",
    "text": "Recall that classical linear regression (parametric) favours interpreatbility when aiming to make inference\nIf goal is accurate prediction, then we can use local regression (non-linear)\n\n\n\n\nUse step function to fit a piecewise constant function\n\n\\[\nC_0(X_i) = I(X_i &lt; c_1) = \\begin{cases} 1 & \\text{if } X_i &lt; c_1 \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\n\\[\nC_1(X_i) = I(c_1 \\leq X_i &lt; c_2) = \\begin{cases} 1 & \\text{if } c_1 \\leq X_i &lt; c_2 \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\n\\[\n\\vdots\n\\]\n\\[\nC_{k-1}(X_i) = I(c_{k-1} \\leq X_i &lt; c_k) = \\begin{cases} 1 & \\text{if } c_{k-1} \\leq X_i &lt; c_k \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\n\\[\nC_k(X_i) = I(X_i \\leq c_k) = \\begin{cases} 1 & \\text{if } X_i \\leq c_k \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\n\\[Y_i = \\beta_0 + \\beta_1 C_1(X_i) + \\beta_2 C_2(X_i) + \\cdots + \\beta_k C_k(X_i) + \\epsilon_i\\]\n\nNo need \\(C_0\\) just by definition of \\(C_0\\)\n\nbreakpoints &lt;- c(10, 20, 30, 40, 50) # or 5 (number of breakpoints)\n\n# create steps\ndata &lt;- data |&gt; mutate(\n    steps = cut(data$var_to_split,\n                breaks = breakpoints,\n                right = FALSE))\nlevels(data$steps) # check levels\n\nmodel &lt;- lm(Y ~ steps, data = data)\n\n\n\n\nAdd interaction terms to the model\n\n\\[Y_i = \\beta_0 + \\beta_1C_1(X_i) + \\cdots + \\beta_kC_k(X_i) \\\\ + \\beta_{k+1}X_i + \\beta_{k+2}X_iC_1(X_i) + \\cdots + \\beta_{2k}X_iC_k(X_i) + \\epsilon_i\\]\nmodel_piecewise_linear &lt;- lm(Y ~ steps * var_to_split, data = data)\n\n\n\n\\[Y_i = \\beta_0 + \\beta_1X_i + \\beta_2(X_i - c_1)_+ + \\cdots + \\beta_k(X_i - c_{k-1})_+ + \\epsilon_i\\]\nWhere:\n\\[(X_i - c_j)_+ = \\begin{cases} X_i - c_j & \\text{if } X_i &gt; c_j \\\\ 0 & \\text{otherwise} \\end{cases}\\]\nmodel_piecewise_cont_linear &lt;- lm(Y ~ var_to_split +\n    I(var_to_split - breakpoint[2]) * I(var_to_split &gt;= breakpoint[2]) +\n    I(var_to_split - breakpoint[3]) * I(var_to_split &gt;= breakpoint[3]) +\n    I(var_to_split - breakpoint[4]) * I(var_to_split &gt;= breakpoint[4]) +\n    I(var_to_split - breakpoint[5]) * I(var_to_split &gt;= breakpoint[5]),\n    data = data)"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#knn-regression",
    "href": "block_4/562_regression_2/562_regression_2.html#knn-regression",
    "title": "Regression II",
    "section": "",
    "text": "In this section:\n\n\\(k\\): number of neighbours\n\\(p\\): number of regressors\n\nkNN is a non-parametric method\nno training phase (lazy learner)\nfinds \\(k\\) closest neighbours to the query point \\(x_0\\) and predicts the average of the neighbours’ responses\n\\(k=1\\) means no training error but overfitting\n\nmodel_knn &lt;- knnreg(Y ~ X, data = data, k = 5)"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#locally-weighted-scatterplot-smoothing-lowess",
    "href": "block_4/562_regression_2/562_regression_2.html#locally-weighted-scatterplot-smoothing-lowess",
    "title": "Regression II",
    "section": "",
    "text": "Idea:\n\nFind closest points to \\(x_i\\) (query point)\nassign weights based on distance\n\ncloser -&gt; more weight\n\nuse weighted least squares for second degree polynomial fit\n\nMinimize sum of squares of weighted residuals\n\n\\[\\sum_{i=1}^n w_i(y_i - \\beta_0 - \\beta_1x_i - \\beta_2x_i^2)^2\\]\n\nThis model can deal with heteroscedasticity (non-constant variance)\n\nWeighted least squares allows different variance for each observation\n\nThings to consider:\n\nspan: between 0 and 1, specifies the proportion of points considered as neighbours (more neighbours -&gt; smoother fit)\ndegree: degree of polynomial to fit\n\n\nmodel_lowess &lt;- lowess(Y ~ X, data = data, span = 0.5, degree = 2)"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#quantile-regression",
    "href": "block_4/562_regression_2/562_regression_2.html#quantile-regression",
    "title": "Regression II",
    "section": "",
    "text": "The quantile \\(Q(\\tau)\\) is the observed value of \\(X\\) such that \\(\\tau * 100\\%\\) of the data is less than or equal to \\(X\\).\n\ni.e. on the left side of the distribution\n\n\n \n\n\n\n\nQuantile regression is a form of regression analysis used to estimate the conditional median or other quantiles of a response variable.\nTypes of questions that can be answered with quantile regression:\n\nFor baseball teams in the upper 75% threshold in runs, are these runs largely associated with a large number of hits?\nFor any given team that scores 1000 hits in future tournaments, how many runs can this team score with 50% chance?\n\n\n\n\n\n\nRecall OLS: \\[\n\\mathbb{E}(Y_i \\mid X_{i,j} = x_{i,j}) = \\beta_0 + \\beta_1 x_{i,1} + \\ldots + \\beta_k x_{i,k};\n\\]\n\n\\[\n\\text{loss} = \\sum_{i = 1}^n (y_i - \\beta_0 - \\beta_1 x_{i,1} - \\ldots - \\beta_k x_{i,k})^2.\n\\]\n\nParametric Quantile Regression:\n\n\\[\nQ_i( \\tau \\mid X_{i,j} = x_{i,j}) = \\beta_0(\\tau) + \\beta_1(\\tau) x_{i,1} + \\ldots + \\beta_k(\\tau) x_{i,k}\n\\]\n(notice the \\(\\beta\\)s are now functions of \\(\\tau\\))\n\nError term: Fidelity\n\n\\[\n\\text{loss} = \\sum_{i} e_i[\\tau - I(e_i &lt; 0)] = \\sum_{i: e_i \\geq 0} \\tau|e_i|+\\sum_{i: e_i &lt; 0}(1-\\tau)|e_i|\n\\]\nWhere\n\\[I(e_i &lt; 0) = \\begin{cases} 1 & \\text{if } e_i &lt; 0 \\\\ 0 & \\text{otherwise} \\end{cases}\\]\n\n\n# Plotting the quantiles\nggplot(data, aes(x, y)) +\n    geom_point() +\n    geom_quantile(quantiles = c(0.25, 0.5, 0.75),\n                    formula = y ~ x)\n\n# fit the model\npqr_model &lt;- rq(y ~ x,\n                tau = c(0.25, 0.5, 0.75),\n                data = data)\n\n\n\n\nTo justify the relationship b/w the \\(\\tau\\)th quantile in response and regressors.\nUse test statistics to test the null hypothesis that the \\(\\tau\\)th quantile is not related to the regressors.\n\n\\(t\\)-value with \\(n - k - 1\\) degrees of freedom:\n\n\n\\[t_j = \\frac{\\hat{\\beta}_j(\\tau)}{\\text{SE}(\\hat{\\beta}_j(\\tau))}\\]\n\nNull hypothesis: \\(H_0 : \\beta_j(\\tau) = 0\\).\nCheck p-value: summary(pqr_model)[1] for \\(\\tau = 0.25\\), summary(pqr_model)[2] for \\(\\tau = 0.5\\), summary(pqr_model)[3] for \\(\\tau = 0.75\\).\n\n\n\n\n\nSimilar to OLS\n\\(\\beta_1(\\tau)\\) is the change in the \\(\\tau\\)th quantile of \\(Y\\) for a unit increase in \\(X_1\\).\ne.g. \\(\\beta_1(0.75) = 0.5\\) means that for a unit increase in \\(X_1\\), the 75th quantile of \\(Y\\) increases by 0.5.\n\n\n\n\npredict(pqr_model, newdata = data.frame(...))\n\n\n\n\n\nImplicates no distributional assumptions and no model function specification.\n\\(\\lambda\\) is the penalty parameter.\n\nChoosing how local the estimation is.\nsmall \\(\\lambda\\): better approx, but more variance (model not smooth)\nlarge \\(\\lambda\\): lose local info, favouring smoothness (global info)\n\n\nmedian_rqss &lt;- rqss(y ~ qss(x, lambda = 0.5),\n            tau = 0.5, # cannot do multiple quantiles\n            data = data)\n\nsummary(median_rqss)\n\npredict(median_rqss, newdata = data.frame(...))"
  },
  {
    "objectID": "block_4/562_regression_2/562_regression_2.html#missing-data",
    "href": "block_4/562_regression_2/562_regression_2.html#missing-data",
    "title": "Regression II",
    "section": "",
    "text": "The probability of missing data is the same for all observations\nMissingness is independent of data (Ideal case because there is no pattern)\nNo systematic differences between missing and non-missing data\n\n\n\n\n\nThe probability of missing data depends on observed data\nCan use imputation to fill in missing data:\n\nHot deck: Replace missing value with a value from the same dataset\nCold deck: Replace missing value with a value from a different dataset\n\n\n\n\n\n\nThe probability of missing data depends on unobservable quantities\nE.g. missing data on income for people who are unemployed\n\n\n\n\n\n\n\n\nRemove all observations with missing data\nIf data is MCAR, this is unbiased\n\nAlso increases standard errors since we’re using less data\n\nIf data is MAR or MNAR, this is biased (CAREFUL)\n\ne.g. if missing data is related to income, lower income will omit telling us their income, so removing them will bias our data to higher income.\n\n\n\n\n\n\nReplace missing data with the mean of the observed data\n\nCan only be used on continuous/ count data\n\nArtificially reduces standard errors (drawback)\n\nAlso reduces variance, which is not good\n\n\nlibrary(mice)\n\ndata &lt;- mice(data, seed = 1, method = \"mean\")\ncomplete(data)\n\n\n\n\nUse a regression model to predict missing data\nUse the predicted value as the imputed value\nWe will reinforce the relationship between the predictor and the variable with missing data\n\nThis is not good if the relationship is not strong\nWill change inference results\n\n\ndata &lt;- mice(data, seed = 1, method = \"norm.predict\")\ncomplete(data)\n\n\n\n\nIdea: Impute missing data multiple times to account for uncertainty\nUse mice package in R. Stands for Multivariate Imputation by Chained Equations\nSteps:\n\nCreate m copies of the dataset\nIn each copy, impute missing data (different values)\nCarry out analysis on each dataset\nCombine models to one pooled model\n\n\nimp_data &lt;- mice(data, seed = 1, m = 15, printFlag = FALSE)\n\ncomplete(data, 3) # Get the third imputed dataset\n\n# estimate OLS regression on each dataset\nmodels &lt;- with(imp_data, lm(y ~ x1 + x2))\n\n# get third model\nmodels$analyses[[3]]\n\n# combine models\npooled_model &lt;- pool(models)\n\nsummary(pooled_model) # remember to exp if using log model"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html",
    "href": "block_3/561_regression/561_regression.html",
    "title": "Regression I",
    "section": "",
    "text": "Basic idea: fit a line to data\nCan help with:\n\nEstimation: how to estimate the true (but unknown) relation between the response and the input variables\nInference: how to use the model to infer information about the unknown relation between variables\nPrediction: how to use the model to predict the value of the response for new observations\n\nBefore starting, normally do some exploratory data analysis (EDA) to get a sense of the data\n\nfind out size\ndistribution of variables\nmissing values/ outliers\nrelationships between variables\n\n\n\n\n\n\nSimple linear regression: one response variable and one explanatory variable\n\nLet \\((X_i, Y_i)\\) for \\(i = 1, \\dots, n\\) random sample of size \\(n\\) from a population.\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\]\nWhere:\n\n\\(Y\\) is the response variable\n\\(X\\) is the explanatory/ input variable\n\\(\\epsilon_i \\sim N(0, \\sigma^2)\\) and \\(\\epsilon_1, \\dots, \\epsilon_n\\) are independent.\n\nIt represents other factors not taken into account in the model\n\n\n\nImage from the book: “Beyond Multiple Linear Regression”, from Paul Roback and Julie Legler\nAssumptions:\n\nconditional expectation of \\(Y\\) is linearly related to \\(X\\) \\[E(Y|X) = \\beta_0 + \\beta_1 X\\]\niid\nall random error \\(\\epsilon_i\\) is normally distributed with mean 0 and variance \\(\\sigma^2\\)\n\n\\[E(\\epsilon | X) = 0 \\\\ and \\\\ Var(\\epsilon | X) = \\sigma^2\\]\n\n\n\nLine that minimizes the sum of squared errors (SSE) or distance between the line and the data points\n\n\\[SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 = \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i)^2\\] [[fact check the equation above]]\n\n\n\n# fit a linear model\nlm(Y ~ X, data = data)\n\n# plot linear line using ggplot2\nggplot(data, aes(x = X, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\nTurn categorical into dummy variables (numerical)\n\ne.g. X = Fireplace in a house, Y = Price of the house\nMake dummy variable: \\(X_2 = 1\\) if house has fireplace, \\(X_2 = 0\\) if house doesn’t have fireplace\n\\(E(Y|X_2) = \\beta_0+ \\beta_2 X_2\\)\n\nSome cool thing:\n\n\\(\\beta_2 = E(Y|X_2 = 1) - E(Y|X_2 = 0) = \\mu_1 - \\mu_0\\)\n\nSAME as null hypothesis for t-test\n\n\\(\\beta_0 = E(Y|X_2 = 0)\\)\n\n\n\n\n\n\\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\]\n\nFor Linear Regression, we are estimating:\n\n\\(\\beta_0\\) and \\(\\beta_1\\)\n\n\n\n\n\nlm &lt;- lm(Y ~ X, data = data) |&gt;\n    tidy() |&gt;\n    mutate_if(is.numeric, round, 3) # round to 3 decimal places\n\n# Returns:\n# term  estimate    std.error   statistic   p.value\n# &lt;chr&gt; &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;             &lt;dbl&gt;\n\n\n\nIf take LR for new samples, we will get different estimates\n\nStandard Error is the standard deviation of the sampling distribution of the estimate\n\nMethods to measure uncertainty in estimation:\n\ncompute SE after taking multiple samples\nrarely take multiple samples, just take it theoretically (what lm does), see third col above\ncan also bootstrap from sample to get distribution of estimates -&gt; get standard error\n\n\n\n\n\nTake multiple samples and compute the variance of the estimates\n\n\n\n\n\nAssumption: conditional distribution of \\(Y_i\\) given \\(X_i\\) is normal with mean \\(\\beta_0 + \\beta_1 X_i\\) and variance \\(\\sigma^2\\), the \\(\\epsilon_i\\) term.\nUse t test with \\(df=n-p\\)\n\nn = sample size\np = number of parameters in the model\n\nUsed by lm to get standard error\nIf conditional dist is normal =&gt; error term is normal =&gt; sampling distribution of \\(\\hat{\\beta_1}\\) is normal\n\nOr if the sample size is large enough, sampling distribution of \\(\\hat{\\beta_1}\\) is approximately normal (CLT)\n\n\n\n\n\n\nBootstrap Normal Theory\n\\[\\beta \\pm z_{\\alpha/2} SE(\\hat{\\beta})\\]\n\nSE = stdev of bootstrap distribution\n\nBootstrap Percentile Method\n\nbasically take the 2.5th and 97.5th percentile of the bootstrap distribution (95% confidence interval)\n\n\\[\\hat{b}_{\\alpha/2} , \\hat{b}_{1-\\alpha/2}\\]\n\n\n\n\n\ntidy(lm_s, conf.int = TRUE)\n\nIf 0 is in the confidence interval, then we can’t reject the null hypothesis (\\(\\beta_1 = 0\\))\n\n\n\n\n\nCan do:\n\n\\(H_0: \\beta_0 = 0\\) vs \\(H_a: \\beta_0 \\neq 0\\)\n\nUsually not interesting\n\n\\(H_0: \\beta_1 = 0\\) vs \\(H_a: \\beta_1 \\neq 0\\)\n\nAnswers: is there a linear relationship between \\(X\\) and \\(Y\\)?\n\n\nRecall, test statistic under null hypothesis:\n\\[ t = \\frac{\\hat{\\beta_1}}{SE(\\hat{\\beta_1})} \\]\nWhich has a t-distribution with \\(n-p\\) degrees of freedom\n\n\n\n\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ip} + \\epsilon_i\\]\ne.g. \\[ Y_i = \\beta_0 + \\beta_1 height_i + \\beta_2 weight_i + \\epsilon_i \\]\n\nLinear regression models the conditional expectation as linear combination of the predictors\n\n\\[ E(Y|X_1, X_2, \\dots, X_p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p \\]\n\n\n# visualize the relationship between variables\n# using ggpairs from GGally package\nggpairs(data)\n\n# fit the model\nlm_s &lt;- lm(Y ~ X1 + X2 + X3, data = data)\n\n\n\n\nFor 2 categories (e.g. Fireplace exits or not) we only need 1 dummy variable\n\ncorresponding to 0 or 1\n\nFor 3 categories (e.g. old, modern, new) we need 2 dummy variables\n\nM corresponds to 1 if house is modern, 0 otherwise\nN corresponds to 1 if house is new, 0 otherwise\n\nFor \\(k\\) categories, we need \\(k-1\\) dummy variables\n\nlm(Y ~ cat_var, data = data) # cat_var is a categorical variable\n\n\n\n\n\nNormally when you have categorical and continuous variables, the linear model will:\n\nfit different intercepts for each category\nfit the same slope for each category (simpler)*\n\nInteraction terms allow us to fit different slopes for each category\n\n*Occam’s razor: if can explain with simpler model, do so\ne.g.\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 Z_i + (\\beta_3 X_i Z_i) + \\epsilon_i\\]\n\n\\(X_i\\) is a continuous variable\n\\(Z_i\\) is a categorical variable\n\\(\\beta_3\\) is the interaction term\n\nfit &lt;- lm(score ~ age*sex, data = dat)\ntidy(fit) |&gt; mutate_if(is.numeric, round, 3)\n# Returns columns:(intercept), age, sexmale, age:sexmale\n\nIntercept: average teaching score of female instructors of zero age\nage: slope for female instructors. Increase in age by 1 year will increase teaching score by age for female instructors\nsexmale: holding age constant, the difference in teaching score between male and female\nage:sexmale is the offset slope. Increase in age by one year is associated with the increase in teaching score of age + age:sexmale for male instructors.\n\n\n\n\n\nIs this better than nothing/ null model (only intercept)?\n\nIn other words is \\(E(Y|X)\\) better than \\(E(Y)\\)?\n\n\n\n\n\nTotal sum of squares (TSS):\n\n\\(TSS = \\sum (y_i - \\bar{y})^2\\)\nSum of squares from the null (intercept only) model\n\nExplained sum of squares (ESS):\n\n\\(ESS = \\sum (\\hat{y}_i - \\bar{y})^2\\)\nmeasures how mich it is explained by the model\n\nbetter model = larger ESS\n\n\nResidual sum of squares (RSS):\n\n\\(RSS = \\sum (y_i - \\hat{y}_i)^2\\)\nRSS == SSE (sum of squared errors) == n * MSE (mean squared error)\nEstimated parameters minimizes RSS (objective function)\n\nWhere:\n\n\\(y_i\\) observed ith value of y\n\\(\\hat{y}_i\\) predicted value of y with a model (of sample)\n\\(\\bar{y}_i\\) mean of y\n\n\nIf using linear regression using least squares:\n\\[ TSS = ESS + RSS \\]\nRecall: Residuals is the difference between the observed value and the predicted value. \\(r_i = y_i - \\hat{y}_i\\)\n\n\n\n\\[ R^2 = \\frac{ESS}{TSS} = 1 - \\frac{RSS}{TSS} \\]\n\n\\(R^2\\) is the proportion of variance explained by the model\n\nTells us how well the regression model explains the variation in the data\n\n\\(R^2\\) is between -inf and 1 (1 being perfect fit)\n\ngenerally between 0 and 1 since expect TSS &gt; RSS\n\nONLY IF the model has an intercept and is estimated by LS\n\nonly less than 0 if model is worse than null model\n\n\nglance(fit)\n\n\n\n\\(R^2\\) increases with number of predictors\n\n\\(R^2\\) will never decrease when adding predictors\n\nComputed based on “in-sample” predictions\n\nWe do not know how well the model will perform on new data\nBasically only on train data, dunno performance on test data\n\nIs it useful?\n\nYes, to compare size of residuals of fitted model to null model\nCannot be used to test any hypothesis since its distribution is unknown\n\n\n\n\n\n\nIs no longer coefficient of determination\nMeasures correlation between the true and predicted values\n\\(R^2 = Cor(Y, \\hat{Y})^2\\) if \\(\\hat{Y}\\) is a prediction obtained from a LR with an intercept estimated by LS.\n\nThis is ued to assess the prediction performance of a model\n\n\n\n\n\n\n\nCompare two models\n\n\\(reduced\\): intercept + some predictors\n\\(full\\): intercept + predictors\n\nIs the full model better than the reduced model? Simultaneously testing if many parameters are 0\nF-test is a global test\n\ntests if all parameters are 0\nif F-test is significant, then at least one parameter is not 0\n\n\n\\[F \\propto \\frac{(RSS_{reduced} - RSS_{full}) / k}{RSS_{full} / (n - p)}\\]\n\nparams:\n\nk = number of parameters tested (difference between models)\np = number of predictors in the full model (s + 1)\n\n\nlm_red &lt;- lm(score~1, data=dat) # intercept only\nlm_full &lt;- lm(score~ age + sex, data=dat)\n\nanova(lm_red,lm_full)\n# F-test tests H0: coef_age = coef_sex = 0\n\nglance(lm_full)\n# also includes F-statistic in \"statistic\" column + p-value\n# compares it to null model (intercept only)\n\nF-test can test multiple parameters at once\n\ne.g. \\(\\beta_1 = \\beta_2 = \\beta_3 = 0\\)\n\nFor one parameter, F-test is equivalent to t-test\n\ne.g. \\(\\beta_1 = 0\\)\nFor one parameter, \\(t^2 = F\\)\n\nwhere t is the t-statistic and F is the F-statistic\n\n\n\n\n\n\nif F-statistic is large (F &gt; 1), then the full model is better than the reduced model\nif F-statistic is small (F &lt; 1), then the full model is not better than the reduced model\n\n\n\n\n\nBoth depend on RSS and TSS, there is a formula to convert between them\nF-test has a known F-distribution (under certain assumptions) so we can use it to make probabilistic statements\n\n\n\n\n\n\n\n\n\nWhat is your goal?\n\nInference: understand the relationship between the response and the predictors\nPrediction: predict the response for future observations\n\n\n\n\n\n\nMean squared error (MSE): average squared error on new data\n\n\\(MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\)\nTrain MSE: MSE on training data\nTest MSE: MSE on test data\n\nResidual Sum of Squares (RSS): n * MSE\n\n\\(\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\)\nsmall is good\nresiduals are measured in training data\n\nResidual Standard Error (RSE): estimates the standard deviation of the residuals\n\n\\(\\sqrt{\\frac{1}{n-p} RSS}\\)\n\np = number of parameters in the model\n\nBased on training data to evaluate fit of the model (small is good)\n\nCoefficient of Determination (\\(R^2\\)):\n\n\\(R^2 = \\frac{ESS}{TSS} = 1 - \\frac{RSS}{TSS}\\)\n*See Interaction Terms &gt; Coefficient of Determination for more info\n\n\n\n\n\n\n\n\n\nUse F-test to test to compare and test nested models\n\nanova(lm_red,lm_full)\n\nt-test to test contribution of individual predictors\n\ntidy(lm_full, conf.int = TRUE)\n\nF-test is equivalent to t-test when there is only one predictor\n\\(R^2\\) is the proportion of variance explained by the model\n\n\\(R^2\\) increases with number of predictors (RSS decreases)\n\nAdjusted \\(R^2\\) penalizes for number of predictors\n\nAdjusted \\(R^2\\) increases only if the new predictor improves the model more than expected by chance\nAdjusted \\(R^2\\) decreases if the new predictor does not improve the model more than expected by chance \\[R^2_{adj} = 1 - \\frac{RSS/(n-p)}{TSS/(n-1)}\\]\nCam be used to compare models with different number of parameters/ predictors\n\n\nNested models: one model is a special case of the other\n\ne.g. lm(y ~ x1 + x2) is nested in lm(y ~ x1 + x2 + x3)\n\n\n\n\n\n\n\nAverage squared error on new data\nSelection criteria:\n\nThe Mallows Cp\nAkaike information criterion (AIC)\nBayesian information criterion (BIC)\n\nThese add different penalties to the training RSS to adjust for the fact that the training error tends to underestimate the test error\n\n\n\n\n\nForward selection\n\nStart with null model\nAdd predictors one at a time (select the best feats at that number of feats)\nStop when no more predictors improve the model\nleaps::regsubsets\n\nBackward selection: start with full model and remove predictors one at a time\nHybrid: after adding a predictor, check if any predictors can be removed\n\ndat_forward &lt;- regsubsets(\n  assess_val ~ age + FIREPLACE + GARAGE + BASEMENT,\n  data = dat_s,\n  nvmax = 5, # max number of feats\n  method = \"forward\", # backward, exhaustive, etc.\n)\n\n# view\nfws_summary &lt;- summary(dat_forward)\n# will have rsq, adj r2, rss, cp, bic, etc.\n\n\n\n\nOffers an alternative to the above methods\ne.g. Ridge, Lasso, Elastic Net\n\n\n\n\n\n\n\nConditional expectation of predictor \\(Y\\) given \\(X\\): \\[E(Y|X)\\]\nAssumes linear form of ^\n\n\n\nTerms:\n\n\\(\\hat{y}_i\\) is the predicted value of the LR model (sample)\n\\(E(Y_i|X_i)\\) is the conditional expectation of \\(Y\\) given \\(X_i\\)\n\nor the average value of \\(Y_i\\) for a given \\(X_i\\)\nor LR model of population\n\n\nWith our model \\(\\hat{y}\\) we can predict:\n\n\\(E(Y|X_0)\\)\n\\(Y_i\\) (Actual value of \\(Y\\) for \\(X_i\\))\n\nA lot harder to predict (individual prediction)\n\n\n2 types of intervals:\n\nConfidence intervals for prediction (CIP)\nPrediction intervals (PI)\n\n\n\n\n\n\n\n\n\nCIP\nPI\n\n\n\n\nUncertainty of the mean of the prediction\nUncertainty of the individual prediction\n\n\nUncertainty of \\(E(Y_i\\|X_i)\\)\nUncertainty of \\(Y_i\\)\n\n\nError from estimation of \\(\\beta\\)\nError from estimation of \\(\\beta\\) and \\(\\epsilon\\)\n\n\nSmaller than PI\nWider than CIP\n\n\nCentered around \\(\\hat{y}_i\\)\nCentered around \\(\\hat{y}_i\\)\n\n\n\n\n\n\nWe are interested in the mean of the prediction, not the individual prediction \\[E(Y_i|X_i)\\]\nUncertainty only comes from the estimation (1 source of uncertainty)\n\nThe model: \\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\]\napproximates, with uncertainty, the conditional expectation: \\[E(Y_i|X_i) = \\beta_0 + \\beta_1 x_i\\]\ni.e. Estimated coeffs (\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)) are estimates of true population coeffs (\\(\\beta_0\\) and \\(\\beta_1\\))\n\n95% CIP is a range where we have 95% confidence that it contains the average value of \\(Y_i\\) for a given \\(X_i\\)\n\nsmaller CIP means smaller confidence interval =&gt; less confidence it contains the true values\n\n\n\n\nmodel_sample &lt;- lm(y ~ x, data = data_sample)\n\ndata_sample |&gt;\n  select(x, y) |&gt;\n  cbind(\n    predict(\n      model_sample,\n      interval = \"confidence\",\n      level = 0.95, # 95% CIP is default\n      se.fit=TRUE # standard error of the fit,\n    )$fit\n  )\n# or\npredict(\n  model_sample,\n  interval = \"confidence\",\n  newdata = tibble(x1 = 1, x2 = 2),\n)\n\n\n\n\n\nWe are interested in the individual prediction: \\(Y_i\\)\nWider than CIP\n\nThe model: \\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i + \\epsilon_i\\] approximates, with uncertainty, an actual observation \\(Y_i\\).\n\nUncertainty comes from 2 sources:\n\nEstimation of the coeffs (\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\))\nVariability of the error term \\(\\epsilon_i\\)\n\nActual observation \\(Y_i\\) differs from the average (population) by \\(\\epsilon_i\\)\n\n\n95% PI is a range where we have 95% confidence that it contains the actual value of \\(Y_i\\) for a given \\(X_i\\)\n\n\n\ndata_sample |&gt;\n  select(x, y) |&gt;\n  cbind(\n    predict(\n      model_sample,\n      interval = \"prediction\", # change from \"confidence\"\n      level = 0.95, # 95% CIP is default\n      se.fit=TRUE # standard error of the fit\n    )$fit\n  )\n\n\n\n\n\n\n\n\nLinearity in regression != linearity in math\nLinearity in regression means that the model is linear in the parameters\nExamples of linear regression models:\n\n$ y = _0 + _1 x_1 + _2 x_2 + _3 x_3 $\n$ y = _0 + _1 x_1 + _2 x_2 +_3 x_1^2 + _4 x_1 x_2$\n$ y = _0 + _1 x_1 + _2 e^{x_1} + _3 (x_2)$\n\nNon-linear examples:\n\n$ y = _0 + _1 x_1 + _2 x_1^{_3} $\n\n\n\n\n\n\nLS estimation do not depend on any normality assumption\nBut if small sample size, normality assumption is needed for inference (since CLT does not apply)\nOLS (Ordinary Least Squares) do not require assumptions of normality. BUT we need to assume it in order to do inference (e.g. CIP, PI, hypothesis testing).\n\n\n\n\nQ-Q plot is a graphical method for assessing whether or not a data set follows a given distribution such as normal distribution\nPoints in the Q-Q plot follow a straight line if the data are distributed normally\n\nmodel &lt;- lm(y ~ x, data = data_sample)\n\nplot(model, which = 2) # which = 2 for Q-Q plot\n\n\n\n\n\n\nWe assumed that \\(\\epsilon_i\\) are iid with mean 0 and variance \\(\\sigma^2\\)\nEstimated using RSE (residual standard error)\nHow to check this assumption?\n\nPlot residuals vs fitted values\n\nNeed to see evenly spread points around 0\nDo not want to see funnel shape\n\n\nheteroscedasticity: variance of residuals is not constant\nhomoscedasticity: variance of residuals is constant\n\nplot(model, which = 1) # which = 1 for residuals vs fitted values\n\n\n\n\n\nSome of the explanatory variables are linearly related\nWhen this happens, the LS are very “unstable”\n\ncontribution of each variable to the model is hard to assess\ncan inflate the standard errors of the coefficients\n\n\n\n\n\nCorrelation matrix\nVariance Inflation Factor (VIF)\n\nonly works for linear models with &gt; 1 explanatory variables\nVIF of \\(x_j\\) is \\(VIF_j = \\frac{1}{1-R_j^2}\\)\n\\(R_j^2\\) is the \\(R^2\\) from the regression of \\(x_j\\) on all other explanatory variables\n\nHow much of the observed variation of \\(X_j\\) is explained by the other explanatory variables\n\nIf \\(VIF_j &gt;&gt; 1\\), then multicollinearity is a problem, remove \\(x_j\\) from the model\n\nRidge deals with multicollinearity\n\nshrinks the coefficients of correlated variables towards each other\n\n\n\n\n\n\n\nCofounding factors are variables, not in model, that are related to both the response and the explanatory variables\nConfounding refers to a situation in which a variable, not included in the model, is related with both the response and at least one covariate in the model.\ne.g. Job hunting, create a LR model to predict salaries based on programming languages\n\nCofounding factor: years of experience, education level, etc.\n\n\n\n\n\n\n\n\n\nWant to use LR to predict a binary outcome (e.g. whether a person will buy a product or not)\nLinear model will predict the “probability” of buying a product\n\nBUT it will predict values outside of the [0, 1] range\n\nIt also violates:\n\nthe assumption of normality (residuals are not normally distributed)\nequal variance (qq plot shows that residuals are not equal).\n\n\n\n\n\nUsing MLE and assume Bernoulli distribution for \\(Y_i\\), we can predict probabolities between 0 and 1.\n\n\\[ Y \\sim Bernoulli(p) \\] \\[ Bernoulli(p) = p^y (1-p)^{1-y} \\]\n\\[ E(Y) = p \\]\n\n\n\n\n\n\n\n\nLogit function is the inverse of the logistic function\nProperties:\n\nRange: \\((-\\infty, \\infty)\\)\nMonotonic: always increasing or always decreasing\nDifferentiable: can be differentiated\n\n\n\\[ logit(p) = log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p \\]\n\\[ p_i = \\frac{e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p}}{1 + e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p}} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p)}} \\]\n\n\n\n\nProbability of success \\[p = \\frac{Odds}{1 + Odds}\\]\nOdds of success: \\[\\frac{p}{1-p}\\]\n\n\n\n\nlogistic_model &lt;- glm(as.factor(y) ~ x1 + x2 + x3,\n    data = data,\n    family = binomial # use binomial distribution\n)\n\n# default is to predict the log odds of success\npredict(logistic_model) # type = \"link\"\n\n# to predict the odds of success\npredict(logistic_model, type = \"response\")\n\n\n\nIntercept: log odds of success when all predictors are 0\nCoefficients: log odds ratio of success for a 1 unit increase in the predictor\n\nTo get the odds ratio, we need to exponentiate the coefficients.\n# returns the exp(log(odds ratio)) = odds ratio\ntidy(logistic_model, exponentiate = TRUE)\nexample:\n\nif \\(\\beta_1 = 0.4\\), then the odds ratio is \\(e^{0.4} = 1.49\\).\nThis means that for a 1 unit increase in \\(x_1\\), it is more likely to be a success by a factor of 1.49.\nCannot say anything about probability of success increasing as \\(x_1\\) increases.\n\n\n\n\n\nWe can determine whether a regressor is statistically associated with the logarithm of the response’s odds through hypothesis testing for \\(\\beta_i\\).\n\ni.e. To determine whether a coefficient is significant\n\nDo the Walad Statistics test \\[ z_i = \\frac{\\hat{\\beta_i}}{SE(\\hat{\\beta_i})} \\]\n\n\\(H_0\\): \\(\\beta_i = 0\\)\n\\(H_a\\): \\(\\beta_i \\neq 0\\)\n\n\ntidy(logistic_model, conf.int = TRUE)\n# then check the p-value to see if the coefficient is significant"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#introduction",
    "href": "block_3/561_regression/561_regression.html#introduction",
    "title": "Regression I",
    "section": "",
    "text": "Basic idea: fit a line to data\nCan help with:\n\nEstimation: how to estimate the true (but unknown) relation between the response and the input variables\nInference: how to use the model to infer information about the unknown relation between variables\nPrediction: how to use the model to predict the value of the response for new observations\n\nBefore starting, normally do some exploratory data analysis (EDA) to get a sense of the data\n\nfind out size\ndistribution of variables\nmissing values/ outliers\nrelationships between variables"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#simple-linear-regression",
    "href": "block_3/561_regression/561_regression.html#simple-linear-regression",
    "title": "Regression I",
    "section": "",
    "text": "Simple linear regression: one response variable and one explanatory variable\n\nLet \\((X_i, Y_i)\\) for \\(i = 1, \\dots, n\\) random sample of size \\(n\\) from a population.\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\]\nWhere:\n\n\\(Y\\) is the response variable\n\\(X\\) is the explanatory/ input variable\n\\(\\epsilon_i \\sim N(0, \\sigma^2)\\) and \\(\\epsilon_1, \\dots, \\epsilon_n\\) are independent.\n\nIt represents other factors not taken into account in the model\n\n\n\nImage from the book: “Beyond Multiple Linear Regression”, from Paul Roback and Julie Legler\nAssumptions:\n\nconditional expectation of \\(Y\\) is linearly related to \\(X\\) \\[E(Y|X) = \\beta_0 + \\beta_1 X\\]\niid\nall random error \\(\\epsilon_i\\) is normally distributed with mean 0 and variance \\(\\sigma^2\\)\n\n\\[E(\\epsilon | X) = 0 \\\\ and \\\\ Var(\\epsilon | X) = \\sigma^2\\]\n\n\n\nLine that minimizes the sum of squared errors (SSE) or distance between the line and the data points\n\n\\[SSE = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 = \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i)^2\\] [[fact check the equation above]]\n\n\n\n# fit a linear model\nlm(Y ~ X, data = data)\n\n# plot linear line using ggplot2\nggplot(data, aes(x = X, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#slr-with-catagorical-covariates",
    "href": "block_3/561_regression/561_regression.html#slr-with-catagorical-covariates",
    "title": "Regression I",
    "section": "",
    "text": "Turn categorical into dummy variables (numerical)\n\ne.g. X = Fireplace in a house, Y = Price of the house\nMake dummy variable: \\(X_2 = 1\\) if house has fireplace, \\(X_2 = 0\\) if house doesn’t have fireplace\n\\(E(Y|X_2) = \\beta_0+ \\beta_2 X_2\\)\n\nSome cool thing:\n\n\\(\\beta_2 = E(Y|X_2 = 1) - E(Y|X_2 = 0) = \\mu_1 - \\mu_0\\)\n\nSAME as null hypothesis for t-test\n\n\\(\\beta_0 = E(Y|X_2 = 0)\\)"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#estimation-with-slr",
    "href": "block_3/561_regression/561_regression.html#estimation-with-slr",
    "title": "Regression I",
    "section": "",
    "text": "\\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\]\n\nFor Linear Regression, we are estimating:\n\n\\(\\beta_0\\) and \\(\\beta_1\\)"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#uncertainty-in-estimation",
    "href": "block_3/561_regression/561_regression.html#uncertainty-in-estimation",
    "title": "Regression I",
    "section": "",
    "text": "lm &lt;- lm(Y ~ X, data = data) |&gt;\n    tidy() |&gt;\n    mutate_if(is.numeric, round, 3) # round to 3 decimal places\n\n# Returns:\n# term  estimate    std.error   statistic   p.value\n# &lt;chr&gt; &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;             &lt;dbl&gt;\n\n\n\nIf take LR for new samples, we will get different estimates\n\nStandard Error is the standard deviation of the sampling distribution of the estimate\n\nMethods to measure uncertainty in estimation:\n\ncompute SE after taking multiple samples\nrarely take multiple samples, just take it theoretically (what lm does), see third col above\ncan also bootstrap from sample to get distribution of estimates -&gt; get standard error\n\n\n\n\n\nTake multiple samples and compute the variance of the estimates\n\n\n\n\n\nAssumption: conditional distribution of \\(Y_i\\) given \\(X_i\\) is normal with mean \\(\\beta_0 + \\beta_1 X_i\\) and variance \\(\\sigma^2\\), the \\(\\epsilon_i\\) term.\nUse t test with \\(df=n-p\\)\n\nn = sample size\np = number of parameters in the model\n\nUsed by lm to get standard error\nIf conditional dist is normal =&gt; error term is normal =&gt; sampling distribution of \\(\\hat{\\beta_1}\\) is normal\n\nOr if the sample size is large enough, sampling distribution of \\(\\hat{\\beta_1}\\) is approximately normal (CLT)\n\n\n\n\n\n\nBootstrap Normal Theory\n\\[\\beta \\pm z_{\\alpha/2} SE(\\hat{\\beta})\\]\n\nSE = stdev of bootstrap distribution\n\nBootstrap Percentile Method\n\nbasically take the 2.5th and 97.5th percentile of the bootstrap distribution (95% confidence interval)\n\n\\[\\hat{b}_{\\alpha/2} , \\hat{b}_{1-\\alpha/2}\\]\n\n\n\n\n\ntidy(lm_s, conf.int = TRUE)\n\nIf 0 is in the confidence interval, then we can’t reject the null hypothesis (\\(\\beta_1 = 0\\))"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#hypothesis-tests-for-slr",
    "href": "block_3/561_regression/561_regression.html#hypothesis-tests-for-slr",
    "title": "Regression I",
    "section": "",
    "text": "Can do:\n\n\\(H_0: \\beta_0 = 0\\) vs \\(H_a: \\beta_0 \\neq 0\\)\n\nUsually not interesting\n\n\\(H_0: \\beta_1 = 0\\) vs \\(H_a: \\beta_1 \\neq 0\\)\n\nAnswers: is there a linear relationship between \\(X\\) and \\(Y\\)?\n\n\nRecall, test statistic under null hypothesis:\n\\[ t = \\frac{\\hat{\\beta_1}}{SE(\\hat{\\beta_1})} \\]\nWhich has a t-distribution with \\(n-p\\) degrees of freedom"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#multiple-linear-regression",
    "href": "block_3/561_regression/561_regression.html#multiple-linear-regression",
    "title": "Regression I",
    "section": "",
    "text": "\\[Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_p X_{ip} + \\epsilon_i\\]\ne.g. \\[ Y_i = \\beta_0 + \\beta_1 height_i + \\beta_2 weight_i + \\epsilon_i \\]\n\nLinear regression models the conditional expectation as linear combination of the predictors\n\n\\[ E(Y|X_1, X_2, \\dots, X_p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p \\]\n\n\n# visualize the relationship between variables\n# using ggpairs from GGally package\nggpairs(data)\n\n# fit the model\nlm_s &lt;- lm(Y ~ X1 + X2 + X3, data = data)\n\n\n\n\nFor 2 categories (e.g. Fireplace exits or not) we only need 1 dummy variable\n\ncorresponding to 0 or 1\n\nFor 3 categories (e.g. old, modern, new) we need 2 dummy variables\n\nM corresponds to 1 if house is modern, 0 otherwise\nN corresponds to 1 if house is new, 0 otherwise\n\nFor \\(k\\) categories, we need \\(k-1\\) dummy variables\n\nlm(Y ~ cat_var, data = data) # cat_var is a categorical variable"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#interaction-terms",
    "href": "block_3/561_regression/561_regression.html#interaction-terms",
    "title": "Regression I",
    "section": "",
    "text": "Normally when you have categorical and continuous variables, the linear model will:\n\nfit different intercepts for each category\nfit the same slope for each category (simpler)*\n\nInteraction terms allow us to fit different slopes for each category\n\n*Occam’s razor: if can explain with simpler model, do so\ne.g.\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 Z_i + (\\beta_3 X_i Z_i) + \\epsilon_i\\]\n\n\\(X_i\\) is a continuous variable\n\\(Z_i\\) is a categorical variable\n\\(\\beta_3\\) is the interaction term\n\nfit &lt;- lm(score ~ age*sex, data = dat)\ntidy(fit) |&gt; mutate_if(is.numeric, round, 3)\n# Returns columns:(intercept), age, sexmale, age:sexmale\n\nIntercept: average teaching score of female instructors of zero age\nage: slope for female instructors. Increase in age by 1 year will increase teaching score by age for female instructors\nsexmale: holding age constant, the difference in teaching score between male and female\nage:sexmale is the offset slope. Increase in age by one year is associated with the increase in teaching score of age + age:sexmale for male instructors."
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#goodness-of-fit",
    "href": "block_3/561_regression/561_regression.html#goodness-of-fit",
    "title": "Regression I",
    "section": "",
    "text": "Is this better than nothing/ null model (only intercept)?\n\nIn other words is \\(E(Y|X)\\) better than \\(E(Y)\\)?\n\n\n\n\n\nTotal sum of squares (TSS):\n\n\\(TSS = \\sum (y_i - \\bar{y})^2\\)\nSum of squares from the null (intercept only) model\n\nExplained sum of squares (ESS):\n\n\\(ESS = \\sum (\\hat{y}_i - \\bar{y})^2\\)\nmeasures how mich it is explained by the model\n\nbetter model = larger ESS\n\n\nResidual sum of squares (RSS):\n\n\\(RSS = \\sum (y_i - \\hat{y}_i)^2\\)\nRSS == SSE (sum of squared errors) == n * MSE (mean squared error)\nEstimated parameters minimizes RSS (objective function)\n\nWhere:\n\n\\(y_i\\) observed ith value of y\n\\(\\hat{y}_i\\) predicted value of y with a model (of sample)\n\\(\\bar{y}_i\\) mean of y\n\n\nIf using linear regression using least squares:\n\\[ TSS = ESS + RSS \\]\nRecall: Residuals is the difference between the observed value and the predicted value. \\(r_i = y_i - \\hat{y}_i\\)\n\n\n\n\\[ R^2 = \\frac{ESS}{TSS} = 1 - \\frac{RSS}{TSS} \\]\n\n\\(R^2\\) is the proportion of variance explained by the model\n\nTells us how well the regression model explains the variation in the data\n\n\\(R^2\\) is between -inf and 1 (1 being perfect fit)\n\ngenerally between 0 and 1 since expect TSS &gt; RSS\n\nONLY IF the model has an intercept and is estimated by LS\n\nonly less than 0 if model is worse than null model\n\n\nglance(fit)\n\n\n\n\\(R^2\\) increases with number of predictors\n\n\\(R^2\\) will never decrease when adding predictors\n\nComputed based on “in-sample” predictions\n\nWe do not know how well the model will perform on new data\nBasically only on train data, dunno performance on test data\n\nIs it useful?\n\nYes, to compare size of residuals of fitted model to null model\nCannot be used to test any hypothesis since its distribution is unknown\n\n\n\n\n\n\nIs no longer coefficient of determination\nMeasures correlation between the true and predicted values\n\\(R^2 = Cor(Y, \\hat{Y})^2\\) if \\(\\hat{Y}\\) is a prediction obtained from a LR with an intercept estimated by LS.\n\nThis is ued to assess the prediction performance of a model\n\n\n\n\n\n\n\nCompare two models\n\n\\(reduced\\): intercept + some predictors\n\\(full\\): intercept + predictors\n\nIs the full model better than the reduced model? Simultaneously testing if many parameters are 0\nF-test is a global test\n\ntests if all parameters are 0\nif F-test is significant, then at least one parameter is not 0\n\n\n\\[F \\propto \\frac{(RSS_{reduced} - RSS_{full}) / k}{RSS_{full} / (n - p)}\\]\n\nparams:\n\nk = number of parameters tested (difference between models)\np = number of predictors in the full model (s + 1)\n\n\nlm_red &lt;- lm(score~1, data=dat) # intercept only\nlm_full &lt;- lm(score~ age + sex, data=dat)\n\nanova(lm_red,lm_full)\n# F-test tests H0: coef_age = coef_sex = 0\n\nglance(lm_full)\n# also includes F-statistic in \"statistic\" column + p-value\n# compares it to null model (intercept only)\n\nF-test can test multiple parameters at once\n\ne.g. \\(\\beta_1 = \\beta_2 = \\beta_3 = 0\\)\n\nFor one parameter, F-test is equivalent to t-test\n\ne.g. \\(\\beta_1 = 0\\)\nFor one parameter, \\(t^2 = F\\)\n\nwhere t is the t-statistic and F is the F-statistic\n\n\n\n\n\n\nif F-statistic is large (F &gt; 1), then the full model is better than the reduced model\nif F-statistic is small (F &lt; 1), then the full model is not better than the reduced model\n\n\n\n\n\nBoth depend on RSS and TSS, there is a formula to convert between them\nF-test has a known F-distribution (under certain assumptions) so we can use it to make probabilistic statements"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#model-evaluation",
    "href": "block_3/561_regression/561_regression.html#model-evaluation",
    "title": "Regression I",
    "section": "",
    "text": "What is your goal?\n\nInference: understand the relationship between the response and the predictors\nPrediction: predict the response for future observations\n\n\n\n\n\n\nMean squared error (MSE): average squared error on new data\n\n\\(MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\)\nTrain MSE: MSE on training data\nTest MSE: MSE on test data\n\nResidual Sum of Squares (RSS): n * MSE\n\n\\(\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\)\nsmall is good\nresiduals are measured in training data\n\nResidual Standard Error (RSE): estimates the standard deviation of the residuals\n\n\\(\\sqrt{\\frac{1}{n-p} RSS}\\)\n\np = number of parameters in the model\n\nBased on training data to evaluate fit of the model (small is good)\n\nCoefficient of Determination (\\(R^2\\)):\n\n\\(R^2 = \\frac{ESS}{TSS} = 1 - \\frac{RSS}{TSS}\\)\n*See Interaction Terms &gt; Coefficient of Determination for more info"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#variable-selection",
    "href": "block_3/561_regression/561_regression.html#variable-selection",
    "title": "Regression I",
    "section": "",
    "text": "Use F-test to test to compare and test nested models\n\nanova(lm_red,lm_full)\n\nt-test to test contribution of individual predictors\n\ntidy(lm_full, conf.int = TRUE)\n\nF-test is equivalent to t-test when there is only one predictor\n\\(R^2\\) is the proportion of variance explained by the model\n\n\\(R^2\\) increases with number of predictors (RSS decreases)\n\nAdjusted \\(R^2\\) penalizes for number of predictors\n\nAdjusted \\(R^2\\) increases only if the new predictor improves the model more than expected by chance\nAdjusted \\(R^2\\) decreases if the new predictor does not improve the model more than expected by chance \\[R^2_{adj} = 1 - \\frac{RSS/(n-p)}{TSS/(n-1)}\\]\nCam be used to compare models with different number of parameters/ predictors\n\n\nNested models: one model is a special case of the other\n\ne.g. lm(y ~ x1 + x2) is nested in lm(y ~ x1 + x2 + x3)\n\n\n\n\n\n\n\nAverage squared error on new data\nSelection criteria:\n\nThe Mallows Cp\nAkaike information criterion (AIC)\nBayesian information criterion (BIC)\n\nThese add different penalties to the training RSS to adjust for the fact that the training error tends to underestimate the test error\n\n\n\n\n\nForward selection\n\nStart with null model\nAdd predictors one at a time (select the best feats at that number of feats)\nStop when no more predictors improve the model\nleaps::regsubsets\n\nBackward selection: start with full model and remove predictors one at a time\nHybrid: after adding a predictor, check if any predictors can be removed\n\ndat_forward &lt;- regsubsets(\n  assess_val ~ age + FIREPLACE + GARAGE + BASEMENT,\n  data = dat_s,\n  nvmax = 5, # max number of feats\n  method = \"forward\", # backward, exhaustive, etc.\n)\n\n# view\nfws_summary &lt;- summary(dat_forward)\n# will have rsq, adj r2, rss, cp, bic, etc.\n\n\n\n\nOffers an alternative to the above methods\ne.g. Ridge, Lasso, Elastic Net"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#prediction-intervals-vs-confidence-intervals-for-prediction",
    "href": "block_3/561_regression/561_regression.html#prediction-intervals-vs-confidence-intervals-for-prediction",
    "title": "Regression I",
    "section": "",
    "text": "Conditional expectation of predictor \\(Y\\) given \\(X\\): \\[E(Y|X)\\]\nAssumes linear form of ^\n\n\n\nTerms:\n\n\\(\\hat{y}_i\\) is the predicted value of the LR model (sample)\n\\(E(Y_i|X_i)\\) is the conditional expectation of \\(Y\\) given \\(X_i\\)\n\nor the average value of \\(Y_i\\) for a given \\(X_i\\)\nor LR model of population\n\n\nWith our model \\(\\hat{y}\\) we can predict:\n\n\\(E(Y|X_0)\\)\n\\(Y_i\\) (Actual value of \\(Y\\) for \\(X_i\\))\n\nA lot harder to predict (individual prediction)\n\n\n2 types of intervals:\n\nConfidence intervals for prediction (CIP)\nPrediction intervals (PI)\n\n\n\n\n\n\n\n\n\nCIP\nPI\n\n\n\n\nUncertainty of the mean of the prediction\nUncertainty of the individual prediction\n\n\nUncertainty of \\(E(Y_i\\|X_i)\\)\nUncertainty of \\(Y_i\\)\n\n\nError from estimation of \\(\\beta\\)\nError from estimation of \\(\\beta\\) and \\(\\epsilon\\)\n\n\nSmaller than PI\nWider than CIP\n\n\nCentered around \\(\\hat{y}_i\\)\nCentered around \\(\\hat{y}_i\\)\n\n\n\n\n\n\nWe are interested in the mean of the prediction, not the individual prediction \\[E(Y_i|X_i)\\]\nUncertainty only comes from the estimation (1 source of uncertainty)\n\nThe model: \\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\]\napproximates, with uncertainty, the conditional expectation: \\[E(Y_i|X_i) = \\beta_0 + \\beta_1 x_i\\]\ni.e. Estimated coeffs (\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)) are estimates of true population coeffs (\\(\\beta_0\\) and \\(\\beta_1\\))\n\n95% CIP is a range where we have 95% confidence that it contains the average value of \\(Y_i\\) for a given \\(X_i\\)\n\nsmaller CIP means smaller confidence interval =&gt; less confidence it contains the true values\n\n\n\n\nmodel_sample &lt;- lm(y ~ x, data = data_sample)\n\ndata_sample |&gt;\n  select(x, y) |&gt;\n  cbind(\n    predict(\n      model_sample,\n      interval = \"confidence\",\n      level = 0.95, # 95% CIP is default\n      se.fit=TRUE # standard error of the fit,\n    )$fit\n  )\n# or\npredict(\n  model_sample,\n  interval = \"confidence\",\n  newdata = tibble(x1 = 1, x2 = 2),\n)\n\n\n\n\n\nWe are interested in the individual prediction: \\(Y_i\\)\nWider than CIP\n\nThe model: \\[\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i + \\epsilon_i\\] approximates, with uncertainty, an actual observation \\(Y_i\\).\n\nUncertainty comes from 2 sources:\n\nEstimation of the coeffs (\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\))\nVariability of the error term \\(\\epsilon_i\\)\n\nActual observation \\(Y_i\\) differs from the average (population) by \\(\\epsilon_i\\)\n\n\n95% PI is a range where we have 95% confidence that it contains the actual value of \\(Y_i\\) for a given \\(X_i\\)\n\n\n\ndata_sample |&gt;\n  select(x, y) |&gt;\n  cbind(\n    predict(\n      model_sample,\n      interval = \"prediction\", # change from \"confidence\"\n      level = 0.95, # 95% CIP is default\n      se.fit=TRUE # standard error of the fit\n    )$fit\n  )"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#potential-problems-in-lr",
    "href": "block_3/561_regression/561_regression.html#potential-problems-in-lr",
    "title": "Regression I",
    "section": "",
    "text": "Linearity in regression != linearity in math\nLinearity in regression means that the model is linear in the parameters\nExamples of linear regression models:\n\n$ y = _0 + _1 x_1 + _2 x_2 + _3 x_3 $\n$ y = _0 + _1 x_1 + _2 x_2 +_3 x_1^2 + _4 x_1 x_2$\n$ y = _0 + _1 x_1 + _2 e^{x_1} + _3 (x_2)$\n\nNon-linear examples:\n\n$ y = _0 + _1 x_1 + _2 x_1^{_3} $\n\n\n\n\n\n\nLS estimation do not depend on any normality assumption\nBut if small sample size, normality assumption is needed for inference (since CLT does not apply)\nOLS (Ordinary Least Squares) do not require assumptions of normality. BUT we need to assume it in order to do inference (e.g. CIP, PI, hypothesis testing).\n\n\n\n\nQ-Q plot is a graphical method for assessing whether or not a data set follows a given distribution such as normal distribution\nPoints in the Q-Q plot follow a straight line if the data are distributed normally\n\nmodel &lt;- lm(y ~ x, data = data_sample)\n\nplot(model, which = 2) # which = 2 for Q-Q plot\n\n\n\n\n\n\nWe assumed that \\(\\epsilon_i\\) are iid with mean 0 and variance \\(\\sigma^2\\)\nEstimated using RSE (residual standard error)\nHow to check this assumption?\n\nPlot residuals vs fitted values\n\nNeed to see evenly spread points around 0\nDo not want to see funnel shape\n\n\nheteroscedasticity: variance of residuals is not constant\nhomoscedasticity: variance of residuals is constant\n\nplot(model, which = 1) # which = 1 for residuals vs fitted values\n\n\n\n\n\nSome of the explanatory variables are linearly related\nWhen this happens, the LS are very “unstable”\n\ncontribution of each variable to the model is hard to assess\ncan inflate the standard errors of the coefficients\n\n\n\n\n\nCorrelation matrix\nVariance Inflation Factor (VIF)\n\nonly works for linear models with &gt; 1 explanatory variables\nVIF of \\(x_j\\) is \\(VIF_j = \\frac{1}{1-R_j^2}\\)\n\\(R_j^2\\) is the \\(R^2\\) from the regression of \\(x_j\\) on all other explanatory variables\n\nHow much of the observed variation of \\(X_j\\) is explained by the other explanatory variables\n\nIf \\(VIF_j &gt;&gt; 1\\), then multicollinearity is a problem, remove \\(x_j\\) from the model\n\nRidge deals with multicollinearity\n\nshrinks the coefficients of correlated variables towards each other\n\n\n\n\n\n\n\nCofounding factors are variables, not in model, that are related to both the response and the explanatory variables\nConfounding refers to a situation in which a variable, not included in the model, is related with both the response and at least one covariate in the model.\ne.g. Job hunting, create a LR model to predict salaries based on programming languages\n\nCofounding factor: years of experience, education level, etc."
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#maximum-likelihood-estimation-mle",
    "href": "block_3/561_regression/561_regression.html#maximum-likelihood-estimation-mle",
    "title": "Regression I",
    "section": "",
    "text": "Want to use LR to predict a binary outcome (e.g. whether a person will buy a product or not)\nLinear model will predict the “probability” of buying a product\n\nBUT it will predict values outside of the [0, 1] range\n\nIt also violates:\n\nthe assumption of normality (residuals are not normally distributed)\nequal variance (qq plot shows that residuals are not equal).\n\n\n\n\n\nUsing MLE and assume Bernoulli distribution for \\(Y_i\\), we can predict probabolities between 0 and 1.\n\n\\[ Y \\sim Bernoulli(p) \\] \\[ Bernoulli(p) = p^y (1-p)^{1-y} \\]\n\\[ E(Y) = p \\]"
  },
  {
    "objectID": "block_3/561_regression/561_regression.html#logistic-regression",
    "href": "block_3/561_regression/561_regression.html#logistic-regression",
    "title": "Regression I",
    "section": "",
    "text": "Logit function is the inverse of the logistic function\nProperties:\n\nRange: \\((-\\infty, \\infty)\\)\nMonotonic: always increasing or always decreasing\nDifferentiable: can be differentiated\n\n\n\\[ logit(p) = log(\\frac{p}{1-p}) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p \\]\n\\[ p_i = \\frac{e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p}}{1 + e^{\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p}} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p)}} \\]\n\n\n\n\nProbability of success \\[p = \\frac{Odds}{1 + Odds}\\]\nOdds of success: \\[\\frac{p}{1-p}\\]\n\n\n\n\nlogistic_model &lt;- glm(as.factor(y) ~ x1 + x2 + x3,\n    data = data,\n    family = binomial # use binomial distribution\n)\n\n# default is to predict the log odds of success\npredict(logistic_model) # type = \"link\"\n\n# to predict the odds of success\npredict(logistic_model, type = \"response\")\n\n\n\nIntercept: log odds of success when all predictors are 0\nCoefficients: log odds ratio of success for a 1 unit increase in the predictor\n\nTo get the odds ratio, we need to exponentiate the coefficients.\n# returns the exp(log(odds ratio)) = odds ratio\ntidy(logistic_model, exponentiate = TRUE)\nexample:\n\nif \\(\\beta_1 = 0.4\\), then the odds ratio is \\(e^{0.4} = 1.49\\).\nThis means that for a 1 unit increase in \\(x_1\\), it is more likely to be a success by a factor of 1.49.\nCannot say anything about probability of success increasing as \\(x_1\\) increases.\n\n\n\n\n\nWe can determine whether a regressor is statistically associated with the logarithm of the response’s odds through hypothesis testing for \\(\\beta_i\\).\n\ni.e. To determine whether a coefficient is significant\n\nDo the Walad Statistics test \\[ z_i = \\frac{\\hat{\\beta_i}}{SE(\\hat{\\beta_i})} \\]\n\n\\(H_0\\): \\(\\beta_i = 0\\)\n\\(H_a\\): \\(\\beta_i \\neq 0\\)\n\n\ntidy(logistic_model, conf.int = TRUE)\n# then check the p-value to see if the coefficient is significant"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html",
    "href": "block_3/573_model_sel/573_model_sel.html",
    "title": "Model Selection",
    "section": "",
    "text": "Need to evaluate the performance of a classifier not just by accuracy\n\n\n\n\nLinear regression with sigmoid function\nHyperparameter: C (default 1.0)\n\nLower C =&gt; more regularization =&gt; smaller coefficients =&gt; underfitting\n\n\nfrom sklearn.linear_model import LogisticRegression\n\npipe_lr = make_pipeline(preprocessor, LogisticRegression())\npd.DataFrame(cross_validate(pipe_lr, X_train, y_train, cv=10, return_train_score=True))\n\n\n\n\n\nChoose the positive case based on which class is more important to us or which class is rare\n\nspotting a class (e.g. fraud/ spam/ disease detection)\n\nCan do this in python:\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_predict\n\n# gets values without plot\nconfusion_matrix(y_train, cross_val_predict(pipe_lr, X_train, y_train, cv=5))\n\n# get values with plot\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nConfusionMatrixDisplay.from_estimator(pipe_lr, X_train, y_train, display_labels=['Not Fraud', 'Fraud']);\nOutput of confusion matrix by default:\n             | Predicted F [0]| Predicted T [1]\nActual F [0] | TN             | FP\nActual T [1] | FN             | TP\n\n\n\nPrecision \\[precision = \\frac{TP}{PP} =  \\frac{TP}{TP + FP}\\]\nRecall or True positive rate (TPR) \\[recall = \\frac{TP}{P} = \\frac{TP}{TP + FN}\\]\nf1-score \\[f1 = 2 \\cdot \\frac{precision * recall}{precision + recall} = 2 \\frac{1}{\\frac{1}{precision} + \\frac{1}{recall}}\\]\n\n\n\n\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_train, cross_val_predict(pipe_lr, X_train, y_train, cv=5)))\n\n# output\n#               precision    recall  f1-score   support\n\n#        Fraud       0.89      0.63      0.74       102\n#    Non fraud       1.00      1.00      1.00     59708\n\n#     accuracy                           1.00     59810\n#    macro avg       0.94      0.81      0.87     59810\n# weighted avg       1.00      1.00      1.00     59810\n\n\n\n\n\ntradeoff between precision and recall\nsetting a threshold is called setting the operating point\nChanging the threshold of the classifier (default of logistic regression is 0.5)\n\nremember predict_proba returns the probability of the positive class, if higher than threshold, then positive class\nas threshold increases, higher bar =&gt; precision increases and recall decreases (less FP but less TP)\nas threshold decreases, lower bar =&gt; precision decreases and recall increases (more TP but more FP)\n\n\n\n\nfrom sklearn.metrics import PrecisionRecallDisplay\n\nPrecisionRecallDisplay.from_estimator(\n    pipe_lr,\n    X_valid,\n    y_valid,\n    pos_label='Fraud',\n    name='LR',  # For Logistic Regression\n);\n\n\nWant the curve to be as close to the top right corner as possible (100% precision and 100% recall)\n\nThreshold of 0 in bottom left =&gt; All points are predicted to be positive =&gt; 100% recall and 0% precision\nThreshold of 1 in top right =&gt; All points are predicted to be negative =&gt; 0% recall and 100% precision\n\n\nOr if want just the values:\nfrom sklearn.metrics import precision_recall_curve\n\npd.DataFrame(\n    precision_recall_curve(\n        y_valid,\n        pipe_lr.predict_proba(X_valid)[:, fraud_column],\n        pos_label='Fraud',\n    ),\n    index=['precision', 'recall', 'threshold']\n).T\n\n\n\n\nArea under the precision recall curve\nHigher is better (0 is worst, 1 is best)\nAlso supports multi-class using one-vs-rest approach + averaging\nIMPORTANT:\n\nF1 score is for a given threshold and measures the quality of predict.\nAP score is a summary across thresholds and measures the quality of predict_proba.\n\n\nfrom sklearn.metrics import average_precision_score\n\nap_lr = average_precision_score(y_valid, pipe_lr.predict_proba(X_valid)[:, fraud_column], pos_label='Fraud')\n\n\n\nAP score\nf1 score\n\n\n\n\nmeasures quality of predict_proba\nmeasures quality of predict\n\n\n\n\nThese two metrics do not always agree*\n\n\n\n\n\n\nReceiver Operating Characteristic: plots the true positive rate (recall) against the false positive rate (1 - specificity)\n\nInstead of plotting precision against recall, plots recall (TPR) against FPR\n\n\n\\[FPR = \\frac{FP}{N} = \\frac{FP}{FP + TN}\\]\nfrom sklearn.metrics import RocCurveDisplay\n\nRocCurveDisplay.from_estimator(\n    pipe_lr,\n    X_valid,\n    y_valid,\n    pos_label='Fraud'\n);\n\n\nIdeal is top left corner (100% TPR and 0% FPR)\nLower left (Threshold of 1) =&gt; 0% TPR and 0% FPR\nUpper right (Threshold of 0) =&gt; 100% TPR and 100% FPR\n\n\n\n\nArea under the ROC curve\nAUC is the probability that a randomly chosen positive point has a higher score than a randomly chosen negative point\n\nAUC of 1.0: all positive points have a higher score than all negative points.\nAUC of 0.5: means random chance.\n\n\n\n\n\n\n\nIf we balance the classes:\n\nPrecision goes down, FP goes up\nRecall goes up, FN goes down\nF1 score goes down\nAP score goes down\nAUC score goes down\ngenerally reduce accuracy\n\n\n\n\n\nClass weights: penalize the minority class more\n\nLogisticRegression(\n  max_iter=500,\n  # give more importance to \"Fraud\" class (10x)\n  class_weight={'Non fraud': 1, 'Fraud': 10}\n\n  # class_weight='balanced' =&gt; automatically give more importance to minority class\n  )\n\n\n\n\n\n\n\n\n\nfrom sklearn.dummy import DummyRegressor\n\ndummy = DummyRegressor(strategy='mean') # default\npd.DataFrame(cross_validate(dummy, X_train, y_train, cv=10, return_train_score=True))\n\n\n\n\nLinear regression with L2 regularization\nHyperparameter: alpha (default 1.0)\n\nHigher alpha =&gt; more regularization =&gt; smaller coefficients =&gt; underfitting\nmore regularization : smaller coefficients =&gt; less sensitive to changes in the input features =&gt; less likely to overfit\n\n\nfrom sklearn.linear_model import Ridge\n\nlr_pipe = make_pipeline(preprocessor, Ridge())\npd.DataFrame(cross_validate(lr_pipe, X_train, y_train, cv=10, return_train_score=True))\n\n# fit model\nlr_pipe.fit(X_train, y_train)\n\n# get coefficients\ndf = pd.DataFrame(\n    data={\"coefficients\": lr_pipe.named_steps[\"ridge\"].coef_}, index=feature_names)\n\ndf.sort_values(\"coefficients\", ascending=False)\n\n\n\n\nRidge with cross-validation to find the best alpha\n\nfrom sklearn.linear_model import RidgeCV\n\nalphas = 10.0 ** np.arange(-6, 6, 1)\n\nridgecv_pipe = make_pipeline(preprocessor,\n  RidgeCV(alphas=alphas, cv=10))\nridgecv_pipe.fit(X_train, y_train);\n\n# best alpha\nbest_alpha = ridgecv_pipe.named_steps[\"ridgecv\"].alpha_\n\n\n\n\n\nCannot use equality since we are predicting a continuous variable (not classification)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetric\nDescription\nFormula\nMin Value\nMax Value\nCode Example (sklearn)\n\n\n\n\nMSE (Mean Squared Error)\nMeasures the average of the squares of the errors, i.e., the average squared difference between the estimated values and the actual value.\n\\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(Y_i - \\hat{Y}_i)^2\\]\n0 (perfect)\n∞\nfrom sklearn.metrics import mean_squared_error  mse = mean_squared_error(y_true, y_pred)\n\n\nRMSE (Root Mean Squared Error)\nSquare root of the MSE. It measures the standard deviation of the prediction errors or residuals.\n\\[RMSE = \\sqrt{MSE}\\]\n0 (perfect)\n∞\nrmse = mean_squared_error(y_true, y_pred, squared=False)\n\n\nMAE (Mean Absolute Error)\nMeasures the average of the absolute errors.\n\\[MAE = \\frac{1}{n}\\sum_{i=1}^{n}\\left\\| Y_i - \\hat{Y}_i \\right\\| \\]\n0 (perfect)\n∞\nfrom sklearn.metrics import mean_absolute_error  mae = mean_absolute_error(y_true, y_pred)\n\n\nMAPE (Mean Absolute Percentage Error)\nMeasures the average of the absolute percentage errors.\n\\[MAPE = \\frac{1}{n}\\sum_{i=1}^{n}\\left\\| \\frac{Y_i - \\hat{Y}_i}{Y_i} \\right\\| \\times 100\\% \\]\n0 % (perfect)\n∞ %\nfrom sklearn.metrics import mean_absolute_percentage_error  mape = mean_absolute_percentage_error(y_true, y_pred)\n\n\nR² (Coefficient of Determination)\nMeasures how well future samples are likely to be predicted by the model.\n\\[R^2 = 1 - \\frac{\\sum_{i=1}^{n}(Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^{n}(Y_i - \\bar{Y})^2}\\]\n-ve [if worst than mean]\n1 (perfect)\nfrom sklearn.metrics import r2_score  r2 = r2_score(y_true, y_pred)\n\n\n\n\nMAE is less sensitive to outliers than MSE\n\n\n\n\n\nCan plot the actual vs predicted values to see the error (and plot line of gradient 1 = perfect prediction)\n\nfrom sklearn.metrics import PredictionErrorDisplay\n\nPredictionErrorDisplay.from_estimator(\n    ridge_tuned,\n    X_train,\n    y_train,\n    kind='actual_vs_predicted',\n    subsample=None  # show all predictions\n)\n\n\nCan plot the residuals (error) vs predicted values to see if there is a pattern (e.g. heteroscedasticity)\n\nfrom sklearn.metrics import PredictionErrorDisplay\n\nPredictionErrorDisplay.from_estimator(\n    ridge_tuned,\n    X_train,\n    y_train,\n    subsample=None  # show all predictions\n)\n\n\n\n\n\n\nNeed to have good data to build a good model.\nBad quality data:\n\nMissing values\nFew observations overall or in a specific group\nBiased sample (not representative of the population)\nNon-independent observations\nInaccurate measurements\nFabricated data\nOut of bounds values\nObsure column names\nTypos (spelling differences in categorical variables)\nUsing multiple values to represent same thing (e.g. NA, NONE, NULL, NaN)\nIncorrect data types\n\n\n\nBetter features help more than a better model.\n\nGood features would ideally:\n\ncapture the most important information\nallow learning with few examples\ngeneralize to new scenarios\n\nTrade-off for simple and expressive features:\n\nsimple features: overfitting risk is low but low score\nexpressive features: overfitting risk is high but high score\n\n\n\n\n\n\n\n\n\n\n\n\nPreprocessing\nEngineering\n\n\n\n\nstep of cleaning and preparing the data for analysis\ncreating new features from existing data\n\n\ngenerally HAVE to do, or error\noptional but can improve model performance\n\n\ne.g. scaling, normalization, imputation\ne.g. one-hot encoding, binning, get more data, group-wise normalization, transformation\n\n\n\n\nFeature Selection: removing irrelevant features, normally done after feature engineering\n\n\n\n\n\nCommon guidelines:\n\nIs not unique or random\nHas a variance (not constant)\nAdds unique variance (is not constant transformation)\n\nchanging units (e.g. from meters to feet) is not feature engineering\n\nIs ideally interpretable\n\nExamples:\n\nLooking up and getting additional data\nDiscretization (binning): e.g. age -&gt; age group\nGroup-wise normalization: express feature relative to group mean/median\nTransformation: e.g. height, weight -&gt; BMI\n\n\n\n\nparameter: degree\n\nhigher degree: more expressive features =&gt; more overfitting\n\nTry to capture non-linear relationships\n\nLinear regression can only capture lines, planes, hyperplanes\n\ne.g. add a squared feature (feat1^2)\n\n\n\n\n\nDo polynomial then StandardScaler\n\navoids polynomials being very large\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2)\n# degree: max degree of polynomial features\nX_enc = poly_feats.fit_transform(X_toy)\npd.DataFrame(X_enc, columns=poly_feats.get_feature_names_out()).head()\nDegree example:\n\n\n\nDimension\nDegree\nExamples\n\n\n\n\n1\n2\n1, x, x^2\n\n\n2\n2\n1, x, y, x^2, xy, y^2\n\n\n\n\n\n\n\\[ \\hat{y} = Xw \\]\n\nX: design matrix (n x d)\nw: weights (d x 1)\ny: target (n x 1)\n\nFor polynomial features:\n\\(Z = [1, x, x^2]\\)\n\\[ \\hat{y} = Zw = \\begin{bmatrix} 1 & x_1 & x_1^2 \\\\ 1 & x_2 & x_2^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x_n^2 \\end{bmatrix} \\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\end{bmatrix} = \\begin{bmatrix} w_0 + w_1x_1 + w_2x_1^2 \\\\ w_0 + w_1x_2 + w_2x_2^2 \\\\ \\vdots \\\\ w_0 + w_1x_n + w_2x_n^2 \\end{bmatrix} \\]\n\n\n\n\nPolynomial features can be expensive to compute\nKernel trick: compute dot product between two vectors in a higher dimensional space without computing the transformation explicitly\n\n\n\n\n\n\n\nimport seaborn as sns\n\nsns.heatmap(df.corr(), annot=True)\n\nShows correlation between features\nExtremely simplistic:\n\nOnly looks at linear correlation\nOnly looks at pairwise correlation (isolates features)\n\n\n\n\n\n\nCorrelation among features might make coefficients completely uninterpretable.\nFairly straightforward to interpret coefficients of ordinal features.\nIn categorical features, it’s often helpful to consider one category as a reference point and think about relative importance.\nFor numeric features, relative importance is meaningful after scaling.\nYou have to be careful about the scale of the feature when interpreting the coefficients.\nRemember that explaining the model explaining the data or explaining how the world works.\nThe coefficients tell us only about the model and they might not accurately reflect the data.\n\n\n\n\n\n\nMore features =&gt; more complex model =&gt; overfitting\nFeature selection find features that are important for prediction and remove the rest.\n\nWhy?\n\nInterpretability: easier to explain with fewer features\nComputation: faster to train simpler models (curse of dimensionality)\nData collection: cheaper to collect fewer features\nStorage: less space to store fewer features\nFundamental Tradeoff: reduce overfitting by reducing complexity\n\nHow?\n\ndomain knowledge\nautomatic feature selection\n\nmodel-based selection\nrecursive feature elimination\nforward/backward selection\n\n\n\n\n\nFast and easy\nArbitrary, does not take into account feature combinations\ne.g. correlation threshold, variance threshold\n\n\n\n\n\nUse a supervised model to judge the importance of each feature, and keep only the most important ones.\nCan use different model than the final estimator.\nUse a model that provides some measure of feature importance. (e.g. decision trees, linear models, etc.)\n\nGeneral Steps:\n\nSelect a threshold for which features to keep.\nDiscard features below the threshold.\n\nfrom sklearn.feature_selection import SelectFromModel\n\nselect_lr = SelectFromModel(Ridge(), threshold=\"median\")\n\n# Add feature selection to pipeline (after preprocessing)\npipe = make_pipeline(preprocessor, select_lr, Ridge())\n\n# access selected features\npipe.named_steps[\"selectfrommodel\"]\n# can do: .n_features_in_, .threshold_, .get_support()\n\n\n\n\nRecursively remove features, build a model on those features that remain, and then repeatedly construct a model and remove the weakest feature until a specified number of features remain.\nIteratively eliminates unimportant features.\nComputationally expensive\n\n\n\n\nDecide the number of features to select.\nAssign importances to features, e.g. by fitting a model and looking at coef_ (Linear models) or feature_importances_ (RandomForestClassifier).\nRemove the least important feature.\nRepeat steps 2-3 until only \\(k\\) features are remaining.\n\nfrom sklearn.feature_selection import RFE\n\nrfe = RFE(Ridge(), n_features_to_select=120)\n\npipe_rf_rfe = make_pipeline(preprocessor, rfe, RandomForestRegressor(random_state=42))\n\nresults[\"rfe + rfe\"] = mean_std_cross_val_scores(\n    pipe_rf_rfe, X_train, y_train, return_train_score=True\n)\nNote: for categorical features, RFE might remove some OHE features. This is an unresolved issue.\n\n\n\n\nRFE but using CV to find the optimal number of features to keep.\nvery slow since there is CV within CV.\n\nfrom sklearn.feature_selection import RFECV\n\nrfecv = RFECV(Ridge())\n\npipe_rf_rfecv = make_pipeline(\n    preprocessor, rfecv, RandomForestRegressor(random_state=42)\n)\npipe_rf_rfecv.fit(X_train, y_train);\n\n# optimal number of features\npipe_rfe_ridgecv.named_steps[\"rfecv\"].n_features_\n# which features were selected\npipe_rfe_ridgecv.named_steps[\"rfecv\"].support_\n\n\n\n\n\nNot based on feature importance.\nDefine a scoring function \\(f(S)\\) that measures the quality of a subset of features \\(S\\).\nSearch through all possible subsets of features and pick the subset with the best score.\n\ne.g. for A, B, C search through {}, {A}, {B}, {C}, {A, B}, {A, C}, {B, C}, {A, B, C}…\n# subsets = \\(2^p\\) where \\(p\\) is the number of features\n\nForward selection: start with no features and add one at a time.\n\nadd the feature that results in the best score.\n\nBackward selection: start with all features and remove one at a time.\n\nremove the feature that results in the best score.\n\n\nfrom sklearn.feature_selection import SequentialFeatureSelector\n\npipe_forward = make_pipeline(\n    preprocessor,\n    SequentialFeatureSelector(Ridge(), direction=\"forward\"),\n    RandomForestRegressor(random_state=42),\n)\n\n# results['rf_forward_fs'] = mean_std_cross_val_scores(pipe_forward, X_train, y_train, return_train_score=True)\n\npipe_forward.fit(X_train, y_train)\n\n\n\n\nA feature relevance is only defines in the context of other features.\n\nAdding/removing features can change the importance of other features.\n\nRelevance != causality\nThe methods don’t always work.\n\n\n\n\n\n\nA function that measures how well a model fits the data\nThe goal is to minimize the loss function\n\nsmaller loss \\(\\rightarrow\\) better model\n\nCaptures what is important to minimize\nCommon loss functions:\n\nLeast squares loss\nAbsolute error loss\nHuber loss\n\n\n\n\n\n\n\n\n\n\n\n\nScoring Metric\nLoss Function\n\n\n\n\nReport the results (often several metrics)\nUsed to train/ fit the model\n\n\nFind best model hyperparams\nFind best model parameters\n\n\nCan pick those suitable for the task\nConstrainted (need to be differentiable)\n\n\n\n\n\n\n\n\n\n\\[J(\\omega) = \\sum_{i=1}^{n} (y_i - \\omega^T x_i)^2 \\]\n\nDefine loss as sum of squared errors (difference between prediction and actual value)\nPenalizes heavily for large errors\n\nsensitive to outliers\n\nFor a well-defined OLS, there is a unique solution (not always the case)\n\n\\(\\omega = (X^T X)^{-1} X^T y\\)\n\n\\(X^T X\\) must be invertible\n\n\n\n\n\n\n\\[J(\\omega) = \\sum_{i=1}^{n} |y_i - \\omega^T x_i| \\]\n\nLess sensitive to outliers\nBut minimization is harder\n\nnot differentiable at 0\nno closed form solution\n\nAlternative: Huber Loss\n\nbehaves like OLS for small errors\n\n\n\n\n\n\n\\[ y_i w^T x_i = \\begin{cases} \\text{correct} & y_i w^T x_i &gt; 0 \\\\ \\text{incorrect} & y_i w^T x_i &lt; 0 \\end{cases} \\]\n\n\\(y_i\\): positive\n\ncorrect: \\(w^T x_i &gt; 0 \\rightarrow y_i w^T x_i &gt; 0\\)\nincorrect: \\(w^T x_i &lt; 0 \\rightarrow y_i w^T x_i &lt; 0\\)\n\n\\(y_i\\): negative\n\ncorrect: \\(w^T x_i &lt; 0 \\rightarrow y_i w^T x_i &gt; 0\\)\nincorrect: \\(w^T x_i &gt; 0 \\rightarrow y_i w^T x_i &lt; 0\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0-1 Loss\n Exponential Loss \n Hinge Loss \n Logistic Loss \n\n\n\n\ndescription\nOnly cares about whether the prediction is correct or not\nPunishes the wrong predictions exponentially, value gets smaller as more confident for correct predictions\n- Confident and correct predictions are not penalized- Grows linearly for incorrect predictions\n- Used in logistic regression- Smooths the degenerate 0-1 loss with log-sum-exp\n\n\nfunction\n\\[\\mathbb{1}\\{y \\neq w^T x\\}\\]\n\\[e^{-y w^T x}\\]\n\\[\\max(0, 1 - y w^T x)\\]\n\\[\\log(1 + e^{-y w^T x})\\]\n\n\nloss\n- when incorrect, loss is 1- when correct, loss is 0- Total loss is the sum of all losses (number of incorrect predictions)\n- when incorrect, loss is large- when correct, loss is small\n- if correct \\(y w^T x &gt; 1\\), \\(1-y w^T x &lt;0\\) - if incorrect \\(y w^T x &lt; 1\\), \\(1-y w^T x &gt;0\\)\n- Convex, differentiable, and smooth\n\n\n\n\nHinge loss + L2 regularization = SVM\n\n\n\n\nLet \\(z = y_i w^T x_i\\)\n\nSigmoid loss: \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)\n\nmaps to [0, 1] (probability)\nfor predict_proba\n\nLogistic loss: \\(\\log(1 + e^{-z})\\)\n\nmaps \\(y_i w^T x_i\\) to \\([0, \\infty]\\) = loss contribution for a single example\nimportant for fit\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nmodel.predict(X_test)\nmodel.predict_proba(X_test)\n\nhyperparameters:\n\nC: inverse of regularization strength\n\nsmaller C \\(\\rightarrow\\) stronger regularization \\(\\rightarrow\\) smaller weights \\(\\rightarrow\\) simpler model \\(\\rightarrow\\) underfitting\n\n\n\n\n\n\n\n\\[J(w) = \\text{loss} + \\lambda \\cdot \\text{regularization} \\]\n\nmore complex model =&gt; higher regularization =&gt; higher cost\n\n\n\n\n\nadding penalty on model complexity (to reduce overfitting)\nBenefits:\n\nreduce overfitting\nless prone to outliers\n\nKey Idea: Pick the line/ hyperplane with the smalles slope (simplest model)\n\n\n\n\n\n\n\n\n\nL0 Norm\nL1 Norm\nL2 Norm\n\n\n\n\nNumber of non-zero elements\nSum of absolute values\nSquare root of the sum of squared values\n\n\n\\(\\|\\|w\\|\\|_0\\)\n\\(\\|\\|w\\|\\|_1\\)\n\\(\\|\\|w\\|\\|_2\\)\n\n\nN/A\n\\(\\sum_{i=1}^{n} \\|x_i\\|\\)\n\\(\\sqrt{\\sum_{i=1}^{n} x_i^2}\\)\n\n\n\n\n\n\n\\[J(\\omega) = ||X\\omega - y||_2^2 + \\lambda ||\\omega||_0\\]\n\nWhere \\(||\\omega||_0\\) is the L0-norm: number of non-zero elements in \\(\\omega\\)\nTo increase the degrees of freedom by 1, need to decrease the error by \\(\\lambda\\)\n\ndegrees of freedom: number of parameters in the model\nsmaller DOF is preferred\n\nHard to optimize since it is non-convex and non-differentiable\n\n\n\n\n\\[J(\\omega) = ||X\\omega - y||_2^2 + \\lambda ||\\omega||_2^2\\]\n\nWhere \\(||\\omega||_2^2\\) is the L2-norm: sum of squared values of \\(\\omega\\)\nWeights decrease, but DO NOT become zero\nBig \\(\\lambda\\) =&gt; more regularization =&gt; lower weights =&gt; simpler model =&gt; underfitting\n\n\\(\\lambda=0\\) same as OLS\n\nAs \\(\\lambda\\) increases (simpler model):\n\n\\(||X\\omega - y||_2^2\\) increases (less accurate)\n\\(||\\omega||_2^2\\) decreases (smaller weights)\n\n\n\n\n\nTheory: As n grows, \\(\\lambda\\) should be in the range of \\([0,\\sqrt{n}]\\)\nDo this by optimizing validation error or CV error\n\n\n\n\nfrom sklearn.linear_model import Ridge\nridge = Ridge(alpha=1.0) # alpha is lambda\nridge.fit(X_train, y_train)\n\n\n\n\n\\[J(\\omega) = ||X\\omega - y||_2^2 + \\lambda ||\\omega||_1\\]\n\nSimilar to L2 but uses L1-norm instead.\n\nboth shrink weights and result in simpler models\n\nWeights can become zero\n\nsome features are completely ignored\nkinda like feature selection\n\n\n\n\n\nsparsity: linear function with many zero-valued coefficients\nL0 and L1 regularization =&gt; sparse models\n\n\n\n\nfrom sklearn.linear_model import Lasso\nlasso = Lasso(alpha=1.0) # alpha is lambda\nlasso.fit(X_train, y_train)\n\n\n\n\n\nScaling does not matter:\n\nDecision trees/ Naive Bayes/ Random Forests: only look at 1 feature at a time\nLeast Squares Loss: just change weights\n\nScaling matters:\n\nKNN: distance will be affected by large values\nRegularized Least Squares: regularization term will be affected by large values\n\n\n\n\n\n\nL2 regularization: only 1 unique solution\n\ne.g. 3 collinear features =&gt; all equal weights (1/3)\n\nL1 regularization: many solutions\n\ne.g. 3 collinear features =&gt; (1,0,0), (0,1,0), (0,0,1), (1/2,1/2,0), etc.\n\n\n\n\n\n\\[J(\\omega) = ||X\\omega - y||_2^2 + \\lambda(\\alpha ||\\omega||_1 + (1-\\alpha)||\\omega||_2^2)\\]\n\n\\(\\lambda\\) is the regularization parameter\n\\(\\alpha\\) promotes:\n\nsparcity in L1\nsmoothness in L2\n\nFunction is strictly convex, so there is a unique solution (no collinearity problem)\n\nfrom sklearn.linear_model import ElasticNet\nelastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5) # alpha is lambda, l1_ratio is alpha\nelastic_net.fit(X_train, y_train)\n\n\n\n\nKey Idea: Groups make better decisions than individuals, especially when the group members are diverse.\nTree-based is the most successful ensemble method\n\nRandom Forest\nGradient Boosted Trees\n\nWays to combine models:\n\nAveraging\nStacking\n\n\n\n\n\nDoes not make the individual models better but adds diversity =&gt; better ensemble\nThink of it visually as high variance (not precise) and low bias (accurate on average)\n((insert image from slides))\n\n\n\n\nRF\nBoosting\n\n\n\n\nMany independent trees\nTrees are dependent on each other\n\n\nrandom diversity\nnon-random diversity\n\n\ncan be parallelized\nsequential\n\n\n\n\n\n\nTrain each model on a different subset of the data\nBootstrap Aggregation: Sampling with replacement.\n\n\n\n\n\n\nBagging + Random set of features at each node.\nDeep trees, normally full depth.\nGeneral Idea:\n\nSingle tree is likely to overfit\nUse a collection of diverse trees\nEach tree overfits on some part of the data but reduce overfitting by averaging\n\nAt each node:\n\nrandomly select a subset of features (independednt from each node)\nfind best split among the subset of features\ngrow tree to full depth\n\nPrediction:\n\nVote the trees\n\n\n\n\n\nuses soft prediction (avg predict_proba)\neasy to fit in parallel (each tree is independent)\nSome important hyperparameters:\n\nn_estimators: number of trees (higher = more complex)\nmax_depth: max depth of each tree (higher = more complex)\nmax_features: max number of features to consider at each split (higher = more complex)\n\nDefaults:\n\nClassification: sqrt(# features)\nRegression: # features / 3 or # features\n\n\n\nmax_features and n_estimators get better scores as values increase but more computationally expensive\n\ndifferent from standard “hyperparameter tuning”\n\n\n\n\n\n\n\nRandom Forest + Random Thresholds (instead of the best threshold)\nIndividually, each tree is not that good\nEach tree is grown to full depth\nIn sklearn: ExtraTreesClassifier\n((insert image about the variance comparison))\n\n\n\n\n\nBoost performance through reweighting of data\n\n\nFit initial model on training data\nreweight the training data so that samples that the initial model got wrong became more important\nFit the next model on the reweighted data\n\nwill prioritize the samples that the previous model performed poorly on\n\nRepeat until you have a set of models\nCombine the models into a single model\n\n\n\n\n\nbetter than AdaBoost\nKey Idea: Trying to decrease the residuals of the previous model by creating new tree that can predict the residuals (then use it as correction)\n\nNo randomization\ncombine multiple shallow (depth 1-5) trees\nbuilt sequentially\n\nWas computationally expensive but now it is not:\n\nXGBoost, LightGBM, CatBoost\n\n\n\nConstruct a base tree (root node), no splits\nBuild next tree based on errors of previous tree\n\nthe errors are the residuals\n\nCombine the tree from step 1 with the tree from step 2\nRepeat until max number of estimators is reached\nCombine the trees into a single model\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightbgm.sklearn import LGBMClassifier\n\nImportant Hyperparameters:\n\nn_estimators: number of boosting rounds (higher = more complex)\nlearning_rate: controls how strongly each tree tries to correct the mistakes of the previous trees (higher = more corrections = more complex)\nmax_depth: max depth of each tree (higher = more complex)\nscale_pos_weight: balancing of positive and negative weights\n\n\n\n\n\n\n\n\nTake vote of all models\n\n\n\n\n\nvoting='soft' uses predict_proba and averages the probabilities\n\ngood if trust predict_proba\n\nvoting='hard' uses predict and takes the majority vote\n\nfrom sklearn.ensemble import VotingClassifier\n\naveraging_model = VotingClassifier(\n    list(classifiers.items()), voting=\"soft\"\n)  # need the list() here for cross-validation to work!\n\n\n\n\nKey idea: Weighted voting via simple model\n\nuse outputs as inputs to a new model\n\nUse a simpler model like logistic regression as “meta-model”\nEach model is a feature for the meta-model\n\n\n\n\n\n\n\n\n\n\nLooking at feature importance can help us understand the model\nWhy?\n\nDebugging: find systematic errors\nDiagnosing bias: find underlying biases\nFeature engineering: find new features\nFeature selection: find unimportant features\nModel interpretability can be leveraged by domain experts to diagnose systematic errors and underlying biases of complex ML systems.\n\n\n\n\n\nKey idea: How much does each feature contribute to the model?\nUse a tree-based model (e.g. random forest) to measure feature importance\n\nDo not use single decision tree because it is not stable\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nEasy to interpret\nNot stable (can change a lot with small changes in data)\n\n\nCan be used for feature selection\nCan be biased towards high cardinality* features (e.g. ID)\n\n\nCan be used for feature engineering (e.g. interaction features)\nCan be biased towards numerical features (e.g. age)\n\n\nna\nCan only be used for tree-based models (e.g. random forest)\n\n\n\n*high cardinality: a feature with a lot of unique values\n\n\n\n\nRandomly shuffle a feature and measure how much the model performance changes\nNo issue with high cardinality features\nCan add a random noise column to measure feature importance\n\nCan be used as a threshold to remove features\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nCan be used for any model (not just tree-based models)\nCorrelated features are still an issue\n\n\nno issue with high cardinality features\nWe do not know if they contribute positively or negatively to the model\n\n\nSimple concept, easy to understand\n\n\n\nUse validation data, so not picking up features that overfit\n\n\n\n\n\n\nfrom sklearn.inspection import permutation_importance\n\nperm_importance = permutation_importance(\n    model,\n    X_test,\n    y_test,\n    n_repeats=10,\n    random_state=42,\n    n_jobs=-1\n)\n\ncan be used on non sklearn models\ndo not have a sign\nonly tell us importance not direction of importance\n\n\n\n\n\n\nWhat is the contribution of each feature to the prediction?\nThe order of the features matters due to interactions\nCould use backward/ forward selection to find the best subset of features but very expensive\nInstead, see how the prediction changes when we add a feature\nShapley values: the average marginal contribution of a feature value over all possible coalitions\n\nMarginal contribution: the difference in the prediction with and without the feature\nCoalition: a subset of features\n\n\nread: https://towardsdatascience.com/using-shap-values-to-explain-how-your-machine-learning-model-works-732b3f40e137\n\n\n\nIndicate the contribution of each feature to the prediction\nSame units as the prediction (for classification, it is probability)\n\n\n\n\nimport shap\n\nX_test_enc = pd.DataFrame(\n    data = pipe[:-1].transform(X_test),\n    columns = feature_names,\n    index = X_test.index\n)\n\nexplainer = shap.Explainer(pipe[-1])\nexplanation = explainer(X_test_enc)\n\n# [sample, feature, class], class 1 is the positive class\nexplanation[:,:,1].values\n\n# visualize\n# also [sample, feature, class]\nshap.plots.waterfall(explanation[2,:,1])\n\n# or force plot: compresses the waterfall plot\nshap.plots.force(explanation[2,:,1])\n\n# summary plit to show impact of each feature\nshap.plots.beeswarm(explanation[:,:,1])\nshap.summary_plot(explanation[:,:,1])"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#classification-metrics",
    "href": "block_3/573_model_sel/573_model_sel.html#classification-metrics",
    "title": "Model Selection",
    "section": "",
    "text": "Need to evaluate the performance of a classifier not just by accuracy\n\n\n\n\nLinear regression with sigmoid function\nHyperparameter: C (default 1.0)\n\nLower C =&gt; more regularization =&gt; smaller coefficients =&gt; underfitting\n\n\nfrom sklearn.linear_model import LogisticRegression\n\npipe_lr = make_pipeline(preprocessor, LogisticRegression())\npd.DataFrame(cross_validate(pipe_lr, X_train, y_train, cv=10, return_train_score=True))\n\n\n\n\n\nChoose the positive case based on which class is more important to us or which class is rare\n\nspotting a class (e.g. fraud/ spam/ disease detection)\n\nCan do this in python:\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_predict\n\n# gets values without plot\nconfusion_matrix(y_train, cross_val_predict(pipe_lr, X_train, y_train, cv=5))\n\n# get values with plot\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nConfusionMatrixDisplay.from_estimator(pipe_lr, X_train, y_train, display_labels=['Not Fraud', 'Fraud']);\nOutput of confusion matrix by default:\n             | Predicted F [0]| Predicted T [1]\nActual F [0] | TN             | FP\nActual T [1] | FN             | TP\n\n\n\nPrecision \\[precision = \\frac{TP}{PP} =  \\frac{TP}{TP + FP}\\]\nRecall or True positive rate (TPR) \\[recall = \\frac{TP}{P} = \\frac{TP}{TP + FN}\\]\nf1-score \\[f1 = 2 \\cdot \\frac{precision * recall}{precision + recall} = 2 \\frac{1}{\\frac{1}{precision} + \\frac{1}{recall}}\\]\n\n\n\n\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_train, cross_val_predict(pipe_lr, X_train, y_train, cv=5)))\n\n# output\n#               precision    recall  f1-score   support\n\n#        Fraud       0.89      0.63      0.74       102\n#    Non fraud       1.00      1.00      1.00     59708\n\n#     accuracy                           1.00     59810\n#    macro avg       0.94      0.81      0.87     59810\n# weighted avg       1.00      1.00      1.00     59810\n\n\n\n\n\ntradeoff between precision and recall\nsetting a threshold is called setting the operating point\nChanging the threshold of the classifier (default of logistic regression is 0.5)\n\nremember predict_proba returns the probability of the positive class, if higher than threshold, then positive class\nas threshold increases, higher bar =&gt; precision increases and recall decreases (less FP but less TP)\nas threshold decreases, lower bar =&gt; precision decreases and recall increases (more TP but more FP)\n\n\n\n\nfrom sklearn.metrics import PrecisionRecallDisplay\n\nPrecisionRecallDisplay.from_estimator(\n    pipe_lr,\n    X_valid,\n    y_valid,\n    pos_label='Fraud',\n    name='LR',  # For Logistic Regression\n);\n\n\nWant the curve to be as close to the top right corner as possible (100% precision and 100% recall)\n\nThreshold of 0 in bottom left =&gt; All points are predicted to be positive =&gt; 100% recall and 0% precision\nThreshold of 1 in top right =&gt; All points are predicted to be negative =&gt; 0% recall and 100% precision\n\n\nOr if want just the values:\nfrom sklearn.metrics import precision_recall_curve\n\npd.DataFrame(\n    precision_recall_curve(\n        y_valid,\n        pipe_lr.predict_proba(X_valid)[:, fraud_column],\n        pos_label='Fraud',\n    ),\n    index=['precision', 'recall', 'threshold']\n).T\n\n\n\n\nArea under the precision recall curve\nHigher is better (0 is worst, 1 is best)\nAlso supports multi-class using one-vs-rest approach + averaging\nIMPORTANT:\n\nF1 score is for a given threshold and measures the quality of predict.\nAP score is a summary across thresholds and measures the quality of predict_proba.\n\n\nfrom sklearn.metrics import average_precision_score\n\nap_lr = average_precision_score(y_valid, pipe_lr.predict_proba(X_valid)[:, fraud_column], pos_label='Fraud')\n\n\n\nAP score\nf1 score\n\n\n\n\nmeasures quality of predict_proba\nmeasures quality of predict\n\n\n\n\nThese two metrics do not always agree*\n\n\n\n\n\n\nReceiver Operating Characteristic: plots the true positive rate (recall) against the false positive rate (1 - specificity)\n\nInstead of plotting precision against recall, plots recall (TPR) against FPR\n\n\n\\[FPR = \\frac{FP}{N} = \\frac{FP}{FP + TN}\\]\nfrom sklearn.metrics import RocCurveDisplay\n\nRocCurveDisplay.from_estimator(\n    pipe_lr,\n    X_valid,\n    y_valid,\n    pos_label='Fraud'\n);\n\n\nIdeal is top left corner (100% TPR and 0% FPR)\nLower left (Threshold of 1) =&gt; 0% TPR and 0% FPR\nUpper right (Threshold of 0) =&gt; 100% TPR and 100% FPR\n\n\n\n\nArea under the ROC curve\nAUC is the probability that a randomly chosen positive point has a higher score than a randomly chosen negative point\n\nAUC of 1.0: all positive points have a higher score than all negative points.\nAUC of 0.5: means random chance.\n\n\n\n\n\n\n\nIf we balance the classes:\n\nPrecision goes down, FP goes up\nRecall goes up, FN goes down\nF1 score goes down\nAP score goes down\nAUC score goes down\ngenerally reduce accuracy\n\n\n\n\n\nClass weights: penalize the minority class more\n\nLogisticRegression(\n  max_iter=500,\n  # give more importance to \"Fraud\" class (10x)\n  class_weight={'Non fraud': 1, 'Fraud': 10}\n\n  # class_weight='balanced' =&gt; automatically give more importance to minority class\n  )"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#regression-metrics",
    "href": "block_3/573_model_sel/573_model_sel.html#regression-metrics",
    "title": "Model Selection",
    "section": "",
    "text": "from sklearn.dummy import DummyRegressor\n\ndummy = DummyRegressor(strategy='mean') # default\npd.DataFrame(cross_validate(dummy, X_train, y_train, cv=10, return_train_score=True))\n\n\n\n\nLinear regression with L2 regularization\nHyperparameter: alpha (default 1.0)\n\nHigher alpha =&gt; more regularization =&gt; smaller coefficients =&gt; underfitting\nmore regularization : smaller coefficients =&gt; less sensitive to changes in the input features =&gt; less likely to overfit\n\n\nfrom sklearn.linear_model import Ridge\n\nlr_pipe = make_pipeline(preprocessor, Ridge())\npd.DataFrame(cross_validate(lr_pipe, X_train, y_train, cv=10, return_train_score=True))\n\n# fit model\nlr_pipe.fit(X_train, y_train)\n\n# get coefficients\ndf = pd.DataFrame(\n    data={\"coefficients\": lr_pipe.named_steps[\"ridge\"].coef_}, index=feature_names)\n\ndf.sort_values(\"coefficients\", ascending=False)\n\n\n\n\nRidge with cross-validation to find the best alpha\n\nfrom sklearn.linear_model import RidgeCV\n\nalphas = 10.0 ** np.arange(-6, 6, 1)\n\nridgecv_pipe = make_pipeline(preprocessor,\n  RidgeCV(alphas=alphas, cv=10))\nridgecv_pipe.fit(X_train, y_train);\n\n# best alpha\nbest_alpha = ridgecv_pipe.named_steps[\"ridgecv\"].alpha_\n\n\n\n\n\nCannot use equality since we are predicting a continuous variable (not classification)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetric\nDescription\nFormula\nMin Value\nMax Value\nCode Example (sklearn)\n\n\n\n\nMSE (Mean Squared Error)\nMeasures the average of the squares of the errors, i.e., the average squared difference between the estimated values and the actual value.\n\\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(Y_i - \\hat{Y}_i)^2\\]\n0 (perfect)\n∞\nfrom sklearn.metrics import mean_squared_error  mse = mean_squared_error(y_true, y_pred)\n\n\nRMSE (Root Mean Squared Error)\nSquare root of the MSE. It measures the standard deviation of the prediction errors or residuals.\n\\[RMSE = \\sqrt{MSE}\\]\n0 (perfect)\n∞\nrmse = mean_squared_error(y_true, y_pred, squared=False)\n\n\nMAE (Mean Absolute Error)\nMeasures the average of the absolute errors.\n\\[MAE = \\frac{1}{n}\\sum_{i=1}^{n}\\left\\| Y_i - \\hat{Y}_i \\right\\| \\]\n0 (perfect)\n∞\nfrom sklearn.metrics import mean_absolute_error  mae = mean_absolute_error(y_true, y_pred)\n\n\nMAPE (Mean Absolute Percentage Error)\nMeasures the average of the absolute percentage errors.\n\\[MAPE = \\frac{1}{n}\\sum_{i=1}^{n}\\left\\| \\frac{Y_i - \\hat{Y}_i}{Y_i} \\right\\| \\times 100\\% \\]\n0 % (perfect)\n∞ %\nfrom sklearn.metrics import mean_absolute_percentage_error  mape = mean_absolute_percentage_error(y_true, y_pred)\n\n\nR² (Coefficient of Determination)\nMeasures how well future samples are likely to be predicted by the model.\n\\[R^2 = 1 - \\frac{\\sum_{i=1}^{n}(Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^{n}(Y_i - \\bar{Y})^2}\\]\n-ve [if worst than mean]\n1 (perfect)\nfrom sklearn.metrics import r2_score  r2 = r2_score(y_true, y_pred)\n\n\n\n\nMAE is less sensitive to outliers than MSE\n\n\n\n\n\nCan plot the actual vs predicted values to see the error (and plot line of gradient 1 = perfect prediction)\n\nfrom sklearn.metrics import PredictionErrorDisplay\n\nPredictionErrorDisplay.from_estimator(\n    ridge_tuned,\n    X_train,\n    y_train,\n    kind='actual_vs_predicted',\n    subsample=None  # show all predictions\n)\n\n\nCan plot the residuals (error) vs predicted values to see if there is a pattern (e.g. heteroscedasticity)\n\nfrom sklearn.metrics import PredictionErrorDisplay\n\nPredictionErrorDisplay.from_estimator(\n    ridge_tuned,\n    X_train,\n    y_train,\n    subsample=None  # show all predictions\n)"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#data-cleaning",
    "href": "block_3/573_model_sel/573_model_sel.html#data-cleaning",
    "title": "Model Selection",
    "section": "",
    "text": "Need to have good data to build a good model.\nBad quality data:\n\nMissing values\nFew observations overall or in a specific group\nBiased sample (not representative of the population)\nNon-independent observations\nInaccurate measurements\nFabricated data\nOut of bounds values\nObsure column names\nTypos (spelling differences in categorical variables)\nUsing multiple values to represent same thing (e.g. NA, NONE, NULL, NaN)\nIncorrect data types\n\n\n\nBetter features help more than a better model.\n\nGood features would ideally:\n\ncapture the most important information\nallow learning with few examples\ngeneralize to new scenarios\n\nTrade-off for simple and expressive features:\n\nsimple features: overfitting risk is low but low score\nexpressive features: overfitting risk is high but high score\n\n\n\n\n\n\n\n\n\n\n\n\nPreprocessing\nEngineering\n\n\n\n\nstep of cleaning and preparing the data for analysis\ncreating new features from existing data\n\n\ngenerally HAVE to do, or error\noptional but can improve model performance\n\n\ne.g. scaling, normalization, imputation\ne.g. one-hot encoding, binning, get more data, group-wise normalization, transformation\n\n\n\n\nFeature Selection: removing irrelevant features, normally done after feature engineering"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#feature-engineering",
    "href": "block_3/573_model_sel/573_model_sel.html#feature-engineering",
    "title": "Model Selection",
    "section": "",
    "text": "Common guidelines:\n\nIs not unique or random\nHas a variance (not constant)\nAdds unique variance (is not constant transformation)\n\nchanging units (e.g. from meters to feet) is not feature engineering\n\nIs ideally interpretable\n\nExamples:\n\nLooking up and getting additional data\nDiscretization (binning): e.g. age -&gt; age group\nGroup-wise normalization: express feature relative to group mean/median\nTransformation: e.g. height, weight -&gt; BMI\n\n\n\n\nparameter: degree\n\nhigher degree: more expressive features =&gt; more overfitting\n\nTry to capture non-linear relationships\n\nLinear regression can only capture lines, planes, hyperplanes\n\ne.g. add a squared feature (feat1^2)\n\n\n\n\n\nDo polynomial then StandardScaler\n\navoids polynomials being very large\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2)\n# degree: max degree of polynomial features\nX_enc = poly_feats.fit_transform(X_toy)\npd.DataFrame(X_enc, columns=poly_feats.get_feature_names_out()).head()\nDegree example:\n\n\n\nDimension\nDegree\nExamples\n\n\n\n\n1\n2\n1, x, x^2\n\n\n2\n2\n1, x, y, x^2, xy, y^2\n\n\n\n\n\n\n\\[ \\hat{y} = Xw \\]\n\nX: design matrix (n x d)\nw: weights (d x 1)\ny: target (n x 1)\n\nFor polynomial features:\n\\(Z = [1, x, x^2]\\)\n\\[ \\hat{y} = Zw = \\begin{bmatrix} 1 & x_1 & x_1^2 \\\\ 1 & x_2 & x_2^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x_n^2 \\end{bmatrix} \\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\end{bmatrix} = \\begin{bmatrix} w_0 + w_1x_1 + w_2x_1^2 \\\\ w_0 + w_1x_2 + w_2x_2^2 \\\\ \\vdots \\\\ w_0 + w_1x_n + w_2x_n^2 \\end{bmatrix} \\]\n\n\n\n\nPolynomial features can be expensive to compute\nKernel trick: compute dot product between two vectors in a higher dimensional space without computing the transformation explicitly"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#feature-importance",
    "href": "block_3/573_model_sel/573_model_sel.html#feature-importance",
    "title": "Model Selection",
    "section": "",
    "text": "import seaborn as sns\n\nsns.heatmap(df.corr(), annot=True)\n\nShows correlation between features\nExtremely simplistic:\n\nOnly looks at linear correlation\nOnly looks at pairwise correlation (isolates features)\n\n\n\n\n\n\nCorrelation among features might make coefficients completely uninterpretable.\nFairly straightforward to interpret coefficients of ordinal features.\nIn categorical features, it’s often helpful to consider one category as a reference point and think about relative importance.\nFor numeric features, relative importance is meaningful after scaling.\nYou have to be careful about the scale of the feature when interpreting the coefficients.\nRemember that explaining the model explaining the data or explaining how the world works.\nThe coefficients tell us only about the model and they might not accurately reflect the data."
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#feature-selection",
    "href": "block_3/573_model_sel/573_model_sel.html#feature-selection",
    "title": "Model Selection",
    "section": "",
    "text": "More features =&gt; more complex model =&gt; overfitting\nFeature selection find features that are important for prediction and remove the rest.\n\nWhy?\n\nInterpretability: easier to explain with fewer features\nComputation: faster to train simpler models (curse of dimensionality)\nData collection: cheaper to collect fewer features\nStorage: less space to store fewer features\nFundamental Tradeoff: reduce overfitting by reducing complexity\n\nHow?\n\ndomain knowledge\nautomatic feature selection\n\nmodel-based selection\nrecursive feature elimination\nforward/backward selection\n\n\n\n\n\nFast and easy\nArbitrary, does not take into account feature combinations\ne.g. correlation threshold, variance threshold\n\n\n\n\n\nUse a supervised model to judge the importance of each feature, and keep only the most important ones.\nCan use different model than the final estimator.\nUse a model that provides some measure of feature importance. (e.g. decision trees, linear models, etc.)\n\nGeneral Steps:\n\nSelect a threshold for which features to keep.\nDiscard features below the threshold.\n\nfrom sklearn.feature_selection import SelectFromModel\n\nselect_lr = SelectFromModel(Ridge(), threshold=\"median\")\n\n# Add feature selection to pipeline (after preprocessing)\npipe = make_pipeline(preprocessor, select_lr, Ridge())\n\n# access selected features\npipe.named_steps[\"selectfrommodel\"]\n# can do: .n_features_in_, .threshold_, .get_support()\n\n\n\n\nRecursively remove features, build a model on those features that remain, and then repeatedly construct a model and remove the weakest feature until a specified number of features remain.\nIteratively eliminates unimportant features.\nComputationally expensive\n\n\n\n\nDecide the number of features to select.\nAssign importances to features, e.g. by fitting a model and looking at coef_ (Linear models) or feature_importances_ (RandomForestClassifier).\nRemove the least important feature.\nRepeat steps 2-3 until only \\(k\\) features are remaining.\n\nfrom sklearn.feature_selection import RFE\n\nrfe = RFE(Ridge(), n_features_to_select=120)\n\npipe_rf_rfe = make_pipeline(preprocessor, rfe, RandomForestRegressor(random_state=42))\n\nresults[\"rfe + rfe\"] = mean_std_cross_val_scores(\n    pipe_rf_rfe, X_train, y_train, return_train_score=True\n)\nNote: for categorical features, RFE might remove some OHE features. This is an unresolved issue.\n\n\n\n\nRFE but using CV to find the optimal number of features to keep.\nvery slow since there is CV within CV.\n\nfrom sklearn.feature_selection import RFECV\n\nrfecv = RFECV(Ridge())\n\npipe_rf_rfecv = make_pipeline(\n    preprocessor, rfecv, RandomForestRegressor(random_state=42)\n)\npipe_rf_rfecv.fit(X_train, y_train);\n\n# optimal number of features\npipe_rfe_ridgecv.named_steps[\"rfecv\"].n_features_\n# which features were selected\npipe_rfe_ridgecv.named_steps[\"rfecv\"].support_\n\n\n\n\n\nNot based on feature importance.\nDefine a scoring function \\(f(S)\\) that measures the quality of a subset of features \\(S\\).\nSearch through all possible subsets of features and pick the subset with the best score.\n\ne.g. for A, B, C search through {}, {A}, {B}, {C}, {A, B}, {A, C}, {B, C}, {A, B, C}…\n# subsets = \\(2^p\\) where \\(p\\) is the number of features\n\nForward selection: start with no features and add one at a time.\n\nadd the feature that results in the best score.\n\nBackward selection: start with all features and remove one at a time.\n\nremove the feature that results in the best score.\n\n\nfrom sklearn.feature_selection import SequentialFeatureSelector\n\npipe_forward = make_pipeline(\n    preprocessor,\n    SequentialFeatureSelector(Ridge(), direction=\"forward\"),\n    RandomForestRegressor(random_state=42),\n)\n\n# results['rf_forward_fs'] = mean_std_cross_val_scores(pipe_forward, X_train, y_train, return_train_score=True)\n\npipe_forward.fit(X_train, y_train)\n\n\n\n\nA feature relevance is only defines in the context of other features.\n\nAdding/removing features can change the importance of other features.\n\nRelevance != causality\nThe methods don’t always work."
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#loss-function",
    "href": "block_3/573_model_sel/573_model_sel.html#loss-function",
    "title": "Model Selection",
    "section": "",
    "text": "A function that measures how well a model fits the data\nThe goal is to minimize the loss function\n\nsmaller loss \\(\\rightarrow\\) better model\n\nCaptures what is important to minimize\nCommon loss functions:\n\nLeast squares loss\nAbsolute error loss\nHuber loss\n\n\n\n\n\n\n\n\n\n\n\n\nScoring Metric\nLoss Function\n\n\n\n\nReport the results (often several metrics)\nUsed to train/ fit the model\n\n\nFind best model hyperparams\nFind best model parameters\n\n\nCan pick those suitable for the task\nConstrainted (need to be differentiable)"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#regression-loss-functions",
    "href": "block_3/573_model_sel/573_model_sel.html#regression-loss-functions",
    "title": "Model Selection",
    "section": "",
    "text": "\\[J(\\omega) = \\sum_{i=1}^{n} (y_i - \\omega^T x_i)^2 \\]\n\nDefine loss as sum of squared errors (difference between prediction and actual value)\nPenalizes heavily for large errors\n\nsensitive to outliers\n\nFor a well-defined OLS, there is a unique solution (not always the case)\n\n\\(\\omega = (X^T X)^{-1} X^T y\\)\n\n\\(X^T X\\) must be invertible\n\n\n\n\n\n\n\\[J(\\omega) = \\sum_{i=1}^{n} |y_i - \\omega^T x_i| \\]\n\nLess sensitive to outliers\nBut minimization is harder\n\nnot differentiable at 0\nno closed form solution\n\nAlternative: Huber Loss\n\nbehaves like OLS for small errors"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#classification-loss-functions",
    "href": "block_3/573_model_sel/573_model_sel.html#classification-loss-functions",
    "title": "Model Selection",
    "section": "",
    "text": "\\[ y_i w^T x_i = \\begin{cases} \\text{correct} & y_i w^T x_i &gt; 0 \\\\ \\text{incorrect} & y_i w^T x_i &lt; 0 \\end{cases} \\]\n\n\\(y_i\\): positive\n\ncorrect: \\(w^T x_i &gt; 0 \\rightarrow y_i w^T x_i &gt; 0\\)\nincorrect: \\(w^T x_i &lt; 0 \\rightarrow y_i w^T x_i &lt; 0\\)\n\n\\(y_i\\): negative\n\ncorrect: \\(w^T x_i &lt; 0 \\rightarrow y_i w^T x_i &gt; 0\\)\nincorrect: \\(w^T x_i &gt; 0 \\rightarrow y_i w^T x_i &lt; 0\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0-1 Loss\n Exponential Loss \n Hinge Loss \n Logistic Loss \n\n\n\n\ndescription\nOnly cares about whether the prediction is correct or not\nPunishes the wrong predictions exponentially, value gets smaller as more confident for correct predictions\n- Confident and correct predictions are not penalized- Grows linearly for incorrect predictions\n- Used in logistic regression- Smooths the degenerate 0-1 loss with log-sum-exp\n\n\nfunction\n\\[\\mathbb{1}\\{y \\neq w^T x\\}\\]\n\\[e^{-y w^T x}\\]\n\\[\\max(0, 1 - y w^T x)\\]\n\\[\\log(1 + e^{-y w^T x})\\]\n\n\nloss\n- when incorrect, loss is 1- when correct, loss is 0- Total loss is the sum of all losses (number of incorrect predictions)\n- when incorrect, loss is large- when correct, loss is small\n- if correct \\(y w^T x &gt; 1\\), \\(1-y w^T x &lt;0\\) - if incorrect \\(y w^T x &lt; 1\\), \\(1-y w^T x &gt;0\\)\n- Convex, differentiable, and smooth\n\n\n\n\nHinge loss + L2 regularization = SVM\n\n\n\n\nLet \\(z = y_i w^T x_i\\)\n\nSigmoid loss: \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)\n\nmaps to [0, 1] (probability)\nfor predict_proba\n\nLogistic loss: \\(\\log(1 + e^{-z})\\)\n\nmaps \\(y_i w^T x_i\\) to \\([0, \\infty]\\) = loss contribution for a single example\nimportant for fit\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nmodel.predict(X_test)\nmodel.predict_proba(X_test)\n\nhyperparameters:\n\nC: inverse of regularization strength\n\nsmaller C \\(\\rightarrow\\) stronger regularization \\(\\rightarrow\\) smaller weights \\(\\rightarrow\\) simpler model \\(\\rightarrow\\) underfitting"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#final-cost-function",
    "href": "block_3/573_model_sel/573_model_sel.html#final-cost-function",
    "title": "Model Selection",
    "section": "",
    "text": "\\[J(w) = \\text{loss} + \\lambda \\cdot \\text{regularization} \\]\n\nmore complex model =&gt; higher regularization =&gt; higher cost"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#regularization",
    "href": "block_3/573_model_sel/573_model_sel.html#regularization",
    "title": "Model Selection",
    "section": "",
    "text": "adding penalty on model complexity (to reduce overfitting)\nBenefits:\n\nreduce overfitting\nless prone to outliers\n\nKey Idea: Pick the line/ hyperplane with the smalles slope (simplest model)\n\n\n\n\n\n\n\n\n\nL0 Norm\nL1 Norm\nL2 Norm\n\n\n\n\nNumber of non-zero elements\nSum of absolute values\nSquare root of the sum of squared values\n\n\n\\(\\|\\|w\\|\\|_0\\)\n\\(\\|\\|w\\|\\|_1\\)\n\\(\\|\\|w\\|\\|_2\\)\n\n\nN/A\n\\(\\sum_{i=1}^{n} \\|x_i\\|\\)\n\\(\\sqrt{\\sum_{i=1}^{n} x_i^2}\\)"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#l0-regularization",
    "href": "block_3/573_model_sel/573_model_sel.html#l0-regularization",
    "title": "Model Selection",
    "section": "",
    "text": "\\[J(\\omega) = ||X\\omega - y||_2^2 + \\lambda ||\\omega||_0\\]\n\nWhere \\(||\\omega||_0\\) is the L0-norm: number of non-zero elements in \\(\\omega\\)\nTo increase the degrees of freedom by 1, need to decrease the error by \\(\\lambda\\)\n\ndegrees of freedom: number of parameters in the model\nsmaller DOF is preferred\n\nHard to optimize since it is non-convex and non-differentiable"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#l2-regularization",
    "href": "block_3/573_model_sel/573_model_sel.html#l2-regularization",
    "title": "Model Selection",
    "section": "",
    "text": "\\[J(\\omega) = ||X\\omega - y||_2^2 + \\lambda ||\\omega||_2^2\\]\n\nWhere \\(||\\omega||_2^2\\) is the L2-norm: sum of squared values of \\(\\omega\\)\nWeights decrease, but DO NOT become zero\nBig \\(\\lambda\\) =&gt; more regularization =&gt; lower weights =&gt; simpler model =&gt; underfitting\n\n\\(\\lambda=0\\) same as OLS\n\nAs \\(\\lambda\\) increases (simpler model):\n\n\\(||X\\omega - y||_2^2\\) increases (less accurate)\n\\(||\\omega||_2^2\\) decreases (smaller weights)\n\n\n\n\n\nTheory: As n grows, \\(\\lambda\\) should be in the range of \\([0,\\sqrt{n}]\\)\nDo this by optimizing validation error or CV error\n\n\n\n\nfrom sklearn.linear_model import Ridge\nridge = Ridge(alpha=1.0) # alpha is lambda\nridge.fit(X_train, y_train)"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#l1-regularization",
    "href": "block_3/573_model_sel/573_model_sel.html#l1-regularization",
    "title": "Model Selection",
    "section": "",
    "text": "\\[J(\\omega) = ||X\\omega - y||_2^2 + \\lambda ||\\omega||_1\\]\n\nSimilar to L2 but uses L1-norm instead.\n\nboth shrink weights and result in simpler models\n\nWeights can become zero\n\nsome features are completely ignored\nkinda like feature selection\n\n\n\n\n\nsparsity: linear function with many zero-valued coefficients\nL0 and L1 regularization =&gt; sparse models\n\n\n\n\nfrom sklearn.linear_model import Lasso\nlasso = Lasso(alpha=1.0) # alpha is lambda\nlasso.fit(X_train, y_train)"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#regularization-and-scaling",
    "href": "block_3/573_model_sel/573_model_sel.html#regularization-and-scaling",
    "title": "Model Selection",
    "section": "",
    "text": "Scaling does not matter:\n\nDecision trees/ Naive Bayes/ Random Forests: only look at 1 feature at a time\nLeast Squares Loss: just change weights\n\nScaling matters:\n\nKNN: distance will be affected by large values\nRegularized Least Squares: regularization term will be affected by large values"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#regularization-and-collinearity",
    "href": "block_3/573_model_sel/573_model_sel.html#regularization-and-collinearity",
    "title": "Model Selection",
    "section": "",
    "text": "L2 regularization: only 1 unique solution\n\ne.g. 3 collinear features =&gt; all equal weights (1/3)\n\nL1 regularization: many solutions\n\ne.g. 3 collinear features =&gt; (1,0,0), (0,1,0), (0,0,1), (1/2,1/2,0), etc."
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#sklearn-elastic-net",
    "href": "block_3/573_model_sel/573_model_sel.html#sklearn-elastic-net",
    "title": "Model Selection",
    "section": "",
    "text": "\\[J(\\omega) = ||X\\omega - y||_2^2 + \\lambda(\\alpha ||\\omega||_1 + (1-\\alpha)||\\omega||_2^2)\\]\n\n\\(\\lambda\\) is the regularization parameter\n\\(\\alpha\\) promotes:\n\nsparcity in L1\nsmoothness in L2\n\nFunction is strictly convex, so there is a unique solution (no collinearity problem)\n\nfrom sklearn.linear_model import ElasticNet\nelastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5) # alpha is lambda, l1_ratio is alpha\nelastic_net.fit(X_train, y_train)"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#ensemble-methods",
    "href": "block_3/573_model_sel/573_model_sel.html#ensemble-methods",
    "title": "Model Selection",
    "section": "",
    "text": "Key Idea: Groups make better decisions than individuals, especially when the group members are diverse.\nTree-based is the most successful ensemble method\n\nRandom Forest\nGradient Boosted Trees\n\nWays to combine models:\n\nAveraging\nStacking\n\n\n\n\n\nDoes not make the individual models better but adds diversity =&gt; better ensemble\nThink of it visually as high variance (not precise) and low bias (accurate on average)\n((insert image from slides))\n\n\n\n\nRF\nBoosting\n\n\n\n\nMany independent trees\nTrees are dependent on each other\n\n\nrandom diversity\nnon-random diversity\n\n\ncan be parallelized\nsequential\n\n\n\n\n\n\nTrain each model on a different subset of the data\nBootstrap Aggregation: Sampling with replacement.\n\n\n\n\n\n\nBagging + Random set of features at each node.\nDeep trees, normally full depth.\nGeneral Idea:\n\nSingle tree is likely to overfit\nUse a collection of diverse trees\nEach tree overfits on some part of the data but reduce overfitting by averaging\n\nAt each node:\n\nrandomly select a subset of features (independednt from each node)\nfind best split among the subset of features\ngrow tree to full depth\n\nPrediction:\n\nVote the trees\n\n\n\n\n\nuses soft prediction (avg predict_proba)\neasy to fit in parallel (each tree is independent)\nSome important hyperparameters:\n\nn_estimators: number of trees (higher = more complex)\nmax_depth: max depth of each tree (higher = more complex)\nmax_features: max number of features to consider at each split (higher = more complex)\n\nDefaults:\n\nClassification: sqrt(# features)\nRegression: # features / 3 or # features\n\n\n\nmax_features and n_estimators get better scores as values increase but more computationally expensive\n\ndifferent from standard “hyperparameter tuning”\n\n\n\n\n\n\n\nRandom Forest + Random Thresholds (instead of the best threshold)\nIndividually, each tree is not that good\nEach tree is grown to full depth\nIn sklearn: ExtraTreesClassifier\n((insert image about the variance comparison))\n\n\n\n\n\nBoost performance through reweighting of data\n\n\nFit initial model on training data\nreweight the training data so that samples that the initial model got wrong became more important\nFit the next model on the reweighted data\n\nwill prioritize the samples that the previous model performed poorly on\n\nRepeat until you have a set of models\nCombine the models into a single model\n\n\n\n\n\nbetter than AdaBoost\nKey Idea: Trying to decrease the residuals of the previous model by creating new tree that can predict the residuals (then use it as correction)\n\nNo randomization\ncombine multiple shallow (depth 1-5) trees\nbuilt sequentially\n\nWas computationally expensive but now it is not:\n\nXGBoost, LightGBM, CatBoost\n\n\n\nConstruct a base tree (root node), no splits\nBuild next tree based on errors of previous tree\n\nthe errors are the residuals\n\nCombine the tree from step 1 with the tree from step 2\nRepeat until max number of estimators is reached\nCombine the trees into a single model\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightbgm.sklearn import LGBMClassifier\n\nImportant Hyperparameters:\n\nn_estimators: number of boosting rounds (higher = more complex)\nlearning_rate: controls how strongly each tree tries to correct the mistakes of the previous trees (higher = more corrections = more complex)\nmax_depth: max depth of each tree (higher = more complex)\nscale_pos_weight: balancing of positive and negative weights\n\n\n\n\n\n\n\n\nTake vote of all models\n\n\n\n\n\nvoting='soft' uses predict_proba and averages the probabilities\n\ngood if trust predict_proba\n\nvoting='hard' uses predict and takes the majority vote\n\nfrom sklearn.ensemble import VotingClassifier\n\naveraging_model = VotingClassifier(\n    list(classifiers.items()), voting=\"soft\"\n)  # need the list() here for cross-validation to work!\n\n\n\n\nKey idea: Weighted voting via simple model\n\nuse outputs as inputs to a new model\n\nUse a simpler model like logistic regression as “meta-model”\nEach model is a feature for the meta-model"
  },
  {
    "objectID": "block_3/573_model_sel/573_model_sel.html#model-interpretability",
    "href": "block_3/573_model_sel/573_model_sel.html#model-interpretability",
    "title": "Model Selection",
    "section": "",
    "text": "Looking at feature importance can help us understand the model\nWhy?\n\nDebugging: find systematic errors\nDiagnosing bias: find underlying biases\nFeature engineering: find new features\nFeature selection: find unimportant features\nModel interpretability can be leveraged by domain experts to diagnose systematic errors and underlying biases of complex ML systems.\n\n\n\n\n\nKey idea: How much does each feature contribute to the model?\nUse a tree-based model (e.g. random forest) to measure feature importance\n\nDo not use single decision tree because it is not stable\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nEasy to interpret\nNot stable (can change a lot with small changes in data)\n\n\nCan be used for feature selection\nCan be biased towards high cardinality* features (e.g. ID)\n\n\nCan be used for feature engineering (e.g. interaction features)\nCan be biased towards numerical features (e.g. age)\n\n\nna\nCan only be used for tree-based models (e.g. random forest)\n\n\n\n*high cardinality: a feature with a lot of unique values\n\n\n\n\nRandomly shuffle a feature and measure how much the model performance changes\nNo issue with high cardinality features\nCan add a random noise column to measure feature importance\n\nCan be used as a threshold to remove features\n\n\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nCan be used for any model (not just tree-based models)\nCorrelated features are still an issue\n\n\nno issue with high cardinality features\nWe do not know if they contribute positively or negatively to the model\n\n\nSimple concept, easy to understand\n\n\n\nUse validation data, so not picking up features that overfit\n\n\n\n\n\n\nfrom sklearn.inspection import permutation_importance\n\nperm_importance = permutation_importance(\n    model,\n    X_test,\n    y_test,\n    n_repeats=10,\n    random_state=42,\n    n_jobs=-1\n)\n\ncan be used on non sklearn models\ndo not have a sign\nonly tell us importance not direction of importance\n\n\n\n\n\n\nWhat is the contribution of each feature to the prediction?\nThe order of the features matters due to interactions\nCould use backward/ forward selection to find the best subset of features but very expensive\nInstead, see how the prediction changes when we add a feature\nShapley values: the average marginal contribution of a feature value over all possible coalitions\n\nMarginal contribution: the difference in the prediction with and without the feature\nCoalition: a subset of features\n\n\nread: https://towardsdatascience.com/using-shap-values-to-explain-how-your-machine-learning-model-works-732b3f40e137\n\n\n\nIndicate the contribution of each feature to the prediction\nSame units as the prediction (for classification, it is probability)\n\n\n\n\nimport shap\n\nX_test_enc = pd.DataFrame(\n    data = pipe[:-1].transform(X_test),\n    columns = feature_names,\n    index = X_test.index\n)\n\nexplainer = shap.Explainer(pipe[-1])\nexplanation = explainer(X_test_enc)\n\n# [sample, feature, class], class 1 is the positive class\nexplanation[:,:,1].values\n\n# visualize\n# also [sample, feature, class]\nshap.plots.waterfall(explanation[2,:,1])\n\n# or force plot: compresses the waterfall plot\nshap.plots.force(explanation[2,:,1])\n\n# summary plit to show impact of each feature\nshap.plots.beeswarm(explanation[:,:,1])\nshap.summary_plot(explanation[:,:,1])"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html",
    "href": "block_2/552_stat_inter/552_stat_inter.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Term\nDefinition\n\n\n\n\npoint_estimate\nA summary statistic calculated from a random sample that estimates an unknown population parameter of interest.\n\n\npopulation\nThe entire set of entities objects of interest.\n\n\npopulation_parameter\nA numerical summary value about the population.\n\n\nsample\nA collected subset of observations from a population.\n\n\nobservation\nA quantity or quality (or a set of these) from a single member of a population.\n\n\nsampling_distribution\nA distribution of point estimates, where each point estimate was calculated from a different random sample coming from the same population.\n\n\n\n\nTrue population parameter is denoted with Greek letters. (e.g. μ for mean)\nEstimated population parameter is denoted with Greek letters with a hat. (e.g. μ̂ for mean)\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nDescriptive\nSummarizes a characteristic of a set of data without interpretation.\n- What is the frequency of bacterial illnesses in a data set?  - How many people live in each US state?\n\n\nExploratory\nAnalyzes data to identify patterns, trends, or relationships between variables.\n- Do certain diets correlate with bacterial illnesses?  - Does air pollution correlate with life expectancy in different US regions?\n\n\nInferential\nAnalyzes patterns, trends, or relationships in a representative sample to quantify applicability to the whole population. Estimation with associated randomness.\n- Is eating 5 servings of fruits and vegetables associated with fewer bacterial illnesses?  - Is gestational length different for first born babies?\n\n\nPredictive\nAims to predict measurements or labels for individuals. Not focused on causes but on predictions.\n- How many viral illnesses will someone have next year?  - What political party will someone vote for in the next US election?\n\n\nCausal\nInquires if changing one factor will change another factor in a population. Sometimes design allows for causal interpretation.\n- Does eating 5 servings of fruits and vegetables cause fewer bacterial illnesses?  - Does smoking lead to cancer?\n\n\nMechanistic\nSeeks to explain the underlying mechanism of observed patterns or relationships.\n- How do changes in diet reduce bacterial illnesses?  - How does airplane wing design affect air flow and decrease drag?\n\n\n\n\n\n\nDefine the population of interest.\nSelect the right sampling method according to the specific characteristics of our population of interest.\nSelect our sample size (Power Analysis).\nCollect the sampled data.\nMeasure and calculate the sample statistic.\nInfer the population value based on this sample statistic while accounting for sampling uncertainty.\n\n\n\n\n\n\n\n\nPopulation\nSample\n\n\n\n\n\\(\\pi_E\\): population proportion\n\\(\\hat{\\pi}_E\\): sample proportion\n\n\n\\(\\mu\\): population mean\n\\(\\bar{\\mu}\\): sample mean\n\n\n\nPopulation Distribution:\n\nThe sample distribution is of a similar shape to the population distribution.\nThe sample point estimates are identical to the values for the true population parameter we are trying to estimate.\n\nSample Distribution of 1 Sample:\n\nTaking a random sample and calculating a point estimate is a “good guess” of the unknown population parameter you are interested in.\nAs the sample size increases:\n\nthe sampling distribution becomes narrower.\nmore sample point estimates are closer to the true population mean.\nthe sampling distribution appears more bell-shaped.\n\n\nSampling Distribution of Sample Means:\n\nThe sampling distribution is centered at the true population mean.\nMost sample means are at or very near the same value as the true population mean.\nThe sample distribution (if representative) is an estimate of the population distribution.\nThe sampling distribution of the sample means is not necessarily the same shape as the distribution of the population distribution and tends to be more symmetrical and bell-shaped.\n\n\n\n\nset.seed(1) # for reproducibility\nsample1 &lt;- rep_sample_n(df, size = 100, reps = 10000, replace = TRUE)\n\ndefault is sampling with replacement, but can be changed with replace = FALSE\nsize is the number of samples to draw\nrep is the number of times to repeat the sampling process\n\n\n\n\npop_dist &lt;- multi_family_strata %&gt;%\n  ggplot(aes(x = current_land_value)) +\n  geom_histogram(bins = 50) +\n  xlab(\"current land value\") +\n  ggtitle(\"Population distribution\")"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#terms-and-definitions",
    "href": "block_2/552_stat_inter/552_stat_inter.html#terms-and-definitions",
    "title": "Statistical Inference",
    "section": "",
    "text": "Term\nDefinition\n\n\n\n\npoint_estimate\nA summary statistic calculated from a random sample that estimates an unknown population parameter of interest.\n\n\npopulation\nThe entire set of entities objects of interest.\n\n\npopulation_parameter\nA numerical summary value about the population.\n\n\nsample\nA collected subset of observations from a population.\n\n\nobservation\nA quantity or quality (or a set of these) from a single member of a population.\n\n\nsampling_distribution\nA distribution of point estimates, where each point estimate was calculated from a different random sample coming from the same population.\n\n\n\n\nTrue population parameter is denoted with Greek letters. (e.g. μ for mean)\nEstimated population parameter is denoted with Greek letters with a hat. (e.g. μ̂ for mean)"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#types-of-statistical-questions",
    "href": "block_2/552_stat_inter/552_stat_inter.html#types-of-statistical-questions",
    "title": "Statistical Inference",
    "section": "",
    "text": "Type\nDescription\nExample\n\n\n\n\nDescriptive\nSummarizes a characteristic of a set of data without interpretation.\n- What is the frequency of bacterial illnesses in a data set?  - How many people live in each US state?\n\n\nExploratory\nAnalyzes data to identify patterns, trends, or relationships between variables.\n- Do certain diets correlate with bacterial illnesses?  - Does air pollution correlate with life expectancy in different US regions?\n\n\nInferential\nAnalyzes patterns, trends, or relationships in a representative sample to quantify applicability to the whole population. Estimation with associated randomness.\n- Is eating 5 servings of fruits and vegetables associated with fewer bacterial illnesses?  - Is gestational length different for first born babies?\n\n\nPredictive\nAims to predict measurements or labels for individuals. Not focused on causes but on predictions.\n- How many viral illnesses will someone have next year?  - What political party will someone vote for in the next US election?\n\n\nCausal\nInquires if changing one factor will change another factor in a population. Sometimes design allows for causal interpretation.\n- Does eating 5 servings of fruits and vegetables cause fewer bacterial illnesses?  - Does smoking lead to cancer?\n\n\nMechanistic\nSeeks to explain the underlying mechanism of observed patterns or relationships.\n- How do changes in diet reduce bacterial illnesses?  - How does airplane wing design affect air flow and decrease drag?\n\n\n\n\n\n\nDefine the population of interest.\nSelect the right sampling method according to the specific characteristics of our population of interest.\nSelect our sample size (Power Analysis).\nCollect the sampled data.\nMeasure and calculate the sample statistic.\nInfer the population value based on this sample statistic while accounting for sampling uncertainty."
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#sampling",
    "href": "block_2/552_stat_inter/552_stat_inter.html#sampling",
    "title": "Statistical Inference",
    "section": "",
    "text": "Population\nSample\n\n\n\n\n\\(\\pi_E\\): population proportion\n\\(\\hat{\\pi}_E\\): sample proportion\n\n\n\\(\\mu\\): population mean\n\\(\\bar{\\mu}\\): sample mean\n\n\n\nPopulation Distribution:\n\nThe sample distribution is of a similar shape to the population distribution.\nThe sample point estimates are identical to the values for the true population parameter we are trying to estimate.\n\nSample Distribution of 1 Sample:\n\nTaking a random sample and calculating a point estimate is a “good guess” of the unknown population parameter you are interested in.\nAs the sample size increases:\n\nthe sampling distribution becomes narrower.\nmore sample point estimates are closer to the true population mean.\nthe sampling distribution appears more bell-shaped.\n\n\nSampling Distribution of Sample Means:\n\nThe sampling distribution is centered at the true population mean.\nMost sample means are at or very near the same value as the true population mean.\nThe sample distribution (if representative) is an estimate of the population distribution.\nThe sampling distribution of the sample means is not necessarily the same shape as the distribution of the population distribution and tends to be more symmetrical and bell-shaped."
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#drawing-samples-in-r-rsamplerep_sample_n",
    "href": "block_2/552_stat_inter/552_stat_inter.html#drawing-samples-in-r-rsamplerep_sample_n",
    "title": "Statistical Inference",
    "section": "",
    "text": "set.seed(1) # for reproducibility\nsample1 &lt;- rep_sample_n(df, size = 100, reps = 10000, replace = TRUE)\n\ndefault is sampling with replacement, but can be changed with replace = FALSE\nsize is the number of samples to draw\nrep is the number of times to repeat the sampling process"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#plotting-histograms-in-r",
    "href": "block_2/552_stat_inter/552_stat_inter.html#plotting-histograms-in-r",
    "title": "Statistical Inference",
    "section": "",
    "text": "pop_dist &lt;- multi_family_strata %&gt;%\n  ggplot(aes(x = current_land_value)) +\n  geom_histogram(bins = 50) +\n  xlab(\"current land value\") +\n  ggtitle(\"Population distribution\")"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#more-on-bootstrapping",
    "href": "block_2/552_stat_inter/552_stat_inter.html#more-on-bootstrapping",
    "title": "Statistical Inference",
    "section": "More on Bootstrapping",
    "text": "More on Bootstrapping\n\nMean of the bootstrap sample is an estimate of the sample mean not the population mean. (unlike the sampling distribution of the sample mean)\nSpread of bootstrap sample is of similar shape to the sampling distribution of the sample mean.\n\nThis is because we used a bootstrap sample size that was the same as the original sample size.\nIf sample size is larger than original sample: underestimate the spread.\nThis is because the empirical sample distribution is an estimate of the population distribution."
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#implementation-in-r",
    "href": "block_2/552_stat_inter/552_stat_inter.html#implementation-in-r",
    "title": "Statistical Inference",
    "section": "Implementation in R",
    "text": "Implementation in R\nset.seed(2485) # DO NOT CHANGE!\n\n# Take a sample from the population\nsample_1 &lt;- multi_family_strata |&gt;\n    rep_sample_n(size = 10) |&gt;\n\nbootstrap_means_10 &lt;- sample_1 |&gt;\n    ungroup() |&gt; # Remove grouping\n    # Removes the replicate column from rep_sample_n\n    select(current_land_value) |&gt; # Only select the column we want\n\n    # Bootstrap from the sample 2000 times\n    rep_sample_n(size = 10, reps = 2000, replace = T) |&gt;\n    # Calculate the mean for each bootstrap sample\n    group_by(replicate) |&gt;\n    summarise(mean_land_value = mean(current_land_value))\n\nbootstrap_means_10"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#calculating-confidence-intervals",
    "href": "block_2/552_stat_inter/552_stat_inter.html#calculating-confidence-intervals",
    "title": "Statistical Inference",
    "section": "Calculating Confidence Intervals",
    "text": "Calculating Confidence Intervals\n\nOne way : use the middle 95% of the distribution of bootstrap sample estimates to determine our endpoints. (2.5th and 97.5th percentiles)\nSpecific to a sample, not a population\nConfidence: yes or no on whether the interval contains the population parameter\nHigher sample size = narrower confidence interval\n\nIncreasing sample size increases the precision of our estimate\n\n\n\nWhat does 95% confidence Interval mean?\n\nIf we were to repeat this process over and over again and calculate the 95% CI many times,\n\n95% of the time expect true population parameter to be in the confidence interval\n\nThe confidence level is the percentage of time that the interval will contain the true population parameter if we were to repeat the process over and over again.\nOther confidence levels: 90%, 99%\n\nhigher confidence level, wider the interval = more likely to contain the population parameter - higher for use cases where we need to be more confident that the interval contains the population parameter (e.g. medical trials)\n\n\nUsing the null distribution, the \\(p\\)-value is the area to the right of \\(\\delta^*\\) and to the left of \\(-\\delta^*\\). In other words, the \\(p\\)-value is doubled for the two-tailed test. Conclusion: If we fail to reject the null hypothesis for a one-sided test, we would definitely not be able to reject it for a two-sided test."
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#steps-to-calculate-a-confidence-interval",
    "href": "block_2/552_stat_inter/552_stat_inter.html#steps-to-calculate-a-confidence-interval",
    "title": "Statistical Inference",
    "section": "Steps to calculate a confidence interval",
    "text": "Steps to calculate a confidence interval\n\nCalculate the true population parameter (e.g. mean), normally we don’t know this\nGet a sample from a population of size n\n\nset.seed(552) # For reproducibility.\nsample &lt;- rep_sample_n(listings, size = 40)\n\nLook at the sample and decide on parameter of interest for distribution (e.g. median)\nBootstrap the sample m times (e.g. 10,000 times) with replacement from the existing sample and get the required statistic (e.g. median) for each bootstrap sample.\n\n\nmust be the same size as the original sample\n\nset.seed(552) # For reproducibility.\nbootstrap_estimates &lt;- sample %&gt;%\n  specify(response = room_type, success = \"Entire home/apt\") %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"prop\")\nbootstrap_estimates\n\nGet the decided parameter of interest (e.g., median) for each of the bootstrap samples and make a distribution\nCalculate the confidence interval (e.g. 95%)\n\nuse infer::get_confidence_interval()\nreturns a tibble with the lower and upper bounds of the confidence interval\n\n\nget_confidence_interval(bootstrap_estimates, level = 0.90, type = \"percentile\")"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#fundamentals-of-hypothesis-testing",
    "href": "block_2/552_stat_inter/552_stat_inter.html#fundamentals-of-hypothesis-testing",
    "title": "Statistical Inference",
    "section": "Fundamentals of Hypothesis Testing",
    "text": "Fundamentals of Hypothesis Testing\n\nNull hypothesis: a statement of “no effect” or “no difference”\n\n\\(H_0: p_{control} - p_{variation} = \\delta = 0\\), where:\n\n\\(p_{control}\\): values of the control group\n\\(p_{variation}\\) values from the variation group\n\n\nAlternative hypothesis / \\(H_a\\): a statement of an effect or difference\n\n\\(H_a: p_{control} - p_{variation} = \\delta \\neq 0\\)\nclaim to seek statistical evidence for\n\nalpha: the probability of rejecting the null hypothesis when it is true, typically 0.05\n\nthe probability of a type I error\nthe probability of a false positive\n\\(\\alpha = P(\\text{reject } H_0 \\mid H_0 \\text{ is true})\\) \n\nProvided a strong enough statistical evidence, we can reject the null hypothesis and accept the alternative hypothesis\nObserved test statistic: \\(\\delta^*\\)\n\ne.g. \\(\\delta^* = \\hat{m}_{chinstrap} - \\hat{m}_{adelie}\\)\n\nwhere \\(\\hat{m}_{chinstrap}\\) is the sample mean (estimator) body mass of Chinstrap penguins"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#framework-for-hypothesis-testing-6-steps",
    "href": "block_2/552_stat_inter/552_stat_inter.html#framework-for-hypothesis-testing-6-steps",
    "title": "Statistical Inference",
    "section": "Framework for Hypothesis Testing (6 Steps)",
    "text": "Framework for Hypothesis Testing (6 Steps)\n\nDefine your null and alternative hypotheses.\n\nNull: The mean body mass of Chinstrap and Adelie are the same. \\(\\mu_{Chinstrap} - \\mu_{Adelie} = 0\\)\nAlt: The mean body mass of Chinstrap and Adelie are different. \\(\\mu_{Chinstrap} - \\mu_{Adelie} \\neq 0\\)\n\nwhere \\(\\mu_{Chinstrap}\\) is the population mean body mass of Chinstrap penguins\n\n\nCompute the observed test statistic coming from your original sample.\n\n\\(\\delta^* = \\hat{m}_{chinstrap} - \\hat{m}_{adelie}\\)\n\n\nchinstrap_adelie # data frame with chinstrap and adelie penguins only\n\nchinstrap_adelie_test_stat &lt;- chinstrap_adelie |&gt;\n  specify(formula = body_mass_g ~ species) |&gt;\n  calculate(\n  stat = \"diff in means\",\n  order = c(\"Chinstrap\", \"Adelie\")\n)\n\nSimulate the null hypothesis being true and calculate their corresponding test statistics.\n\ne.g. by randomly shuffling the data =&gt; any observed difference is due to chance\npermutation test: randomly shuffle the data and calculate the test statistic for each permutation (Without replacement)\nwould be a normal distribution about 0 (two-tailed test)\n\n\n# Running permutation test\nchinstrap_adelie_null_distribution &lt;- chinstrap_adelie |&gt;\n  specify(formula = body_mass_g ~ species) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  calculate(\n  stat = \"diff in means\",\n  order = c(\"Chinstrap\", \"Adelie\")\n  )\n\nGenerate the null distribution using these test statistics.\nObserve where the observed test statistic falls in the distribution\n\nif it falls in the extreme 5% of the distribution, we reject the null hypothesis\ni.e. if the p-value is less than \\(\\alpha\\), we reject the null hypothesis\n\nIf \\(\\delta\\) is near the extremes past some threshold defined with a significance level \\(\\alpha\\), we reject the null hypothesis. Otherwise, we fail to reject the null hypothesis."
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#p-value",
    "href": "block_2/552_stat_inter/552_stat_inter.html#p-value",
    "title": "Statistical Inference",
    "section": "P-Value",
    "text": "P-Value\n\nthe probability of observing a test statistic equally or more extreme than the one you observed, given that the null hypothesis is true\nuse get_p_value function from infer R library\n\nchinstrap_adelie_p_value &lt;- chinstrap_adelie_null_distribution|&gt;\n  get_p_value(chinstrap_adelie_test_stat, direction = \"both\")\n\nResults:\n\nReject Null Hypothesis if p-value &lt; \\(\\alpha\\)\nFail to Reject Null Hypothesis if p-value &gt; \\(\\alpha\\)"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#maximum-likelihood-estimation",
    "href": "block_2/552_stat_inter/552_stat_inter.html#maximum-likelihood-estimation",
    "title": "Statistical Inference",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\nDefinition:\n\nUsed to FIND estimators\nIDEA: Given a set of data and a statistical model (assumed pdf), MLE finds the parameter values that make the observed data most probable.\n\n\nSome key ideas:\n\nNeed to make distributional assumptions about the data, to specify the likelihood function\nNeed to consider nature (discrete/continuous) of the data\nGet the parameters that maximize the likelihood function\n\n\n\n\nLikelihood Function:\n\n\\(P(dataset | params) = l(\\lambda | y_1, y_2, ...)\\)\n\nprob mass/density function of the data given the parameters\n\npdf gives density not probability (area under curve == 1)\n\ngenerally: product of individual pdfs because iid\n\n\n\\[\\prod_{i=1}^n f_{Y_i}(y_i| \\lambda)\\]\n\nContext: Used when you have a specific set of outcomes (data) and you want to understand how likely different parameter values are, given that data.\n\nexample: Have a dataset of 3 obs (\\(y_1, y_2, y_3\\)) and want to know how likely it is that \\(\\lambda = 2\\). Assume exponential distribution.\n\\[l(\\lambda | y_1, y_2, y_3) = \\prod_{i=1}^3 f_{Y_i}(y_i| \\lambda) = \\prod_{i=1}^3 \\lambda e^{-\\lambda y_i}\\]\n\nin R: dexp(y, rate = lambda, log = FALSE)\n\n\n\nObjective of MLE:\n\nThe aim is to find the parameter values that maximize this likelihood function.\n\n\\[w \\in argmax\\{P(dataset | params)\\} = \\prod_{i=1}^3 f_{Y_i}(y_i| \\lambda)\\]\nbut values are very small, so we take the log of the likelihood function to simplify the math.\n\\[w \\in argmax\\{\\log(P(dataset | params))\\} = \\sum_{i=1}^n \\ln f_{Y_i}(y_i| \\lambda) \\]\n\n\nProcedure:\n\nChoose likelihood function\nTake the log of the likelihood function\nDifferentiate the log-likelihood function with respect to the parameters to get max\nSolve for the parameters\n\nin R can do this:\nexp_values &lt;- tibble(\n  possible_lambdas = seq(0.01, 0.2, 0.001),\n  likelihood = map_dbl(possible_lambdas, ~ prod(dexp(sample_n30$values, .))),\n  log_likelihood = map_dbl(possible_lambdas, ~ log(prod(dexp(sample_n30$values, .))))\n)\n\nR optim::optimize()\n\noptimize(f, interval, maximum = TRUE)\n\nf: function to be optimized\ninterval: vector of length 2 giving the start and end of the interval\nmaximum: logical, should the function be maximized?\n\ne.g.\n# Log-likelihood function\nLL &lt;- function(l) log(prod(dexp(sample$values, l)))\n\noptimize(LL, c(0.01, 0.2), maximum = TRUE)\n\n\n\n\nProperties of MLE:\n\nConsistency: As the sample size grows, the MLE converges in probability to the true parameter value.\nAsymptotic Normality: For many models, as the sample size grows large, the distribution of the MLE approaches a normal distribution.\nEfficiency: Among the class of consistent estimators, MLE often has the smallest variance (is the most efficient) under certain regularity conditions.\nMLE can be biased, but it is asymptotically unbiased (i.e. as the sample size increases, the bias goes to 0)\n\n\n\nLimitations:\n\nRequires a specified model for the underlying data distribution. If the model is incorrect, MLE can give biased estimates.\nComputation can be challenging, especially for complex models or when there’s no closed-form solution."
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#central-limit-theorem",
    "href": "block_2/552_stat_inter/552_stat_inter.html#central-limit-theorem",
    "title": "Statistical Inference",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nPopulation and Sampling\n\nLets say population of N samples, each with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nGet a sample of n iid random samples \\(X_1, X_2, ..., X_n\\) from the population.\n\n\\[ \\bar{X} = \\frac{1}{n} \\sum*{i=1}^n X_i \\] \\[ S^2 = \\frac{1}{n-1} \\sum*{i=1}^n (X_i - \\bar{X})^2 \\]\n\nn-1 is to make it unbiased\n\n\n\nDefinition:\n\\(\\bar{X} \\dot\\sim N(\\mu, \\frac{\\sigma^2}{n})\\) as \\(n \\rightarrow \\infty\\)\n\nThe sampling distribution of the sample mean of a population distribution converges to a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\) as the sample size \\(n\\) gets larger when we sample with replacement. \nNOTE: standard deviation of the sampling distribution of the sample mean is called the standard error of the sample mean.\n\n\n\nAssumptions\n\nThe sample size is large enough (at least 30)\n\nunless the population is normal\n\nThe sample is drawn from a population with a finite variance and mean.\nThe samples are iid (independent and identically distributed) from the population.\nEach category in the population should have a sufficient number of samples. (e.g. #success at least 10)\n\nWhen we do inference using the CLT, we required a large sample for two reasons:\n\nThe sampling distribution of \\(\\bar{X}\\) tends to be closer to normal when the sample size is large.\nThe calculated standard error is typically very accurate when using a large sample.\n\n\n\nConfidence Interval using CLT\n\\[ CI = Point \\; Estimate \\pm Margin \\; of \\; Error \\]\n\\[ MoE = Z_x \\times SE = Z_x \\times \\frac{\\sigma}{\\sqrt{n}} \\]\nwhere \\(Z_x\\) is the z-score for the desired confidence level.\n\n95% confidence level: \\(Z_x = Z_{0.025} = 1.96\\)\n\nrecal in R: qnorm(0.975) = 1.96\n\n\n\n\nConfidence Interval for Proportions\n\nsample proportion: \\(\\hat{p} = \\frac{\\sum{X_i}}{n}\\)\n\nit is a sample mean of 0 and 1\n\nRule of thumb:\n\nsuccess: at least 10\nfailure: at least 10\n\n\nCI for proportions:\n\\[ \\hat{p} \\pm Z_x \\times \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\]\nwhere standard error is \\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) since the variance of a bernoulli distribution is \\(p(1-p)\\)"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#hypothesis-testing-via-normal-and-t-testing",
    "href": "block_2/552_stat_inter/552_stat_inter.html#hypothesis-testing-via-normal-and-t-testing",
    "title": "Statistical Inference",
    "section": "Hypothesis Testing via normal and T-testing",
    "text": "Hypothesis Testing via normal and T-testing\n\nSimple confidence interval Test\n\nIf we want to check if mean of population is equal to a value\n\nCan reasonably conclude that the population mean is not equal to the value if the value is not in the confidence interval\nIf the value is in the confidence interval, we cannot conclude that the population mean is equal to the value\n\nNEED TO DO A HYPOTHESIS TEST\n\n\n\n\n\nHypothesis testing\n\nPermutation test (can work on any estimator)\nStudent’s t-test (only works on sample mean)\nProportion test (only works on sample proportion)\n\nNote: All hypothesis tests are done under the null hypothesis\n\n\nP-Value\n\nthe probability of observing a test statistic equally or more extreme than the one you observed, given that the null hypothesis is true\n\nsmall p-value: observed test statistic is unlikely under the null hypothesis\n\nuse get_p_value function from infer R library\nResults:\n\nReject Null Hypothesis if p-value &lt; \\(\\alpha\\)\nFail to Reject Null Hypothesis if p-value &gt; \\(\\alpha\\)\n\n\n\nR code examples\n\nget p-value of normal distribution: pnorm(Z, mean = 0, sd = 1, lower.tail = FALSE)\n\n\n\n\nPermutation Hypothesis test (6 Steps)\n\nDefine your estimator (mean, median, sd, etc.)\nDefine your null and alternative hypotheses. (population)\nCompute the observed test statistic (sample) coming from your original sample.\nSimulate the null hypothesis being true and calculate their corresponding test statistics.\n\ne.g. by randomly shuffling the data =&gt; any observed difference is due to chance\nwould be a normal distribution about 0 (two-tailed test)\n\nGenerate the null distribution using these test statistics.\nObserve where the observed test statistic falls in the distribution\n\nif it falls in the extreme 5% of the distribution, we reject the null hypothesis\ni.e. if the p-value is less than \\(\\alpha\\), we reject the null hypothesis\n\nIf \\(\\delta\\) is near the extremes past some threshold defined with a significance level \\(\\alpha\\), we reject the null hypothesis. Otherwise, we fail to reject the null hypothesis.\n\n\n\nProportion test\n\nDefine test statistic \\(\\delta = p_{control} - p_{variation}\\)\n\n\nAlso define null and alternative hypothesis\n\n\nDefine theory-based CLT test statistic \\[Z = \\frac{\\hat{p}_{control} - \\hat{p}_{variation}}{\\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_C} + \\frac{1}{n_V})}}\\] where: \\[\\hat{p} = \\frac{\\sum_{i=1}^{n_C} X_{iC} + \\sum_{i=1}^{n_V} X_{iV}}{n_C + n_V}\\]\n\nnumer: sample stat + effect size (difference in proportions)\ndenom: standard error of 2 sample\n\nSimulates to Normal distribution with mean 0 and standard deviation\n\n\nFind p-value and see if it is less than \\(\\alpha\\)\nOr find the CI and see if Z is in the CI\n\nIf Z is in the CI, we fail to reject the null hypothesis\n\n\n\nproportion test in R\n\nuse prop.test() function in R\n\nx is the number of successes in Bernoulli trials\nn is the number of trials\nalternative is the alternative hypothesis\n\nless, greater, two.sided\n\nconf.level is the confidence level\ncorrect = FALSE means we are using the normal approximation\n\nif correct = TRUE, we are using the exact binomial test\n\n\n\nprop.test(x = click_summary$success, n = click_summary$n,\n  correct = FALSE, alternative = \"less\"\n)\n\n\n\nPearson’s Chi-Squared Test\n\nTo identify whether two categorical variables are independent or not\n\nSteps:\n\nMake a contingency table\n\nlibrary(janitor)\n\n# Makes table with webpage, n, and num_success as columns\ncont_table_AB &lt;- click_through %&gt;%\n  tabyl(webpage, click_target)\n\nDo some calulations, e.g. for 2x2 table:\n\n\\[\\chi^2 = \\sum_{i=1}^2 \\sum_{j=1}^2 \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\]\n\n\nChi-Squared Test in R\nchisq.test(cont_table_AB, correct = FALSE)\nwhere:\n\n\nT Test\n\nUses the CLT to approximate the sampling distribution of the sample mean\n\nONLY works on sample mean (because CLT only works on sample mean)\nONLY works on continuous data (because CLT only works on continuous data)\ncan be any distribution, because CLT makes it normal\n\nBasically similar to normal test but we have small sample size\nDegrees of freedom = n - 1 where n is the sample size\n\nas df increases, the t-distribution approaches the standard normal distribution\n\nTest Statistic = (sample mean - population mean) / standard error\n\nstandard error = standard deviation / sqrt(n)\nmore formally: \\[t = \\frac{\\bar{x} - \\mu}{s/\\sqrt{n}}\\]\n\nnumerator: observed difference in sample means\ndenominator: standard error of the sampling distribution of the two-sample difference in means for the sample size we collected.\n\n\nOne sample t-test\n\nNull hypothesis: the population mean is equal to a value \\(\\mu = \\mu_0\\)\nTest statistic: \\[t = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}\\]\n\ns : sample standard deviation\n\nIn R use t.test() function\n\nx is the sample data\nmu is the value of the population mean\nalternative is the alternative hypothesis\n\nless, greater, two.sided\n\nconf.level is the confidence level\ncorrect = FALSE means we are using the normal approximation\n\nif correct = TRUE, we are using the exact binomial test\n\n\n\n\n\nTwo sample t-test\n\nNull hypothesis: the population mean of two groups are equal \\(\\delta = \\mu_1 - \\mu_2 = 0\\)\nTest statistic (if equal variance, $ &lt; 2):\n\n\\[t = \\frac{(\\bar{x}_1 - \\bar{x}_2) - \\delta_0}{S_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]\n\n\\(S_p\\) is the pooled standard deviation \\[S_p = \\sqrt{\\frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}}\\]\ndegrees of freedom: \\(df = n_1 + n_2 - 2\\)\nin R, use t.test() function\n\nformula is the formula for the test statistic variable ~ group\ndata is the data frame\nmu hypothesis value of \\(\\delta\\)\nalternative is the alternative hypothesis\n\nless, greater, two.sided\n\nconf.level is the confidence level\nvar.equal = TRUE means we are assuming equal variance\n\nif var.equal = FALSE, we are assuming unequal variance\n\n\n\n\n\n\nComparison with simulation hypothesis test\n\n\n\n\n\n\n\n\n\nTest Type\nFactor\nImpact on Distribution / Statistic\nImpact on p-value\n\n\n\n\nSimulation Hypothesis\nSample Size\nIncrease =&gt; narrower null distribution\nDecrease\n\n\n\nSample Variance\nIncrease =&gt; increase simulated null variance\nIncrease\n\n\n\nEffect Size\nIncrease =&gt; more extreme test statistic\nIncrease (more likely to reject null)\n\n\nNormal/T-Testing\nEffect Size\nIn numerator of test statistic\nIncrease (if other factors remain same)\n\n\n\nSample Variance\nIncrease =&gt; increase standard error\nIncrease\n\n\n\nSample Size\nIncrease =&gt; decrease standard error\nDecrease"
  },
  {
    "objectID": "block_2/552_stat_inter/552_stat_inter.html#errors-in-inference",
    "href": "block_2/552_stat_inter/552_stat_inter.html#errors-in-inference",
    "title": "Statistical Inference",
    "section": "Errors in Inference",
    "text": "Errors in Inference\n\nTypes of Errors\n\nType I error: False positive $ P( H_0 | H_0 ) = $ \nType II error: False negative $ P( H_0 | H_0 ) = $\n\n\n\n\n\n\n\n\n\nDecision\nTrue Condition Positive\nTrue Condition Negative\n\n\n\n\nTest Positive\nCorrect (True Positive)\nType I Error (False Positive)\n\n\nTest Negative\nType II Error (False Negative)\nCorrect (True Negative)\n\n\n\n\n\nVisual representation of errors\n\nParameters:\n\n\\(\\beta\\), power = 1 - \\(\\beta\\)\n\\(\\alpha\\): sets type I error rate (how often we reject the null hypothesis when it is true)\ncohen’s d : effect size\n\n\\(d = \\frac{\\mu_1 - \\mu_2}{\\sigma}\\), where:\n\n\\(\\mu_1\\): mean of group 1\n\\(\\mu_2\\): mean of group 2\n\\(\\sigma\\): standard deviation of the population"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html",
    "href": "block_2/531_viz/531_viz.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Easier to understand data when it is visualized\nAnscombe’s quartet: 4 datasets with the same mean, variance, correlation, and linear regression line but look very different when plotted\nWe are using high-level declarative (altair, ggplot) instead of low-level imperative (matplotlib) \n\n\n\n\n\ngrammar: alt.Chart(data).mark_type().encode(x,y).properties()\nexample gallery: https://altair-viz.github.io/gallery/index.html\n\nimport altair as alt\n\n# to unlimit the number of rows displayed (5000 by default)\nalt.data_transformers.enable('vegafusion')\n\nalt.Chart(cars, title=\"My plot title\").mark_line(opacity=0.6).encode(\n    x=alt.X(\"Year:T\"), # encoders\n    y=alt.Y(\"mean(Miles_per_Gallon)\"),  # same as doing `y=cars.groupby('Year')['Miles_per_Gallon'].mean()`\n    color=alt.Color(\"Origin\"),\n).facet(\n    \"region\",\n    columns=2, # number of columns in the facet\n)\n\n\n\n\n\n\n\n\n\n\n\nMark Type\nDescription\nExample Use Case\n\n\n\n\nmark_area\nFilled area plots.\nDisplay quantitative data over an interval.\n\n\nmark_bar\nBar charts.\nCompare discrete categories or time intervals.\n\n\nmark_circle, mark_square, mark_geoshape\nGeometric shape marks.\nVisualize point data in 2D space (circle, square) or geographic regions and shapes (geoshape).\n\n\nmark_line\nLine charts.\nShow trends over a continuous domain.\n\n\nmark_point\nScatter plots with customizable point shapes.\nVisualize point data with various symbols.\n\n\nmark_rect\nRectangular marks, used in heatmaps.\nDisplay data in 2D grid-like structures.\n\n\nmark_boxplot\nBoxplots.\nSummarize data distribution with quartiles.\n\n\n\n\nno violin plot but can do mark_circle with y and color the same variable and size as count().\npandas explode df.explode() to explode a column of lists into multiple rows\n\n\n\n\nalt.X(\"Year:T\") is the same as alt.X(\"Year\", type=\"temporal\")\ncan do alt.X().scale() to change the scale of the axis\n\n.stack() to stack the axis\n.scale() to change the scale\n\n.scale(type=\"log\") to change to log scale\n.scale(range=(0, 100)) to change the domain\n.scale(zero=False) to not include 0 in the domain\n\n.bin() to bin the axis, makes histogram if used on mark_bar()\n\ndefault: stack=True\ntakes in binwidth or maxbins\n\n.sort() to sort the axis\n\n.sort(“x”) to sort by x (ascending)\n.sort(“-x”) to sort by x (descending)\n\n.title(\"\"): change the title of the axis\n\n\n\n\n\n\n\n\nData Type\nShorthand Code\nDescription\n\n\n\n\nquantitative\nQ\na continuous real-valued quantity\n\n\nordinal\nO\na discrete ordered quantity\n\n\nnominal\nN\na discrete unordered category\n\n\ntemporal\nT\na time or date value\n\n\ngeojson\nG\na geographic shape\n\n\n\nhttps://altair-viz.github.io/user_guide/encodings/index.html#encoding-data-types\n\n\n\nalt.Chart(movies).mark_point(opacity=0.3, size=10).encode(\n     alt.X(alt.repeat('row')).type('quantitative'),\n     alt.Y(alt.repeat('column')).type('quantitative')\n).properties(\n    width=200,\n    height=200\n).repeat(\n    column=['runtime', 'revenue', 'budget'],\n    row=['runtime', 'revenue', 'budget']\n)\n\n\n\ncorr_df = (\n    movies\n    .corr('spearman', numeric_only=True)\n    .abs()                      # Use abs for negative correlation to stand out\n    .stack()                    # Get df into long format for altair\n    .reset_index(name='corr'))  # Name the index that is reset to avoid name collision\n\nalt.Chart(corr_df).mark_circle().encode(\n    x='level_0',\n    y='level_1',\n    size='corr',\n    color='corr'\n)\n\n\n\n\n\ngrammar: ggplot(data, aes(x, y)) + geom_type() + facet_wrap() + theme()\n\nread_csv(url) |&gt;\n    ggplot(aes(x = Year, y = Miles_per_Gallon, color = Origin)) +\n    geom_line(stat = \"summary\", fun = mean, alpha = 0.6) +\n    # stat_summary(geom = \"line\", fun = mean) # alternative way of doing the same thing\n    facet_wrap(~ region, ncol = 2) +\n    # properties\n    ggtitle(\"My plot title\") +\n    labs(x = \"Year\", y = \"Miles per Gallon\")\n\nin ggplot, fill and color are different\n\nfill is the color of the inside of the shape (bars, area, etc.)\ncolor is the color of the outline of the shape (points, lines, etc.)\n\n\n\n\n\n\n\n\n\n\n\n\nGeom Function\nDescription\nExample Use Case\n\n\n\n\ngeom_area\nFilled area plots.\nDisplay quantitative data over an interval.\n\n\ngeom_bar & geom_col\nBar charts.\nCompare discrete categories. geom_col is a special case of geom_bar where height is pre-determined.\n\n\ngeom_point, geom_tile, geom_polygon\nGeometric shape marks.\ngeom_point for scatter plots, geom_tile for heatmaps, and geom_polygon for custom shapes.\n\n\ngeom_line\nLine charts.\nShow trends over a continuous domain.\n\n\ngeom_smooth\nAdds a smoothed conditional mean.\nFit and display a trend line for scattered data.\n\n\ngeom_boxplot\nBoxplots.\nSummarize data distribution with quartiles.\n\n\ngeom_histogram\nHistograms.\nDisplay frequency distributions.\n\n\ngeom_text & geom_label\nDisplay text and labels.\nAnnotate plots with text or labeled rectangles.\n\n\ngeom_jitter\nPoints with a small amount of random noise.\nDisplay individual data points without overlap.\n\n\ngeom_path\nConnect observations in the order they appear.\nDisplay paths or trajectories.\n\n\ngeom_violin\nViolin plots.\nCombine boxplot and kernel density estimation.\n\n\ngeom_rug\nMarginal rug plots.\nDisplay 1D data distribution on plot margins.\n\n\n\n\n\n\n\nreorder() to reorder the axis\n\ngm2018 %&gt;%\n    add_count(region) %&gt;%\n    ggplot(aes(y = reorder(region, n))) + # y is region, reorder by its count (n)\n    geom_bar()\n\n\n\n\nRepeated Charts\n\nlibrary(GGally)\nGGally::ggpairs(movies %&gt;% select_if(is.numeric), progress = FALSE)\n\nCorrelation plot\n\nGGally::ggcorr(movies)\n\n\n\n\n\nFaceting displays groups based on a dataset variable in separate subplots.\nIt’s useful for splitting data over additional categorical variables.\nAvoids overloading a single chart with too much information.\nUsing color groupings in a histogram can obscure differences between groups.\nFaceting each group in separate subplots clarifies distribution comparisons.\n\n\n\n\n\n\n\n\n\n\n\nshows only 1 value of distribution, can show 3 with error bars:\n\nDot plots\nBar plots\n\n\n\n\n\nbox plots: normally shows 5 values (min, max, median, 25th percentile, 75th percentile)\nHistograms\n\ncons: binning required and distribution sensitive to bin settings\n\nKDE\n\nshows density of data points\ngives a smooth curve because uses “gaussian” (default) centered at each data point\nNo binning required (not sensitive to bin width or bin location)\nless affected by sparse data\ncons: y-axis is density, not count (hard to interpret)\n\nViolin plots: mirror of density estimates (KDE)\n\nmore would show higher density, unlike box plots (smaller box plots would show higher density)\ncons: over-smoothing of data (when too little data points)\n\n\n\n\n\n\nscatter plots: shows all data points (can overlap and saturate)\njitter plots: scatter plots with jittering over a small width in y-axis\nraincloud plots: violin plots with jittering\n\n\n\n\n\n\n\nalt.Chart(df).transform_density('x', as_=['x', 'density']).mark_area().encode(x='x:Q', y='density:Q')\nalt.Chart(df).explode('col_name'): explode a column (of list normally) into multiple rows"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#intro-to-data-visualization",
    "href": "block_2/531_viz/531_viz.html#intro-to-data-visualization",
    "title": "Data Visualization",
    "section": "",
    "text": "Easier to understand data when it is visualized\nAnscombe’s quartet: 4 datasets with the same mean, variance, correlation, and linear regression line but look very different when plotted\nWe are using high-level declarative (altair, ggplot) instead of low-level imperative (matplotlib)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#plotting-in-python-altair",
    "href": "block_2/531_viz/531_viz.html#plotting-in-python-altair",
    "title": "Data Visualization",
    "section": "",
    "text": "grammar: alt.Chart(data).mark_type().encode(x,y).properties()\nexample gallery: https://altair-viz.github.io/gallery/index.html\n\nimport altair as alt\n\n# to unlimit the number of rows displayed (5000 by default)\nalt.data_transformers.enable('vegafusion')\n\nalt.Chart(cars, title=\"My plot title\").mark_line(opacity=0.6).encode(\n    x=alt.X(\"Year:T\"), # encoders\n    y=alt.Y(\"mean(Miles_per_Gallon)\"),  # same as doing `y=cars.groupby('Year')['Miles_per_Gallon'].mean()`\n    color=alt.Color(\"Origin\"),\n).facet(\n    \"region\",\n    columns=2, # number of columns in the facet\n)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#different-mark_types",
    "href": "block_2/531_viz/531_viz.html#different-mark_types",
    "title": "Data Visualization",
    "section": "",
    "text": "Mark Type\nDescription\nExample Use Case\n\n\n\n\nmark_area\nFilled area plots.\nDisplay quantitative data over an interval.\n\n\nmark_bar\nBar charts.\nCompare discrete categories or time intervals.\n\n\nmark_circle, mark_square, mark_geoshape\nGeometric shape marks.\nVisualize point data in 2D space (circle, square) or geographic regions and shapes (geoshape).\n\n\nmark_line\nLine charts.\nShow trends over a continuous domain.\n\n\nmark_point\nScatter plots with customizable point shapes.\nVisualize point data with various symbols.\n\n\nmark_rect\nRectangular marks, used in heatmaps.\nDisplay data in 2D grid-like structures.\n\n\nmark_boxplot\nBoxplots.\nSummarize data distribution with quartiles.\n\n\n\n\nno violin plot but can do mark_circle with y and color the same variable and size as count().\npandas explode df.explode() to explode a column of lists into multiple rows\n\n\n\n\nalt.X(\"Year:T\") is the same as alt.X(\"Year\", type=\"temporal\")\ncan do alt.X().scale() to change the scale of the axis\n\n.stack() to stack the axis\n.scale() to change the scale\n\n.scale(type=\"log\") to change to log scale\n.scale(range=(0, 100)) to change the domain\n.scale(zero=False) to not include 0 in the domain\n\n.bin() to bin the axis, makes histogram if used on mark_bar()\n\ndefault: stack=True\ntakes in binwidth or maxbins\n\n.sort() to sort the axis\n\n.sort(“x”) to sort by x (ascending)\n.sort(“-x”) to sort by x (descending)\n\n.title(\"\"): change the title of the axis\n\n\n\n\n\n\n\n\nData Type\nShorthand Code\nDescription\n\n\n\n\nquantitative\nQ\na continuous real-valued quantity\n\n\nordinal\nO\na discrete ordered quantity\n\n\nnominal\nN\na discrete unordered category\n\n\ntemporal\nT\na time or date value\n\n\ngeojson\nG\na geographic shape\n\n\n\nhttps://altair-viz.github.io/user_guide/encodings/index.html#encoding-data-types\n\n\n\nalt.Chart(movies).mark_point(opacity=0.3, size=10).encode(\n     alt.X(alt.repeat('row')).type('quantitative'),\n     alt.Y(alt.repeat('column')).type('quantitative')\n).properties(\n    width=200,\n    height=200\n).repeat(\n    column=['runtime', 'revenue', 'budget'],\n    row=['runtime', 'revenue', 'budget']\n)\n\n\n\ncorr_df = (\n    movies\n    .corr('spearman', numeric_only=True)\n    .abs()                      # Use abs for negative correlation to stand out\n    .stack()                    # Get df into long format for altair\n    .reset_index(name='corr'))  # Name the index that is reset to avoid name collision\n\nalt.Chart(corr_df).mark_circle().encode(\n    x='level_0',\n    y='level_1',\n    size='corr',\n    color='corr'\n)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#plotting-in-r-ggplot2",
    "href": "block_2/531_viz/531_viz.html#plotting-in-r-ggplot2",
    "title": "Data Visualization",
    "section": "",
    "text": "grammar: ggplot(data, aes(x, y)) + geom_type() + facet_wrap() + theme()\n\nread_csv(url) |&gt;\n    ggplot(aes(x = Year, y = Miles_per_Gallon, color = Origin)) +\n    geom_line(stat = \"summary\", fun = mean, alpha = 0.6) +\n    # stat_summary(geom = \"line\", fun = mean) # alternative way of doing the same thing\n    facet_wrap(~ region, ncol = 2) +\n    # properties\n    ggtitle(\"My plot title\") +\n    labs(x = \"Year\", y = \"Miles per Gallon\")\n\nin ggplot, fill and color are different\n\nfill is the color of the inside of the shape (bars, area, etc.)\ncolor is the color of the outline of the shape (points, lines, etc.)\n\n\n\n\n\n\n\n\n\n\n\n\nGeom Function\nDescription\nExample Use Case\n\n\n\n\ngeom_area\nFilled area plots.\nDisplay quantitative data over an interval.\n\n\ngeom_bar & geom_col\nBar charts.\nCompare discrete categories. geom_col is a special case of geom_bar where height is pre-determined.\n\n\ngeom_point, geom_tile, geom_polygon\nGeometric shape marks.\ngeom_point for scatter plots, geom_tile for heatmaps, and geom_polygon for custom shapes.\n\n\ngeom_line\nLine charts.\nShow trends over a continuous domain.\n\n\ngeom_smooth\nAdds a smoothed conditional mean.\nFit and display a trend line for scattered data.\n\n\ngeom_boxplot\nBoxplots.\nSummarize data distribution with quartiles.\n\n\ngeom_histogram\nHistograms.\nDisplay frequency distributions.\n\n\ngeom_text & geom_label\nDisplay text and labels.\nAnnotate plots with text or labeled rectangles.\n\n\ngeom_jitter\nPoints with a small amount of random noise.\nDisplay individual data points without overlap.\n\n\ngeom_path\nConnect observations in the order they appear.\nDisplay paths or trajectories.\n\n\ngeom_violin\nViolin plots.\nCombine boxplot and kernel density estimation.\n\n\ngeom_rug\nMarginal rug plots.\nDisplay 1D data distribution on plot margins.\n\n\n\n\n\n\n\nreorder() to reorder the axis\n\ngm2018 %&gt;%\n    add_count(region) %&gt;%\n    ggplot(aes(y = reorder(region, n))) + # y is region, reorder by its count (n)\n    geom_bar()\n\n\n\n\nRepeated Charts\n\nlibrary(GGally)\nGGally::ggpairs(movies %&gt;% select_if(is.numeric), progress = FALSE)\n\nCorrelation plot\n\nGGally::ggcorr(movies)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#faceting",
    "href": "block_2/531_viz/531_viz.html#faceting",
    "title": "Data Visualization",
    "section": "",
    "text": "Faceting displays groups based on a dataset variable in separate subplots.\nIt’s useful for splitting data over additional categorical variables.\nAvoids overloading a single chart with too much information.\nUsing color groupings in a histogram can obscure differences between groups.\nFaceting each group in separate subplots clarifies distribution comparisons."
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#most-basic-fast-to-make",
    "href": "block_2/531_viz/531_viz.html#most-basic-fast-to-make",
    "title": "Data Visualization",
    "section": "",
    "text": "shows only 1 value of distribution, can show 3 with error bars:\n\nDot plots\nBar plots"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#a-bit-better",
    "href": "block_2/531_viz/531_viz.html#a-bit-better",
    "title": "Data Visualization",
    "section": "",
    "text": "box plots: normally shows 5 values (min, max, median, 25th percentile, 75th percentile)\nHistograms\n\ncons: binning required and distribution sensitive to bin settings\n\nKDE\n\nshows density of data points\ngives a smooth curve because uses “gaussian” (default) centered at each data point\nNo binning required (not sensitive to bin width or bin location)\nless affected by sparse data\ncons: y-axis is density, not count (hard to interpret)\n\nViolin plots: mirror of density estimates (KDE)\n\nmore would show higher density, unlike box plots (smaller box plots would show higher density)\ncons: over-smoothing of data (when too little data points)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#showing-all-data",
    "href": "block_2/531_viz/531_viz.html#showing-all-data",
    "title": "Data Visualization",
    "section": "",
    "text": "scatter plots: shows all data points (can overlap and saturate)\njitter plots: scatter plots with jittering over a small width in y-axis\nraincloud plots: violin plots with jittering"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#plotting-density",
    "href": "block_2/531_viz/531_viz.html#plotting-density",
    "title": "Data Visualization",
    "section": "",
    "text": "alt.Chart(df).transform_density('x', as_=['x', 'density']).mark_area().encode(x='x:Q', y='density:Q')\nalt.Chart(df).explode('col_name'): explode a column (of list normally) into multiple rows"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#some-notes",
    "href": "block_2/531_viz/531_viz.html#some-notes",
    "title": "Data Visualization",
    "section": "Some notes",
    "text": "Some notes\n\nsame data can give different messages\nkeep same color mapping for same categories across plots\nlabels horizontally &gt; vertically\nPie charts aint it when:\n\ncategories &gt; 3\ncomparing &gt; 1 pie charts\n\nDon’t have too many lines in a line chart\n\nunless you gray out the lines and highlight the one you want to show\n\n\n\nOverplotting\n\nToo many points in a plot\nSome solutions:\n\nlower size\nlower opacity (alpha in ggplot)\n\nIf you have too many points, you can use a heatmap\n\nggplot: gromm::geom_bin2d(bins=40) or gromm::geom_hex(bins=40)\n\nhex is preferred because better granularity\n\n\nmatch group colours in different charts\n\nggplot(diamonds) +\n    aes(x=carat, y=price) +\n    geom_hex(bins=40)\n\naltair:\n\nalt.Chart(diamonds).mark_rect().encode(\n  alt.X('carat', bin=alt.Bin(maxbins=40)),\n  alt.Y('price', binsalt.Bin(maxbins=40)),\n  alt.Color('count()'))\n\n\nAxes Formatting\n\nPython Altair:\n\n# clip=True to remove points outside of the plot axis\nalt.Chart(diamonds,\n        title='Diamonds',\n        subtitle='Price vs. Carat'\n    ).mark_rect(clip=True).encode(\n  alt.X('carat',\n    bin=alt.Bin(maxbins=40),\n    title='Carat', # set the axis title, blank if no title\n    scale=alt.Scale(domain=(0, 3)), # set the axis extent\n    reverse=True # reverse=True to flip the axis (largest value on the left)\n    # axis=None # remove the axis (no labels no titles)\n    ),\n  alt.Y('price',\n    binsalt.Bin(maxbins=40),\n    title='Price',\n    scale=alt.Scale(domain=(0, 2000)),\n    axis=alt.Axis(format='$s') # set the axis format 1000 -&gt; $1.0k\n    ),\n  alt.Color('count()'))\n\nR ggplot:\n\nggplot(diamonds) +\n    aes(x=carat, y=price) +\n    geom_hex(bins=40) +\n    scale_x_continuous(\n        limits=c(0, 3),\n        expand=c(0, 0)) + # expand=c(0, 0) to remove padding [c(mult, add)] for both sides\n    scale_y_continuous(\n        limits=c(0, 2000),\n        trans=\"reverse\", # to flip the axis\n        labels=scales::label_dollar()) + # to format the axis labels\n    labs(x=\"Carat\", y=\"Price\", fill=\"Number\", # to set the axis labels\n        title=\"Diamonds\", subtitle=\"Price vs. Carat\") + # to set the title and subtitle\n    theme(axis.title.x=element_blank(), # to remove the axis title\n        axis.text.x=element_blank(), # to remove the axis labels\n        axis.ticks.x=element_blank()) +# to remove the axis ticks\n    # theme_void() # to remove everything (no labels no titles)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#trendlines",
    "href": "block_2/531_viz/531_viz.html#trendlines",
    "title": "Data Visualization",
    "section": "Trendlines",
    "text": "Trendlines\n\n(Rolling) Mean: average of a subset of data\n\nto communicate to general public\n\nLinear Regression: line of best fit\n\nfor extrapolation\n\npoints +  points.mark_line(size=3).transform_regression(\n  'Year',\n  'Horsepower',\n  groupby=['Origin'],\n  method='poly', # 'linear' (default), 'log', 'exp', 'pow', 'quad', 'poly'\n)\nLoess: linear regression with a moving window (subset of data)\n\npython:\n\n\npoints +  points.mark_line(size=3).transform_loess(\n    'Year',\n    'Horsepower',\n    groupby=['Origin'],\n    bandwidth=0.8, # 0.3 (default), 1 (max is similar to linear regression)\n)\n\nR:\n\nggplot(cars) +\n    aes(x = Year,\n        y = Horsepower,\n        color = Origin,\n        fill = Origin) + # fill the CI area around the line\n    geom_point() +\n    geom_smooth(se = FALSE,\n        span = 0.8, # similar to bandwidth\n        method = \"loess\") # method = \"lm\" for linear regression"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#some-general-rules",
    "href": "block_2/531_viz/531_viz.html#some-general-rules",
    "title": "Data Visualization",
    "section": "Some general rules",
    "text": "Some general rules\n\nin Altair: `alt.Color(‘species’).scale(scheme=‘dark2’, reverse=True)\nin ggplot: scale_color_brewer(palette='Dark2')\n\n\n\n\n\n\n\n\n\n\nData Type\nVariation\nColor Map Type\nExample\n\n\n\n\nNumerical\nVary value\nSequential (Perceptually uniform)\nNumber of people (0-100)\n\n\nCategorical\nVary hue\nCategorical\nSubject (math, science, english)\n\n\nOrdered\nVary hue and value\nNot specified\nLow, medium, high\n\n\nCyclic\nVary hue and value\nCyclic\nDegrees (0-360) or days of week (0-6)\n\n\nData with natural center\nNot specified\nDiverging\nTemperature (0-100)\n\n\n\n \n \n\nAlso consider intuitive colors\n\n(e.g. red for hot, blue for cold)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#add-annotations",
    "href": "block_2/531_viz/531_viz.html#add-annotations",
    "title": "Data Visualization",
    "section": "Add annotations",
    "text": "Add annotations\n\nin Altair:\n\nbars = alt.Chart(wheat).mark_bar().encode(\n    x='year:O',\n    y=\"wheat\",\n    color=alt.Color('highlight').legend(None)\n)\nbars + bars.mark_text(dy=-5).encode(text='wheat')\n\n# Or for line plot\nlines = alt.Chart(stocks).mark_line().encode(\n    x='date',\n    y='price',\n    color=alt.Color('symbol').legend(None)\n)\n\ntext = alt.Chart(stock_order).mark_text(dx=20).encode(\n    x='date',\n    y='price',\n    text='symbol',\n    color='symbol'\n)\n\nlines + text\n\nin ggplot:\n\nggplot(wheat) +\n    aes(x = year,\n        y = wheat,\n        fill = highlight,\n        label = wheat) +\n    geom_bar(stat = 'identity', color = 'white') +\n    geom_text(vjust=-0.3)\n\n# Or for line plot\nggplot(stocks) +\n    aes(x = date,\n        y = price,\n        color = symbol,\n        label = symbol) +\n    geom_line() +\n    geom_text(data = stock_order, vjust=-1) +\n    ggthemes::scale_color_tableau() +\n    theme(legend.position = 'none')\n\nFrequency Framing\n\npeople normally judge probabilities wrongly\nNormalize the counts and show each individual count"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#error-bars",
    "href": "block_2/531_viz/531_viz.html#error-bars",
    "title": "Data Visualization",
    "section": "Error bars",
    "text": "Error bars\n\nSpecify what kind of error bar\n\nmin, max\nstd dev\nstd error\n95% confidence interval\n\nin Altair:\n\npoints.mark_errorband(extent='ci') # always 95% confidence interval\n\n# good way\nerr_bars = alt.Chart(cars).mark_errorbar(extent='ci', rule=alt.LineConfig(size=2)).encode(\n  x='Horsepower',\n  y='Origin'\n)\n\n(err_bars.mark_tick(color='lightgrey') + # show ticks\nerr_bars + # error bars\nerr_bars.mark_point(color='black').encode(x='mean(Horsepower)')) # mean as a point\n\nIn ggplot: use Hmisc::mean_cl_boot()\n\nggplot(cars) +\n    aes(x = Horsepower,\n        y = Origin) +\n    geom_point(shape = '|', color='grey', size=5) + # show ticks\n    geom_pointrange(stat = 'summary', fun.data = mean_cl_boot, size = 0.7) # error bar + mean as a point\n\n# For line mean and errobar\n... + geom_line(stat = 'summary', fun = mean) + # mean as a line\n    geom_ribbon(stat = 'summary', fun.data = mean_cl_boot, alpha=0.5, color = NA) # error bar as a ribbon"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#statistical-vs-practical-significance",
    "href": "block_2/531_viz/531_viz.html#statistical-vs-practical-significance",
    "title": "Data Visualization",
    "section": "Statistical vs practical significance",
    "text": "Statistical vs practical significance\n\nStatistical significance: is the difference between two groups statistically significant?\nPractical significance: is the difference between two groups practically significant?"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#figure-composition",
    "href": "block_2/531_viz/531_viz.html#figure-composition",
    "title": "Data Visualization",
    "section": "Figure composition",
    "text": "Figure composition\n\nPython\n\nvertically: plot1 & plot2 or alt.vconcat(plot1, plot2)\nhorizontally: plot1 | plot2 or alt.hconcat(plot1, plot2)\nadd title: (plot1 | plot2).properties(title='title')\n\n\n\nR\n\nuse package patchwork\nvertically: plot_grid(plot1, plot2, ncol=1)\nhorizontally: plot_grid(plot1, plot2, nrow=1)\nAdd labels: plot_grid(plot1, plot2, labels=c('A', 'B'))\nSet width: plot_grid(plot1, plot2, rel_widths=c(1, 2))"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#interactive-visualization",
    "href": "block_2/531_viz/531_viz.html#interactive-visualization",
    "title": "Data Visualization",
    "section": "Interactive visualization",
    "text": "Interactive visualization\n\nPanning and zooming\n\nAdd .interactive() to the end of the chart\n\nalt.Chart(cars).mark_circle().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n).interactive()\n\n\nDetails on demand\n\nAdd a tooltip to show details on demand\n\nalt.Chart(cars).mark_circle().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    tooltip=['Name', 'Origin']\n)\n\n\nInterval selection\n\nUse alt.selection_interval() to select a range of data points\n\nformat: alt.condition(check, if_true, if_false)\n\n\nbrush = alt.selection_interval(\n    encodings=['x'], # only select x axis, default is both x and y\n    resolve='union' # default is 'global', which means all charts are linked\n  )\n\npoints = alt.Chart(cars).mark_circle().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    # Use alt.condition to do a selection\n    color=alt.condition(brush, 'Origin', alt.value('lightgray'))\n).add_params(\n    brush\n)\n\n# linking different plots\npoints | points.encode(x='Acceleration')\n\n\nClick selection\n\ndefault: on='click'\n\nclick = alt.selection_point(fields=['Origin'], on='mouseover', bind='legend')\n\nbars = alt.Chart(cars).mark_bar().encode(\n    x='count()',\n    y='Origin',\n    color='Origin',\n    opacity=alt.condition(click, alt.value(0.9), alt.value(0.2))\n).add_params(\n    click\n)\n\n\nFiltering data based on selection\n\nuse transform_filter to filter data based on selection\n\nbrush = alt.selection_interval()\nclick = alt.selection_point(fields=['Origin'], bind='legend')\n\npoints = alt.Chart(cars).mark_circle().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    color=alt.condition(brush, 'Origin', alt.value('lightgray')),\n    opacity=alt.condition(click, alt.value(0.9), alt.value(0.2))\n).add_params(\n    brush\n)\n\nbars = alt.Chart(cars).mark_bar().encode(\n    x='count()',\n    y='Origin',\n    color='Origin',\n    opacity=alt.condition(click, alt.value(0.9), alt.value(0.2))\n).transform_filter( # changes bar plot based on selection of points\n    brush\n)\n\n(points & bars).add_params(click)"
  },
  {
    "objectID": "block_2/531_viz/531_viz.html#pairwise-comparisons",
    "href": "block_2/531_viz/531_viz.html#pairwise-comparisons",
    "title": "Data Visualization",
    "section": "Pairwise comparisons",
    "text": "Pairwise comparisons\n\nPython:\n\npoints = alt.Chart(scores_this_year).mark_circle(size=50, color='black', opacity=1).encode(\n    alt.X('score_type'),\n    alt.Y('score'),\n    alt.Detail('time')).properties(width=300)\npoints.mark_line(size=1.8, opacity=0.8\n    ).encode(\n        alt.Color('diff',\n        scale=alt.Scale(scheme='blueorange', domain=(-6, 6))) # diverging colormap\n        # scale(range=['coral', 'green', 'steelblue']) # for categorial [negative, neutral, positive]\n    ) +\n    points\n\nR:\n\nggplot(scores_this_year) +\n    aes(x = score_type,\n        y = score,\n       group = time) +\n    geom_line(aes(color = self_belief), size = 0.8) +\n    geom_point(size=3) + labs(x='') +\n    # colour the lines by diverging colormap (sometimes not a good idea)\n    scale_color_distiller(palette = 'PuOr', limits = c(-5, 5))"
  },
  {
    "objectID": "list.html",
    "href": "list.html",
    "title": "List of Notes",
    "section": "",
    "text": "This is the list of MDS Courses.\nBelow are my list of notes for each course.\n\n\n\nCourse Code\nBlock #\nTitle\n\n\n\n\n511\n1\nIntro to Python\n\n\n521\n1\nPlatforms\n\n\n523\n1\nR Programming\n\n\n551\n1\nStatistics and Probability\n\n\n512\n2\nAlgorithms and Data Structures\n\n\n531\n2\nData Visualization\n\n\n552\n2\nStatistical Interference\n\n\n571\n2\nSupervised Learning\n\n\n513\n3\nDatabases\n\n\n522\n3\nWorkflows\n\n\n561\n3\nRegression I\n\n\n573\n3\nModel Selection\n\n\n562\n4\nRegression II\n\n\n572\n4\nSupervised Learning II"
  },
  {
    "objectID": "block_1/523_R/523_R.html",
    "href": "block_1/523_R/523_R.html",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "read_csv(url): reads a csv file\n\nread_csv(url, col_types = cols()): reads a csv file with no column types\nread_csv(url, col_types = cols(col_name = col_type)): reads a csv file with column types\nread_csv(url, skip = n, n_max = m): reads a csv file skipping n rows and reading m rows\n\nread_csv2(url): reads a csv file with a comma as decimal separator\nread_tsv(url): reads a tsv file\nread_delim(url, delim = \"\"): reads a file with a delimiter\n\n\n\n\n\nread_excel(file_path, sheet=\"name\"): reads an excel file\n\nto read url need to do download.file(url, destfile = \"file.xlsx\", mode = \"wb\")\n\n\n\n\n\n\nclean_names(df): cleans column names to match them with R conventions (e.g., col_name1)\n\n\n\n\n\nselect(df, col_name1, col_name2): selects cols\nfilter(df, col_name1 == \"value\", col_name2 &gt; 5): filters rows\n\nfilter(df, col_name1 %in% c(\"value1\", \"value2\")): filters if col_name1 is in a vector of values\n\narrange(df, col_name1): sorts rows, default is ascending\n\narrange(df, desc(col_name1)): sorts rows descending\n\nmutate(df, new_col_name = col_name1 + col_name2): creates new cols\nslice(df, 1:10): selects rows\n\nslice(df, 1): selects first row\n\npull(df, col_name1): extracts a column as a vector\n\n\n\n\n\nstr_detect(df$col_name, \"value\"): detects if a string contains a value\nstr_subset(df$col_name, \"value\"): subsets a string if it contains a value\nstr_split(df$col_name, \"value\"): splits a string by a value\n\nstr_split_fixed(df$col_name, \"value\", n): splits a string by a value and returns n columns (gets character matrix)\n\nseparate(df, col_name, into = c(\"new_col_name1\", \"new_col_name2\"), sep = \"value\"): separates a column into two columns\nstr_length(df$col_name): gets length of string\nstr_sub(df$col_name, start = n, end = m): gets substring from n to m\nstr_c(df$col_name1, df$col_name2, sep = \"value\"): concatenates two strings\n\nstr_c(df$col_name1, sep = \"value\", collapse = \"value\"): concatenates vector of string and collapses them into one string\n\nstr_replace(df$col_name, \"value\", \"new_value\"): replaces a value in a string\n\n\n\n\n\nfct_drop(df$col_name): drops unused levels\nfct_infreq(df$col_name): orders levels by frequency\nfct_reorder(df$col_name, df$col_name2): orders levels by another column\nfct_relevel(df$col_name, \"value\"): moves a level to the front\nfct_rev(df$col_name): reverses order of levels\n\n\n\n\n\npivot_longer(df, cols = c(col_name1, col_name2), names_to = \"new_col_name\", values_to = \"new_col_name\"): pivots cols to rows\npivot_wider(df, names_from = \"col_name1\", values_from = \"col_name2\"): pivots rows to cols\n\n\n\n\nCriteria:\n\nEach row is a single observation\nEach variable is a single column\nEach value is a single cell\n\n\n\n\n\nwe use &lt;- to assign values to variables.\nThis is because when we do median(x &lt;- c(1, 2, 3)) x is assigned to c(1, 2, 3) globally. \n\n\n\nobjects that contain 1 or more elements of the same type\nelements are ordered\nHeirarchy for coercion: character &gt; double &gt; integer &gt; logical\nto change type of vector use as.character(), as.double(), as.integer(), as.logical()\nto check if vector is of a certain type use is.character(), is.double(), is.integer(), is.logical()\nto check length of vector use length()\nto check type of vector use typeof()\n\nCan get vector from df using: df$col_name\n\n\n\nname &lt;- c(\"a\", \"b\", \"c\")\nname[1] # \"a\"\nname[2:3] # \"b\" \"c\"\nname[-1] # \"b\" \"c\"\nname[length(name)] # \"c\"]\n\n# Also...\nx &lt;- c(1, 2, 3)\ny &lt;- x\n\ny[3] &lt;- 4\ny\n#&gt; [1] 1 2 4\n\n\n\n\n\nTibles inherit from data frames but are more strict. They are more consistent and have better printing.\nImportant properties:\n\nTibbles only output first 10 rows and all columns that fit on screen\nwhen you subset a tibble you always get a tibble, in data frames you get a vector\n\n\n\n\nuses lubridate package\n\ntoday(): gets today’s date, class is Date\nnow(): gets today’s date and time, class is POSIXct\nymd(), ydm(), mdy(), myd(), dmy(), dym(): converts character to date\nymd_hms(): converts character to date and time\nCan mutate date: dates |&gt; mutate = make_date(year, month, day)\nwdays(): gets day of week\n\nwdays(date, label = TRUE): gets abbreviated day of week (e.g., Mon)\nwday(date, label = TRUE, abbr = FALSE): gets day of week as full name (e.g., Monday)\n\nmdays(): gets day of month\nydays(): gets day of year\n\n\n\n\n\nbind_rows(df1, df2): binds rows of two dfs\nbind_cols(df1, df2): binds cols of two dfs\ninner_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, only keeps rows that match\nleft_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps all rows from df1\nsemi_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps only column names from df1, only keeps rows that match\nanti_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps only column names from df1, keeps only rows that don’t match\nfull_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps all rows from both dfs\n\n\n\n\n\n\n\ncase_when(): selectively modify column values based on conditions\n\ngapminder |&gt;\n    mutate(country = case_when(country == \"Cambodia\" ~ \"Kingdom of Cambodia\",\n    # only work if country is character (not factor)\n                            TRUE ~ country))\n\n# For multiple values\ngapminder |&gt;\n    mutate(continent = case_when(continent == \"Asia\" ~ \"Asie\",\n                                 continent == \"Europe\" ~ \"L'Europe\",\n                                 continent == \"Africa\" ~ \"Afrique\",\n                                 TRUE ~ continent)) #This is to keep the original value (not NA)\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ndrop_na()\nRemove rows based on NA in cols x to y\ndf %&gt;% drop_na(x:y)\n\n\n\nRemove rows if any column has NA\ndf %&gt;% drop_na()\n\n\n\n\n\n\n\n\nsummarise() or summarize(): returns a single value for each group\ngroup_by(): groups rows by a column\n\n# calculate the average life expectancy for the entire dataset\ngapminder |&gt;\n    summarise(mean_life_exp = mean(lifeExp))\n\n# calculate the average life expectancy for each continent and year\ngapminder |&gt;\n    group_by(continent, year) |&gt;\n    summarise(mean_life_exp = mean(lifeExp, na.rm = TRUE))\n    # na.rm = TRUE removes NAs from calculation\n\n# does not collapse the data frame, compute with group\ngapminder %&gt;%\n    group_by(country) %&gt;%\n    mutate(life_exp_gain = lifeExp - first(lifeExp)) %&gt;%\n    # first() returns the first value of a vector\n    head()\n\n\n\n\n\nmap(df, mean, na.rm = TRUE): retuirns a list\n\nna.rm = TRUE removes NAs from calculation\n\nmap_dfc(df, median): returns a tibble\nmap_dbl(df, max): returns a double vector\n\nCan use anonymous functions with map():\n# Long form\nmap_*(data, function(arg) function_being_called(arg, other_arg))\n# e.g.\nmap_df(data_entry, function(vect) str_replace(vect, pattern = \"Cdn\", replacement = \"Canadian\"))\n\n# short form\nmap_*(data, ~ function_being_called(., other_arg))\n# e.g.\nmap_df(data_entry, ~str_replace(., pattern = \"Cdn\", replacement = \"Canadian\"))\n\n\n\n\nHas roxygen comments, same as python docstrings\n\n#' Calculates the variance of a vector of numbers.\n#'\n#' Calculates the sample variance of data generated from a normal/Gaussian distribution,\n#' omitting NA's in the data.\n#'\n#' @param data numeric vector of numbers whose length is &gt; 1.\n#'\n#' @return numeric vector of length one, the variance.\n#'\n#' @examples\n#' variance(c(1, 2, 3))\nvariance &lt;- function(observations) {\n  if (!is.numeric(observations)) {\n    # Throws an error\n    stop(\"All inputs must be numeric.\")\n  }\n  sum((observations - mean(observations)) ^ 2) / (length(observations) - 1)\n}\n\nName Masking: if a variable is defined in the function, it will be used instead of the global variable\n\nif not in function, looks one level up, until it reaches the global environment\n\nR looks for values when the function is run, not when it is defined\nEach run is independent of the other\nLazy Evaluation: R only evaluates the arguments that are needed\n\nforce() forces R to evaluate an argument\n\n\n\n\n\ntest_that(\"Message to print if test fails\", expect_*(...))\n\ntest_that('variance expects a numeric vector', {\n    expect_error(variance(list(1, 2, 3)))\n    expect_error(variance(data.frame(1, 2, 3)))\n    expect_error(variance(c(\"one\", \"two\", \"three\")))\n})\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nexpect_identical\nTest two objects for being exactly equal\n\n\nexpect_equal\nCompare R objects x and y testing ‘near equality’ (can set a tolerance)\n\n\n\n- expect_equal(x, y, tolerance = 0.00001)\n\n\nexpect_equivalent\nCompare R objects x and y testing ‘near equality’ (can set a tolerance) and does not assess attributes\n\n\nexpect_error\nTests if an expression throws an error\n\n\nexpect_warning\nTests whether an expression outputs a warning\n\n\nexpect_output\nTests that print output matches a specified value\n\n\nexpect_true\nTests if the object returns TRUE\n\n\nexpect_false\nTests if the object returns FALSE\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nsource(\"path/to/script.R\")\nTake functions from another script\n\n\nlibrary(package_name)\nImport a package\n\n\n\n\n\n(function(x) x + 1)(1) (see purrr package for examples)\n\n\n\n\n# create a nested data frame\nby_country &lt;- gapminder %&gt;%\n    group_by(continent, country) %&gt;%\n    nest() # turns all other columns into a column called data (list of data frames)\nCommon workflow:\n\ngroup_by() + nest() to create a nested data frame\nmutate() + map() to add new columns\nunnest() to return to a regular data frame\n\nweather |&gt;\n# step 1\n  group_by(origin, month) |&gt;\n  nest() |&gt;\n# step 2\n  mutate(min_temp = map_dbl(data, ~min(.$temp, na.rm = T)),\n         max_temp = map_dbl(data, ~max(.$temp, na.rm = T)),\n         avg_temp = map_dbl(data, ~mean(.$temp, na.rm = T)))\n# step 3\nunnest(avg_temp) # only unnest if we get some intermediate list-columns from map\nAlternative to above code:\nweather_nested_2 &lt;- weather |&gt;\n  group_by(origin, month) |&gt;\n  summarise(min_temp = min(temp, na.rm = T),\n            max_temp = max(temp, na.rm = T),\n            avg_temp = mean(temp, na.rm = T))\n\n\n\nmetaprogramming: writing code that writes code\nWith tidyverse, they have a feautre called “non-standard evaluation” (NSE). Part of this is data masking.\n\nData Masking: data frame is promised to be first argument (data mask)\n\ncolumns act as if they are variables, filter(gapminder, country == \"Canada\", year == 1952)\nchecks dataframe first before global environment\n\nDelay in Evaluation: expressions are captured and evaluated later\nenquo(): quotes the argument\nsym(): turns column name into a function as a string\n!!: unquotes the argument\n{{ arg_name }}: unquotes and quotes the argument\n:=: Walrus operator - needed when assigning values\n\n# e.g.\nfilter_gap &lt;- function(col, val) {\n    col &lt;- enquo(col)\n    filter(gapminder, !!col == val)\n}\n\n# better way\nfilter_gap &lt;- function(col, val) {\n    filter(gapminder, {{col}} == val)\n}\n\nfilter_gap(country, \"Canada\")\n# e.g. of walrus operator\nfunction(data, group, col, fun) {\n    data %&gt;%\n        group_by({{ group }}) %&gt;%\n        summarise( {{ col }} := fun({{ col }}))\n}\n\n\n\nif passing varibales to tidyverse functions, use ...\n\nwhen variable not used in logical comparisons or variable assignment\n\nshould be last argument in function\ncan add multiple arguments\n\n\n\nsort_gap &lt;- function(x, ...) {\n    print(x + 1)\n    arrange(gapminder, ...)\n}\n\nsort_gap(1, year, continent, country)"
  },
  {
    "objectID": "block_1/523_R/523_R.html#r-packages",
    "href": "block_1/523_R/523_R.html#r-packages",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "read_csv(url): reads a csv file\n\nread_csv(url, col_types = cols()): reads a csv file with no column types\nread_csv(url, col_types = cols(col_name = col_type)): reads a csv file with column types\nread_csv(url, skip = n, n_max = m): reads a csv file skipping n rows and reading m rows\n\nread_csv2(url): reads a csv file with a comma as decimal separator\nread_tsv(url): reads a tsv file\nread_delim(url, delim = \"\"): reads a file with a delimiter\n\n\n\n\n\nread_excel(file_path, sheet=\"name\"): reads an excel file\n\nto read url need to do download.file(url, destfile = \"file.xlsx\", mode = \"wb\")\n\n\n\n\n\n\nclean_names(df): cleans column names to match them with R conventions (e.g., col_name1)\n\n\n\n\n\nselect(df, col_name1, col_name2): selects cols\nfilter(df, col_name1 == \"value\", col_name2 &gt; 5): filters rows\n\nfilter(df, col_name1 %in% c(\"value1\", \"value2\")): filters if col_name1 is in a vector of values\n\narrange(df, col_name1): sorts rows, default is ascending\n\narrange(df, desc(col_name1)): sorts rows descending\n\nmutate(df, new_col_name = col_name1 + col_name2): creates new cols\nslice(df, 1:10): selects rows\n\nslice(df, 1): selects first row\n\npull(df, col_name1): extracts a column as a vector\n\n\n\n\n\nstr_detect(df$col_name, \"value\"): detects if a string contains a value\nstr_subset(df$col_name, \"value\"): subsets a string if it contains a value\nstr_split(df$col_name, \"value\"): splits a string by a value\n\nstr_split_fixed(df$col_name, \"value\", n): splits a string by a value and returns n columns (gets character matrix)\n\nseparate(df, col_name, into = c(\"new_col_name1\", \"new_col_name2\"), sep = \"value\"): separates a column into two columns\nstr_length(df$col_name): gets length of string\nstr_sub(df$col_name, start = n, end = m): gets substring from n to m\nstr_c(df$col_name1, df$col_name2, sep = \"value\"): concatenates two strings\n\nstr_c(df$col_name1, sep = \"value\", collapse = \"value\"): concatenates vector of string and collapses them into one string\n\nstr_replace(df$col_name, \"value\", \"new_value\"): replaces a value in a string\n\n\n\n\n\nfct_drop(df$col_name): drops unused levels\nfct_infreq(df$col_name): orders levels by frequency\nfct_reorder(df$col_name, df$col_name2): orders levels by another column\nfct_relevel(df$col_name, \"value\"): moves a level to the front\nfct_rev(df$col_name): reverses order of levels\n\n\n\n\n\npivot_longer(df, cols = c(col_name1, col_name2), names_to = \"new_col_name\", values_to = \"new_col_name\"): pivots cols to rows\npivot_wider(df, names_from = \"col_name1\", values_from = \"col_name2\"): pivots rows to cols\n\n\n\n\nCriteria:\n\nEach row is a single observation\nEach variable is a single column\nEach value is a single cell"
  },
  {
    "objectID": "block_1/523_R/523_R.html#assignment-environment",
    "href": "block_1/523_R/523_R.html#assignment-environment",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "we use &lt;- to assign values to variables.\nThis is because when we do median(x &lt;- c(1, 2, 3)) x is assigned to c(1, 2, 3) globally. \n\n\n\nobjects that contain 1 or more elements of the same type\nelements are ordered\nHeirarchy for coercion: character &gt; double &gt; integer &gt; logical\nto change type of vector use as.character(), as.double(), as.integer(), as.logical()\nto check if vector is of a certain type use is.character(), is.double(), is.integer(), is.logical()\nto check length of vector use length()\nto check type of vector use typeof()\n\nCan get vector from df using: df$col_name\n\n\n\nname &lt;- c(\"a\", \"b\", \"c\")\nname[1] # \"a\"\nname[2:3] # \"b\" \"c\"\nname[-1] # \"b\" \"c\"\nname[length(name)] # \"c\"]\n\n# Also...\nx &lt;- c(1, 2, 3)\ny &lt;- x\n\ny[3] &lt;- 4\ny\n#&gt; [1] 1 2 4"
  },
  {
    "objectID": "block_1/523_R/523_R.html#tibbles-vs-data-frames",
    "href": "block_1/523_R/523_R.html#tibbles-vs-data-frames",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "Tibles inherit from data frames but are more strict. They are more consistent and have better printing.\nImportant properties:\n\nTibbles only output first 10 rows and all columns that fit on screen\nwhen you subset a tibble you always get a tibble, in data frames you get a vector"
  },
  {
    "objectID": "block_1/523_R/523_R.html#dates-and-times",
    "href": "block_1/523_R/523_R.html#dates-and-times",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "uses lubridate package\n\ntoday(): gets today’s date, class is Date\nnow(): gets today’s date and time, class is POSIXct\nymd(), ydm(), mdy(), myd(), dmy(), dym(): converts character to date\nymd_hms(): converts character to date and time\nCan mutate date: dates |&gt; mutate = make_date(year, month, day)\nwdays(): gets day of week\n\nwdays(date, label = TRUE): gets abbreviated day of week (e.g., Mon)\nwday(date, label = TRUE, abbr = FALSE): gets day of week as full name (e.g., Monday)\n\nmdays(): gets day of month\nydays(): gets day of year"
  },
  {
    "objectID": "block_1/523_R/523_R.html#joining-data",
    "href": "block_1/523_R/523_R.html#joining-data",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "bind_rows(df1, df2): binds rows of two dfs\nbind_cols(df1, df2): binds cols of two dfs\ninner_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, only keeps rows that match\nleft_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps all rows from df1\nsemi_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps only column names from df1, only keeps rows that match\nanti_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps only column names from df1, keeps only rows that don’t match\nfull_join(df1, df2, by = \"col_name\"): joins two dfs by col_name, keeps all rows from both dfs"
  },
  {
    "objectID": "block_1/523_R/523_R.html#change-or-remove-specific-values",
    "href": "block_1/523_R/523_R.html#change-or-remove-specific-values",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "case_when(): selectively modify column values based on conditions\n\ngapminder |&gt;\n    mutate(country = case_when(country == \"Cambodia\" ~ \"Kingdom of Cambodia\",\n    # only work if country is character (not factor)\n                            TRUE ~ country))\n\n# For multiple values\ngapminder |&gt;\n    mutate(continent = case_when(continent == \"Asia\" ~ \"Asie\",\n                                 continent == \"Europe\" ~ \"L'Europe\",\n                                 continent == \"Africa\" ~ \"Afrique\",\n                                 TRUE ~ continent)) #This is to keep the original value (not NA)\n\n\n\n\n\n\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ndrop_na()\nRemove rows based on NA in cols x to y\ndf %&gt;% drop_na(x:y)\n\n\n\nRemove rows if any column has NA\ndf %&gt;% drop_na()"
  },
  {
    "objectID": "block_1/523_R/523_R.html#iterate-over-groups-of-rows",
    "href": "block_1/523_R/523_R.html#iterate-over-groups-of-rows",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "summarise() or summarize(): returns a single value for each group\ngroup_by(): groups rows by a column\n\n# calculate the average life expectancy for the entire dataset\ngapminder |&gt;\n    summarise(mean_life_exp = mean(lifeExp))\n\n# calculate the average life expectancy for each continent and year\ngapminder |&gt;\n    group_by(continent, year) |&gt;\n    summarise(mean_life_exp = mean(lifeExp, na.rm = TRUE))\n    # na.rm = TRUE removes NAs from calculation\n\n# does not collapse the data frame, compute with group\ngapminder %&gt;%\n    group_by(country) %&gt;%\n    mutate(life_exp_gain = lifeExp - first(lifeExp)) %&gt;%\n    # first() returns the first value of a vector\n    head()"
  },
  {
    "objectID": "block_1/523_R/523_R.html#purrr-package",
    "href": "block_1/523_R/523_R.html#purrr-package",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "map(df, mean, na.rm = TRUE): retuirns a list\n\nna.rm = TRUE removes NAs from calculation\n\nmap_dfc(df, median): returns a tibble\nmap_dbl(df, max): returns a double vector\n\nCan use anonymous functions with map():\n# Long form\nmap_*(data, function(arg) function_being_called(arg, other_arg))\n# e.g.\nmap_df(data_entry, function(vect) str_replace(vect, pattern = \"Cdn\", replacement = \"Canadian\"))\n\n# short form\nmap_*(data, ~ function_being_called(., other_arg))\n# e.g.\nmap_df(data_entry, ~str_replace(., pattern = \"Cdn\", replacement = \"Canadian\"))"
  },
  {
    "objectID": "block_1/523_R/523_R.html#functions",
    "href": "block_1/523_R/523_R.html#functions",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "Has roxygen comments, same as python docstrings\n\n#' Calculates the variance of a vector of numbers.\n#'\n#' Calculates the sample variance of data generated from a normal/Gaussian distribution,\n#' omitting NA's in the data.\n#'\n#' @param data numeric vector of numbers whose length is &gt; 1.\n#'\n#' @return numeric vector of length one, the variance.\n#'\n#' @examples\n#' variance(c(1, 2, 3))\nvariance &lt;- function(observations) {\n  if (!is.numeric(observations)) {\n    # Throws an error\n    stop(\"All inputs must be numeric.\")\n  }\n  sum((observations - mean(observations)) ^ 2) / (length(observations) - 1)\n}\n\nName Masking: if a variable is defined in the function, it will be used instead of the global variable\n\nif not in function, looks one level up, until it reaches the global environment\n\nR looks for values when the function is run, not when it is defined\nEach run is independent of the other\nLazy Evaluation: R only evaluates the arguments that are needed\n\nforce() forces R to evaluate an argument\n\n\n\n\n\ntest_that(\"Message to print if test fails\", expect_*(...))\n\ntest_that('variance expects a numeric vector', {\n    expect_error(variance(list(1, 2, 3)))\n    expect_error(variance(data.frame(1, 2, 3)))\n    expect_error(variance(c(\"one\", \"two\", \"three\")))\n})\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nexpect_identical\nTest two objects for being exactly equal\n\n\nexpect_equal\nCompare R objects x and y testing ‘near equality’ (can set a tolerance)\n\n\n\n- expect_equal(x, y, tolerance = 0.00001)\n\n\nexpect_equivalent\nCompare R objects x and y testing ‘near equality’ (can set a tolerance) and does not assess attributes\n\n\nexpect_error\nTests if an expression throws an error\n\n\nexpect_warning\nTests whether an expression outputs a warning\n\n\nexpect_output\nTests that print output matches a specified value\n\n\nexpect_true\nTests if the object returns TRUE\n\n\nexpect_false\nTests if the object returns FALSE"
  },
  {
    "objectID": "block_1/523_R/523_R.html#importing-packages-or-scripts",
    "href": "block_1/523_R/523_R.html#importing-packages-or-scripts",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "Command\nDescription\n\n\n\n\nsource(\"path/to/script.R\")\nTake functions from another script\n\n\nlibrary(package_name)\nImport a package\n\n\n\n\n\n(function(x) x + 1)(1) (see purrr package for examples)"
  },
  {
    "objectID": "block_1/523_R/523_R.html#nested-data-frames",
    "href": "block_1/523_R/523_R.html#nested-data-frames",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "# create a nested data frame\nby_country &lt;- gapminder %&gt;%\n    group_by(continent, country) %&gt;%\n    nest() # turns all other columns into a column called data (list of data frames)\nCommon workflow:\n\ngroup_by() + nest() to create a nested data frame\nmutate() + map() to add new columns\nunnest() to return to a regular data frame\n\nweather |&gt;\n# step 1\n  group_by(origin, month) |&gt;\n  nest() |&gt;\n# step 2\n  mutate(min_temp = map_dbl(data, ~min(.$temp, na.rm = T)),\n         max_temp = map_dbl(data, ~max(.$temp, na.rm = T)),\n         avg_temp = map_dbl(data, ~mean(.$temp, na.rm = T)))\n# step 3\nunnest(avg_temp) # only unnest if we get some intermediate list-columns from map\nAlternative to above code:\nweather_nested_2 &lt;- weather |&gt;\n  group_by(origin, month) |&gt;\n  summarise(min_temp = min(temp, na.rm = T),\n            max_temp = max(temp, na.rm = T),\n            avg_temp = mean(temp, na.rm = T))"
  },
  {
    "objectID": "block_1/523_R/523_R.html#tidy-evaluation",
    "href": "block_1/523_R/523_R.html#tidy-evaluation",
    "title": "R Programming Cheatsheet",
    "section": "",
    "text": "metaprogramming: writing code that writes code\nWith tidyverse, they have a feautre called “non-standard evaluation” (NSE). Part of this is data masking.\n\nData Masking: data frame is promised to be first argument (data mask)\n\ncolumns act as if they are variables, filter(gapminder, country == \"Canada\", year == 1952)\nchecks dataframe first before global environment\n\nDelay in Evaluation: expressions are captured and evaluated later\nenquo(): quotes the argument\nsym(): turns column name into a function as a string\n!!: unquotes the argument\n{{ arg_name }}: unquotes and quotes the argument\n:=: Walrus operator - needed when assigning values\n\n# e.g.\nfilter_gap &lt;- function(col, val) {\n    col &lt;- enquo(col)\n    filter(gapminder, !!col == val)\n}\n\n# better way\nfilter_gap &lt;- function(col, val) {\n    filter(gapminder, {{col}} == val)\n}\n\nfilter_gap(country, \"Canada\")\n# e.g. of walrus operator\nfunction(data, group, col, fun) {\n    data %&gt;%\n        group_by({{ group }}) %&gt;%\n        summarise( {{ col }} := fun({{ col }}))\n}\n\n\n\nif passing varibales to tidyverse functions, use ...\n\nwhen variable not used in logical comparisons or variable assignment\n\nshould be last argument in function\ncan add multiple arguments\n\n\n\nsort_gap &lt;- function(x, ...) {\n    print(x + 1)\n    arrange(gapminder, ...)\n}\n\nsort_gap(1, year, continent, country)"
  },
  {
    "objectID": "block_1/551_stats-and-prob/551_stats.html",
    "href": "block_1/551_stats-and-prob/551_stats.html",
    "title": "Statistics and Probability Cheat Sheet",
    "section": "",
    "text": "Statistics and Probability Cheat Sheet\n\n&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;Nando’s MDS Notes&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;Nando’s MDS Notes&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Home\"&gt;Home&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/index.html\"&gt;/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:List of Notes\"&gt;List of Notes&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/list.html\"&gt;/list.html&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;Nando’s MDS Notes&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/section&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  }
]