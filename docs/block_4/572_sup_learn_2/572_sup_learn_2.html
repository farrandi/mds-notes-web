<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Nando's MDS Notes – sup_learn_2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../images/white_cartoon.PNG" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../images/mds_logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Nando’s MDS Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../list.html" rel="" target="">
 <span class="menu-text">List of Notes</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#supervised-learning-ii" id="toc-supervised-learning-ii" class="nav-link active" data-scroll-target="#supervised-learning-ii">Supervised Learning II</a>
  <ul class="collapse">
  <li><a href="#rounding-errors-in-programming" id="toc-rounding-errors-in-programming" class="nav-link" data-scroll-target="#rounding-errors-in-programming">Rounding Errors in Programming</a>
  <ul class="collapse">
  <li><a href="#why-is-this-relevant-in-ml" id="toc-why-is-this-relevant-in-ml" class="nav-link" data-scroll-target="#why-is-this-relevant-in-ml">Why is this relevant in ML?</a></li>
  <li><a href="#binary-numbers-and-integers" id="toc-binary-numbers-and-integers" class="nav-link" data-scroll-target="#binary-numbers-and-integers">Binary Numbers and Integers</a></li>
  <li><a href="#fractional-numbers-in-binary" id="toc-fractional-numbers-in-binary" class="nav-link" data-scroll-target="#fractional-numbers-in-binary">Fractional Numbers in Binary</a></li>
  <li><a href="#fixed-point-numbers" id="toc-fixed-point-numbers" class="nav-link" data-scroll-target="#fixed-point-numbers">Fixed Point Numbers</a></li>
  <li><a href="#floating-point-numbers" id="toc-floating-point-numbers" class="nav-link" data-scroll-target="#floating-point-numbers">*Floating Point Numbers*</a></li>
  <li><a href="#rounding-errors-and-spacing" id="toc-rounding-errors-and-spacing" class="nav-link" data-scroll-target="#rounding-errors-and-spacing">Rounding Errors and Spacing</a></li>
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples">Examples</a></li>
  </ul></li>
  <li><a href="#optimization" id="toc-optimization" class="nav-link" data-scroll-target="#optimization">Optimization</a>
  <ul class="collapse">
  <li><a href="#optimization-terminology" id="toc-optimization-terminology" class="nav-link" data-scroll-target="#optimization-terminology">Optimization Terminology</a></li>
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function">Loss Function</a></li>
  <li><a href="#notation" id="toc-notation" class="nav-link" data-scroll-target="#notation">Notation</a></li>
  </ul></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">Gradient Descent</a>
  <ul class="collapse">
  <li><a href="#gd-with-a-single-parameter" id="toc-gd-with-a-single-parameter" class="nav-link" data-scroll-target="#gd-with-a-single-parameter">GD with a Single Parameter</a></li>
  <li><a href="#gd-with-multiple-parameters" id="toc-gd-with-multiple-parameters" class="nav-link" data-scroll-target="#gd-with-multiple-parameters">GD with Multiple Parameters</a></li>
  <li><a href="#general-process" id="toc-general-process" class="nav-link" data-scroll-target="#general-process">General process</a></li>
  <li><a href="#other-optimization-algorithms" id="toc-other-optimization-algorithms" class="nav-link" data-scroll-target="#other-optimization-algorithms">Other Optimization Algorithms</a></li>
  </ul></li>
  <li><a href="#stochastic-gradient-descent" id="toc-stochastic-gradient-descent" class="nav-link" data-scroll-target="#stochastic-gradient-descent">Stochastic Gradient Descent</a>
  <ul class="collapse">
  <li><a href="#mini-batch-gradient-descent" id="toc-mini-batch-gradient-descent" class="nav-link" data-scroll-target="#mini-batch-gradient-descent">Mini-batch Gradient Descent</a></li>
  <li><a href="#terminology" id="toc-terminology" class="nav-link" data-scroll-target="#terminology">Terminology</a></li>
  </ul></li>
  <li><a href="#neural-networks" id="toc-neural-networks" class="nav-link" data-scroll-target="#neural-networks">Neural Networks</a>
  <ul class="collapse">
  <li><a href="#components" id="toc-components" class="nav-link" data-scroll-target="#components">Components</a></li>
  <li><a href="#deep-learning" id="toc-deep-learning" class="nav-link" data-scroll-target="#deep-learning">Deep Learning</a></li>
  </ul></li>
  <li><a href="#pytorch-for-neural-networks" id="toc-pytorch-for-neural-networks" class="nav-link" data-scroll-target="#pytorch-for-neural-networks">PyTorch for Neural Networks</a>
  <ul class="collapse">
  <li><a href="#tensors" id="toc-tensors" class="nav-link" data-scroll-target="#tensors">Tensors</a></li>
  <li><a href="#gpu-with-pytorch" id="toc-gpu-with-pytorch" class="nav-link" data-scroll-target="#gpu-with-pytorch">GPU with PyTorch</a></li>
  <li><a href="#linear-regression-with-pytorch" id="toc-linear-regression-with-pytorch" class="nav-link" data-scroll-target="#linear-regression-with-pytorch">Linear Regression with PyTorch</a></li>
  <li><a href="#non-linear-regression-with-pytorch" id="toc-non-linear-regression-with-pytorch" class="nav-link" data-scroll-target="#non-linear-regression-with-pytorch">Non-linear Regression with PyTorch</a></li>
  <li><a href="#common-criteria-and-optimizers-for-pytorch" id="toc-common-criteria-and-optimizers-for-pytorch" class="nav-link" data-scroll-target="#common-criteria-and-optimizers-for-pytorch">Common Criteria and Optimizers for PyTorch</a></li>
  </ul></li>
  <li><a href="#backpropagation" id="toc-backpropagation" class="nav-link" data-scroll-target="#backpropagation">Backpropagation</a>
  <ul class="collapse">
  <li><a href="#basic-concept" id="toc-basic-concept" class="nav-link" data-scroll-target="#basic-concept">Basic concept</a></li>
  <li><a href="#torch-autograd" id="toc-torch-autograd" class="nav-link" data-scroll-target="#torch-autograd">Torch: Autograd</a></li>
  <li><a href="#vanishing-and-exploding-gradients" id="toc-vanishing-and-exploding-gradients" class="nav-link" data-scroll-target="#vanishing-and-exploding-gradients">Vanishing and Exploding Gradients</a></li>
  <li><a href="#training-neural-networks-in-pytorch" id="toc-training-neural-networks-in-pytorch" class="nav-link" data-scroll-target="#training-neural-networks-in-pytorch">Training Neural Networks in PyTorch</a></li>
  <li><a href="#pytorch-trainer-code" id="toc-pytorch-trainer-code" class="nav-link" data-scroll-target="#pytorch-trainer-code">PyTorch Trainer Code</a></li>
  <li><a href="#universal-approximation-theorem" id="toc-universal-approximation-theorem" class="nav-link" data-scroll-target="#universal-approximation-theorem">Universal Approximation Theorem</a></li>
  </ul></li>
  <li><a href="#convolutional-neural-networks-cnn" id="toc-convolutional-neural-networks-cnn" class="nav-link" data-scroll-target="#convolutional-neural-networks-cnn">Convolutional Neural Networks (CNN)</a>
  <ul class="collapse">
  <li><a href="#convolution" id="toc-convolution" class="nav-link" data-scroll-target="#convolution">Convolution</a></li>
  <li><a href="#cnn-structure" id="toc-cnn-structure" class="nav-link" data-scroll-target="#cnn-structure">CNN Structure</a></li>
  <li><a href="#cnn-in-pytorch" id="toc-cnn-in-pytorch" class="nav-link" data-scroll-target="#cnn-in-pytorch">CNN in PyTorch</a></li>
  <li><a href="#preparing-data" id="toc-preparing-data" class="nav-link" data-scroll-target="#preparing-data">Preparing Data</a></li>
  <li><a href="#hyperparameter-tuning" id="toc-hyperparameter-tuning" class="nav-link" data-scroll-target="#hyperparameter-tuning">Hyperparameter Tuning</a></li>
  <li><a href="#transfer-learning" id="toc-transfer-learning" class="nav-link" data-scroll-target="#transfer-learning">Transfer Learning</a></li>
  </ul></li>
  <li><a href="#advanced-cnn" id="toc-advanced-cnn" class="nav-link" data-scroll-target="#advanced-cnn">Advanced CNN</a>
  <ul class="collapse">
  <li><a href="#generative-vs-discriminative-models" id="toc-generative-vs-discriminative-models" class="nav-link" data-scroll-target="#generative-vs-discriminative-models">Generative vs Discriminative Models</a></li>
  <li><a href="#autoencoders" id="toc-autoencoders" class="nav-link" data-scroll-target="#autoencoders">Autoencoders</a></li>
  </ul></li>
  <li><a href="#generative-adversarial-networks-gans" id="toc-generative-adversarial-networks-gans" class="nav-link" data-scroll-target="#generative-adversarial-networks-gans">Generative Adversarial Networks (GANs)</a>
  <ul class="collapse">
  <li><a href="#training-gans" id="toc-training-gans" class="nav-link" data-scroll-target="#training-gans">Training GANs</a></li>
  <li><a href="#pytorch-implementation" id="toc-pytorch-implementation" class="nav-link" data-scroll-target="#pytorch-implementation">Pytorch Implementation</a></li>
  <li><a href="#multi-input-networks" id="toc-multi-input-networks" class="nav-link" data-scroll-target="#multi-input-networks">Multi-Input Networks</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">



<section id="supervised-learning-ii" class="level1">
<h1>Supervised Learning II</h1>
<section id="rounding-errors-in-programming" class="level2">
<h2 class="anchored" data-anchor-id="rounding-errors-in-programming">Rounding Errors in Programming</h2>
<ul>
<li>Infinite amount of numbers but finite amount of bits to represent them</li>
<li>These small errors will accumulate and cause problems</li>
</ul>
<section id="why-is-this-relevant-in-ml" class="level3">
<h3 class="anchored" data-anchor-id="why-is-this-relevant-in-ml">Why is this relevant in ML?</h3>
<ul>
<li>large datasets with millions of params</li>
<li>small errors can accumulate and cause problems</li>
</ul>
</section>
<section id="binary-numbers-and-integers" class="level3">
<h3 class="anchored" data-anchor-id="binary-numbers-and-integers">Binary Numbers and Integers</h3>
<ul>
<li>Binary numbers are represented as a sum of powers of 2</li>
<li>e.g.&nbsp;104 in binary is 1101000 = <span class="math inline">\(1(2^6) + 1(2^5) + 0(2^4) + 1(2^3) + 0(2^2) + 0(2^1) + 0(2^0) = 64 + 32 + 8 = 104\)</span></li>
<li><strong>Unsigned Integers</strong>: <span class="math inline">\(2^n - 1\)</span> is the largest number that can be represented with n bits
<ul>
<li>e.g.&nbsp;8 bits can represent 0 to 255</li>
<li><code>np.iinfo(np.uint8)</code> gives the min and max values</li>
</ul></li>
<li><strong>Signed Integers</strong>: <span class="math inline">\(2^{n-1} - 1\)</span> is the largest positive number that can be represented with n bits
<ul>
<li><span class="math inline">\(-2^{n-1}\)</span> is the smallest negative number that can be represented with n bits</li>
<li>e.g.&nbsp;8 bits can represent -128 to 127 (0 is included in the positive numbers)</li>
<li>1 bit is used to represent the sign</li>
<li><code>np.iinfo(np.int8)</code> gives the min and max values</li>
</ul></li>
</ul>
</section>
<section id="fractional-numbers-in-binary" class="level3">
<h3 class="anchored" data-anchor-id="fractional-numbers-in-binary">Fractional Numbers in Binary</h3>
<ul>
<li>14.75 in binary is 1110.11</li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th>2^3</th>
<th>2^2</th>
<th>2^1</th>
<th>2^0</th>
<th>2^-1</th>
<th>2^-2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>8</td>
<td>4</td>
<td>2</td>
<td>0</td>
<td>0.5</td>
<td>0.25</td>
</tr>
</tbody>
</table>
<p>$ 8 + 4 + 2 + 0 + 0.5 + 0.25 = 14.75 $</p>
</section>
<section id="fixed-point-numbers" class="level3">
<h3 class="anchored" data-anchor-id="fixed-point-numbers">Fixed Point Numbers</h3>
<ul>
<li>We typically have a fixed number of bits to represent the fractional part</li>
<li>e.g.&nbsp;8 bits total, 4 bits for the integer part and 4 bits for the fractional part
<ul>
<li>max value is 15.9375 (<span class="math inline">\(2^3 + 2^2 + 2^1 + 2^0 + 2^{-1} + 2^{-2} + 2^{-3} + 2^{-4}\)</span>)
<ul>
<li>overflow if try a higher value</li>
</ul></li>
<li>min value (bigger than 0) is 0.0625 (<span class="math inline">\(2^{-4}\)</span>)
<ul>
<li>or precision of 0.0625 (any less =&gt; underflow)</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="floating-point-numbers" class="level3">
<h3 class="anchored" data-anchor-id="floating-point-numbers">*Floating Point Numbers*</h3>
<ul>
<li>Rather than having a fixed location for the binary point, we let it “float” around.
<ul>
<li>like how we write 0.1234 as 1.234 x 10^-1</li>
</ul></li>
<li><strong>Format</strong>: <span class="math display">\[(-1)^S \times 1. M \times 2^E\]</span>
<ul>
<li>S is the sign bit</li>
<li>M is the mantissa, always between 1 and 2 (1.0 is implied)</li>
<li>E is the exponent</li>
</ul></li>
</ul>
<p><em>Float 64</em> (double precision) <img src="images/1_64float.png" width="400"></p>
<p><em>Float 32</em> (single precision) <img src="images/1_32float.png" width="400"></p>
</section>
<section id="rounding-errors-and-spacing" class="level3">
<h3 class="anchored" data-anchor-id="rounding-errors-and-spacing">Rounding Errors and Spacing</h3>
<section id="spacing" class="level4">
<h4 class="anchored" data-anchor-id="spacing">Spacing</h4>
<ul>
<li>The spacing changes depending on the floating point number (because of the exponent)</li>
</ul>
<section id="ways-to-calculate-the-spacing" class="level5">
<h5 class="anchored" data-anchor-id="ways-to-calculate-the-spacing">Ways to calculate the spacing</h5>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>np.spacing(<span class="fl">1e16</span>) <span class="co"># 1.0</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>np.nextafter(<span class="fl">1e16</span>, <span class="fl">2e16</span>) <span class="op">-</span> <span class="fl">1e16</span> <span class="co"># 1.0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
</section>
<section id="examples" class="level3">
<h3 class="anchored" data-anchor-id="examples">Examples</h3>
<ul>
<li><code>1.0 + 2.0 + 3.0 == 6.0</code> True</li>
<li><code>0.1 + 0.2 == 0.3</code> False
<ul>
<li>0.1, 0.2, and 0.3 are not exactly representable in binary</li>
</ul></li>
<li><code>1e16 + 1 == 1e16</code> True
<ul>
<li>1 is less than the spacing, so it is rounded back</li>
</ul></li>
<li><code>1e16 + 2.0 == 1e16</code> False
<ul>
<li>2.0 is greater than the spacing, so it is rounded up</li>
</ul></li>
<li><code>1e16 + 1.0 + 1.0  == 1e16</code> True
<ul>
<li>1.0 is less than the spacing, so it is rounded back, then 1.0 is added, which is less than the spacing, so it is rounded back again</li>
</ul></li>
</ul>
</section>
</section>
<section id="optimization" class="level2">
<h2 class="anchored" data-anchor-id="optimization">Optimization</h2>
<ul>
<li>In ML, we want to minimize a loss function
<ul>
<li>typically a sum of losses over the training set</li>
</ul></li>
<li>Can think of ML as a 3 step process:
<ol type="1">
<li>Choose <strong>model</strong>: controls space of possible functions that map X to y</li>
<li>Choose <strong>loss function</strong>: measures how well the model fits the data</li>
<li>Choose <strong>optimization</strong> algorithm: finds the best model</li>
</ol></li>
</ul>
<section id="optimization-terminology" class="level3">
<h3 class="anchored" data-anchor-id="optimization-terminology">Optimization Terminology</h3>
<ul>
<li><strong>Optimization</strong>: process to min/max a function</li>
<li><strong>Objective Function</strong>: function to be optimized</li>
<li><strong>Domain</strong>: set to search for optimal value</li>
<li><strong>Minimizer</strong>: value that minimizes the objective function</li>
</ul>
</section>
<section id="loss-function" class="level3">
<h3 class="anchored" data-anchor-id="loss-function">Loss Function</h3>
<p>Common loss function is MSE (mean squared error):</p>
<p><span class="math display">\[L(w) = \frac{1}{n} \sum_{i=1}^n (\hat{y}_i - y_i)^2\]</span></p>
<p>Using a simple linear regression model <span class="math inline">\(y = w_0 + w_1x\)</span>, we can rewrite the loss function as:</p>
<p><span class="math display">\[L(w) = \frac{1}{n} \sum_{i=1}^n ((w_0 + w_1x_i) - y_i)^2\]</span></p>
<p>So optimization is finding the values of <span class="math inline">\(w_0\)</span> and <span class="math inline">\(w_1\)</span> that minimize the loss function, <span class="math inline">\(L(w)\)</span>.</p>
<p>In vector format:</p>
<p><span class="math display">\[\text{MSE} = \mathcal{L}(\mathbf{w}) = \frac{1}{n}\sum^{n}_{i=1}(\mathbf{x}_i \mathbf{w} - y_i)^2\]</span></p>
<p>In full-matrix format</p>
<p><span class="math display">\[\text{MSE} = \mathcal{L}(\mathbf{w}) = \frac{1}{n}(\mathbf{X} \mathbf{w} - \mathbf{y})^T (\mathbf{X} \mathbf{w} - \mathbf{y}) \]</span></p>
</section>
<section id="notation" class="level3">
<h3 class="anchored" data-anchor-id="notation">Notation</h3>
<p><span class="math display">\[
\mathbf{y}=
\left[
\begin{array}{c} y_1 \\
\vdots \\
y_i \\
\vdots\\
y_n
\end{array}
\right]_{n \times 1}, \quad
\mathbf{X}=
\left[
\begin{array}{c} \mathbf{x}_1 \\
\vdots \\
\mathbf{x}_i \\
\vdots\\
\mathbf{x}_n
\end{array}
\right]_{n \times d}
= \left[\begin{array}{cccc}
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1 d} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{i 1} &amp; x_{i 2} &amp; \cdots &amp; x_{i d}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{n 1} &amp; x_{n 2} &amp; \cdots &amp; x_{n d}
\end{array}\right]_{n \times d},
\quad
\mathbf{w}=
\left[
\begin{array}{c} w_1 \\
\vdots\\
w_d
\end{array}
\right]_{d \times 1}
\]</span></p>
<ul>
<li><span class="math inline">\(n\)</span>: number of examples</li>
<li><span class="math inline">\(d\)</span>: number of input features/dimensions</li>
</ul>
<p>The goal is to find the weights <span class="math inline">\(\mathbf{w}\)</span> that minimize the loss function.</p>
<p><strong>Formulas:</strong></p>
<p><span class="math display">\[\mathbf{y} = \mathbf{X} \mathbf{w}\]</span></p>
<p><span class="math display">\[\hat{\mathbf{y}_i} = \mathbf{w}^T \mathbf{x}_i\]</span></p>
</section>
</section>
<section id="gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent">Gradient Descent</h2>
<ul>
<li><p>One of the most important optimization algorithms in ML</p></li>
<li><p>Iterative optimization algorithm</p></li>
<li><p>Steps:</p>
<ol type="1">
<li><p>start with some arbitrary <span class="math inline">\(\mathbf{w}\)</span></p></li>
<li><p>calculate the gradient using all training examples</p></li>
<li><p>use the gradient to adjust <span class="math inline">\(\mathbf{w}\)</span></p></li>
<li><p>repeat for <span class="math inline">\(I\)</span> iterations or until the step-size is sufficiently small</p></li>
</ol></li>
<li><p>Cost: <span class="math inline">\(O(ndt)\)</span> for t iterations, better than brute force search <span class="math inline">\(O(nd^2 + d^3)\)</span></p></li>
</ul>
<p><span class="math display">\[w_{t+1} = w_t - \alpha \nabla= L(w_t)\]</span></p>
<ul>
<li><span class="math inline">\(w_t\)</span>: current value of the weights</li>
<li><span class="math inline">\(\alpha\)</span>: learning rate</li>
<li><span class="math inline">\(\nabla L(w_t)\)</span>: gradient of the loss function at <span class="math inline">\(w_t\)</span></li>
</ul>
<section id="gd-with-a-single-parameter" class="level3">
<h3 class="anchored" data-anchor-id="gd-with-a-single-parameter">GD with a Single Parameter</h3>
<ul>
<li>Loss function: <span class="math inline">\(L(w) = \frac{1}{n} \sum_{i=1}^n (\hat{y}_i - y_i)^2\)</span></li>
<li>Gradient: <span class="math inline">\(\nabla L(w) = \frac{d}{dw} L(w) = \frac{2}{n} \sum_{i=1}^n x_{i1} (x_{i1} w_1 - y_i)\)</span>
<ul>
<li>Or in Matrix form: <span class="math inline">\(\nabla L(w) = \frac{2}{n} \mathbf{X}^T (\mathbf{X} \mathbf{w} - \mathbf{y})\)</span></li>
</ul></li>
</ul>
</section>
<section id="gd-with-multiple-parameters" class="level3">
<h3 class="anchored" data-anchor-id="gd-with-multiple-parameters">GD with Multiple Parameters</h3>
<ul>
<li>Need to scale for the contour plot to be more “round”
<ul>
<li>better for gradient descent</li>
</ul></li>
</ul>
<p><img src="images/2_contour_plot.png" width="600"></p>
<ul>
<li>In real life, contour plots are not so nice</li>
</ul>
</section>
<section id="general-process" class="level3">
<h3 class="anchored" data-anchor-id="general-process">General process</h3>
<p><img src="images/2_gd.png" width="200"></p>
<ul>
<li><strong>Initialization:</strong> Start with an initial set of parameters, often randomly chosen.</li>
<li><strong>Forward pass:</strong> Generate predictions using the current values of the parameters. (E.g., <span class="math inline">\(\hat{y_i} = x_{1}w_1 + Bias\)</span> in the toy example above)</li>
<li><strong>Loss calculation:</strong> Evaluate the loss, which quantifies the discrepancy between the model’s predictions and the actual target values.</li>
<li><strong>Gradient calculation:</strong> Compute the gradient of the loss function with respect to each parameter either on a batch or the full dataset. This gradient indicates the direction in which the loss is increasing and its magnitude.</li>
<li><strong>Parameter Update</strong>: Adjust the parameters in the opposite direction of the calculated gradient, scaled by the learning rate. This step aims to reduce the loss by moving the parameters toward values that minimize it.</li>
</ul>
</section>
<section id="other-optimization-algorithms" class="level3">
<h3 class="anchored" data-anchor-id="other-optimization-algorithms">Other Optimization Algorithms</h3>
<ul>
<li>Use <code>minimize</code> function from <code>scipy.optimize</code></li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse(w, X, y):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Mean squared error."""</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean((X <span class="op">@</span> w <span class="op">-</span> y) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_grad(w, X, y):</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Gradient of mean squared error."""</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="dv">2</span><span class="op">/</span>n) <span class="op">*</span> X.T <span class="op">@</span> (X <span class="op">@</span> w <span class="op">-</span> y)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> minimize(mse, w, jac<span class="op">=</span>mse_grad, args<span class="op">=</span>(X_scaled_ones, toy_y), method<span class="op">=</span><span class="st">"BFGS"</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># jac: function to compute the gradient (optional)</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># - will use finite difference approximation if not provided</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Other methods:
<ul>
<li><code>BFGS</code>: Broyden–Fletcher–Goldfarb–Shanno algorithm</li>
<li><code>CG</code>: Conjugate gradient algorithm</li>
<li><code>L-BFGS-B</code>: Limited-memory BFGS with bounds on the variables</li>
<li><code>SLSQP</code>: Sequential Least SQuares Programming</li>
<li><code>TNC</code>: Truncated Newton algorithm</li>
</ul></li>
</ul>
</section>
</section>
<section id="stochastic-gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="stochastic-gradient-descent">Stochastic Gradient Descent</h2>
<ul>
<li>Instead of updating our parameters based on a gradient calculated using all training data, we simply use <strong>one of our data points</strong> (the <span class="math inline">\(i\)</span>-th one)</li>
</ul>
<p><strong>Gradient Descent</strong></p>
<p>Loss function:</p>
<p><span class="math display">\[\text{MSE} = \mathcal{L}(\mathbf{w}) = \frac{1}{n}\sum^{n}_{i=1} (\mathbf{x}_i \mathbf{w} - y_i)^2\]</span></p>
<p>Update procedure:</p>
<p><span class="math display">\[\mathbf{w}^{j+1} = \mathbf{w}^{j} - \alpha \nabla_\mathbf{w} \mathcal{L}(\mathbf{w}^{j})\]</span></p>
<p><strong>Stochastic Gradient Descent</strong></p>
<p>Loss function:</p>
<p><span class="math display">\[\text{MSE}_i = \mathcal{L}_i(\mathbf{w}) = (\mathbf{x}_i \mathbf{w} - y_i)^2\]</span></p>
<p>Update procedure: <span class="math display">\[\mathbf{w}^{j+1} = \mathbf{w}^{j} - \alpha \nabla_\mathbf{w} \mathcal{L}_i(\mathbf{w}^{j})\]</span></p>
<section id="mini-batch-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="mini-batch-gradient-descent">Mini-batch Gradient Descent</h3>
<table class="table">
<thead>
<tr class="header">
<th>Gradient Descent</th>
<th>Stochastic Gradient Descent</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Use all data points</td>
<td>Use one data point</td>
</tr>
<tr class="even">
<td>Slow</td>
<td>Fast</td>
</tr>
<tr class="odd">
<td>Accurate</td>
<td>Less Accurate</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Mini-batch Gradient Descent</strong> is a (in-between) compromise between the two</li>
<li>Instead of using a single data point, we use a small batch of data points d</li>
</ul>
<section id="mini-batch-creation" class="level4">
<h4 class="anchored" data-anchor-id="mini-batch-creation">Mini-batch Creation</h4>
<ol type="1">
<li>Shuffle and divide all data into <span class="math inline">\(k\)</span> batches, every example is used once
<ul>
<li><strong>Default in PyTorch</strong></li>
<li>An example will only show up in one batch</li>
</ul></li>
<li>Choose some examples for each batch <strong>without replacement</strong>
<ul>
<li>An example may show up in multiple batches</li>
<li>The same example cannot show up in the same batch more than once</li>
</ul></li>
<li>Choose some examples for each batch <strong>with replacement</strong>
<ul>
<li>An example may show up in multiple batches</li>
<li>The same example may show up in the same batch more than once</li>
</ul></li>
</ol>
</section>
</section>
<section id="terminology" class="level3">
<h3 class="anchored" data-anchor-id="terminology">Terminology</h3>
<p>Assume we have a dataset of <span class="math inline">\(n\)</span> observations (also known as <em>rows, samples, examples, data points, or points</em>)</p>
<ul>
<li><p><strong>Iteration</strong>: each time you update model weights</p></li>
<li><p><strong>Batch</strong>: a subset of data used in an iteration</p></li>
<li><p><strong>Epoch</strong>: One full pass through the dataset to look at all <span class="math inline">\(n\)</span> observations</p></li>
</ul>
<p>In other words,</p>
<ul>
<li>In <strong>GD</strong>, each iteration involves computing the gradient over all examples, so</li>
</ul>
<p><span class="math display">\[1 \: \text{iteration} = 1 \: \text{epoch}\]</span></p>
<ul>
<li>In <strong>SGD</strong>, each iteration involves one data point, so</li>
</ul>
<p><span class="math display">\[n \text{ iterations} = 1 \: \text{epoch}\]</span></p>
<ul>
<li>In <strong>MGD</strong>, each iteration involves a batch of data, so</li>
</ul>
<p><span class="math display">\[
\begin{align}
\frac{n}{\text{batch size}} \text{iterations} &amp;= 1 \text{ epoch}\\
\end{align}
\]</span></p>
<p><strong>*Note</strong>: nobody really says “minibatch SGD”, we just say SGD: in SGD you can specify a batch size of anything between 1 and <span class="math inline">\(n\)</span></p>
</section>
</section>
<section id="neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="neural-networks">Neural Networks</h2>
<ul>
<li>Models that does a good job of approximating complex non-linear functions</li>
<li>It is a sequence of layers, each of which is a linear transformation followed by a non-linear transformation</li>
</ul>
<section id="components" class="level3">
<h3 class="anchored" data-anchor-id="components">Components</h3>
<ul>
<li><strong>Node (or neuron)</strong>: a single unit in a layer</li>
<li><strong>Input layer</strong>: the features of the data</li>
<li><strong>Hidden layer</strong>: the layer(s) between the input and output layers</li>
<li><strong>Output layer</strong>: the prediction(s) of the model</li>
<li><strong>Weights</strong>: the parameters of the model</li>
<li><strong>Activation function</strong>: the non-linear transformation (e.g.&nbsp;ReLU, Sigmoid, Tanh, etc.)</li>
</ul>
<p><img src="images/3_nn.png" width="600"></p>
<p><em>X : (n x d), W : (h x d), b : (n x h), where h is the number of hidden nodes</em> <em>b is actually 1 x hs, but we can think of it as n x hs because it is broadcasted</em></p>
<p><span class="math display">\[\mathbf{H}^{(1)} = \phi^{(1)} (\mathbf{X}\mathbf{W}^{(1)\text{T}} + \mathbf{b}^{(1)})\]</span></p>
<p><span class="math display">\[\mathbf{H}^{(2)} = \phi^{(2)} (\mathbf{H}^{(1)}\mathbf{W}^{(2)\text{T}} + \mathbf{b}^{(2)})\]</span></p>
<p><span class="math display">\[\mathbf{Y} = (\mathbf{H}^{(2)}\mathbf{W}^{(3)\text{T}} + \mathbf{b}^{(3)})\]</span></p>
<ul>
<li>In a layer, <span class="math display">\[\text{ num of weights} = \text{num of nodes in previous layer} \times \text{num of nodes in current layer}\]</span></li>
</ul>
<p><span class="math display">\[\text{num of biases} = \text{num of nodes in current layer}\]</span></p>
<p><span class="math display">\[\text{num of parameters} = \text{num of weights} + \text{num of biases}\]</span></p>
<section id="activation-functions" class="level4">
<h4 class="anchored" data-anchor-id="activation-functions">Activation Functions</h4>
<p><img src="images/4_act_funcs.png" width="600"></p>
</section>
<section id="finding-gradient-of-loss-in-a-neural-network" class="level4">
<h4 class="anchored" data-anchor-id="finding-gradient-of-loss-in-a-neural-network">Finding gradient of loss in a neural network</h4>
<ul>
<li><strong>Backpropagation</strong>: a method to calculate the gradient of the loss function with respect to the weights</li>
<li><strong>Chain rule</strong>: a method to calculate the gradient of a function composed of multiple functions</li>
<li>It is pretty complicated, but PyTorch does it for us</li>
</ul>
</section>
</section>
<section id="deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="deep-learning">Deep Learning</h3>
<ul>
<li>Neural networks with &gt; 1 hidden layer
<ul>
<li>NN with 1 hidden layer: shallow neural network</li>
</ul></li>
</ul>
</section>
</section>
<section id="pytorch-for-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-for-neural-networks">PyTorch for Neural Networks</h2>
<ul>
<li>PyTorch is a popular open-source machine learning library by Facebook based on Torch</li>
<li>It is a Python package that provides two high-level features:
<ul>
<li>Tensor computation (like NumPy) with strong GPU acceleration</li>
<li>Gradient computation through automatic differentiation</li>
</ul></li>
</ul>
<section id="tensors" class="level3">
<h3 class="anchored" data-anchor-id="tensors">Tensors</h3>
<ul>
<li>Similar to <code>ndarray</code> in NumPy</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a tensor</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]) <span class="co"># int</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="fl">5.</span>]) <span class="co"># float</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>], [<span class="dv">5</span>, <span class="dv">6</span>]])</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.zeros(<span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.ones(<span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.rand(<span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the shape, dimensions, and data type</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>x.shape</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>x.ndim</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>x.dtype</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Operations</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.rand(<span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.rand(<span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>a <span class="op">+</span> b <span class="co"># broadcasting</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>a <span class="op">*</span> b <span class="co"># element-wise multiplication</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>a <span class="op">@</span> b <span class="co"># matrix multiplication</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>a.mean()</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>a.<span class="bu">sum</span>()</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Indexing</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>a[<span class="dv">0</span>,:] <span class="co"># first row</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>a[<span class="dv">0</span>] <span class="co"># first row</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>a[:,<span class="dv">0</span>] <span class="co"># first column</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to NumPy</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>x.numpy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="gpu-with-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="gpu-with-pytorch">GPU with PyTorch</h3>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if GPU is available</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>torch.backends.mps.is_available() <span class="co"># mac M chips</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>torch.cuda.is_available() <span class="co"># Nvidia GPU</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># To activate GPU</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'mps'</span> <span class="cf">if</span> torch.backends.mps.is_available() <span class="cf">else</span> <span class="st">'cpu'</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>x.to(<span class="st">'cpu'</span>) <span class="co"># move tensor to cpu</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="gradient-computation" class="level4">
<h4 class="anchored" data-anchor-id="gradient-computation">Gradient Computation</h4>
<ul>
<li>use <code>backward()</code> to compute the gradient, backpropagation</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.tensor([<span class="fl">1.0</span>, <span class="fl">2.0</span>, <span class="fl">3.0</span>], requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> torch.tensor([<span class="fl">1.0</span>], requires_grad<span class="op">=</span><span class="va">True</span>)  <span class="co"># Random initial weight</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.tensor([<span class="fl">2.0</span>, <span class="fl">4.0</span>, <span class="fl">6.0</span>], requires_grad<span class="op">=</span><span class="va">False</span>)  <span class="co"># Target values</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>mse <span class="op">=</span> ((X <span class="op">*</span> w <span class="op">-</span> y)<span class="op">**</span><span class="dv">2</span>).mean()</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>mse.backward()</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>w.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="linear-regression-with-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="linear-regression-with-pytorch">Linear Regression with PyTorch</h3>
<ul>
<li>Every NN model has to inherit from <code>torch.nn.Module</code></li>
</ul>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> linearRegression(nn.Module):  <span class="co"># inherit from nn.Module</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, output_size):</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()  <span class="co"># call the constructor of the parent class</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> nn.Linear(input_size, output_size,)  <span class="co"># wX + b</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.linear(x)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a model</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> linearRegression(<span class="dv">1</span>, <span class="dv">1</span>) <span class="co"># input size, output size</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co"># View model</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>summary(model)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="co">## Train the model</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>LEARNING_RATE <span class="op">=</span> <span class="fl">0.02</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.MSELoss()  <span class="co"># loss function</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span>LEARNING_RATE)  <span class="co"># optimization algorithm is SGD</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="co"># DataLoader for mini-batch</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, TensorDataset</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> TensorDataset(X_t, y_t)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>dataloader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span>BATCH_SIZE, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Training</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> trainer(model, criterion, optimizer, dataloader, epochs<span class="op">=</span><span class="dv">5</span>, verbose<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Simple training wrapper for PyTorch network."""</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>        losses <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X, y <span class="kw">in</span> dataloader:</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()       <span class="co"># Clear gradients w.r.t. parameters</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>            y_hat <span class="op">=</span> model(X).flatten()  <span class="co"># Forward pass to get output</span></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_hat, y)  <span class="co"># Calculate loss</span></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>            loss.backward()             <span class="co"># Getting gradients w.r.t. parameters</span></span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>            optimizer.step()            <span class="co"># Update parameters</span></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>            losses <span class="op">+=</span> loss.item()       <span class="co"># Add loss for this batch to running total</span></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="ss">f"epoch: </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, loss: </span><span class="sc">{</span>losses <span class="op">/</span> <span class="bu">len</span>(dataloader)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>trainer(model, criterion, optimizer, dataloader, epochs<span class="op">=</span><span class="dv">30</span>, verbose<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="non-linear-regression-with-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="non-linear-regression-with-pytorch">Non-linear Regression with PyTorch</h3>
<ul>
<li>use <code>torch.nn.Sequential</code> to create a model</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> nonlinRegression(nn.Module):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, hidden_size, output_size):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.main <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>            nn.Linear(input_size, hidden_size),  <span class="co"># input -&gt; hidden layer</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>            nn.Sigmoid(),                        <span class="co"># sigmoid activation function in hidden layer</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_size, output_size)  <span class="co"># hidden -&gt; output layer</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.main(x)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="common-criteria-and-optimizers-for-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="common-criteria-and-optimizers-for-pytorch">Common Criteria and Optimizers for PyTorch</h3>
<table class="table">
<thead>
<tr class="header">
<th>Task</th>
<th>Criterion (Loss)</th>
<th>Optimizer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Regression</td>
<td>MSELoss</td>
<td>SGD</td>
</tr>
<tr class="even">
<td>Binary Classification</td>
<td>BCELoss</td>
<td>Adam</td>
</tr>
<tr class="odd">
<td>Multi-class Classification</td>
<td>CrossEntropyLoss</td>
<td>Adam</td>
</tr>
</tbody>
</table>
<ul>
<li>Input of CrossEntropyLoss doesn’t need to be normalized (i.e.&nbsp;no need to sum to 1/ no need to use <code>nn.Softmax</code>)</li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># criterions</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>reg_criterion <span class="op">=</span> torch.nn.MSELoss()</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>bc_criterion <span class="op">=</span> torch.nn.BCEWithLogitsLoss()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>mse_criterion <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># optimizers</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> optim</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>reg_optim <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>class_optim <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>LEARNING_RATE)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="backpropagation" class="level2">
<h2 class="anchored" data-anchor-id="backpropagation">Backpropagation</h2>
<section id="basic-concept" class="level3">
<h3 class="anchored" data-anchor-id="basic-concept">Basic concept</h3>
<ul>
<li><p>It is to calculate the gradient of the loss function with respect to the weights</p></li>
<li><p>It is a special case of the chain rule of calculus</p></li>
<li><p><strong>Process:</strong></p>
<ol type="1">
<li>Do “forward pass” to calculate the output of the network (prediction and loss)</li>
</ol>
<p><img src="images/5_back1.png" width="600"></p>
<ol start="2" type="1">
<li>Do “backward pass” to calculate the gradients of the loss function with respect to the weights. Below is an example of reverse-mode autmatic differentiation (backpropagation):</li>
</ol>
<p><img src="images/5_back2.png" width="450"></p></li>
</ul>
</section>
<section id="torch-autograd" class="level3">
<h3 class="anchored" data-anchor-id="torch-autograd">Torch: Autograd</h3>
<ul>
<li><code>torch.autograd</code> is PyTorch’s automatic differentiation engine that powers neural network training</li>
</ul>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create model</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> network(torch.nn.Module):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(network, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer1 <span class="op">=</span> torch.nn.Linear(<span class="dv">1</span>, <span class="dv">6</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> torch.nn.Dropout(<span class="fl">0.2</span>) <span class="co"># dropout layer</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        ...</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.layer1(x)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        ...</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> network()</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> torch.nn.MSELoss()</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward pass</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> criterion(model(x), y)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Backward pass</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>loss.backward()</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Access gradients</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.layer1.weight.grad) <span class="co"># or model.layer1.weight.bias.grad</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Update weights</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>model.state_dict() <span class="co"># get the current weights</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>optimizer.step() <span class="co"># update weights</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="vanishing-and-exploding-gradients" class="level3">
<h3 class="anchored" data-anchor-id="vanishing-and-exploding-gradients">Vanishing and Exploding Gradients</h3>
<ul>
<li>Backpropagation can suffer from two problems because of multiple chain rule applications:
<ul>
<li><strong>Vanishing gradients</strong>: the gradients of the loss function with respect to the weights become very small
<ul>
<li>0 gradients because of underflow</li>
</ul></li>
<li><strong>Exploding gradients</strong>: the gradients of the loss function with respect to the weights become very large</li>
</ul></li>
<li>Possible solutions:
<ul>
<li>Use <strong>ReLU</strong> activation function: but it can also suffer from the dying ReLU problem (gradients are 0)</li>
<li><strong>Weight initialization</strong>: initialize the weights with small values</li>
<li><strong>Batch normalization</strong>: normalize the input layer by adjusting and scaling the activations</li>
<li><strong>Skip connections</strong>: add connections that skip one or more layers</li>
<li><strong>Gradient clipping</strong>: clip the gradients during backpropagation</li>
</ul></li>
</ul>
</section>
<section id="training-neural-networks-in-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="training-neural-networks-in-pytorch">Training Neural Networks in PyTorch</h3>
<section id="preventing-overfitting" class="level4">
<h4 class="anchored" data-anchor-id="preventing-overfitting">Preventing Overfitting</h4>
<ul>
<li>Add validation loss to the training loop</li>
<li><strong>Early stopping</strong>: if we see the validation loss is increasing, we stop training
<ul>
<li>Define a patience parameter: if the validation loss increases for <code>patience</code> epochs, we stop training</li>
</ul></li>
<li><strong>Regularization</strong>: add a penalty term to the loss function to prevent overfitting
<ul>
<li>See <a href="https://mds.farrandi.com/block_3/573_model_sel/573_model_sel#regularization">573 notes</a> for more details</li>
<li><code>weight_decay</code> parameter in the optimizer</li>
</ul></li>
<li><strong>Dropout</strong>: randomly set some neurons to 0 during training
<ul>
<li>It prevents overfitting by reducing the complexity of the model</li>
<li><code>torch.nn.Dropout(0.2)</code></li>
</ul></li>
</ul>
</section>
</section>
<section id="pytorch-trainer-code" class="level3">
<h3 class="anchored" data-anchor-id="pytorch-trainer-code">PyTorch Trainer Code</h3>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> trainer(model, criterion, optimizer, trainloader, validloader, epochs<span class="op">=</span><span class="dv">5</span>, patience<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Simple training wrapper for PyTorch network."""</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> []</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    valid_loss <span class="op">=</span> []</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):  <span class="co"># for each epoch</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        train_batch_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        valid_batch_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Training</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X, y <span class="kw">in</span> trainloader:</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()       <span class="co"># Zero all the gradients w.r.t. parameters</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>            y_hat <span class="op">=</span> model(X).flatten()  <span class="co"># Forward pass to get output</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_hat, y)  <span class="co"># Calculate loss based on output</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>            loss.backward()             <span class="co"># Calculate gradients w.r.t. parameters</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>            optimizer.step()            <span class="co"># Update parameters</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>            train_batch_loss <span class="op">+=</span> loss.item()  <span class="co"># Add loss for this batch to running total</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>        train_loss.append(train_batch_loss <span class="op">/</span> <span class="bu">len</span>(trainloader))</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Validation</span></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():  <span class="co"># this stops pytorch doing computational graph stuff under-the-hood and saves memory and time</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> X_valid, y_valid <span class="kw">in</span> validloader:</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>                y_hat <span class="op">=</span> model(X_valid).flatten()  <span class="co"># Forward pass to get output</span></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> criterion(y_hat, y_valid)  <span class="co"># Calculate loss based on output</span></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>                valid_batch_loss <span class="op">+=</span> loss.item()</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>        valid_loss.append(valid_batch_loss <span class="op">/</span> <span class="bu">len</span>(validloader))</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Early stopping</span></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> epoch <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> valid_loss[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> valid_loss[<span class="op">-</span><span class="dv">2</span>]:</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>            consec_increases <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>            consec_increases <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> consec_increases <span class="op">==</span> patience:</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Stopped early at epoch </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss"> - val loss increased for </span><span class="sc">{</span>consec_increases<span class="sc">}</span><span class="ss"> consecutive epochs!"</span>)</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_loss, valid_loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Using the <code>trainer</code> function:</li>
</ul>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> network(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">1</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> torch.nn.MSELoss()</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.05</span>) <span class="co"># weight_decay=0.01 for L2 regularization</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>train_loss, valid_loss <span class="op">=</span> trainer(model, criterion, optimizer, trainloader, validloader, epochs<span class="op">=</span><span class="dv">201</span>, patience<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>plot_loss(train_loss, valid_loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="universal-approximation-theorem" class="level3">
<h3 class="anchored" data-anchor-id="universal-approximation-theorem">Universal Approximation Theorem</h3>
<ul>
<li>Any continuous function can be approximated arbitrarily well by a neural network with a single hidden layer
<ul>
<li>In other words, NN are universal function approximators</li>
</ul></li>
</ul>
</section>
</section>
<section id="convolutional-neural-networks-cnn" class="level2">
<h2 class="anchored" data-anchor-id="convolutional-neural-networks-cnn">Convolutional Neural Networks (CNN)</h2>
<p><img src="images/6_cnn_ex.png" width="500"></p>
<ul>
<li>Drastically reduces the number of params (compared to NN):
<ul>
<li>have activations depend on small number of inputs</li>
<li>same parameters (convolutional filter) are used for different parts of the image</li>
</ul></li>
<li>Can capture spatial information (preserves the structure of the image)</li>
</ul>
<section id="convolution" class="level3">
<h3 class="anchored" data-anchor-id="convolution">Convolution</h3>
<ul>
<li>Idea: use a small filter/kernel to extract features from the image
<ul>
<li>Filter: a small matrix of weights (normally odd dimensioned -&gt; for symmetry)</li>
</ul></li>
</ul>
<p><img src="images/6_conv.gif" width="250"></p>
<ul>
<li>Notice that the filter results in a smaller output image
<ul>
<li>This is because we are not padding the image</li>
<li>We can add padding to the image to keep the same size
<ul>
<li>Padding: add zeros around the image</li>
</ul></li>
<li>Can also add stride to move the filter more than 1 pixel at a time</li>
</ul></li>
</ul>
</section>
<section id="cnn-structure" class="level3">
<h3 class="anchored" data-anchor-id="cnn-structure">CNN Structure</h3>
<p><img src="images/6_cnn_struct.png" width="500"></p>
<p><em><a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53">img src</a></em></p>
</section>
<section id="cnn-in-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="cnn-in-pytorch">CNN in PyTorch</h3>
<section id="convolutional-layer" class="level4">
<h4 class="anchored" data-anchor-id="convolutional-layer">1. Convolutional Layer</h4>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>conv_1 <span class="op">=</span> torch.nn.Conv2d(in_channels<span class="op">=</span><span class="dv">1</span>, out_channels<span class="op">=</span><span class="dv">6</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Arguments:
<ul>
<li><code>in_channels</code>: number of input channels (gray scale image has 1 channel, RGB has 3)</li>
<li><code>out_channels</code>: number of output channels (similar to hidden nodes in NN)</li>
<li><code>kernel_size</code>: size of the filter</li>
<li><code>stride</code>: how many pixels to move the filter each time</li>
<li><code>padding</code>: how many pixels to add around the image</li>
</ul></li>
</ul>
<p><img src="images/6_conv_layer.png" width="350"></p>
<ul>
<li>Size of input image (e.g.&nbsp;256x256) doesn’t matter, what matters is: <code>in_channels</code>, <code>out_channels</code>, <code>kernel_size</code></li>
</ul>
<p><span class="math display">\[\text{total params} = (\text{out channels} \times \text{in channels} \times \text{kernel size}^2) + \text{out channels}\]</span></p>
<p><span class="math display">\[\text{output size} = \frac{\text{input size} - \text{kernel size} + 2 \times \text{padding}}{\text{stride}} + 1\]</span></p>
<section id="dimensions-of-images-and-kernel-tensors-in-pytorch" class="level5">
<h5 class="anchored" data-anchor-id="dimensions-of-images-and-kernel-tensors-in-pytorch">Dimensions of images and kernel tensors in PyTorch</h5>
<ul>
<li>Images: <code>[batch_size, channels, height, width]</code></li>
<li>Kernel: <code>[out_channels, in_channels, kernel_height, kernel_width]</code></li>
</ul>
<p>Note: before passing the image to the convolutional layer, we need to reshape it to the correct dimensions. Also if you want to <code>plt.imshow()</code> the image, you need to reshape it back to <code>[height, width, channels]</code>.</p>
</section>
</section>
<section id="flattening" class="level4">
<h4 class="anchored" data-anchor-id="flattening">2. Flattening</h4>
<ul>
<li><code>feature learning</code> -&gt; <code>classification</code></li>
<li>Use <code>torch.nn.Flatten()</code> to flatten the image</li>
<li>At the end need to either do regression or classification</li>
</ul>
</section>
<section id="pooling" class="level4">
<h4 class="anchored" data-anchor-id="pooling">3. Pooling</h4>
<ul>
<li>Idea: reduce the size of the image
<ul>
<li>less params</li>
<li>less overfitting</li>
</ul></li>
<li>Common types:
<ul>
<li><strong>Max pooling</strong>: take the max value in each region
<ul>
<li>Works well since it takes the sharpest features</li>
</ul></li>
<li><strong>Average pooling</strong>: take the average value in each region</li>
</ul></li>
</ul>
</section>
<section id="putting-it-all-together" class="level4">
<h4 class="anchored" data-anchor-id="putting-it-all-together">Putting it all together</h4>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CNN(torch.nn.Module):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.main <span class="op">=</span> torch.nn.Sequential(</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>            torch.nn.Conv2d(in_channels<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>                out_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>                kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>                padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>            torch.nn.ReLU(), <span class="co"># activation function</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>            torch.nn.MaxPool2d((<span class="dv">2</span>, <span class="dv">2</span>)),</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>            torch.nn.Conv2d(in_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>                out_channels<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>                kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>                padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>            torch.nn.ReLU(),</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>            torch.nn.MaxPool2d((<span class="dv">2</span>, <span class="dv">2</span>)),</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>            torch.nn.Flatten(),</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>            torch.nn.Linear(<span class="dv">1250</span>, <span class="dv">1</span>)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.main(x)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Trainer code</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> trainer(</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    model, criterion, optimizer, trainloader, validloader, epochs<span class="op">=</span><span class="dv">5</span>, verbose<span class="op">=</span><span class="va">True</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    train_loss, train_accuracy, valid_loss, valid_accuracy <span class="op">=</span> [], [], [], []</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):  <span class="co"># for each epoch</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        train_batch_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        train_batch_acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        valid_batch_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        valid_batch_acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Training</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X, y <span class="kw">in</span> trainloader:</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> device.<span class="bu">type</span> <span class="kw">in</span> [<span class="st">'cuda'</span>, <span class="st">'mps'</span>]:</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>                    X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()  <span class="co"># Zero all the gradients w.r.t. parameters</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>            y_hat <span class="op">=</span> model(X)  <span class="co"># Forward pass to get output</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>            idx <span class="op">=</span> torch.softmax(y_hat, dim<span class="op">=</span><span class="dv">1</span>).argmax(dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># Multiclass classification</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_hat, y)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>            loss.backward()  <span class="co"># Calculate gradients w.r.t. parameters</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>            optimizer.step()  <span class="co"># Update parameters</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>            train_batch_loss <span class="op">+=</span> loss.item()  <span class="co"># Add loss for this batch to running total</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>            train_batch_acc <span class="op">+=</span> (</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>                    (idx.squeeze() <span class="op">==</span> y).<span class="bu">type</span>(</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>                        torch.float32).mean().item()</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        train_loss.append(train_batch_loss <span class="op">/</span> <span class="bu">len</span>(trainloader))</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>        train_accuracy.append(train_batch_acc <span class="op">/</span> <span class="bu">len</span>(trainloader))</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Validation</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():  <span class="co"># this stops pytorch doing computational graph stuff under-the-hood and saves memory and time</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> X, y <span class="kw">in</span> validloader:</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> device.<span class="bu">type</span> <span class="kw">in</span> [<span class="st">'cuda'</span>, <span class="st">'mps'</span>]:</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>                    X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>                y_hat <span class="op">=</span> model(X)</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>                idx <span class="op">=</span> torch.softmax(y_hat, dim<span class="op">=</span><span class="dv">1</span>).argmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> criterion(y_hat, y)</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>                valid_batch_loss <span class="op">+=</span> loss.item()</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>                valid_batch_acc <span class="op">+=</span> (</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>                    (idx.squeeze() <span class="op">==</span> y).<span class="bu">type</span>(</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>                        torch.float32).mean().item()</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>        valid_loss.append(valid_batch_loss <span class="op">/</span> <span class="bu">len</span>(validloader))</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>        valid_accuracy.append(valid_batch_acc <span class="op">/</span> <span class="bu">len</span>(validloader))  <span class="co"># accuracy</span></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Print progress</span></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> verbose:</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"Epoch </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">:"</span>,</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"Train Loss: </span><span class="sc">{</span>train_loss[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">."</span>,</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"Valid Loss: </span><span class="sc">{</span>valid_loss[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss">."</span>,</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"Train Accuracy: </span><span class="sc">{</span>train_accuracy[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.2f}</span><span class="ss">."</span>,</span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"Valid Accuracy: </span><span class="sc">{</span>valid_accuracy[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.2f}</span><span class="ss">."</span>,</span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> {</span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>        <span class="st">"train_loss"</span>: train_loss,</span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>        <span class="st">"train_accuracy"</span>: train_accuracy,</span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a>        <span class="st">"valid_loss"</span>: valid_loss,</span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a>        <span class="st">"valid_accuracy"</span>: valid_accuracy,</span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="using-torchsummary" class="level4">
<h4 class="anchored" data-anchor-id="using-torchsummary">Using torchsummary</h4>
<ul>
<li>To get a summary of the model
<ul>
<li>No need to manually calculate the output size of each layer</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchsummary <span class="im">import</span> summary</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CNN()</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>summary(model, (<span class="dv">1</span>, <span class="dv">256</span>, <span class="dv">256</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="preparing-data" class="level3">
<h3 class="anchored" data-anchor-id="preparing-data">Preparing Data</h3>
<section id="turning-images-to-tensors" class="level4">
<h4 class="anchored" data-anchor-id="turning-images-to-tensors">Turning images to tensors</h4>
<ul>
<li>Normally there are 2 steps:
<ol type="1">
<li>create a <code>dataset</code> object: the raw data</li>
<li>create a <code>dataloader</code> object: batches the data, shuffles, etc.</li>
</ol></li>
<li>Use <code>torchvision</code> to load the data
<ul>
<li><code>torchvision.datasets.ImageFolder</code>: loads images from folders</li>
<li>Assumes structure: <code>root/class_1/xxx.png</code>, <code>root/class_2/xxx.png</code>, …</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets, transforms</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>IMAGE_SIZE <span class="op">=</span> (<span class="dv">256</span>, <span class="dv">256</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co"># create transform object</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>data_transforms <span class="op">=</span> transforms.Compose([</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    transforms.Resize(IMAGE_SIZE),</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor()</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="co"># create dataset object</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> datasets.ImageFolder(root<span class="op">=</span><span class="st">'path/to/data'</span>, transform<span class="op">=</span>data_transforms)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="co"># check out the data</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>train_dataset.classes <span class="co"># list of classes</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>train_dataset.targets <span class="co"># list of labels</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>train_dataset.samples <span class="co"># list of (path, label) tuples</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="co"># create dataloader object</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    train_dataset,          <span class="co"># our raw data</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,  <span class="co"># the size of batches we want the dataloader to return</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,           <span class="co"># shuffle our data before batching</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>    drop_last<span class="op">=</span><span class="va">False</span>         <span class="co"># don't drop the last batch even if it's smaller than batch_size</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a><span class="co"># get a batch of data</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>images, labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_loader))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="saving-and-loading-pytorch-models" class="level4">
<h4 class="anchored" data-anchor-id="saving-and-loading-pytorch-models">Saving and loading PyTorch models</h4>
<ul>
<li><a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">PyTorch documentation</a></li>
<li>Convention: <code>.pt</code> or <code>.pth</code> file extension</li>
</ul>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="st">"models/my_cnn.pt"</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co"># load model</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> bitmoji_CNN() <span class="co"># must have defined the model class</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>model.load_state_dict(torch.load(PATH))</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>() <span class="co"># set model to evaluation mode (not training mode)</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co"># save model</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>torch.save(model.state_dict(), PATH)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="data-augmentation" class="level4">
<h4 class="anchored" data-anchor-id="data-augmentation">Data augmentation</h4>
<ul>
<li><p>To make CNN more robust to different images + increase the size of the dataset</p></li>
<li><p>Common augmentations:</p>
<ul>
<li>Crop</li>
<li>Rotate</li>
<li>Flip</li>
<li>Color jitter</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>data_transforms <span class="op">=</span> transforms.Compose([</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    transforms.Resize(IMAGE_SIZE),</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    transforms.RandomVerticalFlip(p<span class="op">=</span><span class="fl">0.5</span>), <span class="co"># p=0.5 means 50% chance of applying this augmentation</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    transforms.RandomHorizontalFlip(p<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor()</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="hyperparameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter-tuning">Hyperparameter Tuning</h3>
<ul>
<li><p>NN has a lot of hyperparameters</p>
<ul>
<li>Grid search will take a long time</li>
<li>Need a smarter approach: <strong>Optimization Algorithms</strong></li>
</ul></li>
<li><p>Examples: Ax (we will use this), Raytune, Neptune, skorch.</p></li>
</ul>
</section>
<section id="transfer-learning" class="level3">
<h3 class="anchored" data-anchor-id="transfer-learning">Transfer Learning</h3>
<ul>
<li>Idea: use a pre-trained model and fine-tune it to our specific task</li>
<li>Install from <code>torchvision.models</code>
<ul>
<li>All models have been trained on ImageNet dataset (224x224 images)</li>
</ul></li>
<li>See <a href="https://pages.github.ubc.ca/MDS-2023-24/DSCI_572_sup-learn-2_students/lectures/07_cnns-pt2.html#using-pre-trained-models-out-of-the-box">here for code</a></li>
</ul>
<section id="approach-1-adding-layers-to-pre-trained-model" class="level4">
<h4 class="anchored" data-anchor-id="approach-1-adding-layers-to-pre-trained-model">Approach 1: Adding layers to pre-trained model</h4>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>densenet <span class="op">=</span> models.densenet121(weights<span class="op">=</span><span class="st">'DenseNet121_Weights.DEFAULT'</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> densenet.parameters():  <span class="co"># Freeze parameters so we don't update them</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># can fine-tune to freeze only some layers</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(densenet.named_children())[<span class="op">-</span><span class="dv">1</span>] <span class="co"># check the last layer</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co"># update the last layer</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>new_layers <span class="op">=</span> nn.Sequential(</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">1024</span>, <span class="dv">500</span>),</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">500</span>, <span class="dv">1</span>)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>densenet.classifier <span class="op">=</span> new_layers</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Then train the model as usual.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>densenet.to(device)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.BCEWithLogitsLoss()</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(densenet.parameters(), lr<span class="op">=</span><span class="fl">2e-3</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> trainer(densenet, criterion, optimizer, train_loader, valid_loader, device, epochs<span class="op">=</span><span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="approach-2-use-extracted-features-in-a-new-model" class="level4">
<h4 class="anchored" data-anchor-id="approach-2-use-extracted-features-in-a-new-model">Approach 2: Use Extracted Features in a New Model</h4>
<ul>
<li>Idea:
<ol type="1">
<li>Take output from pre-trained model</li>
<li>Feed output to a new model</li>
</ol></li>
</ul>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_features(model, train_loader, valid_loader):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Extract features from both training and validation datasets using the provided model.</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co">    This function passes data through a given neural network model to extract features. It's designed</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co">    to work with datasets loaded using PyTorch's DataLoader. The function operates under the assumption</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co">    that gradients are not required, optimizing memory and computation for inference tasks.</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Disable gradient computation for efficiency during inference</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize empty tensors for training features and labels</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>        Z_train <span class="op">=</span> torch.empty((<span class="dv">0</span>, <span class="dv">1024</span>))  <span class="co"># Assuming each feature vector has 1024 elements</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        y_train <span class="op">=</span> torch.empty((<span class="dv">0</span>))</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize empty tensors for validation features and labels</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>        Z_valid <span class="op">=</span> torch.empty((<span class="dv">0</span>, <span class="dv">1024</span>))</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>        y_valid <span class="op">=</span> torch.empty((<span class="dv">0</span>))</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Process training data</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X, y <span class="kw">in</span> train_loader:</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Extract features and concatenate them to the corresponding tensors</span></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>            Z_train <span class="op">=</span> torch.cat((Z_train, model(X)), dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>            y_train <span class="op">=</span> torch.cat((y_train, y))</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Process validation data</span></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X, y <span class="kw">in</span> valid_loader:</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Extract features and concatenate them to the corresponding tensors</span></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>            Z_valid <span class="op">=</span> torch.cat((Z_valid, model(X)), dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>            y_valid <span class="op">=</span> torch.cat((y_valid, y))</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return the feature and label tensors</span></span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Z_train, y_train, Z_valid, y_valid</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now we can use the extracted features to train a new model.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract features from the pre-trained model</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>densenet <span class="op">=</span> models.densenet121(weights<span class="op">=</span><span class="st">'DenseNet121_Weights.DEFAULT'</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>densenet.classifier <span class="op">=</span> nn.Identity()  <span class="co"># remove that last "classification" layer</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>Z_train, y_train, Z_valid, y_valid <span class="op">=</span> get_features(densenet, train_loader, valid_loader)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a new model using the extracted features</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's scale our data</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>Z_train <span class="op">=</span> scaler.fit_transform(Z_train)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>Z_valid <span class="op">=</span> scaler.transform(Z_valid)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a model</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>model.fit(Z_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
</section>
<section id="advanced-cnn" class="level2">
<h2 class="anchored" data-anchor-id="advanced-cnn">Advanced CNN</h2>
<section id="generative-vs-discriminative-models" class="level3">
<h3 class="anchored" data-anchor-id="generative-vs-discriminative-models">Generative vs Discriminative Models</h3>
<table class="table">
<colgroup>
<col style="width: 53%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th>Generative Models</th>
<th>Discriminative Models</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Directly model the joint probability distribution of the input and output</td>
<td>Model the conditional probability of the output given the input</td>
</tr>
<tr class="even">
<td>Directly model <span class="math inline">\(P(y\|x)\)</span></td>
<td>Estimate <span class="math inline">\(P(x\|y)\)</span> to then deduce <span class="math inline">\(P(y\|x)\)</span></td>
</tr>
<tr class="odd">
<td>Build model for each class</td>
<td>Make boundary between classes</td>
</tr>
<tr class="even">
<td>“Generate or draw a cat”</td>
<td>“Distinquish between cats and dogs”</td>
</tr>
<tr class="odd">
<td>Examples: Naibe bayes, ChatGPT</td>
<td>Examples: Logistic Regression, SVM, Tree based models, CNN</td>
</tr>
</tbody>
</table>
</section>
<section id="autoencoders" class="level3">
<h3 class="anchored" data-anchor-id="autoencoders">Autoencoders</h3>
<p><img src="images/8_autoencoder.png" width="300"></p>
<ul>
<li>Designed to reconstruct the input</li>
<li>Encoder and a decoder</li>
<li>Why do we need autoencoders?
<ul>
<li>Dimensionality reduction</li>
<li>Denoising</li>
</ul></li>
</ul>
<section id="dimensionality-reduction" class="level4">
<h4 class="anchored" data-anchor-id="dimensionality-reduction">Dimensionality Reduction</h4>
<ul>
<li>Maybe the z axis is unimportant in the input space for classification</li>
</ul>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> autoencoder(torch.nn.Module):</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, hidden_size):</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(input_size, <span class="dv">2</span>),</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>            nn.Sigmoid()</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">2</span>, input_size),</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>            nn.Sigmoid()</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.encoder(x)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.decoder(x)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up the training</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>X_tensor <span class="op">=</span> torch.tensor(X, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>dataloader <span class="op">=</span> DataLoader(X_tensor,</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>                        batch_size<span class="op">=</span>BATCH_SIZE)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> autoencoder(<span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.MSELoss()</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters())</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>EPOCHS <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(EPOCHS):</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> dataloader:</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()           <span class="co"># Clear gradients w.r.t. parameters</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>        y_hat <span class="op">=</span> model(batch)            <span class="co"># Forward pass to get output</span></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(y_hat, batch)  <span class="co"># Calculate loss</span></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>        loss.backward()                 <span class="co"># Getting gradients w.r.t. parameters</span></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>        optimizer.step()                <span class="co"># Update parameters</span></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Use encoder</span></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>X_encoded <span class="op">=</span> model.encoder(X_tensor)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="denoising" class="level4">
<h4 class="anchored" data-anchor-id="denoising">Denoising</h4>
<ul>
<li>Remove noise from the input</li>
<li>Use <em>Transposed Convolution Layers</em> to upsample the input
<ul>
<li>Normal convolution: downsample (output is smaller than input)</li>
<li>Transposed convolution: upsample (output is larger than input)</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv_block(input_channels, output_channels):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Sequential(</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>        nn.Conv2d(input_channels, output_channels, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>        nn.ReLU(),</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>        nn.MaxPool2d(<span class="dv">2</span>)  <span class="co"># reduce x-y dims by two; window and stride of 2</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> deconv_block(input_channels, output_channels, kernel_size):</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Sequential(</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>        nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>        nn.ReLU()</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> autoencoder(torch.nn.Module):</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>            conv_block(<span class="dv">1</span>, <span class="dv">32</span>),</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>            conv_block(<span class="dv">32</span>, <span class="dv">16</span>),</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>            conv_block(<span class="dv">16</span>, <span class="dv">8</span>)</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>            deconv_block(<span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">3</span>),</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>            deconv_block(<span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">2</span>),</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>            deconv_block(<span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">2</span>),</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">32</span>, <span class="dv">1</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)  <span class="co"># final conv layer to decrease channel back to 1</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.encoder(x)</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.decoder(x)</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.sigmoid(x)  <span class="co"># get pixels between 0 and 1</span></span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up the training</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>EPOCHS <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.MSELoss()</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters())</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>img_list <span class="op">=</span> []</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(EPOCHS):</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch, _ <span class="kw">in</span> trainloader:</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>        noisy_batch <span class="op">=</span> batch <span class="op">+</span> noise <span class="op">*</span> torch.randn(<span class="op">*</span>batch.shape)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>        noisy_batch <span class="op">=</span> torch.clip(noisy_batch, <span class="fl">0.0</span>, <span class="fl">1.0</span>)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>        y_hat <span class="op">=</span> model(noisy_batch)</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(y_hat, batch)</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>        losses <span class="op">+=</span> loss.item()</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"epoch: </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, loss: </span><span class="sc">{</span>losses <span class="op">/</span> <span class="bu">len</span>(trainloader)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save example results each epoch so we can see what's going on</span></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>        noisy_8 <span class="op">=</span> noisy_batch[:<span class="dv">1</span>, :<span class="dv">1</span>, :, :]</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>        model_8 <span class="op">=</span> model(input_8)</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>        real_8 <span class="op">=</span> batch[:<span class="dv">1</span>, :<span class="dv">1</span>, :, :]</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>    img_list.append(utils.make_grid([noisy_8[<span class="dv">0</span>], model_8[<span class="dv">0</span>], real_8[<span class="dv">0</span>]], padding<span class="op">=</span><span class="dv">1</span>))```</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
</section>
<section id="generative-adversarial-networks-gans" class="level2">
<h2 class="anchored" data-anchor-id="generative-adversarial-networks-gans">Generative Adversarial Networks (GANs)</h2>
<ul>
<li>Model used to generate new data (indistinguishable from real data)</li>
<li>No need for labels (unsupervised learning)</li>
<li>See <a href="https://developers.google.com/machine-learning/gan/gan_structure">here</a></li>
</ul>
<p><img src="images/8_gan_struct.png" width="500"></p>
<ul>
<li>Two networks:
<ul>
<li>Generator: creates new data</li>
<li>Discriminator: tries to distinguish between real and fake data</li>
</ul></li>
<li>Both are battling each other:
<ul>
<li>Generator tries to create data that the discriminator can’t distinguish from real data</li>
<li>Discriminator tries to distinguish between real and fake data</li>
</ul></li>
</ul>
<section id="training-gans" class="level3">
<h3 class="anchored" data-anchor-id="training-gans">Training GANs</h3>
<ol type="1">
<li>Train the discriminator (simple binary classification)
<ul>
<li>Train the discriminator on real data</li>
<li>Train the discriminator on fake data (generated by the generator)</li>
</ul></li>
<li>Train the generator
<ul>
<li>Generate fake images with the generator and label them as real</li>
<li>Pass to discriminator and ask it to classify them (real or fake)</li>
<li>Pass judgement to a loss function (see how far it is from the ideal output)
<ul>
<li>ideal output: all fake images are classified as real</li>
</ul></li>
<li>Do backpropagation and update the generator</li>
</ul></li>
<li>Repeat</li>
</ol>
</section>
<section id="pytorch-implementation" class="level3">
<h3 class="anchored" data-anchor-id="pytorch-implementation">Pytorch Implementation</h3>
<ol type="1">
<li><p>Creating the data loader</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>DATA_DIR <span class="op">=</span> <span class="st">"../input/face-recognition-dataset/Extracted Faces"</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>IMAGE_SIZE <span class="op">=</span> (<span class="dv">128</span>, <span class="dv">128</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>data_transforms <span class="op">=</span> transforms.Compose([</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    transforms.Resize(IMAGE_SIZE), <span class="co"># uses CPU (bottleneck)</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize((<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>), (<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>))</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> datasets.ImageFolder(root<span class="op">=</span>DATA_DIR, transform<span class="op">=</span>data_transforms)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> torch.utils.data.DataLoader(dataset, batch_size<span class="op">=</span>BATCH_SIZE, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Creating the generator</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Generator(nn.Module):</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a> <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, LATENT_SIZE):</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>     <span class="bu">super</span>(Generator, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>     <span class="va">self</span>.main <span class="op">=</span> nn.Sequential(</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>         nn.ConvTranspose2d(LATENT_SIZE, <span class="dv">1024</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>         nn.BatchNorm2d(<span class="dv">1024</span>),</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>         nn.LeakyReLU(<span class="fl">0.2</span>, inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>         nn.ConvTranspose2d(<span class="dv">1024</span>, <span class="dv">512</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>         nn.BatchNorm2d(<span class="dv">512</span>),</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>         nn.LeakyReLU(<span class="fl">0.2</span>, inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>         nn.ConvTranspose2d(<span class="dv">512</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>         nn.BatchNorm2d(<span class="dv">128</span>),</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>         nn.LeakyReLU(<span class="fl">0.2</span>, inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>         nn.ConvTranspose2d(<span class="dv">128</span>, <span class="dv">3</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>         nn.BatchNorm2d(<span class="dv">3</span>),</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>         nn.Tanh()</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>     )</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a> <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>     <span class="cf">return</span> <span class="va">self</span>.main(<span class="bu">input</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Creating the discriminator</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Discriminator(nn.Module):</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a> <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>     <span class="bu">super</span>(Discriminator, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>     <span class="va">self</span>.main <span class="op">=</span> nn.Sequential(</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>         nn.Conv2d(<span class="dv">3</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>         nn.BatchNorm2d(<span class="dv">128</span>),</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>         nn.LeakyReLU(<span class="fl">0.2</span>, inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>         nn.Conv2d(<span class="dv">128</span>, <span class="dv">512</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>         nn.BatchNorm2d(<span class="dv">512</span>),</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>         nn.LeakyReLU(<span class="fl">0.2</span>, inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>         nn.Conv2d(<span class="dv">512</span>, <span class="dv">1</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>         nn.Flatten(),</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>         nn.Sigmoid()</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>     )</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a> <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>     <span class="cf">return</span> <span class="va">self</span>.main(<span class="bu">input</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Instantiating the models</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'mps'</span> <span class="cf">if</span> torch.backends.mps.is_available() <span class="cf">else</span> <span class="st">'cpu'</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>LATENT_SIZE <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> Generator(LATENT_SIZE).to(device)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>discriminator <span class="op">=</span> Discriminator().to(device)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.BCELoss()</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>optimizerG <span class="op">=</span> optim.Adam(generator.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>, betas<span class="op">=</span>(<span class="fl">0.5</span>, <span class="fl">0.999</span>))</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>optimizerD <span class="op">=</span> optim.Adam(discriminator.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>, betas<span class="op">=</span>(<span class="fl">0.5</span>, <span class="fl">0.999</span>))</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a> <span class="kw">def</span> weights_init(m):</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>     <span class="cf">if</span> <span class="bu">isinstance</span>(m, (nn.Conv2d, nn.ConvTranspose2d)):</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>         nn.init.normal_(m.weight.data, <span class="fl">0.0</span>, <span class="fl">0.02</span>)</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>     <span class="cf">elif</span> <span class="bu">isinstance</span>(m, nn.BatchNorm2d):</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>         nn.init.normal_(m.weight.data, <span class="fl">1.0</span>, <span class="fl">0.02</span>)</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>         nn.init.constant_(m.bias.data, <span class="dv">0</span>)</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a> generator.<span class="bu">apply</span>(weights_init)</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a> discriminator.<span class="bu">apply</span>(weights_init)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Training the GAN</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a> img_list <span class="op">=</span> []</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>fixed_noise <span class="op">=</span> torch.randn(BATCH_SIZE, LATENT_SIZE, <span class="dv">1</span>, <span class="dv">1</span>).to(device)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a> NUM_EPOCHS <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statistics <span class="im">import</span> mean</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Training started:</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a> D_real_epoch, D_fake_epoch, loss_dis_epoch, loss_gen_epoch <span class="op">=</span> [], [], [], []</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a> <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(NUM_EPOCHS):</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>     D_real_iter, D_fake_iter, loss_dis_iter, loss_gen_iter <span class="op">=</span> [], [], [], []</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>     <span class="cf">for</span> real_batch, _ <span class="kw">in</span> data_loader:</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>         <span class="co"># STEP 1: train discriminator</span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>         <span class="co"># ==================================</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>         optimizerD.zero_grad()</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>         real_batch <span class="op">=</span> real_batch.to(device)</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>         real_labels <span class="op">=</span> torch.ones((real_batch.shape[<span class="dv">0</span>],), dtype<span class="op">=</span>torch.<span class="bu">float</span>).to(device)</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>         output <span class="op">=</span> discriminator(real_batch).view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>         loss_real <span class="op">=</span> criterion(output, real_labels)</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>         <span class="co"># Iteration book-keeping</span></span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>         D_real_iter.append(output.mean().item())</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>         <span class="co"># Train with fake data</span></span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>         noise <span class="op">=</span> torch.randn(real_batch.shape[<span class="dv">0</span>], LATENT_SIZE, <span class="dv">1</span>, <span class="dv">1</span>).to(device)</span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a>         fake_batch <span class="op">=</span> generator(noise)</span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a>         fake_labels <span class="op">=</span> torch.zeros_like(real_labels)</span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a>         output <span class="op">=</span> discriminator(fake_batch.detach()).view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a>         loss_fake <span class="op">=</span> criterion(output, fake_labels)</span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a>         <span class="co"># Update discriminator weights</span></span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a>         loss_dis <span class="op">=</span> loss_real <span class="op">+</span> loss_fake</span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a>         loss_dis.backward()</span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true" tabindex="-1"></a>         optimizerD.step()</span>
<span id="cb31-41"><a href="#cb31-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-42"><a href="#cb31-42" aria-hidden="true" tabindex="-1"></a>         <span class="co"># Iteration book-keeping</span></span>
<span id="cb31-43"><a href="#cb31-43" aria-hidden="true" tabindex="-1"></a>         loss_dis_iter.append(loss_dis.mean().item())</span>
<span id="cb31-44"><a href="#cb31-44" aria-hidden="true" tabindex="-1"></a>         D_fake_iter.append(output.mean().item())</span>
<span id="cb31-45"><a href="#cb31-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-46"><a href="#cb31-46" aria-hidden="true" tabindex="-1"></a>         <span class="co"># STEP 2: train generator</span></span>
<span id="cb31-47"><a href="#cb31-47" aria-hidden="true" tabindex="-1"></a>         <span class="co"># ==================================</span></span>
<span id="cb31-48"><a href="#cb31-48" aria-hidden="true" tabindex="-1"></a>         optimizerG.zero_grad()</span>
<span id="cb31-49"><a href="#cb31-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-50"><a href="#cb31-50" aria-hidden="true" tabindex="-1"></a>         <span class="co"># Calculate the output with the updated weights of the discriminator</span></span>
<span id="cb31-51"><a href="#cb31-51" aria-hidden="true" tabindex="-1"></a>         output <span class="op">=</span> discriminator(fake_batch).view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb31-52"><a href="#cb31-52" aria-hidden="true" tabindex="-1"></a>         loss_gen <span class="op">=</span> criterion(output, real_labels)</span>
<span id="cb31-53"><a href="#cb31-53" aria-hidden="true" tabindex="-1"></a>         loss_gen.backward()</span>
<span id="cb31-54"><a href="#cb31-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-55"><a href="#cb31-55" aria-hidden="true" tabindex="-1"></a>         <span class="co"># Book-keeping</span></span>
<span id="cb31-56"><a href="#cb31-56" aria-hidden="true" tabindex="-1"></a>         loss_gen_iter.append(loss_gen.mean().item())</span>
<span id="cb31-57"><a href="#cb31-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-58"><a href="#cb31-58" aria-hidden="true" tabindex="-1"></a>         <span class="co"># Update generator weights and store loss</span></span>
<span id="cb31-59"><a href="#cb31-59" aria-hidden="true" tabindex="-1"></a>         optimizerG.step()</span>
<span id="cb31-60"><a href="#cb31-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-61"><a href="#cb31-61" aria-hidden="true" tabindex="-1"></a>     <span class="bu">print</span>(<span class="ss">f"Epoch (</span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>NUM_EPOCHS<span class="sc">}</span><span class="ss">)</span><span class="ch">\t</span><span class="ss">"</span>,</span>
<span id="cb31-62"><a href="#cb31-62" aria-hidden="true" tabindex="-1"></a>         <span class="ss">f"Loss_G: </span><span class="sc">{</span>mean(loss_gen_iter)<span class="sc">:.4f}</span><span class="ss">"</span>,</span>
<span id="cb31-63"><a href="#cb31-63" aria-hidden="true" tabindex="-1"></a>         <span class="ss">f"Loss_D: </span><span class="sc">{</span>mean(loss_dis_iter)<span class="sc">:.4f}</span><span class="ch">\t</span><span class="ss">"</span>,</span>
<span id="cb31-64"><a href="#cb31-64" aria-hidden="true" tabindex="-1"></a>         <span class="ss">f"D_real: </span><span class="sc">{</span>mean(D_real_iter)<span class="sc">:.4f}</span><span class="ss">"</span>,</span>
<span id="cb31-65"><a href="#cb31-65" aria-hidden="true" tabindex="-1"></a>         <span class="ss">f"D_fake: </span><span class="sc">{</span>mean(D_fake_iter)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb31-66"><a href="#cb31-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-67"><a href="#cb31-67" aria-hidden="true" tabindex="-1"></a>     <span class="co"># Epoch book-keeping</span></span>
<span id="cb31-68"><a href="#cb31-68" aria-hidden="true" tabindex="-1"></a>     loss_gen_epoch.append(mean(loss_gen_iter))</span>
<span id="cb31-69"><a href="#cb31-69" aria-hidden="true" tabindex="-1"></a>     loss_dis_epoch.append(mean(loss_dis_iter))</span>
<span id="cb31-70"><a href="#cb31-70" aria-hidden="true" tabindex="-1"></a>     D_real_epoch.append(mean(D_real_iter))</span>
<span id="cb31-71"><a href="#cb31-71" aria-hidden="true" tabindex="-1"></a>     D_fake_epoch.append(mean(D_fake_iter))</span>
<span id="cb31-72"><a href="#cb31-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-73"><a href="#cb31-73" aria-hidden="true" tabindex="-1"></a>     <span class="co"># Keeping track of the evolution of a fixed noise latent vector</span></span>
<span id="cb31-74"><a href="#cb31-74" aria-hidden="true" tabindex="-1"></a>     <span class="cf">with</span> torch.no_grad():</span>
<span id="cb31-75"><a href="#cb31-75" aria-hidden="true" tabindex="-1"></a>         fake_images <span class="op">=</span> generator(fixed_noise).detach().cpu()</span>
<span id="cb31-76"><a href="#cb31-76" aria-hidden="true" tabindex="-1"></a>         <span class="co">#img_list.append(utils.make_grid(fake_images, normalize=True, nrows=10))</span></span>
<span id="cb31-77"><a href="#cb31-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-78"><a href="#cb31-78" aria-hidden="true" tabindex="-1"></a> <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Training ended."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Visualize training process</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a> plt.plot(np.array(loss_gen_epoch), label<span class="op">=</span><span class="st">'loss_gen'</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a> plt.plot(np.array(loss_dis_epoch), label<span class="op">=</span><span class="st">'loss_dis'</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a> plt.xlabel(<span class="st">"Epoch"</span>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a> plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a> plt.legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a> plt.plot(np.array(D_real_epoch), label<span class="op">=</span><span class="st">'D_real'</span>)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a> plt.plot(np.array(D_fake_epoch), label<span class="op">=</span><span class="st">'D_fake'</span>)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a> plt.xlabel(<span class="st">"Epoch"</span>)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a> plt.ylabel(<span class="st">"Probability"</span>)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a> plt.legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ol>
</section>
<section id="multi-input-networks" class="level3">
<h3 class="anchored" data-anchor-id="multi-input-networks">Multi-Input Networks</h3>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> multiModel(nn.Module):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>        ...</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, image, data):</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>        x_cnn <span class="op">=</span> <span class="va">self</span>.cnn(image) <span class="co"># 1st model: CNN</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>        x_fc <span class="op">=</span> <span class="va">self</span>.fc(data) <span class="co"># 2nd model: Fully connected</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.cat((x_cnn, x_fc), dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># concatenate the two outputs</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>